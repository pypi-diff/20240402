# Comparing `tmp/nrgpy-2.3.1-py3-none-any.whl.zip` & `tmp/nrgpy-2.3.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,9 +1,9 @@
-Zip file size: 73116 bytes, number of entries: 35
--rw-rw-r--  2.0 unx     2886 b- defN 24-Feb-13 18:27 nrgpy/__init__.py
+Zip file size: 73211 bytes, number of entries: 35
+-rw-rw-r--  2.0 unx     2886 b- defN 24-Feb-14 20:08 nrgpy/__init__.py
 -rw-r--r--  2.0 unx      193 b- defN 23-Sep-08 00:38 nrgpy/api/__init__.py
 -rw-r--r--  2.0 unx     4775 b- defN 23-Sep-08 00:38 nrgpy/api/auth.py
 -rw-r--r--  2.0 unx     3019 b- defN 23-Sep-08 00:38 nrgpy/api/catalog.py
 -rw-r--r--  2.0 unx     8086 b- defN 23-Sep-08 00:38 nrgpy/api/convert.py
 -rw-r--r--  2.0 unx     6486 b- defN 23-Sep-08 00:38 nrgpy/api/export.py
 -rw-r--r--  2.0 unx     3283 b- defN 23-Sep-08 00:38 nrgpy/api/upload.py
 -rw-r--r--  2.0 unx      297 b- defN 23-Sep-12 15:08 nrgpy/cloud_api/__init__.py
@@ -13,25 +13,25 @@
 -rw-r--r--  2.0 unx    11374 b- defN 24-Jan-17 21:16 nrgpy/cloud_api/jobs.py
 -rw-r--r--  2.0 unx     5682 b- defN 24-Jan-17 21:16 nrgpy/cloud_api/sites.py
 -rw-r--r--  2.0 unx     8536 b- defN 24-Jan-17 21:16 nrgpy/cloud_api/upload.py
 -rw-r--r--  2.0 unx       83 b- defN 23-Sep-08 00:38 nrgpy/convert/__init__.py
 -rw-r--r--  2.0 unx    11940 b- defN 23-Sep-12 15:08 nrgpy/convert/convert_rld.py
 -rw-r--r--  2.0 unx    12261 b- defN 23-Sep-12 15:08 nrgpy/convert/convert_rwd.py
 -rw-r--r--  2.0 unx       82 b- defN 23-Sep-08 00:38 nrgpy/quality/__init__.py
--rw-r--r--  2.0 unx     8985 b- defN 24-Feb-06 15:30 nrgpy/quality/quality.py
+-rw-r--r--  2.0 unx     9356 b- defN 24-Apr-02 17:04 nrgpy/quality/quality.py
 -rw-r--r--  2.0 unx      202 b- defN 24-Feb-06 15:30 nrgpy/read/__init__.py
 -rw-r--r--  2.0 unx     1956 b- defN 23-Sep-08 00:38 nrgpy/read/channel_info_arrays.py
--rw-r--r--  2.0 unx    30024 b- defN 24-Feb-13 18:27 nrgpy/read/logr.py
+-rw-r--r--  2.0 unx    29609 b- defN 24-Mar-04 03:08 nrgpy/read/logr.py
 -rwxr-xr-x  2.0 unx     7522 b- defN 23-Sep-12 15:08 nrgpy/read/spidar_txt.py
--rw-r--r--  2.0 unx    47744 b- defN 23-Sep-12 15:08 nrgpy/read/sympro_txt.py
+-rw-r--r--  2.0 unx    46414 b- defN 24-Mar-04 03:08 nrgpy/read/sympro_txt.py
 -rw-r--r--  2.0 unx    13227 b- defN 23-Sep-12 15:08 nrgpy/read/txt_utils.py
 -rwxr-xr-x  2.0 unx      347 b- defN 23-Sep-08 00:38 nrgpy/utils/__init__.py
 -rw-r--r--  2.0 unx     2265 b- defN 24-Jan-17 21:16 nrgpy/utils/encodings.py
 -rw-r--r--  2.0 unx     8615 b- defN 23-Sep-08 00:38 nrgpy/utils/ipk2lgr.py
 -rw-r--r--  2.0 unx     9525 b- defN 23-Sep-08 00:38 nrgpy/utils/nsd_functions.py
--rwxr-xr-x  2.0 unx    11161 b- defN 24-Feb-06 15:30 nrgpy/utils/utilities.py
--rw-r--r--  2.0 unx     1061 b- defN 24-Feb-13 18:28 nrgpy-2.3.1.dist-info/LICENSE
--rw-rw-r--  2.0 unx     8510 b- defN 24-Feb-13 18:28 nrgpy-2.3.1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 24-Feb-13 18:28 nrgpy-2.3.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        6 b- defN 24-Feb-13 18:28 nrgpy-2.3.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2786 b- defN 24-Feb-13 18:28 nrgpy-2.3.1.dist-info/RECORD
-35 files, 261595 bytes uncompressed, 68734 bytes compressed:  73.7%
+-rwxr-xr-x  2.0 unx    11367 b- defN 24-Apr-02 17:04 nrgpy/utils/utilities.py
+-rw-r--r--  2.0 unx     1061 b- defN 24-Apr-02 17:30 nrgpy-2.3.4.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     8510 b- defN 24-Apr-02 17:30 nrgpy-2.3.4.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 24-Apr-02 17:30 nrgpy-2.3.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx        6 b- defN 24-Apr-02 17:30 nrgpy-2.3.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2786 b- defN 24-Apr-02 17:30 nrgpy-2.3.4.dist-info/RECORD
+35 files, 260427 bytes uncompressed, 68829 bytes compressed:  73.6%
```

## zipnote {}

```diff
@@ -84,23 +84,23 @@
 
 Filename: nrgpy/utils/nsd_functions.py
 Comment: 
 
 Filename: nrgpy/utils/utilities.py
 Comment: 
 
-Filename: nrgpy-2.3.1.dist-info/LICENSE
+Filename: nrgpy-2.3.4.dist-info/LICENSE
 Comment: 
 
-Filename: nrgpy-2.3.1.dist-info/METADATA
+Filename: nrgpy-2.3.4.dist-info/METADATA
 Comment: 
 
-Filename: nrgpy-2.3.1.dist-info/WHEEL
+Filename: nrgpy-2.3.4.dist-info/WHEEL
 Comment: 
 
-Filename: nrgpy-2.3.1.dist-info/top_level.txt
+Filename: nrgpy-2.3.4.dist-info/top_level.txt
 Comment: 
 
-Filename: nrgpy-2.3.1.dist-info/RECORD
+Filename: nrgpy-2.3.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nrgpy/quality/quality.py

```diff
@@ -1,13 +1,23 @@
+from collections import Counter
 from datetime import datetime
+import math
+import numpy as np
+import pandas as pd
+from typing import Union
 
 
 def check_intervals(
-    df, verbose=True, return_info=False, show_all_missing_timestamps=False, show_all_duplicate_timestamps=False, interval=""
-):
+    df: pd.DataFrame,
+    verbose: bool=True,
+    return_info: bool=False,
+    show_all_missing_timestamps: bool=False,
+    show_all_duplicate_timestamps: bool=False,
+    interval: Union[int, str]="",
+) -> Union[dict, None]:
     """checks for missing or duplicate intervals in a pandas dataframe with a "Timestamp" column
 
     Parameters
     ----------
     df : object
         the dataframe to be checked
     interval : int
@@ -34,15 +44,15 @@
             range of time represented in export file
         first_interval : str
             file starting timestamp
         last_interval : str
             file ending timestamp
         missing_timestamps : list
             a list of missing timestamps
-        duplicate_timestamps : list 
+        duplicate_timestamps : list
             a list of duplicate timestamps
 
     Examples
     ----------
     ex. pass a reader.data dataframe for an interval check:
 
     >>>  reader = nrgpy.sympro_txt_read()
@@ -57,15 +67,15 @@
     Actual rows in data set   : 26093
     Data set complete.
     """
     if "horz" in "".join(df.columns).lower() or isinstance(
         df["Timestamp"][0], datetime
     ):
         df2 = df.copy()
-        #  df2.Timestamp = df2.Timestamp.apply(lambda x: x.strftime("%Y-%m-%d %H:%M:%S"))
+        #  df2.Timestamp = df2.Timestamp.apply(lambda x: x.strftime("%Y-%m-%d %H:%M:%S"))  # noqa: E501
         df2.reset_index(level=0, inplace=True)
         _df = df2
         first_interval = _df["Timestamp"].min()
         last_interval = _df["Timestamp"].max()
     else:
         _df = df.copy()
         time_fmt = "%Y-%m-%d %H:%M:%S"
@@ -73,19 +83,20 @@
         last_interval = datetime.strptime(_df["Timestamp"].max(), time_fmt)
 
     duplicate_timestamps, _df = find_duplicate_intervals(_df)
     # delete duplicate intervals before doing the rest of the interval checks
     _df = _df.drop_duplicates(subset=["Timestamp"], keep="first")
     _df.reset_index(drop=True, inplace=True)
 
-    interval = select_interval_length(_df)
+    interval = int(select_interval_length(_df))
     time_range = last_interval - first_interval
     expected_rows = int(time_range.total_seconds() / interval)
     actual_rows = len(_df) - 1
-    loss_pct = 100*((expected_rows - actual_rows) / expected_rows)
+    loss_pct = 100 * ((expected_rows - actual_rows) / expected_rows)
+
     if abs(loss_pct) > 1:
         loss_pct = round(loss_pct)
     else:
         loss_pct = round(loss_pct, 3)
 
     if expected_rows != actual_rows:
         missing_timestamps, _df = find_missing_intervals(_df, interval)
@@ -101,110 +112,113 @@
         if expected_rows == actual_rows:
             print("\nData set complete.")
 
         else:
             print("Interval loss percentage  : {0}".format(loss_pct))
             print("\nMissing {0} timestamps:".format(len(missing_timestamps)))
 
-            if len(missing_timestamps) <= 8 or show_all_missing_timestamps == True:
+            if len(missing_timestamps) <= 8 or show_all_missing_timestamps:
                 for i, timestamp in enumerate(missing_timestamps):
                     print("\t{0}\t{1}".format(i + 1, timestamp))
 
             else:
                 for timestamp in (
                     missing_timestamps[0:3] + ["..."] + missing_timestamps[-3:]
                 ):
                     print("\t{0}\t{1}".format(" ", timestamp))
 
-            print("\nDuplicate {0} timestamps:".format(len(duplicate_timestamps)))
+            print("Duplicate {0} timestamps:".format(len(duplicate_timestamps)))
 
-            if len(duplicate_timestamps) <= 8 or show_all_duplicate_timestamps == True: 
+            if len(duplicate_timestamps) <= 8 or show_all_duplicate_timestamps:
                 for i, timestamp in enumerate(duplicate_timestamps):
-                    print ("\t{0}\t{1}".format(i + 1, timestamp))
+                    print("\t{0}\t{1}".format(i + 1, timestamp))
             else:
                 for timestamp in (
                     duplicate_timestamps[0:3] + ["..."] + duplicate_timestamps[-3:]
                 ):
                     print("\t{0}\t{1}".format(" ", timestamp))
 
     if return_info:
-
         interval_info = {}
         interval_info["actual_rows"] = actual_rows
         interval_info["expected_rows"] = expected_rows
+        interval_info["interval_length"] = interval
         interval_info["first_interval"] = first_interval
         interval_info["last_interval"] = last_interval
         interval_info["time_range"] = time_range
         interval_info["loss_pct"] = loss_pct
 
         try:
             interval_info["missing_timestamps"] = missing_timestamps
-        except:
-            interval_info["missing_timestamps"] = False
+        except Exception:
+            interval_info["missing_timestamps"] = []
 
         interval_info["duplicate_timestamps"] = duplicate_timestamps
 
         return interval_info
+    
+    return None
 
 
-def find_missing_intervals(__df, interval):
+def find_missing_intervals(__df: pd.DataFrame, interval: Union[int, str]) -> tuple:
     """find gaps in data dataframe
 
     returns
     ----------
     list
         a list of all missing intervals
     """
     _df = __df.copy()
-    import pandas as pd
-
     _df["data"] = True
     _df["Timestamp"] = pd.to_datetime(_df["Timestamp"])
     _df.set_index("Timestamp", inplace=True)
     _df = _df.reindex(
         pd.date_range(
             start=_df.index[0], end=_df.index[-1], freq="{0}s".format(interval)
         )
     )
 
     missing_timestamps = []
 
     for index, row in _df.iterrows():
-
-        if row["data"] != True:
+        try:
+            if math.isnan(row["data"]):
+                missing_timestamps.append(index)
+        except TypeError:
             missing_timestamps.append(index)
 
     return missing_timestamps, _df
 
-def find_duplicate_intervals(__df): 
+
+def find_duplicate_intervals(__df: pd.DataFrame) -> tuple:
     """find duplicate interval timestamps
-    
-    returns 
+
+    returns
     -------
-    list 
-        a list of all duplicate intervals 
+    list
+        a list of all duplicate intervals
     """
     _df = __df.copy()
-    from collections import Counter
-    import pandas as pd
-
     time_fmt = "%Y-%m-%d %H:%M:%S"
     observed_intervals = pd.to_datetime(_df["Timestamp"].values, format=time_fmt)
     unique_timestamps_set = set(observed_intervals)
 
     if len(observed_intervals) == len(unique_timestamps_set):
-        duplicate_timestamps = False
+        duplicate_timestamps = []
     else:
         # Record duplicates using Counter
         timestamp_counter = Counter(observed_intervals)
-        duplicate_timestamps = [timestamp for timestamp, count in timestamp_counter.items() if count > 1]
-    
+        duplicate_timestamps = [
+            timestamp for timestamp, count in timestamp_counter.items() if count > 1
+        ]
+
     return duplicate_timestamps, _df
 
-def select_interval_length(df, seconds=True):
+
+def select_interval_length(df: pd.DataFrame, seconds: bool = True) -> Union[int, float]:
     """returns the mode of the first 10 intervals of the data set
 
     parameters
     ----------
         reader : nrgpy reader object
         seconds : bool
             (True) set to False to get interval length in minutes
@@ -215,44 +229,41 @@
 
     """
     from datetime import datetime
 
     formatter = "%Y-%m-%d %H:%M:%S"
     interval = []
 
-    for i in range(10):
+    for i in range(5):
         try:
             interval.append(
                 int(
                     (
                         datetime.strptime(df["Timestamp"].loc[i + 1], formatter)
                         - datetime.strptime(df["Timestamp"].loc[i], formatter)
                     ).seconds
                 )
             )
 
-        except:
+        except Exception:
             formatter = "%Y-%m-%d %H:%M:%S.%f"
             interval.append(int((df["Timestamp"][i + 1] - df["Timestamp"][i]).seconds))
 
         # except:
         #    pass
 
-    interval_s = select_mode_from_list(interval)
-    interval_m = interval_s / 60
-
     try:
         if seconds:
             return select_mode_from_list(interval)
         return select_mode_from_list(interval) / 60
-    except:
+    except Exception:
         return False
 
 
-def select_mode_from_list(lst):
+def select_mode_from_list(lst: list) -> int:
     return max(set(lst), key=lst.count)
 
 
 def check_for_missing_txt_files(txt_file_names):
     """check list of files for missing file numbers
 
     parameters
@@ -266,17 +277,16 @@
         "missing" text file numbers
 
     """
 
     missing_file_numbers = []
 
     for i, f in enumerate(sorted(txt_file_names)):
-
         file_number = int(f.split("_")[-2])
 
         if i > 0:
-            if file_number - _file_number > 1:
+            if file_number - _file_number > 1:  # noqa: F821
                 missing_file_numbers.append(f)
 
         _file_number = file_number
 
     return missing_file_numbers
```

## nrgpy/read/logr.py

```diff
@@ -4,14 +4,15 @@
     pass
 from datetime import datetime, timedelta
 from glob import glob
 import os
 import pandas as pd
 from nrgpy.utils.utilities import (
     check_platform,
+    locate_text_in_df_column,
     windows_folder_path,
     linux_folder_path,
     draw_progress_bar,
     string_date_check,
     renamer,
 )
 import traceback
@@ -105,17 +106,17 @@
         )
 
         self.site_info = self.site_info.iloc[
             : self.site_info.loc[self.site_info[0] == "Data"].index.tolist()[0] + 1
         ]
 
         self.create_data_df(header_len)
-
-        # if not hasattr(self, "site_details"):
         self.format_site_data()
+        if str(self.filename).lower().endswith("dat"):
+            self.arrange_ch_info()
 
     def create_data_df(self, header_len: int) -> None:
         suffix = str(self.filename).lower().split(".")[-1]
         if suffix == "log":
             try:
                 self.data = pd.read_csv(
                     self.filename,
@@ -462,15 +463,15 @@
                         file_path,
                         text_timestamps=self.text_timestamps,
                         logger_local_time=self.logger_local_time,
                     )
                     base.data = pd.concat(
                         [base.data, s.data], ignore_index=True, axis=0, join="outer"
                     )
-                    if not s.filename.lower().endswith(("log", "diag")):
+                    if not str(s.filename).lower().endswith(("log", "diag")):
                         base.ch_info = pd.concat(
                             [base.ch_info, s.ch_info],
                             ignore_index=True,
                             axis=0,
                             join="outer",
                         )
                     if not progress_bar:
@@ -493,24 +494,25 @@
         if out_file != "":
             self.out_file = out_file
 
         if output_txt:
             base.data.to_csv(os.path.join(dat_dir, out_file), sep=",", index=False)
 
         try:
-
-            if self.filename.lower().endswith("dat"):
+            if str(self.dat_file_names[-1]).lower().endswith("dat"):
                 self.ch_info = s.ch_info
                 self.ch_list = s.ch_list
                 self.array = s.array
-          
+
                 if drop_duplicates:
                     logger.info("Dropping duplicate timestamps")
-                    self.data = base.data.drop_duplicates(subset=["Timestamp"], keep="first")
-                else: 
+                    self.data = base.data.drop_duplicates(
+                        subset=["Timestamp"], keep="first"
+                    )
+                else:
                     self.data = base.data
                 self.data.reset_index(drop=True, inplace=True)
                 base.ch_info["ch"] = base.ch_info["Channel:"].astype(int)
 
                 try:
                     self.ch_info = (
                         base.ch_info.sort_values(by=["ch"])
@@ -533,16 +535,14 @@
             else:
                 self.data = base.data
             self.data.reset_index(drop=True, inplace=True)
 
             self.first_timestamp = base.first_timestamp
             self.site_info = s.site_info
             self.format_site_data()
-            if base.filename.lower().endswith("dat"):
-                self.arrange_ch_info()
             print("\n")
             logger.info(f"Concatenation of {len(self.data)} rows complete")
 
         except UnboundLocalError:
             print("No files match to contatenate.")
             logger.error(f"No files in {self.dat_dir} match to contatenate.")
 
@@ -598,15 +598,15 @@
                     logger.debug(traceback.format_exc())
                 self.site_info.to_csv(
                     f,
                     header=False,
                     sep="\t",
                     index=False,
                     index_label=False,
-                    line_terminator="\n",
+                    lineterminator="\n",
                 )
 
             output_file.close()
 
             with open(output_name, "U") as f:
                 text = f.read()
                 while "\t\n" in text:
@@ -618,15 +618,15 @@
             with open(output_name, "a", encoding="utf-8") as f:
                 self.data.round(6).to_csv(
                     f,
                     header=True,
                     sep="\t",
                     index=False,
                     index_label=False,
-                    line_terminator="\n",
+                    lineterminator="\n",
                 )
 
             output_file.close()
             self.insert_blank_header_rows(output_name)
 
         if standard:
             if out_file != "":
@@ -640,48 +640,46 @@
                 flush=True,
             )
             logger.info("\nOutputting file: {0}   ...   ".format(output_name))
 
             try:
                 output_file = open(output_name, "w+", encoding="utf-8")
                 output_file.truncate()
-                output_file.write(self.head)
+                # output_file.write(self.head)
                 output_file.close()
 
                 # write header
                 with open(output_name, "a", encoding="utf-8") as f:
                     self.site_info.to_csv(
                         f,
                         header=False,
                         sep="\t",
                         index=False,
                         index_label=False,
-                        line_terminator="\n",
+                        lineterminator="\n",
                     )
                 output_file.close()
 
                 # write data
                 with open(output_name, "a", encoding="utf-8") as f:
                     self.data.round(6).to_csv(
                         f,
                         header=True,
                         sep="\t",
                         index=False,
                         index_label=False,
-                        line_terminator="\n",
+                        lineterminator="\n",
                     )
                 output_file.close()
                 self.insert_blank_header_rows(output_name)
                 print("[OK]")
 
             except Exception:
                 print("[FAILED]")
-                print(traceback.format_exc())
-                logger.error(f"Outputting {output_name} failed")
-                logger.debug(traceback.format_exc())
+                logger.exception(f"Outputting {output_name} failed")
 
     def insert_blank_header_rows(self, filename: str):
         """insert blank rows when using shift_timestamps()
 
         ensures the resulting text file looks and feels like an
         original LOGR  export
         """
@@ -689,53 +687,45 @@
             "Site Properties",
             "File Properties",
             "Sensor History",
             "Data",
         ]
 
         blank_list = []
-        for i in self.site_info[
-            self.site_info[0].str.contains("Site Properties") is True
-        ].index:
-            blank_list.append(i)
-            site_properties_line = i + 2
-
-        for i in self.site_info[
-            self.site_info[0].str.contains("File Properties") is True
-        ].index:
-            blank_list.append(i)
-            file_properties_line = i + 2
-
-        for i in self.site_info[
-            self.site_info[0].str.contains("Sensor History") is True
-        ].index:
-            blank_list.append(i)
-            sensor_history_line = i + 2
+        field_lines = {}
+
+        for h in header_section_headings:
+            for i in locate_text_in_df_column(self.site_info, h):
+                blank_list.append(i)
+                field_lines[h] = i + 2
 
         skip_first_channel = True
-        for i in self.site_info[
-            self.site_info[0].str.contains("Channel:") is True
-        ].index:
+        _channel = locate_text_in_df_column(self.site_info, "Channel:")
+        for i in _channel:
             if skip_first_channel:
                 skip_first_channel = False
             else:
                 blank_list.append(i)
 
-        for i in self.site_info[self.site_info[0].str.match("Data") is True].index:
-            blank_list.append(i)
-            data_line = i + 2
-
         f_read = open(filename, "r")
         contents = f_read.readlines()
         f_read.close()
 
-        contents[site_properties_line] = header_section_headings[0] + "\n"
-        contents[file_properties_line] = header_section_headings[1] + "\n"
-        contents[sensor_history_line] = header_section_headings[2] + "\n"
-        contents[data_line] = header_section_headings[3] + "\n"
+        contents[field_lines[header_section_headings[0]]] = (
+            header_section_headings[0] + "\n"
+        )
+        contents[field_lines[header_section_headings[1]]] = (
+            header_section_headings[1] + "\n"
+        )
+        contents[field_lines[header_section_headings[2]]] = (
+            header_section_headings[2] + "\n"
+        )
+        contents[field_lines[header_section_headings[3]]] = (
+            header_section_headings[3] + "\n"
+        )
 
         for i in list(reversed(sorted(blank_list))):
             contents.insert(i + 2, "\n")
 
         f_write = open(filename, "w")
         contents = "".join(contents)
         f_write.write(contents)
```

## nrgpy/read/sympro_txt.py

```diff
@@ -5,14 +5,15 @@
 import datetime
 from datetime import datetime, timedelta
 from glob import glob
 import os
 import pandas as pd
 from nrgpy.utils.utilities import (
     check_platform,
+    locate_text_in_df_column,
     windows_folder_path,
     linux_folder_path,
     draw_progress_bar,
     string_date_check,
     renamer,
 )
 import traceback
@@ -24,37 +25,37 @@
         filename: str = "",
         out_file: str = "",
         text_timestamps: bool = False,
         **kwargs,
     ):
         """Class of pandas dataframes created from SymPRO standard txt output.
 
-        If a filename is passed when calling class, the file is read in alone. 
-        Otherwise, an instance of the class is created, and the concat_txt function may 
+        If a filename is passed when calling class, the file is read in alone.
+        Otherwise, an instance of the class is created, and the concat_txt function may
         be called to combine all txt files in a directory.
 
-        Filters may be used on any part of the filename, to combine a subset of text 
+        Filters may be used on any part of the filename, to combine a subset of text
         files in a directory.
 
         Parameters
         ----------
         filename : str, optional
             path to filename
         out_file : str, optional
             path to outputted file
         text_timestamps : boolean
             set to True for text timestamps
 
         Attributes
         ---------
         ch_info : obj
-            pandas dataframe of ch_list (below) pulled out of file with 
+            pandas dataframe of ch_list (below) pulled out of file with
             sympro_txt_read.arrange_ch_info()
         ch_list : list
-            list of channel info; can be converted to json w/ import json ... 
+            list of channel info; can be converted to json w/ import json ...
             json.dumps(fut.ch_info)
         data : obj
             pandas dataframe of all data
         head : obj
             lines at the top of the txt file..., used when rebuilding timeshifted files
         site_info : obj
             pandas dataframe of site information
@@ -274,18 +275,18 @@
             filename to write data dataframe too if output_txt = True
         progress_bar : bool
             show bar on concat [True] or list of files [False]
 
         Returns
         -------
         ch_info : obj
-            pandas dataframe of ch_list (below) pulled out of file with 
+            pandas dataframe of ch_list (below) pulled out of file with
             sympro_txt_read.arrange_ch_info()
         ch_list : list
-            list of channel info; can be converted to json w/ import json ... 
+            list of channel info; can be converted to json w/ import json ...
             json.dumps(fut.ch_info)
         data : obj
             pandas dataframe of all data
         head : obj
             lines at the top of the txt file..., used when rebuilding timeshifted files
         site_info : obj
             pandas dataframe of site information
@@ -470,15 +471,15 @@
 
         except UnboundLocalError:
             print("No files match to contatenate.")
             logger.error(f"No files in {self.txt_dir} match to contatenate.")
             return None
 
     def select_channels_for_reformat(self, epe=False, soiling=False):
-        """determines which of the channel headers fit those required for 
+        """determines which of the channel headers fit those required for
         post-processing for either
 
             a. EPE formatting
             b. soiling ratio calculation
 
         Note that this formatting requires the the channel headers to be full (requires
         Local export of text files, as of 0.1.8.
@@ -1144,17 +1145,17 @@
                             index_label=False,
                             lineterminator="\n",
                         )
                     output_file.close()
                     self.insert_blank_header_rows(output_name)
                     print("[OK]")
 
-                except Exception as e:
+                except Exception:
                     print("[FAILED]")
-                    print(e)
+                    logger.exception(f"Outputting {output_name} failed")
 
     def insert_blank_header_rows(self, filename):
         """insert blank rows when using shift_timestamps()
 
         ensures the resulting text file looks and feels like an
         original Sympro Desktop exported
         """
@@ -1163,110 +1164,69 @@
             "Site Properties",
             "Logger History",
             "iPack History",
             "Sensor History",
             "Data",
         ]
 
+        ignore_headers = [
+            "Data Type:",
+            "Data Logging Mode:",
+            "Math Function:",
+            "GHI Channel:",
+            "RHI Channel:",
+            "DIF Channel:",
+        ]
+
         blank_list = []
-        for i in self.site_info[
-            self.site_info[0].str.contains("Export Parameters") == True
-        ].index:
-            blank_list.append(i)
-            export_parameter_line = i + 2
-
-        for i in self.site_info[
-            self.site_info[0].str.contains("Site Properties") == True
-        ].index:
-            blank_list.append(i)
-            site_properties_line = i + 2
-
-        for i in self.site_info[
-            self.site_info[0].str.contains("Logger History") == True
-        ].index:
-            blank_list.append(i)
-            logger_history_line = i + 2
-
-        for i in self.site_info[
-            self.site_info[0].str.contains("iPack History") == True
-        ].index:
-            blank_list.append(i)
-            ipack_history_line = i + 2
-
-        for i in self.site_info[
-            self.site_info[0].str.contains("Sensor History") == True
-        ].index:
-            blank_list.append(i)
-            sensor_history_line = i + 2
+        field_lines = {}
+
+        for h in header_section_headings:
+            for i in locate_text_in_df_column(self.site_info, h):
+                blank_list.append(i)
+                field_lines[h] = i + 2
 
         skip_first_channel = True
-        for i in self.site_info[
-            self.site_info[0].str.contains("Channel:") == True
-        ].index:
+        _channel = locate_text_in_df_column(self.site_info, "Channel:")
+        for i in _channel:
             if skip_first_channel:
                 skip_first_channel = False
             else:
                 blank_list.append(i)
 
-        for i in self.site_info[self.site_info[0].str.match("Data") == True].index:
-            blank_list.append(i)
-            data_line = i + 2
-
-        for i in self.site_info[
-            self.site_info[0].str.contains("Data Type:") == True
-        ].index:
-            blank_list.remove(i)
-
-        for i in self.site_info[
-            self.site_info[0].str.contains("Data Logging Mode:") == True
-        ].index:
-            blank_list.remove(i)
-
-        try:
-            for i in self.site_info[
-                self.site_info[0].str.contains("Math Function:") == True
-            ].index:
-                blank_list.remove(i)
-        except Exception:
-            pass
-
-        try:
-            for i in self.site_info[
-                self.site_info[0].str.contains("GHI Channel:") == True
-            ].index:
-                blank_list.remove(i)
-        except Exception:
-            pass
-
-        try:
-            for i in self.site_info[
-                self.site_info[0].str.contains("RHI Channel:") == True
-            ].index:
-                blank_list.remove(i)
-        except Exception:
-            pass
-
-        try:
-            for i in self.site_info[
-                self.site_info[0].str.contains("DIF Channel:") == True
-            ].index:
-                blank_list.remove(i)
-        except Exception:
-            pass
+        for ih in ignore_headers:
+            _ignore = locate_text_in_df_column(self.site_info, ih)
+            try:
+                for i in _ignore:
+                    blank_list.remove(i)
+            except ValueError:
+                pass
 
         f_read = open(filename, "r")
         contents = f_read.readlines()
         f_read.close()
 
-        contents[export_parameter_line] = header_section_headings[0] + "\n"
-        contents[site_properties_line] = header_section_headings[1] + "\n"
-        contents[logger_history_line] = header_section_headings[2] + "\n"
-        contents[ipack_history_line] = header_section_headings[3] + "\n"
-        contents[sensor_history_line] = header_section_headings[4] + "\n"
-        contents[data_line] = header_section_headings[5] + "\n"
+        contents[field_lines[header_section_headings[0]]] = (
+            header_section_headings[0] + "\n"
+        )
+        contents[field_lines[header_section_headings[1]]] = (
+            header_section_headings[1] + "\n"
+        )
+        contents[field_lines[header_section_headings[2]]] = (
+            header_section_headings[2] + "\n"
+        )
+        contents[field_lines[header_section_headings[3]]] = (
+            header_section_headings[3] + "\n"
+        )
+        contents[field_lines[header_section_headings[4]]] = (
+            header_section_headings[4] + "\n"
+        )
+        contents[field_lines[header_section_headings[5]]] = (
+            header_section_headings[5] + "\n"
+        )
 
         for i in list(reversed(sorted(blank_list))):
             contents.insert(i + 2, "\n")
 
         f_write = open(filename, "w")
         contents = "".join(contents)
         f_write.write(contents)
```

## nrgpy/utils/utilities.py

```diff
@@ -2,14 +2,15 @@
     from nrgpy import logger
 except ImportError:
     pass
 from datetime import datetime
 import os
 import pathlib
 import pickle
+import pandas as pd
 import psutil
 import re
 import sys
 import traceback
 
 
 def affirm_directory(directory: str) -> None:
@@ -91,21 +92,20 @@
     # LOGR & SymphonieClassic
     date_format_no_dash = "([0-9]{4}[0-9]{2}[0-9]{2})"
     strp_format_no_dash = "%Y%m%d"
     # ZX datafile
     date_format_zx = "(Y[0-9]{4}_M[0-9]{2}_D[0-9]{2})"
     strp_format_zx = "Y%Y_M%m_D%d"
     # SymphoniePRO
-    date_format_with_dash = "([0-9]{4}\-[0-9]{2}\-[0-9]{2})"
+    date_format_with_dash = r"([0-9]{4}\-[0-9]{2}\-[0-9]{2})"
     strp_format_with_dash = "%Y-%m-%d"
     # NRG Cloud TXT Export
-    date_format_with_dot = "([0-9]{4}\.[0-9]{2}\.[0-9]{2})"
+    date_format_with_dot = r"([0-9]{4}\.[0-9]{2}\.[0-9]{2})"
     strp_format_with_dot = "%Y.%m.%d"
 
-
     try:
         start = datetime.strptime(start_date, strp_format_with_dash)
         end = datetime.strptime(end_date, strp_format_with_dash)
     except TypeError:
         print(traceback.format_exc())
         start = start_date
         end = end_date
@@ -212,15 +212,15 @@
         except Exception:
             filename = f"{reader.serial_number}_reader.pkl"
 
     with open(filename, "wb") as pkl:
         pickle.dump(reader, pkl, protocol=pickle.HIGHEST_PROTOCOL)
 
 
-def load(site_number: str="", serial_number: str="", filename: str="") -> object:
+def load(site_number: str = "", serial_number: str = "", filename: str = "") -> object:
     """recall a reader from a pickle file by site number or filename
 
     parameters
     ----------
     site_number : str
         6-digit site number of stored reader OR spidar serial number
     filename : str
@@ -240,15 +240,15 @@
 
     with open(filename, "rb") as pkl:
         reader = pickle.load(pkl)
 
     return reader
 
 
-def data_months(start_date: str, end_date: str, output: str="string") -> list:
+def data_months(start_date: str, end_date: str, output: str = "string") -> list:
     """returns list of months for a date range in YYYY-mm-dd format
 
     parameters
     ----------
     start_date : str or datetime
         YYYY-mm-dd formatted date or datetime object
     end_date : str or datetime
@@ -258,15 +258,15 @@
 
     returns
     -------
     list
     """
 
     if isinstance(start_date, str) and re.match(
-        "^\d{4}\-(0[1-9]|1[012])\-(0[1-9]|[12][0-9]|3[01])", start_date
+        r"^\d{4}\-(0[1-9]|1[012])\-(0[1-9]|[12][0-9]|3[01])", start_date
     ):
         start_year = start_date.split("-")[0]
         start_month = start_date.split("-")[1]
         start_day = start_date.split("-")[2]
         end_year = end_date.split("-")[0]
         end_month = end_date.split("-")[1]
         end_day = end_date.split("-")[2]
@@ -371,15 +371,21 @@
             p = psutil.Process(pid)
             if p.name() == "SymPRODesktop.exe":
                 return True
 
     return False
 
 
-def set_start_stop(reader: object, with_time: bool=False) -> None:
+def set_start_stop(reader: object, with_time: bool = False) -> None:
     """ """
     reader.start_date = reader.data["Timestamp"].loc[0]
     reader.end_date = reader.data["Timestamp"].loc[len(reader.data) - 1]
 
     if not with_time:
         reader.start_date = reader.start_date.date()
         reader.end_date = reader.end_date.date()
+
+
+def locate_text_in_df_column(
+    dataframe: pd.DataFrame, text: str, column: str | int = 0
+) -> list:
+    return dataframe.loc[dataframe[column].str.contains(text)].index
```

## Comparing `nrgpy-2.3.1.dist-info/LICENSE` & `nrgpy-2.3.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `nrgpy-2.3.1.dist-info/METADATA` & `nrgpy-2.3.4.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: nrgpy
-Version: 2.3.1
+Version: 2.3.4
 Summary: library for handling NRG Systems data files
 Author-email: David Carlson <drc@nrgsystems.com>, Natalie Babij <natalie@chickadeemail.com>, NRG Technical Services <support@nrgsystems.com>
 Maintainer-email: David Carlson <davec.vt@gmail.com>
 License: Copyright (c) 2023 NRG Systems, Inc.
         
         Permission is hereby granted, free of charge, to any person obtaining a copy
         of this software and associated documentation files (the "Software"), to deal
```

## Comparing `nrgpy-2.3.1.dist-info/RECORD` & `nrgpy-2.3.4.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -12,24 +12,24 @@
 nrgpy/cloud_api/jobs.py,sha256=nq0_40VF-FWx-8Ne1FnLvx6paomHINYhJTxRgIcmyWM,11374
 nrgpy/cloud_api/sites.py,sha256=Mfbhucy7WSCspoPUeW01WdGlc0DvzG7QrKHbm8fxiwg,5682
 nrgpy/cloud_api/upload.py,sha256=ZCyh5munpuHmbBsNTb3aDfaPWEDEgwUA0xKgoURvvSs,8536
 nrgpy/convert/__init__.py,sha256=mQcjMYhOB8xKpa1xM4DB8CDby-rgk9zQUQKeO0wSejo,83
 nrgpy/convert/convert_rld.py,sha256=Wyy4p0MkB_WPpJwP45g_q-U6kqt0lvpJ7cGUk2aPkF0,11940
 nrgpy/convert/convert_rwd.py,sha256=Dw2D_6mAR41Dm_AnQsGdgT18qrGs3i5QHCFxzGLfOkg,12261
 nrgpy/quality/__init__.py,sha256=Od0a-Er-va8GgZtwNT5Z5tQa_Ap_67UFHgnok7krofY,82
-nrgpy/quality/quality.py,sha256=5GssqcNiBznW54hQMe_nXF_28urYJZjso-wESG86Tu4,8985
+nrgpy/quality/quality.py,sha256=fuMnFigalCUvLG9MaW7i9gEhDJI0cbkprF-TT9MwSc0,9356
 nrgpy/read/__init__.py,sha256=On-WaXw5li8_4wSvTh6t-SFrbBlgE2ljUvSkydU81Yk,202
 nrgpy/read/channel_info_arrays.py,sha256=63dL4iw8Xcap_OFd0pHBikYeAHoIXni96wUP4vEItLQ,1956
-nrgpy/read/logr.py,sha256=kuLbpZ9vze24KNEiCVHfQNbaOvWlcsU561dkwFdXhK0,30024
+nrgpy/read/logr.py,sha256=sGq6yPDzZKuZgh34-hYkNt-39B5U_k-XYVogbhS07fA,29609
 nrgpy/read/spidar_txt.py,sha256=e9T1lHFebQuryDyktZjBaIuiRAcxvahaI5vDG92YmJo,7522
-nrgpy/read/sympro_txt.py,sha256=mVSzpeRT87RNDC6NcbHKtlOW7FdRkkV-A-yVnWHxZ-M,47744
+nrgpy/read/sympro_txt.py,sha256=0sDyd6p_8XdH1E5x6UFkPo7ymPxDjfWK0qRpeNgbOnU,46414
 nrgpy/read/txt_utils.py,sha256=oa5Hacbi61lPPXE2UXOA47XO-SceuiNKGVp0_PbAHpA,13227
 nrgpy/utils/__init__.py,sha256=hsf06srC4XupoySZcUzAcYa_a5FYuWSMWcronz23re4,347
 nrgpy/utils/encodings.py,sha256=LAVXa9q0D8jNqXK1WNp1KwWu2-aKK26vgsmiNVFS5HQ,2265
 nrgpy/utils/ipk2lgr.py,sha256=TpBAwtUK52tE-1Hh76HKiRH09UPEyxj7AiDCCiCF4JI,8615
 nrgpy/utils/nsd_functions.py,sha256=cNnrDaqagjSIqJB-0lYEMh7Uf0GKwT73BPU9jVMFefM,9525
-nrgpy/utils/utilities.py,sha256=6sAsnGM-I_e_UH2w6IaKTMHpPKGhFcdbHIIyB7wh7Bo,11161
-nrgpy-2.3.1.dist-info/LICENSE,sha256=Of0D5x0cKuHCPV-4PIWFBTo3jl8kBFQ1NCgNwMk6WFg,1061
-nrgpy-2.3.1.dist-info/METADATA,sha256=Gu5mPyhsvSJtCgKXHekh7_l6UTg13cZ44zbOzVaMR1I,8510
-nrgpy-2.3.1.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-nrgpy-2.3.1.dist-info/top_level.txt,sha256=hRuwdiH5HWFvmtNyYdICwAxvbtr7rf0nshO2Bo_6VdQ,6
-nrgpy-2.3.1.dist-info/RECORD,,
+nrgpy/utils/utilities.py,sha256=BYsxIs-lowE9Ilv6BsRNarht_hMxrv5NKlaTJ4hE7zg,11367
+nrgpy-2.3.4.dist-info/LICENSE,sha256=Of0D5x0cKuHCPV-4PIWFBTo3jl8kBFQ1NCgNwMk6WFg,1061
+nrgpy-2.3.4.dist-info/METADATA,sha256=gjlz6_f6f7_QMbg2Ze7PpatQkuASnXxmfWJHkkUs9Mw,8510
+nrgpy-2.3.4.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+nrgpy-2.3.4.dist-info/top_level.txt,sha256=hRuwdiH5HWFvmtNyYdICwAxvbtr7rf0nshO2Bo_6VdQ,6
+nrgpy-2.3.4.dist-info/RECORD,,
```

