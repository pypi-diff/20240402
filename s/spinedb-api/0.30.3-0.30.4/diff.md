# Comparing `tmp/spinedb_api-0.30.3.tar.gz` & `tmp/spinedb_api-0.30.4.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "spinedb_api-0.30.3.tar", last modified: Mon Dec  4 11:59:41 2023, max compression
+gzip compressed data, was "spinedb_api-0.30.4.tar", last modified: Thu Mar 28 10:33:16 2024, max compression
```

## Comparing `spinedb_api-0.30.3.tar` & `spinedb_api-0.30.4.tar`

### file list

```diff
@@ -1,184 +1,184 @@
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:41.560317 spinedb_api-0.30.3/
--rw-rw-rw-   0        0        0      174 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/.gitattributes
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:39.900175 spinedb_api-0.30.3/.github/
--rw-rw-rw-   0        0        0      306 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/.github/pull_request_template.md
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:39.908153 spinedb_api-0.30.3/.github/workflows/
--rw-rw-rw-   0        0        0     1311 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/.github/workflows/run_unit_tests.yml
--rw-rw-rw-   0        0        0      296 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/.gitignore
--rw-rw-rw-   0        0        0      591 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/.readthedocs.yml
--rw-rw-rw-   0        0        0    33114 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/COPYING
--rw-rw-rw-   0        0        0     7816 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/COPYING.LESSER
--rw-rw-rw-   0        0        0       53 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/MANIFEST.in
--rw-rw-rw-   0        0        0     3940 2023-12-04 11:59:41.559322 spinedb_api-0.30.3/PKG-INFO
--rw-rw-rw-   0        0        0     2786 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/README.md
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:39.935993 spinedb_api-0.30.3/bin/
--rwxrwxrwx   0        0        0       80 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/bin/build_doc.bat
--rw-rw-rw-   0        0        0      359 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/bin/build_doc.py
--rw-rw-rw-   0        0        0     1301 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/bin/update_copyrights.py
--rw-rw-rw-   0        0        0     3248 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/deploy-key.enc
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:39.965414 spinedb_api-0.30.3/docs/
--rw-rw-rw-   0        0        0      602 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/docs/Makefile
--rwxrwxrwx   0        0        0      791 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/docs/make.bat
--rw-rw-rw-   0        0        0      432 2023-10-05 12:58:08.000000 spinedb_api-0.30.3/docs/requirements.txt
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.027580 spinedb_api-0.30.3/docs/source/
--rw-rw-rw-   0        0        0     6906 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/docs/source/conf.py
--rw-rw-rw-   0        0        0     1026 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/docs/source/front_matter.rst
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.043451 spinedb_api-0.30.3/docs/source/img/
--rw-rw-rw-   0        0        0     5205 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/docs/source/img/spinetoolbox_on_wht.svg
--rw-rw-rw-   0        0        0      698 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/docs/source/index.rst
--rw-rw-rw-   0        0        0     3066 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/docs/source/metadata_description.rst
--rw-rw-rw-   0        0        0    10790 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/docs/source/parameter_value_format.rst
--rw-rw-rw-   0        0        0      955 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/docs/source/results_metadata_description.rst
--rw-rw-rw-   0        0        0     2327 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/docs/source/tutorial.rst
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.045441 spinedb_api-0.30.3/fig/
--rw-rw-rw-   0        0        0    55634 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/fig/eu-emblem-low-res.jpg
--rw-rw-rw-   0        0        0    16474 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/pylintrc
--rw-rw-rw-   0        0        0     1791 2023-12-04 07:51:31.000000 spinedb_api-0.30.3/pyproject.toml
--rw-rw-rw-   0        0        0        6 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/requirements.txt
--rw-rw-rw-   0        0        0       42 2023-12-04 11:59:41.561313 spinedb_api-0.30.3/setup.cfg
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.369516 spinedb_api-0.30.3/spinedb_api/
--rw-rw-rw-   0        0        0     4438 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/__init__.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.431958 spinedb_api-0.30.3/spinedb_api/alembic/
--rw-rw-rw-   0        0        0       38 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/README
--rw-rw-rw-   0        0        0     2088 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/env.py
--rw-rw-rw-   0        0        0      518 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/script.py.mako
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.604997 spinedb_api-0.30.3/spinedb_api/alembic/versions/
--rw-rw-rw-   0        0        0     1862 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/070a0eb89e88_drop_category_tables.py
--rw-rw-rw-   0        0        0     4103 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/0c7d199ae915_add_list_value_table.py
--rw-rw-rw-   0        0        0     2442 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/1892adebc00f_create_metadata_tables.py
--rw-rw-rw-   0        0        0     3012 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/1e4997105288_separate_type_from_value.py
--rw-rw-rw-   0        0        0     6138 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/39e860a11b05_add_alternatives_and_scenarios.py
--rw-rw-rw-   0        0        0     2751 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/51fd7b69acf7_add_parameter_tag_and_parameter_value_list.py
--rw-rw-rw-   0        0        0     1323 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/738d494a08ac_fix_foreign_key_constraints_in_object_.py
--rw-rw-rw-   0        0        0     1349 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/7d0b467f2f4e_fix_foreign_key_constraints_in_entity_.py
--rw-rw-rw-   0        0        0     2517 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/8c19c53d5701_rename_parameter_to_parameter_definition.py
--rw-rw-rw-   0        0        0     2615 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/989fccf80441_replace_values_with_reference_to_list_.py
--rw-rw-rw-   0        0        0     1282 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/9da58d2def22_create_entity_group_table.py
--rw-rw-rw-   0        0        0    20419 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/bba1e2ef5153_move_to_entity_based_design.py
--rw-rw-rw-   0        0        0     4423 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/bf255c179bce_get_rid_of_unused_fields_in_parameter_.py
--rw-rw-rw-   0        0        0     3708 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/defbda3bf2b5_add_tool_feature_tables.py
--rw-rw-rw-   0        0        0     6146 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/fbb540efbf15_add_support_for_mysql.py
--rw-rw-rw-   0        0        0     1271 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic/versions/fd542cebf699_drop_on_update_clauses_from_object_and_.py
--rw-rw-rw-   0        0        0     1771 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/alembic.ini
--rw-rw-rw-   0        0        0    29962 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/check_functions.py
--rw-rw-rw-   0        0        0    25818 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/db_cache.py
--rw-rw-rw-   0        0        0     2416 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/db_mapping.py
--rw-rw-rw-   0        0        0    25878 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/db_mapping_add_mixin.py
--rw-rw-rw-   0        0        0    94052 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/db_mapping_base.py
--rw-rw-rw-   0        0        0    42708 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/db_mapping_check_mixin.py
--rw-rw-rw-   0        0        0     3943 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/db_mapping_commit_mixin.py
--rw-rw-rw-   0        0        0    15578 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/db_mapping_query_mixin.py
--rw-rw-rw-   0        0        0    16349 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/db_mapping_remove_mixin.py
--rw-rw-rw-   0        0        0    16715 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/db_mapping_update_mixin.py
--rw-rw-rw-   0        0        0     9018 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/diff_db_mapping.py
--rw-rw-rw-   0        0        0     6541 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/diff_db_mapping_base.py
--rw-rw-rw-   0        0        0     5140 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/diff_db_mapping_commit_mixin.py
--rw-rw-rw-   0        0        0     3597 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/exception.py
--rw-rw-rw-   0        0        0    13504 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/export_functions.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.682643 spinedb_api-0.30.3/spinedb_api/export_mapping/
--rw-rw-rw-   0        0        0     1684 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/export_mapping/__init__.py
--rw-rw-rw-   0        0        0    63981 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/export_mapping/export_mapping.py
--rw-rw-rw-   0        0        0     4357 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/export_mapping/generator.py
--rw-rw-rw-   0        0        0     4181 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/export_mapping/group_functions.py
--rw-rw-rw-   0        0        0    10559 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/export_mapping/pivot.py
--rw-rw-rw-   0        0        0    30774 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/export_mapping/settings.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.774625 spinedb_api-0.30.3/spinedb_api/filters/
--rw-rw-rw-   0        0        0      989 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/filters/__init__.py
--rw-rw-rw-   0        0        0     6824 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/filters/alternative_filter.py
--rw-rw-rw-   0        0        0     6389 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/filters/execution_filter.py
--rw-rw-rw-   0        0        0     9981 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/filters/renamer.py
--rw-rw-rw-   0        0        0    10341 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/filters/scenario_filter.py
--rw-rw-rw-   0        0        0    10201 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/filters/tool_filter.py
--rw-rw-rw-   0        0        0    11044 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/filters/tools.py
--rw-rw-rw-   0        0        0    12953 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/filters/value_transformer.py
--rw-rw-rw-   0        0        0     7399 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/graph_layout_generator.py
--rw-rw-rw-   0        0        0    38413 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/helpers.py
--rw-rw-rw-   0        0        0    87687 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/import_functions.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.820547 spinedb_api-0.30.3/spinedb_api/import_mapping/
--rw-rw-rw-   0        0        0      989 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/import_mapping/__init__.py
--rw-rw-rw-   0        0        0    15765 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/import_mapping/generator.py
--rw-rw-rw-   0        0        0    44670 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/import_mapping/import_mapping.py
--rw-rw-rw-   0        0        0    18074 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/import_mapping/import_mapping_compat.py
--rw-rw-rw-   0        0        0     4221 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/import_mapping/type_conversion.py
--rw-rw-rw-   0        0        0     9148 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/mapping.py
--rw-rw-rw-   0        0        0    57050 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/parameter_value.py
--rw-rw-rw-   0        0        0     8552 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/perfect_split.py
--rw-rw-rw-   0        0        0     3357 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/purge.py
--rw-rw-rw-   0        0        0     4775 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/server_client_helpers.py
--rw-rw-rw-   0        0        0     4200 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/spine_db_client.py
--rw-rw-rw-   0        0        0    22540 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/spine_db_server.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.842398 spinedb_api-0.30.3/spinedb_api/spine_io/
--rw-rw-rw-   0        0        0     1057 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/spine_io/__init__.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:40.911707 spinedb_api-0.30.3/spinedb_api/spine_io/exporters/
--rw-rw-rw-   0        0        0     1067 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/spine_io/exporters/__init__.py
--rw-rw-rw-   0        0        0     2723 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/spine_io/exporters/csv_writer.py
--rw-rw-rw-   0        0        0    13129 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/spine_io/exporters/excel.py
--rw-rw-rw-   0        0        0     4568 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/spine_io/exporters/excel_writer.py
--rw-rw-rw-   0        0        0     5825 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/spine_io/exporters/gdx_writer.py
--rw-rw-rw-   0        0        0     6336 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/spine_io/exporters/sql_writer.py
--rw-rw-rw-   0        0        0     5347 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/spine_io/exporters/writer.py
--rw-rw-rw-   0        0        0     2687 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/spine_io/gdx_utils.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:41.040042 spinedb_api-0.30.3/spinedb_api/spine_io/importers/
--rw-rw-rw-   0        0        0     1025 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/spinedb_api/spine_io/importers/__init__.py
--rw-rw-rw-   0        0        0     7562 2023-11-30 06:59:46.000000 spinedb_api-0.30.3/spinedb_api/spine_io/importers/csv_reader.py
--rw-rw-rw-   0        0        0     4189 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/spine_io/importers/datapackage_reader.py
--rw-rw-rw-   0        0        0    10867 2023-11-30 06:59:46.000000 spinedb_api-0.30.3/spinedb_api/spine_io/importers/excel_reader.py
--rw-rw-rw-   0        0        0     4929 2023-11-30 06:59:46.000000 spinedb_api-0.30.3/spinedb_api/spine_io/importers/gdx_connector.py
--rw-rw-rw-   0        0        0     3755 2023-11-30 06:59:46.000000 spinedb_api-0.30.3/spinedb_api/spine_io/importers/json_reader.py
--rw-rw-rw-   0        0        0     6695 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/spinedb_api/spine_io/importers/reader.py
--rw-rw-rw-   0        0        0     3614 2023-11-30 06:59:46.000000 spinedb_api-0.30.3/spinedb_api/spine_io/importers/sqlalchemy_connector.py
--rw-rw-rw-   0        0        0      429 2023-12-04 11:59:39.000000 spinedb_api-0.30.3/spinedb_api/version.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:41.557326 spinedb_api-0.30.3/spinedb_api.egg-info/
--rw-rw-rw-   0        0        0     3940 2023-12-04 11:59:39.000000 spinedb_api-0.30.3/spinedb_api.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     5976 2023-12-04 11:59:39.000000 spinedb_api-0.30.3/spinedb_api.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-12-04 11:59:39.000000 spinedb_api-0.30.3/spinedb_api.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0        2 2023-08-30 13:04:27.000000 spinedb_api-0.30.3/spinedb_api.egg-info/not-zip-safe
--rw-rw-rw-   0        0        0      239 2023-12-04 11:59:39.000000 spinedb_api-0.30.3/spinedb_api.egg-info/requires.txt
--rw-rw-rw-   0        0        0       17 2023-12-04 11:59:39.000000 spinedb_api-0.30.3/spinedb_api.egg-info/top_level.txt
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:41.195513 spinedb_api-0.30.3/tests/
--rw-rw-rw-   0        0        0     1047 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/__init__.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:41.241986 spinedb_api-0.30.3/tests/export_mapping/
--rw-rw-rw-   0        0        0      989 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/export_mapping/__init__.py
--rw-rw-rw-   0        0        0    82266 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/export_mapping/test_export_mapping.py
--rw-rw-rw-   0        0        0     8932 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/export_mapping/test_pivot.py
--rw-rw-rw-   0        0        0    15264 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/export_mapping/test_settings.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:41.327172 spinedb_api-0.30.3/tests/filters/
--rw-rw-rw-   0        0        0      989 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/filters/__init__.py
--rw-rw-rw-   0        0        0     8921 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/filters/test_alternative_filter.py
--rw-rw-rw-   0        0        0    12406 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/filters/test_renamer.py
--rw-rw-rw-   0        0        0    22462 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/filters/test_scenario_filter.py
--rw-rw-rw-   0        0        0    10459 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/filters/test_tool_filter.py
--rw-rw-rw-   0        0        0    14028 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/filters/test_tools.py
--rw-rw-rw-   0        0        0    10647 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/filters/test_value_transformer.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:41.379462 spinedb_api-0.30.3/tests/import_mapping/
--rw-rw-rw-   0        0        0      989 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/import_mapping/__init__.py
--rw-rw-rw-   0        0        0    24607 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/import_mapping/test_generator.py
--rw-rw-rw-   0        0        0    91995 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/import_mapping/test_import_mapping.py
--rw-rw-rw-   0        0        0     4740 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/import_mapping/test_type_conversion.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:41.402670 spinedb_api-0.30.3/tests/spine_io/
--rw-rw-rw-   0        0        0     1063 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/spine_io/__init__.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:41.474633 spinedb_api-0.30.3/tests/spine_io/exporters/
--rw-rw-rw-   0        0        0     1073 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/spine_io/exporters/__init__.py
--rw-rw-rw-   0        0        0     4682 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/spine_io/exporters/test_csv_writer.py
--rw-rw-rw-   0        0        0     6651 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/spine_io/exporters/test_excel_writer.py
--rw-rw-rw-   0        0        0    16167 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/spine_io/exporters/test_gdx_writer.py
--rw-rw-rw-   0        0        0    12478 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/spine_io/exporters/test_sql_writer.py
--rw-rw-rw-   0        0        0     3457 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/spine_io/exporters/test_writer.py
-drwxrwxrwx   0        0        0        0 2023-12-04 11:59:41.556330 spinedb_api-0.30.3/tests/spine_io/importers/
--rw-rw-rw-   0        0        0     1073 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/spine_io/importers/__init__.py
--rw-rw-rw-   0        0        0     3687 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/spine_io/importers/test_CSVConnector.py
--rw-rw-rw-   0        0        0    12708 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/spine_io/importers/test_GdxConnector.py
--rw-rw-rw-   0        0        0     3490 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/spine_io/importers/test_datapackage_reader.py
--rw-rw-rw-   0        0        0     1414 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/spine_io/importers/test_excel_reader.py
--rw-rw-rw-   0        0        0     2030 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/spine_io/importers/test_json_reader.py
--rw-rw-rw-   0        0        0     1436 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/spine_io/importers/test_sqlalchemy_connector.py
--rw-rw-rw-   0        0        0     6394 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/spine_io/test_excel_integration.py
--rw-rw-rw-   0        0        0    52554 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/test_DatabaseMapping.py
--rw-rw-rw-   0        0        0    74831 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/test_DiffDatabaseMapping.py
--rw-rw-rw-   0        0        0     4278 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/test_check_functions.py
--rw-rw-rw-   0        0        0     9168 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/test_export_functions.py
--rw-rw-rw-   0        0        0     1888 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/test_helpers.py
--rw-rw-rw-   0        0        0    76291 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/test_import_functions.py
--rw-rw-rw-   0        0        0     2567 2023-08-30 10:51:10.000000 spinedb_api-0.30.3/tests/test_mapping.py
--rw-rw-rw-   0        0        0     8579 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/test_migration.py
--rw-rw-rw-   0        0        0    46881 2023-12-04 07:19:23.000000 spinedb_api-0.30.3/tests/test_parameter_value.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.651777 spinedb_api-0.30.4/
+-rw-r--r--   0 runner    (1001) docker     (127)      161 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/.gitattributes
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.623777 spinedb_api-0.30.4/.github/
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/.github/pull_request_template.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.623777 spinedb_api-0.30.4/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (127)     1262 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/.github/workflows/run_unit_tests.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      278 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)      567 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/.readthedocs.yml
+-rw-r--r--   0 runner    (1001) docker     (127)    32493 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/COPYING
+-rw-r--r--   0 runner    (1001) docker     (127)     7651 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/COPYING.LESSER
+-rw-r--r--   0 runner    (1001) docker     (127)       52 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)     3836 2024-03-28 10:33:16.651777 spinedb_api-0.30.4/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.627777 spinedb_api-0.30.4/bin/
+-rw-r--r--   0 runner    (1001) docker     (127)       76 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/bin/build_doc.bat
+-rw-r--r--   0 runner    (1001) docker     (127)      346 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/bin/build_doc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1265 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/bin/update_copyrights.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3248 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/deploy-key.enc
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.627777 spinedb_api-0.30.4/docs/
+-rw-r--r--   0 runner    (1001) docker     (127)      584 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/Makefile
+-rw-r--r--   0 runner    (1001) docker     (127)      756 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/make.bat
+-rw-r--r--   0 runner    (1001) docker     (127)      423 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.627777 spinedb_api-0.30.4/docs/source/
+-rw-r--r--   0 runner    (1001) docker     (127)     6693 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/source/conf.py
+-rw-r--r--   0 runner    (1001) docker     (127)      981 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/source/front_matter.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.627777 spinedb_api-0.30.4/docs/source/img/
+-rw-r--r--   0 runner    (1001) docker     (127)     5205 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/source/img/spinetoolbox_on_wht.svg
+-rw-r--r--   0 runner    (1001) docker     (127)      669 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/source/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2961 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/source/metadata_description.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    10457 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/source/parameter_value_format.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      912 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/source/results_metadata_description.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2250 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/docs/source/tutorial.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.627777 spinedb_api-0.30.4/fig/
+-rw-r--r--   0 runner    (1001) docker     (127)    55634 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/fig/eu-emblem-low-res.jpg
+-rw-r--r--   0 runner    (1001) docker     (127)    16474 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/pylintrc
+-rw-r--r--   0 runner    (1001) docker     (127)     1721 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (127)        5 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-03-28 10:33:16.651777 spinedb_api-0.30.4/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.635777 spinedb_api-0.30.4/spinedb_api/
+-rw-r--r--   0 runner    (1001) docker     (127)     4307 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.635777 spinedb_api-0.30.4/spinedb_api/alembic/
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/README
+-rw-r--r--   0 runner    (1001) docker     (127)     2015 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/env.py
+-rw-r--r--   0 runner    (1001) docker     (127)      494 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/script.py.mako
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.639777 spinedb_api-0.30.4/spinedb_api/alembic/versions/
+-rw-r--r--   0 runner    (1001) docker     (127)     1816 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/070a0eb89e88_drop_category_tables.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4013 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/0c7d199ae915_add_list_value_table.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2371 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/1892adebc00f_create_metadata_tables.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2929 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/1e4997105288_separate_type_from_value.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5987 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/39e860a11b05_add_alternatives_and_scenarios.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2685 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/51fd7b69acf7_add_parameter_tag_and_parameter_value_list.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1280 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/738d494a08ac_fix_foreign_key_constraints_in_object_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1308 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/7d0b467f2f4e_fix_foreign_key_constraints_in_entity_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2461 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/8c19c53d5701_rename_parameter_to_parameter_definition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2542 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/989fccf80441_replace_values_with_reference_to_list_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1240 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/9da58d2def22_create_entity_group_table.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19946 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/bba1e2ef5153_move_to_entity_based_design.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4341 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/bf255c179bce_get_rid_of_unused_fields_in_parameter_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3618 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/defbda3bf2b5_add_tool_feature_tables.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6031 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/fbb540efbf15_add_support_for_mysql.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1230 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic/versions/fd542cebf699_drop_on_update_clauses_from_object_and_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1697 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/alembic.ini
+-rw-r--r--   0 runner    (1001) docker     (127)    29319 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/check_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25153 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/db_cache.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2368 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/db_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25298 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/db_mapping_add_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    91853 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/db_mapping_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41812 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/db_mapping_check_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3853 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/db_mapping_commit_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15289 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/db_mapping_query_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16048 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/db_mapping_remove_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16363 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/db_mapping_update_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8840 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/diff_db_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6398 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/diff_db_mapping_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5036 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/diff_db_mapping_commit_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3492 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/exception.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13191 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/export_functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.639777 spinedb_api-0.30.4/spinedb_api/export_mapping/
+-rw-r--r--   0 runner    (1001) docker     (127)     1649 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/export_mapping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    62024 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/export_mapping/export_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4246 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/export_mapping/generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4039 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/export_mapping/group_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10321 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/export_mapping/pivot.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30010 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/export_mapping/settings.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.639777 spinedb_api-0.30.4/spinedb_api/filters/
+-rw-r--r--   0 runner    (1001) docker     (127)      979 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/filters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6647 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/filters/alternative_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6217 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/filters/execution_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9696 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/filters/renamer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10075 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/filters/scenario_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10062 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/filters/tool_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10710 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/filters/tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12624 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/filters/value_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7238 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/graph_layout_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37437 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    85683 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/import_functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.639777 spinedb_api-0.30.4/spinedb_api/import_mapping/
+-rw-r--r--   0 runner    (1001) docker     (127)      979 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/import_mapping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15373 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/import_mapping/generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    43513 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/import_mapping/import_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17679 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/import_mapping/import_mapping_compat.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4103 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/import_mapping/type_conversion.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8855 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    55459 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/parameter_value.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8370 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/perfect_split.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3277 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/purge.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4636 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/server_client_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4089 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_db_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21936 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_db_server.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.643777 spinedb_api-0.30.4/spinedb_api/spine_io/
+-rw-r--r--   0 runner    (1001) docker     (127)     1042 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.643777 spinedb_api-0.30.4/spinedb_api/spine_io/exporters/
+-rw-r--r--   0 runner    (1001) docker     (127)     1052 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/exporters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2655 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/exporters/csv_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12860 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/exporters/excel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4453 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/exporters/excel_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5687 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/exporters/gdx_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6188 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/exporters/sql_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5205 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/exporters/writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2624 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/gdx_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.643777 spinedb_api-0.30.4/spinedb_api/spine_io/importers/
+-rw-r--r--   0 runner    (1001) docker     (127)     1010 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/importers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7377 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/importers/csv_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4077 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/importers/datapackage_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10593 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/importers/excel_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4808 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/importers/gdx_connector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3654 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/importers/json_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6536 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/importers/reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3520 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/spinedb_api/spine_io/importers/sqlalchemy_connector.py
+-rw-r--r--   0 runner    (1001) docker     (127)      413 2024-03-28 10:33:16.000000 spinedb_api-0.30.4/spinedb_api/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.651777 spinedb_api-0.30.4/spinedb_api.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)     3836 2024-03-28 10:33:16.000000 spinedb_api-0.30.4/spinedb_api.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     5976 2024-03-28 10:33:16.000000 spinedb_api-0.30.4/spinedb_api.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-03-28 10:33:16.000000 spinedb_api-0.30.4/spinedb_api.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-03-28 10:33:16.000000 spinedb_api-0.30.4/spinedb_api.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (127)      239 2024-03-28 10:33:16.000000 spinedb_api-0.30.4/spinedb_api.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       17 2024-03-28 10:33:16.000000 spinedb_api-0.30.4/spinedb_api.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.647777 spinedb_api-0.30.4/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     1032 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.647777 spinedb_api-0.30.4/tests/export_mapping/
+-rw-r--r--   0 runner    (1001) docker     (127)      979 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/export_mapping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    80599 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/export_mapping/test_export_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8695 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/export_mapping/test_pivot.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14923 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/export_mapping/test_settings.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.647777 spinedb_api-0.30.4/tests/filters/
+-rw-r--r--   0 runner    (1001) docker     (127)      979 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/filters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8745 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/filters/test_alternative_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12146 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/filters/test_renamer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22023 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/filters/test_scenario_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10229 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/filters/test_tool_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13710 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/filters/test_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10429 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/filters/test_value_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.647777 spinedb_api-0.30.4/tests/import_mapping/
+-rw-r--r--   0 runner    (1001) docker     (127)      979 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/import_mapping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24167 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/import_mapping/test_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    89876 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/import_mapping/test_import_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4632 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/import_mapping/test_type_conversion.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.647777 spinedb_api-0.30.4/tests/spine_io/
+-rw-r--r--   0 runner    (1001) docker     (127)     1048 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.651777 spinedb_api-0.30.4/tests/spine_io/exporters/
+-rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/exporters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4584 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/exporters/test_csv_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6507 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/exporters/test_excel_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15865 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/exporters/test_gdx_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12196 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/exporters/test_sql_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3367 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/exporters/test_writer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-28 10:33:16.651777 spinedb_api-0.30.4/tests/spine_io/importers/
+-rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/importers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3606 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/importers/test_CSVConnector.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12476 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/importers/test_GdxConnector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3415 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/importers/test_datapackage_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1385 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/importers/test_excel_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1986 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/importers/test_json_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1407 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/importers/test_sqlalchemy_connector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6253 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/spine_io/test_excel_integration.py
+-rw-r--r--   0 runner    (1001) docker     (127)    51540 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/test_DatabaseMapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    73511 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/test_DiffDatabaseMapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4195 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/test_check_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9000 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/test_export_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1844 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/test_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    74718 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/test_import_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2515 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/test_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8438 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/test_migration.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45877 2024-03-28 10:33:06.000000 spinedb_api-0.30.4/tests/test_parameter_value.py
```

### Comparing `spinedb_api-0.30.3/.readthedocs.yml` & `spinedb_api-0.30.4/.readthedocs.yml`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-# .readthedocs.yml
-# Read the Docs configuration file
-# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details
-
-# Required
-version: 2
-
-# Build documentation in the docs/ directory with Sphinx
-sphinx:
-  builder: html
-  configuration: docs/source/conf.py
-
-# Optionally build your docs in additional formats such as PDF and ePub
-formats:
-  - htmlzip
-  - pdf
-
-# Optionally set the version of Python and requirements required to build your docs
-python:
-  version: 3.8
-  install:
-    - method: pip
-      path: .
-    - requirements: docs/requirements.txt
+# .readthedocs.yml
+# Read the Docs configuration file
+# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details
+
+# Required
+version: 2
+
+# Build documentation in the docs/ directory with Sphinx
+sphinx:
+  builder: html
+  configuration: docs/source/conf.py
+
+# Optionally build your docs in additional formats such as PDF and ePub
+formats:
+  - htmlzip
+  - pdf
+
+# Optionally set the version of Python and requirements required to build your docs
+python:
+  version: 3.8
+  install:
+    - method: pip
+      path: .
+    - requirements: docs/requirements.txt
```

### Comparing `spinedb_api-0.30.3/COPYING` & `spinedb_api-0.30.4/COPYING`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,622 +1,622 @@
-                    GNU GENERAL PUBLIC LICENSE
-                       Version 3, 29 June 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-                            Preamble
-
-  The GNU General Public License is a free, copyleft license for
-software and other kinds of works.
-
-  The licenses for most software and other practical works are designed
-to take away your freedom to share and change the works.  By contrast,
-the GNU General Public License is intended to guarantee your freedom to
-share and change all versions of a program--to make sure it remains free
-software for all its users.  We, the Free Software Foundation, use the
-GNU General Public License for most of our software; it applies also to
-any other work released this way by its authors.  You can apply it to
-your programs, too.
-
-  When we speak of free software, we are referring to freedom, not
-price.  Our General Public Licenses are designed to make sure that you
-have the freedom to distribute copies of free software (and charge for
-them if you wish), that you receive source code or can get it if you
-want it, that you can change the software or use pieces of it in new
-free programs, and that you know you can do these things.
-
-  To protect your rights, we need to prevent others from denying you
-these rights or asking you to surrender the rights.  Therefore, you have
-certain responsibilities if you distribute copies of the software, or if
-you modify it: responsibilities to respect the freedom of others.
-
-  For example, if you distribute copies of such a program, whether
-gratis or for a fee, you must pass on to the recipients the same
-freedoms that you received.  You must make sure that they, too, receive
-or can get the source code.  And you must show them these terms so they
-know their rights.
-
-  Developers that use the GNU GPL protect your rights with two steps:
-(1) assert copyright on the software, and (2) offer you this License
-giving you legal permission to copy, distribute and/or modify it.
-
-  For the developers' and authors' protection, the GPL clearly explains
-that there is no warranty for this free software.  For both users' and
-authors' sake, the GPL requires that modified versions be marked as
-changed, so that their problems will not be attributed erroneously to
-authors of previous versions.
-
-  Some devices are designed to deny users access to install or run
-modified versions of the software inside them, although the manufacturer
-can do so.  This is fundamentally incompatible with the aim of
-protecting users' freedom to change the software.  The systematic
-pattern of such abuse occurs in the area of products for individuals to
-use, which is precisely where it is most unacceptable.  Therefore, we
-have designed this version of the GPL to prohibit the practice for those
-products.  If such problems arise substantially in other domains, we
-stand ready to extend this provision to those domains in future versions
-of the GPL, as needed to protect the freedom of users.
-
-  Finally, every program is threatened constantly by software patents.
-States should not allow patents to restrict development and use of
-software on general-purpose computers, but in those that do, we wish to
-avoid the special danger that patents applied to a free program could
-make it effectively proprietary.  To prevent this, the GPL assures that
-patents cannot be used to render the program non-free.
-
-  The precise terms and conditions for copying, distribution and
-modification follow.
-
-                       TERMS AND CONDITIONS
-
-  0. Definitions.
-
-  "This License" refers to version 3 of the GNU General Public License.
-
-  "Copyright" also means copyright-like laws that apply to other kinds of
-works, such as semiconductor masks.
-
-  "The Program" refers to any copyrightable work licensed under this
-License.  Each licensee is addressed as "you".  "Licensees" and
-"recipients" may be individuals or organizations.
-
-  To "modify" a work means to copy from or adapt all or part of the work
-in a fashion requiring copyright permission, other than the making of an
-exact copy.  The resulting work is called a "modified version" of the
-earlier work or a work "based on" the earlier work.
-
-  A "covered work" means either the unmodified Program or a work based
-on the Program.
-
-  To "propagate" a work means to do anything with it that, without
-permission, would make you directly or secondarily liable for
-infringement under applicable copyright law, except executing it on a
-computer or modifying a private copy.  Propagation includes copying,
-distribution (with or without modification), making available to the
-public, and in some countries other activities as well.
-
-  To "convey" a work means any kind of propagation that enables other
-parties to make or receive copies.  Mere interaction with a user through
-a computer network, with no transfer of a copy, is not conveying.
-
-  An interactive user interface displays "Appropriate Legal Notices"
-to the extent that it includes a convenient and prominently visible
-feature that (1) displays an appropriate copyright notice, and (2)
-tells the user that there is no warranty for the work (except to the
-extent that warranties are provided), that licensees may convey the
-work under this License, and how to view a copy of this License.  If
-the interface presents a list of user commands or options, such as a
-menu, a prominent item in the list meets this criterion.
-
-  1. Source Code.
-
-  The "source code" for a work means the preferred form of the work
-for making modifications to it.  "Object code" means any non-source
-form of a work.
-
-  A "Standard Interface" means an interface that either is an official
-standard defined by a recognized standards body, or, in the case of
-interfaces specified for a particular programming language, one that
-is widely used among developers working in that language.
-
-  The "System Libraries" of an executable work include anything, other
-than the work as a whole, that (a) is included in the normal form of
-packaging a Major Component, but which is not part of that Major
-Component, and (b) serves only to enable use of the work with that
-Major Component, or to implement a Standard Interface for which an
-implementation is available to the public in source code form.  A
-"Major Component", in this context, means a major essential component
-(kernel, window system, and so on) of the specific operating system
-(if any) on which the executable work runs, or a compiler used to
-produce the work, or an object code interpreter used to run it.
-
-  The "Corresponding Source" for a work in object code form means all
-the source code needed to generate, install, and (for an executable
-work) run the object code and to modify the work, including scripts to
-control those activities.  However, it does not include the work's
-System Libraries, or general-purpose tools or generally available free
-programs which are used unmodified in performing those activities but
-which are not part of the work.  For example, Corresponding Source
-includes interface definition files associated with source files for
-the work, and the source code for shared libraries and dynamically
-linked subprograms that the work is specifically designed to require,
-such as by intimate data communication or control flow between those
-subprograms and other parts of the work.
-
-  The Corresponding Source need not include anything that users
-can regenerate automatically from other parts of the Corresponding
-Source.
-
-  The Corresponding Source for a work in source code form is that
-same work.
-
-  2. Basic Permissions.
-
-  All rights granted under this License are granted for the term of
-copyright on the Program, and are irrevocable provided the stated
-conditions are met.  This License explicitly affirms your unlimited
-permission to run the unmodified Program.  The output from running a
-covered work is covered by this License only if the output, given its
-content, constitutes a covered work.  This License acknowledges your
-rights of fair use or other equivalent, as provided by copyright law.
-
-  You may make, run and propagate covered works that you do not
-convey, without conditions so long as your license otherwise remains
-in force.  You may convey covered works to others for the sole purpose
-of having them make modifications exclusively for you, or provide you
-with facilities for running those works, provided that you comply with
-the terms of this License in conveying all material for which you do
-not control copyright.  Those thus making or running the covered works
-for you must do so exclusively on your behalf, under your direction
-and control, on terms that prohibit them from making any copies of
-your copyrighted material outside their relationship with you.
-
-  Conveying under any other circumstances is permitted solely under
-the conditions stated below.  Sublicensing is not allowed; section 10
-makes it unnecessary.
-
-  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
-
-  No covered work shall be deemed part of an effective technological
-measure under any applicable law fulfilling obligations under article
-11 of the WIPO copyright treaty adopted on 20 December 1996, or
-similar laws prohibiting or restricting circumvention of such
-measures.
-
-  When you convey a covered work, you waive any legal power to forbid
-circumvention of technological measures to the extent such circumvention
-is effected by exercising rights under this License with respect to
-the covered work, and you disclaim any intention to limit operation or
-modification of the work as a means of enforcing, against the work's
-users, your or third parties' legal rights to forbid circumvention of
-technological measures.
-
-  4. Conveying Verbatim Copies.
-
-  You may convey verbatim copies of the Program's source code as you
-receive it, in any medium, provided that you conspicuously and
-appropriately publish on each copy an appropriate copyright notice;
-keep intact all notices stating that this License and any
-non-permissive terms added in accord with section 7 apply to the code;
-keep intact all notices of the absence of any warranty; and give all
-recipients a copy of this License along with the Program.
-
-  You may charge any price or no price for each copy that you convey,
-and you may offer support or warranty protection for a fee.
-
-  5. Conveying Modified Source Versions.
-
-  You may convey a work based on the Program, or the modifications to
-produce it from the Program, in the form of source code under the
-terms of section 4, provided that you also meet all of these conditions:
-
-    a) The work must carry prominent notices stating that you modified
-    it, and giving a relevant date.
-
-    b) The work must carry prominent notices stating that it is
-    released under this License and any conditions added under section
-    7.  This requirement modifies the requirement in section 4 to
-    "keep intact all notices".
-
-    c) You must license the entire work, as a whole, under this
-    License to anyone who comes into possession of a copy.  This
-    License will therefore apply, along with any applicable section 7
-    additional terms, to the whole of the work, and all its parts,
-    regardless of how they are packaged.  This License gives no
-    permission to license the work in any other way, but it does not
-    invalidate such permission if you have separately received it.
-
-    d) If the work has interactive user interfaces, each must display
-    Appropriate Legal Notices; however, if the Program has interactive
-    interfaces that do not display Appropriate Legal Notices, your
-    work need not make them do so.
-
-  A compilation of a covered work with other separate and independent
-works, which are not by their nature extensions of the covered work,
-and which are not combined with it such as to form a larger program,
-in or on a volume of a storage or distribution medium, is called an
-"aggregate" if the compilation and its resulting copyright are not
-used to limit the access or legal rights of the compilation's users
-beyond what the individual works permit.  Inclusion of a covered work
-in an aggregate does not cause this License to apply to the other
-parts of the aggregate.
-
-  6. Conveying Non-Source Forms.
-
-  You may convey a covered work in object code form under the terms
-of sections 4 and 5, provided that you also convey the
-machine-readable Corresponding Source under the terms of this License,
-in one of these ways:
-
-    a) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by the
-    Corresponding Source fixed on a durable physical medium
-    customarily used for software interchange.
-
-    b) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by a
-    written offer, valid for at least three years and valid for as
-    long as you offer spare parts or customer support for that product
-    model, to give anyone who possesses the object code either (1) a
-    copy of the Corresponding Source for all the software in the
-    product that is covered by this License, on a durable physical
-    medium customarily used for software interchange, for a price no
-    more than your reasonable cost of physically performing this
-    conveying of source, or (2) access to copy the
-    Corresponding Source from a network server at no charge.
-
-    c) Convey individual copies of the object code with a copy of the
-    written offer to provide the Corresponding Source.  This
-    alternative is allowed only occasionally and noncommercially, and
-    only if you received the object code with such an offer, in accord
-    with subsection 6b.
-
-    d) Convey the object code by offering access from a designated
-    place (gratis or for a charge), and offer equivalent access to the
-    Corresponding Source in the same way through the same place at no
-    further charge.  You need not require recipients to copy the
-    Corresponding Source along with the object code.  If the place to
-    copy the object code is a network server, the Corresponding Source
-    may be on a different server (operated by you or a third party)
-    that supports equivalent copying facilities, provided you maintain
-    clear directions next to the object code saying where to find the
-    Corresponding Source.  Regardless of what server hosts the
-    Corresponding Source, you remain obligated to ensure that it is
-    available for as long as needed to satisfy these requirements.
-
-    e) Convey the object code using peer-to-peer transmission, provided
-    you inform other peers where the object code and Corresponding
-    Source of the work are being offered to the general public at no
-    charge under subsection 6d.
-
-  A separable portion of the object code, whose source code is excluded
-from the Corresponding Source as a System Library, need not be
-included in conveying the object code work.
-
-  A "User Product" is either (1) a "consumer product", which means any
-tangible personal property which is normally used for personal, family,
-or household purposes, or (2) anything designed or sold for incorporation
-into a dwelling.  In determining whether a product is a consumer product,
-doubtful cases shall be resolved in favor of coverage.  For a particular
-product received by a particular user, "normally used" refers to a
-typical or common use of that class of product, regardless of the status
-of the particular user or of the way in which the particular user
-actually uses, or expects or is expected to use, the product.  A product
-is a consumer product regardless of whether the product has substantial
-commercial, industrial or non-consumer uses, unless such uses represent
-the only significant mode of use of the product.
-
-  "Installation Information" for a User Product means any methods,
-procedures, authorization keys, or other information required to install
-and execute modified versions of a covered work in that User Product from
-a modified version of its Corresponding Source.  The information must
-suffice to ensure that the continued functioning of the modified object
-code is in no case prevented or interfered with solely because
-modification has been made.
-
-  If you convey an object code work under this section in, or with, or
-specifically for use in, a User Product, and the conveying occurs as
-part of a transaction in which the right of possession and use of the
-User Product is transferred to the recipient in perpetuity or for a
-fixed term (regardless of how the transaction is characterized), the
-Corresponding Source conveyed under this section must be accompanied
-by the Installation Information.  But this requirement does not apply
-if neither you nor any third party retains the ability to install
-modified object code on the User Product (for example, the work has
-been installed in ROM).
-
-  The requirement to provide Installation Information does not include a
-requirement to continue to provide support service, warranty, or updates
-for a work that has been modified or installed by the recipient, or for
-the User Product in which it has been modified or installed.  Access to a
-network may be denied when the modification itself materially and
-adversely affects the operation of the network or violates the rules and
-protocols for communication across the network.
-
-  Corresponding Source conveyed, and Installation Information provided,
-in accord with this section must be in a format that is publicly
-documented (and with an implementation available to the public in
-source code form), and must require no special password or key for
-unpacking, reading or copying.
-
-  7. Additional Terms.
-
-  "Additional permissions" are terms that supplement the terms of this
-License by making exceptions from one or more of its conditions.
-Additional permissions that are applicable to the entire Program shall
-be treated as though they were included in this License, to the extent
-that they are valid under applicable law.  If additional permissions
-apply only to part of the Program, that part may be used separately
-under those permissions, but the entire Program remains governed by
-this License without regard to the additional permissions.
-
-  When you convey a copy of a covered work, you may at your option
-remove any additional permissions from that copy, or from any part of
-it.  (Additional permissions may be written to require their own
-removal in certain cases when you modify the work.)  You may place
-additional permissions on material, added by you to a covered work,
-for which you have or can give appropriate copyright permission.
-
-  Notwithstanding any other provision of this License, for material you
-add to a covered work, you may (if authorized by the copyright holders of
-that material) supplement the terms of this License with terms:
-
-    a) Disclaiming warranty or limiting liability differently from the
-    terms of sections 15 and 16 of this License; or
-
-    b) Requiring preservation of specified reasonable legal notices or
-    author attributions in that material or in the Appropriate Legal
-    Notices displayed by works containing it; or
-
-    c) Prohibiting misrepresentation of the origin of that material, or
-    requiring that modified versions of such material be marked in
-    reasonable ways as different from the original version; or
-
-    d) Limiting the use for publicity purposes of names of licensors or
-    authors of the material; or
-
-    e) Declining to grant rights under trademark law for use of some
-    trade names, trademarks, or service marks; or
-
-    f) Requiring indemnification of licensors and authors of that
-    material by anyone who conveys the material (or modified versions of
-    it) with contractual assumptions of liability to the recipient, for
-    any liability that these contractual assumptions directly impose on
-    those licensors and authors.
-
-  All other non-permissive additional terms are considered "further
-restrictions" within the meaning of section 10.  If the Program as you
-received it, or any part of it, contains a notice stating that it is
-governed by this License along with a term that is a further
-restriction, you may remove that term.  If a license document contains
-a further restriction but permits relicensing or conveying under this
-License, you may add to a covered work material governed by the terms
-of that license document, provided that the further restriction does
-not survive such relicensing or conveying.
-
-  If you add terms to a covered work in accord with this section, you
-must place, in the relevant source files, a statement of the
-additional terms that apply to those files, or a notice indicating
-where to find the applicable terms.
-
-  Additional terms, permissive or non-permissive, may be stated in the
-form of a separately written license, or stated as exceptions;
-the above requirements apply either way.
-
-  8. Termination.
-
-  You may not propagate or modify a covered work except as expressly
-provided under this License.  Any attempt otherwise to propagate or
-modify it is void, and will automatically terminate your rights under
-this License (including any patent licenses granted under the third
-paragraph of section 11).
-
-  However, if you cease all violation of this License, then your
-license from a particular copyright holder is reinstated (a)
-provisionally, unless and until the copyright holder explicitly and
-finally terminates your license, and (b) permanently, if the copyright
-holder fails to notify you of the violation by some reasonable means
-prior to 60 days after the cessation.
-
-  Moreover, your license from a particular copyright holder is
-reinstated permanently if the copyright holder notifies you of the
-violation by some reasonable means, this is the first time you have
-received notice of violation of this License (for any work) from that
-copyright holder, and you cure the violation prior to 30 days after
-your receipt of the notice.
-
-  Termination of your rights under this section does not terminate the
-licenses of parties who have received copies or rights from you under
-this License.  If your rights have been terminated and not permanently
-reinstated, you do not qualify to receive new licenses for the same
-material under section 10.
-
-  9. Acceptance Not Required for Having Copies.
-
-  You are not required to accept this License in order to receive or
-run a copy of the Program.  Ancillary propagation of a covered work
-occurring solely as a consequence of using peer-to-peer transmission
-to receive a copy likewise does not require acceptance.  However,
-nothing other than this License grants you permission to propagate or
-modify any covered work.  These actions infringe copyright if you do
-not accept this License.  Therefore, by modifying or propagating a
-covered work, you indicate your acceptance of this License to do so.
-
-  10. Automatic Licensing of Downstream Recipients.
-
-  Each time you convey a covered work, the recipient automatically
-receives a license from the original licensors, to run, modify and
-propagate that work, subject to this License.  You are not responsible
-for enforcing compliance by third parties with this License.
-
-  An "entity transaction" is a transaction transferring control of an
-organization, or substantially all assets of one, or subdividing an
-organization, or merging organizations.  If propagation of a covered
-work results from an entity transaction, each party to that
-transaction who receives a copy of the work also receives whatever
-licenses to the work the party's predecessor in interest had or could
-give under the previous paragraph, plus a right to possession of the
-Corresponding Source of the work from the predecessor in interest, if
-the predecessor has it or can get it with reasonable efforts.
-
-  You may not impose any further restrictions on the exercise of the
-rights granted or affirmed under this License.  For example, you may
-not impose a license fee, royalty, or other charge for exercise of
-rights granted under this License, and you may not initiate litigation
-(including a cross-claim or counterclaim in a lawsuit) alleging that
-any patent claim is infringed by making, using, selling, offering for
-sale, or importing the Program or any portion of it.
-
-  11. Patents.
-
-  A "contributor" is a copyright holder who authorizes use under this
-License of the Program or a work on which the Program is based.  The
-work thus licensed is called the contributor's "contributor version".
-
-  A contributor's "essential patent claims" are all patent claims
-owned or controlled by the contributor, whether already acquired or
-hereafter acquired, that would be infringed by some manner, permitted
-by this License, of making, using, or selling its contributor version,
-but do not include claims that would be infringed only as a
-consequence of further modification of the contributor version.  For
-purposes of this definition, "control" includes the right to grant
-patent sublicenses in a manner consistent with the requirements of
-this License.
-
-  Each contributor grants you a non-exclusive, worldwide, royalty-free
-patent license under the contributor's essential patent claims, to
-make, use, sell, offer for sale, import and otherwise run, modify and
-propagate the contents of its contributor version.
-
-  In the following three paragraphs, a "patent license" is any express
-agreement or commitment, however denominated, not to enforce a patent
-(such as an express permission to practice a patent or covenant not to
-sue for patent infringement).  To "grant" such a patent license to a
-party means to make such an agreement or commitment not to enforce a
-patent against the party.
-
-  If you convey a covered work, knowingly relying on a patent license,
-and the Corresponding Source of the work is not available for anyone
-to copy, free of charge and under the terms of this License, through a
-publicly available network server or other readily accessible means,
-then you must either (1) cause the Corresponding Source to be so
-available, or (2) arrange to deprive yourself of the benefit of the
-patent license for this particular work, or (3) arrange, in a manner
-consistent with the requirements of this License, to extend the patent
-license to downstream recipients.  "Knowingly relying" means you have
-actual knowledge that, but for the patent license, your conveying the
-covered work in a country, or your recipient's use of the covered work
-in a country, would infringe one or more identifiable patents in that
-country that you have reason to believe are valid.
-
-  If, pursuant to or in connection with a single transaction or
-arrangement, you convey, or propagate by procuring conveyance of, a
-covered work, and grant a patent license to some of the parties
-receiving the covered work authorizing them to use, propagate, modify
-or convey a specific copy of the covered work, then the patent license
-you grant is automatically extended to all recipients of the covered
-work and works based on it.
-
-  A patent license is "discriminatory" if it does not include within
-the scope of its coverage, prohibits the exercise of, or is
-conditioned on the non-exercise of one or more of the rights that are
-specifically granted under this License.  You may not convey a covered
-work if you are a party to an arrangement with a third party that is
-in the business of distributing software, under which you make payment
-to the third party based on the extent of your activity of conveying
-the work, and under which the third party grants, to any of the
-parties who would receive the covered work from you, a discriminatory
-patent license (a) in connection with copies of the covered work
-conveyed by you (or copies made from those copies), or (b) primarily
-for and in connection with specific products or compilations that
-contain the covered work, unless you entered into that arrangement,
-or that patent license was granted, prior to 28 March 2007.
-
-  Nothing in this License shall be construed as excluding or limiting
-any implied license or other defenses to infringement that may
-otherwise be available to you under applicable patent law.
-
-  12. No Surrender of Others' Freedom.
-
-  If conditions are imposed on you (whether by court order, agreement or
-otherwise) that contradict the conditions of this License, they do not
-excuse you from the conditions of this License.  If you cannot convey a
-covered work so as to satisfy simultaneously your obligations under this
-License and any other pertinent obligations, then as a consequence you may
-not convey it at all.  For example, if you agree to terms that obligate you
-to collect a royalty for further conveying from those to whom you convey
-the Program, the only way you could satisfy both those terms and this
-License would be to refrain entirely from conveying the Program.
-
-  13. Use with the GNU Affero General Public License.
-
-  Notwithstanding any other provision of this License, you have
-permission to link or combine any covered work with a work licensed
-under version 3 of the GNU Affero General Public License into a single
-combined work, and to convey the resulting work.  The terms of this
-License will continue to apply to the part which is the covered work,
-but the special requirements of the GNU Affero General Public License,
-section 13, concerning interaction through a network will apply to the
-combination as such.
-
-  14. Revised Versions of this License.
-
-  The Free Software Foundation may publish revised and/or new versions of
-the GNU General Public License from time to time.  Such new versions will
-be similar in spirit to the present version, but may differ in detail to
-address new problems or concerns.
-
-  Each version is given a distinguishing version number.  If the
-Program specifies that a certain numbered version of the GNU General
-Public License "or any later version" applies to it, you have the
-option of following the terms and conditions either of that numbered
-version or of any later version published by the Free Software
-Foundation.  If the Program does not specify a version number of the
-GNU General Public License, you may choose any version ever published
-by the Free Software Foundation.
-
-  If the Program specifies that a proxy can decide which future
-versions of the GNU General Public License can be used, that proxy's
-public statement of acceptance of a version permanently authorizes you
-to choose that version for the Program.
-
-  Later license versions may give you additional or different
-permissions.  However, no additional obligations are imposed on any
-author or copyright holder as a result of your choosing to follow a
-later version.
-
-  15. Disclaimer of Warranty.
-
-  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
-APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
-HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
-OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
-THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
-IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
-ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
-
-  16. Limitation of Liability.
-
-  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
-WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
-THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
-GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
-USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
-DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
-PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
-EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
-SUCH DAMAGES.
-
-  17. Interpretation of Sections 15 and 16.
-
-  If the disclaimer of warranty and limitation of liability provided
-above cannot be given local legal effect according to their terms,
-reviewing courts shall apply local law that most closely approximates
-an absolute waiver of all civil liability in connection with the
-Program, unless a warranty or assumption of liability accompanies a
-copy of the Program in return for a fee.
-
-                     END OF TERMS AND CONDITIONS
+                    GNU GENERAL PUBLIC LICENSE
+                       Version 3, 29 June 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The GNU General Public License is a free, copyleft license for
+software and other kinds of works.
+
+  The licenses for most software and other practical works are designed
+to take away your freedom to share and change the works.  By contrast,
+the GNU General Public License is intended to guarantee your freedom to
+share and change all versions of a program--to make sure it remains free
+software for all its users.  We, the Free Software Foundation, use the
+GNU General Public License for most of our software; it applies also to
+any other work released this way by its authors.  You can apply it to
+your programs, too.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+them if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces of it in new
+free programs, and that you know you can do these things.
+
+  To protect your rights, we need to prevent others from denying you
+these rights or asking you to surrender the rights.  Therefore, you have
+certain responsibilities if you distribute copies of the software, or if
+you modify it: responsibilities to respect the freedom of others.
+
+  For example, if you distribute copies of such a program, whether
+gratis or for a fee, you must pass on to the recipients the same
+freedoms that you received.  You must make sure that they, too, receive
+or can get the source code.  And you must show them these terms so they
+know their rights.
+
+  Developers that use the GNU GPL protect your rights with two steps:
+(1) assert copyright on the software, and (2) offer you this License
+giving you legal permission to copy, distribute and/or modify it.
+
+  For the developers' and authors' protection, the GPL clearly explains
+that there is no warranty for this free software.  For both users' and
+authors' sake, the GPL requires that modified versions be marked as
+changed, so that their problems will not be attributed erroneously to
+authors of previous versions.
+
+  Some devices are designed to deny users access to install or run
+modified versions of the software inside them, although the manufacturer
+can do so.  This is fundamentally incompatible with the aim of
+protecting users' freedom to change the software.  The systematic
+pattern of such abuse occurs in the area of products for individuals to
+use, which is precisely where it is most unacceptable.  Therefore, we
+have designed this version of the GPL to prohibit the practice for those
+products.  If such problems arise substantially in other domains, we
+stand ready to extend this provision to those domains in future versions
+of the GPL, as needed to protect the freedom of users.
+
+  Finally, every program is threatened constantly by software patents.
+States should not allow patents to restrict development and use of
+software on general-purpose computers, but in those that do, we wish to
+avoid the special danger that patents applied to a free program could
+make it effectively proprietary.  To prevent this, the GPL assures that
+patents cannot be used to render the program non-free.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                       TERMS AND CONDITIONS
+
+  0. Definitions.
+
+  "This License" refers to version 3 of the GNU General Public License.
+
+  "Copyright" also means copyright-like laws that apply to other kinds of
+works, such as semiconductor masks.
+
+  "The Program" refers to any copyrightable work licensed under this
+License.  Each licensee is addressed as "you".  "Licensees" and
+"recipients" may be individuals or organizations.
+
+  To "modify" a work means to copy from or adapt all or part of the work
+in a fashion requiring copyright permission, other than the making of an
+exact copy.  The resulting work is called a "modified version" of the
+earlier work or a work "based on" the earlier work.
+
+  A "covered work" means either the unmodified Program or a work based
+on the Program.
+
+  To "propagate" a work means to do anything with it that, without
+permission, would make you directly or secondarily liable for
+infringement under applicable copyright law, except executing it on a
+computer or modifying a private copy.  Propagation includes copying,
+distribution (with or without modification), making available to the
+public, and in some countries other activities as well.
+
+  To "convey" a work means any kind of propagation that enables other
+parties to make or receive copies.  Mere interaction with a user through
+a computer network, with no transfer of a copy, is not conveying.
+
+  An interactive user interface displays "Appropriate Legal Notices"
+to the extent that it includes a convenient and prominently visible
+feature that (1) displays an appropriate copyright notice, and (2)
+tells the user that there is no warranty for the work (except to the
+extent that warranties are provided), that licensees may convey the
+work under this License, and how to view a copy of this License.  If
+the interface presents a list of user commands or options, such as a
+menu, a prominent item in the list meets this criterion.
+
+  1. Source Code.
+
+  The "source code" for a work means the preferred form of the work
+for making modifications to it.  "Object code" means any non-source
+form of a work.
+
+  A "Standard Interface" means an interface that either is an official
+standard defined by a recognized standards body, or, in the case of
+interfaces specified for a particular programming language, one that
+is widely used among developers working in that language.
+
+  The "System Libraries" of an executable work include anything, other
+than the work as a whole, that (a) is included in the normal form of
+packaging a Major Component, but which is not part of that Major
+Component, and (b) serves only to enable use of the work with that
+Major Component, or to implement a Standard Interface for which an
+implementation is available to the public in source code form.  A
+"Major Component", in this context, means a major essential component
+(kernel, window system, and so on) of the specific operating system
+(if any) on which the executable work runs, or a compiler used to
+produce the work, or an object code interpreter used to run it.
+
+  The "Corresponding Source" for a work in object code form means all
+the source code needed to generate, install, and (for an executable
+work) run the object code and to modify the work, including scripts to
+control those activities.  However, it does not include the work's
+System Libraries, or general-purpose tools or generally available free
+programs which are used unmodified in performing those activities but
+which are not part of the work.  For example, Corresponding Source
+includes interface definition files associated with source files for
+the work, and the source code for shared libraries and dynamically
+linked subprograms that the work is specifically designed to require,
+such as by intimate data communication or control flow between those
+subprograms and other parts of the work.
+
+  The Corresponding Source need not include anything that users
+can regenerate automatically from other parts of the Corresponding
+Source.
+
+  The Corresponding Source for a work in source code form is that
+same work.
+
+  2. Basic Permissions.
+
+  All rights granted under this License are granted for the term of
+copyright on the Program, and are irrevocable provided the stated
+conditions are met.  This License explicitly affirms your unlimited
+permission to run the unmodified Program.  The output from running a
+covered work is covered by this License only if the output, given its
+content, constitutes a covered work.  This License acknowledges your
+rights of fair use or other equivalent, as provided by copyright law.
+
+  You may make, run and propagate covered works that you do not
+convey, without conditions so long as your license otherwise remains
+in force.  You may convey covered works to others for the sole purpose
+of having them make modifications exclusively for you, or provide you
+with facilities for running those works, provided that you comply with
+the terms of this License in conveying all material for which you do
+not control copyright.  Those thus making or running the covered works
+for you must do so exclusively on your behalf, under your direction
+and control, on terms that prohibit them from making any copies of
+your copyrighted material outside their relationship with you.
+
+  Conveying under any other circumstances is permitted solely under
+the conditions stated below.  Sublicensing is not allowed; section 10
+makes it unnecessary.
+
+  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
+
+  No covered work shall be deemed part of an effective technological
+measure under any applicable law fulfilling obligations under article
+11 of the WIPO copyright treaty adopted on 20 December 1996, or
+similar laws prohibiting or restricting circumvention of such
+measures.
+
+  When you convey a covered work, you waive any legal power to forbid
+circumvention of technological measures to the extent such circumvention
+is effected by exercising rights under this License with respect to
+the covered work, and you disclaim any intention to limit operation or
+modification of the work as a means of enforcing, against the work's
+users, your or third parties' legal rights to forbid circumvention of
+technological measures.
+
+  4. Conveying Verbatim Copies.
+
+  You may convey verbatim copies of the Program's source code as you
+receive it, in any medium, provided that you conspicuously and
+appropriately publish on each copy an appropriate copyright notice;
+keep intact all notices stating that this License and any
+non-permissive terms added in accord with section 7 apply to the code;
+keep intact all notices of the absence of any warranty; and give all
+recipients a copy of this License along with the Program.
+
+  You may charge any price or no price for each copy that you convey,
+and you may offer support or warranty protection for a fee.
+
+  5. Conveying Modified Source Versions.
+
+  You may convey a work based on the Program, or the modifications to
+produce it from the Program, in the form of source code under the
+terms of section 4, provided that you also meet all of these conditions:
+
+    a) The work must carry prominent notices stating that you modified
+    it, and giving a relevant date.
+
+    b) The work must carry prominent notices stating that it is
+    released under this License and any conditions added under section
+    7.  This requirement modifies the requirement in section 4 to
+    "keep intact all notices".
+
+    c) You must license the entire work, as a whole, under this
+    License to anyone who comes into possession of a copy.  This
+    License will therefore apply, along with any applicable section 7
+    additional terms, to the whole of the work, and all its parts,
+    regardless of how they are packaged.  This License gives no
+    permission to license the work in any other way, but it does not
+    invalidate such permission if you have separately received it.
+
+    d) If the work has interactive user interfaces, each must display
+    Appropriate Legal Notices; however, if the Program has interactive
+    interfaces that do not display Appropriate Legal Notices, your
+    work need not make them do so.
+
+  A compilation of a covered work with other separate and independent
+works, which are not by their nature extensions of the covered work,
+and which are not combined with it such as to form a larger program,
+in or on a volume of a storage or distribution medium, is called an
+"aggregate" if the compilation and its resulting copyright are not
+used to limit the access or legal rights of the compilation's users
+beyond what the individual works permit.  Inclusion of a covered work
+in an aggregate does not cause this License to apply to the other
+parts of the aggregate.
+
+  6. Conveying Non-Source Forms.
+
+  You may convey a covered work in object code form under the terms
+of sections 4 and 5, provided that you also convey the
+machine-readable Corresponding Source under the terms of this License,
+in one of these ways:
+
+    a) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by the
+    Corresponding Source fixed on a durable physical medium
+    customarily used for software interchange.
+
+    b) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by a
+    written offer, valid for at least three years and valid for as
+    long as you offer spare parts or customer support for that product
+    model, to give anyone who possesses the object code either (1) a
+    copy of the Corresponding Source for all the software in the
+    product that is covered by this License, on a durable physical
+    medium customarily used for software interchange, for a price no
+    more than your reasonable cost of physically performing this
+    conveying of source, or (2) access to copy the
+    Corresponding Source from a network server at no charge.
+
+    c) Convey individual copies of the object code with a copy of the
+    written offer to provide the Corresponding Source.  This
+    alternative is allowed only occasionally and noncommercially, and
+    only if you received the object code with such an offer, in accord
+    with subsection 6b.
+
+    d) Convey the object code by offering access from a designated
+    place (gratis or for a charge), and offer equivalent access to the
+    Corresponding Source in the same way through the same place at no
+    further charge.  You need not require recipients to copy the
+    Corresponding Source along with the object code.  If the place to
+    copy the object code is a network server, the Corresponding Source
+    may be on a different server (operated by you or a third party)
+    that supports equivalent copying facilities, provided you maintain
+    clear directions next to the object code saying where to find the
+    Corresponding Source.  Regardless of what server hosts the
+    Corresponding Source, you remain obligated to ensure that it is
+    available for as long as needed to satisfy these requirements.
+
+    e) Convey the object code using peer-to-peer transmission, provided
+    you inform other peers where the object code and Corresponding
+    Source of the work are being offered to the general public at no
+    charge under subsection 6d.
+
+  A separable portion of the object code, whose source code is excluded
+from the Corresponding Source as a System Library, need not be
+included in conveying the object code work.
+
+  A "User Product" is either (1) a "consumer product", which means any
+tangible personal property which is normally used for personal, family,
+or household purposes, or (2) anything designed or sold for incorporation
+into a dwelling.  In determining whether a product is a consumer product,
+doubtful cases shall be resolved in favor of coverage.  For a particular
+product received by a particular user, "normally used" refers to a
+typical or common use of that class of product, regardless of the status
+of the particular user or of the way in which the particular user
+actually uses, or expects or is expected to use, the product.  A product
+is a consumer product regardless of whether the product has substantial
+commercial, industrial or non-consumer uses, unless such uses represent
+the only significant mode of use of the product.
+
+  "Installation Information" for a User Product means any methods,
+procedures, authorization keys, or other information required to install
+and execute modified versions of a covered work in that User Product from
+a modified version of its Corresponding Source.  The information must
+suffice to ensure that the continued functioning of the modified object
+code is in no case prevented or interfered with solely because
+modification has been made.
+
+  If you convey an object code work under this section in, or with, or
+specifically for use in, a User Product, and the conveying occurs as
+part of a transaction in which the right of possession and use of the
+User Product is transferred to the recipient in perpetuity or for a
+fixed term (regardless of how the transaction is characterized), the
+Corresponding Source conveyed under this section must be accompanied
+by the Installation Information.  But this requirement does not apply
+if neither you nor any third party retains the ability to install
+modified object code on the User Product (for example, the work has
+been installed in ROM).
+
+  The requirement to provide Installation Information does not include a
+requirement to continue to provide support service, warranty, or updates
+for a work that has been modified or installed by the recipient, or for
+the User Product in which it has been modified or installed.  Access to a
+network may be denied when the modification itself materially and
+adversely affects the operation of the network or violates the rules and
+protocols for communication across the network.
+
+  Corresponding Source conveyed, and Installation Information provided,
+in accord with this section must be in a format that is publicly
+documented (and with an implementation available to the public in
+source code form), and must require no special password or key for
+unpacking, reading or copying.
+
+  7. Additional Terms.
+
+  "Additional permissions" are terms that supplement the terms of this
+License by making exceptions from one or more of its conditions.
+Additional permissions that are applicable to the entire Program shall
+be treated as though they were included in this License, to the extent
+that they are valid under applicable law.  If additional permissions
+apply only to part of the Program, that part may be used separately
+under those permissions, but the entire Program remains governed by
+this License without regard to the additional permissions.
+
+  When you convey a copy of a covered work, you may at your option
+remove any additional permissions from that copy, or from any part of
+it.  (Additional permissions may be written to require their own
+removal in certain cases when you modify the work.)  You may place
+additional permissions on material, added by you to a covered work,
+for which you have or can give appropriate copyright permission.
+
+  Notwithstanding any other provision of this License, for material you
+add to a covered work, you may (if authorized by the copyright holders of
+that material) supplement the terms of this License with terms:
+
+    a) Disclaiming warranty or limiting liability differently from the
+    terms of sections 15 and 16 of this License; or
+
+    b) Requiring preservation of specified reasonable legal notices or
+    author attributions in that material or in the Appropriate Legal
+    Notices displayed by works containing it; or
+
+    c) Prohibiting misrepresentation of the origin of that material, or
+    requiring that modified versions of such material be marked in
+    reasonable ways as different from the original version; or
+
+    d) Limiting the use for publicity purposes of names of licensors or
+    authors of the material; or
+
+    e) Declining to grant rights under trademark law for use of some
+    trade names, trademarks, or service marks; or
+
+    f) Requiring indemnification of licensors and authors of that
+    material by anyone who conveys the material (or modified versions of
+    it) with contractual assumptions of liability to the recipient, for
+    any liability that these contractual assumptions directly impose on
+    those licensors and authors.
+
+  All other non-permissive additional terms are considered "further
+restrictions" within the meaning of section 10.  If the Program as you
+received it, or any part of it, contains a notice stating that it is
+governed by this License along with a term that is a further
+restriction, you may remove that term.  If a license document contains
+a further restriction but permits relicensing or conveying under this
+License, you may add to a covered work material governed by the terms
+of that license document, provided that the further restriction does
+not survive such relicensing or conveying.
+
+  If you add terms to a covered work in accord with this section, you
+must place, in the relevant source files, a statement of the
+additional terms that apply to those files, or a notice indicating
+where to find the applicable terms.
+
+  Additional terms, permissive or non-permissive, may be stated in the
+form of a separately written license, or stated as exceptions;
+the above requirements apply either way.
+
+  8. Termination.
+
+  You may not propagate or modify a covered work except as expressly
+provided under this License.  Any attempt otherwise to propagate or
+modify it is void, and will automatically terminate your rights under
+this License (including any patent licenses granted under the third
+paragraph of section 11).
+
+  However, if you cease all violation of this License, then your
+license from a particular copyright holder is reinstated (a)
+provisionally, unless and until the copyright holder explicitly and
+finally terminates your license, and (b) permanently, if the copyright
+holder fails to notify you of the violation by some reasonable means
+prior to 60 days after the cessation.
+
+  Moreover, your license from a particular copyright holder is
+reinstated permanently if the copyright holder notifies you of the
+violation by some reasonable means, this is the first time you have
+received notice of violation of this License (for any work) from that
+copyright holder, and you cure the violation prior to 30 days after
+your receipt of the notice.
+
+  Termination of your rights under this section does not terminate the
+licenses of parties who have received copies or rights from you under
+this License.  If your rights have been terminated and not permanently
+reinstated, you do not qualify to receive new licenses for the same
+material under section 10.
+
+  9. Acceptance Not Required for Having Copies.
+
+  You are not required to accept this License in order to receive or
+run a copy of the Program.  Ancillary propagation of a covered work
+occurring solely as a consequence of using peer-to-peer transmission
+to receive a copy likewise does not require acceptance.  However,
+nothing other than this License grants you permission to propagate or
+modify any covered work.  These actions infringe copyright if you do
+not accept this License.  Therefore, by modifying or propagating a
+covered work, you indicate your acceptance of this License to do so.
+
+  10. Automatic Licensing of Downstream Recipients.
+
+  Each time you convey a covered work, the recipient automatically
+receives a license from the original licensors, to run, modify and
+propagate that work, subject to this License.  You are not responsible
+for enforcing compliance by third parties with this License.
+
+  An "entity transaction" is a transaction transferring control of an
+organization, or substantially all assets of one, or subdividing an
+organization, or merging organizations.  If propagation of a covered
+work results from an entity transaction, each party to that
+transaction who receives a copy of the work also receives whatever
+licenses to the work the party's predecessor in interest had or could
+give under the previous paragraph, plus a right to possession of the
+Corresponding Source of the work from the predecessor in interest, if
+the predecessor has it or can get it with reasonable efforts.
+
+  You may not impose any further restrictions on the exercise of the
+rights granted or affirmed under this License.  For example, you may
+not impose a license fee, royalty, or other charge for exercise of
+rights granted under this License, and you may not initiate litigation
+(including a cross-claim or counterclaim in a lawsuit) alleging that
+any patent claim is infringed by making, using, selling, offering for
+sale, or importing the Program or any portion of it.
+
+  11. Patents.
+
+  A "contributor" is a copyright holder who authorizes use under this
+License of the Program or a work on which the Program is based.  The
+work thus licensed is called the contributor's "contributor version".
+
+  A contributor's "essential patent claims" are all patent claims
+owned or controlled by the contributor, whether already acquired or
+hereafter acquired, that would be infringed by some manner, permitted
+by this License, of making, using, or selling its contributor version,
+but do not include claims that would be infringed only as a
+consequence of further modification of the contributor version.  For
+purposes of this definition, "control" includes the right to grant
+patent sublicenses in a manner consistent with the requirements of
+this License.
+
+  Each contributor grants you a non-exclusive, worldwide, royalty-free
+patent license under the contributor's essential patent claims, to
+make, use, sell, offer for sale, import and otherwise run, modify and
+propagate the contents of its contributor version.
+
+  In the following three paragraphs, a "patent license" is any express
+agreement or commitment, however denominated, not to enforce a patent
+(such as an express permission to practice a patent or covenant not to
+sue for patent infringement).  To "grant" such a patent license to a
+party means to make such an agreement or commitment not to enforce a
+patent against the party.
+
+  If you convey a covered work, knowingly relying on a patent license,
+and the Corresponding Source of the work is not available for anyone
+to copy, free of charge and under the terms of this License, through a
+publicly available network server or other readily accessible means,
+then you must either (1) cause the Corresponding Source to be so
+available, or (2) arrange to deprive yourself of the benefit of the
+patent license for this particular work, or (3) arrange, in a manner
+consistent with the requirements of this License, to extend the patent
+license to downstream recipients.  "Knowingly relying" means you have
+actual knowledge that, but for the patent license, your conveying the
+covered work in a country, or your recipient's use of the covered work
+in a country, would infringe one or more identifiable patents in that
+country that you have reason to believe are valid.
+
+  If, pursuant to or in connection with a single transaction or
+arrangement, you convey, or propagate by procuring conveyance of, a
+covered work, and grant a patent license to some of the parties
+receiving the covered work authorizing them to use, propagate, modify
+or convey a specific copy of the covered work, then the patent license
+you grant is automatically extended to all recipients of the covered
+work and works based on it.
+
+  A patent license is "discriminatory" if it does not include within
+the scope of its coverage, prohibits the exercise of, or is
+conditioned on the non-exercise of one or more of the rights that are
+specifically granted under this License.  You may not convey a covered
+work if you are a party to an arrangement with a third party that is
+in the business of distributing software, under which you make payment
+to the third party based on the extent of your activity of conveying
+the work, and under which the third party grants, to any of the
+parties who would receive the covered work from you, a discriminatory
+patent license (a) in connection with copies of the covered work
+conveyed by you (or copies made from those copies), or (b) primarily
+for and in connection with specific products or compilations that
+contain the covered work, unless you entered into that arrangement,
+or that patent license was granted, prior to 28 March 2007.
+
+  Nothing in this License shall be construed as excluding or limiting
+any implied license or other defenses to infringement that may
+otherwise be available to you under applicable patent law.
+
+  12. No Surrender of Others' Freedom.
+
+  If conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot convey a
+covered work so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you may
+not convey it at all.  For example, if you agree to terms that obligate you
+to collect a royalty for further conveying from those to whom you convey
+the Program, the only way you could satisfy both those terms and this
+License would be to refrain entirely from conveying the Program.
+
+  13. Use with the GNU Affero General Public License.
+
+  Notwithstanding any other provision of this License, you have
+permission to link or combine any covered work with a work licensed
+under version 3 of the GNU Affero General Public License into a single
+combined work, and to convey the resulting work.  The terms of this
+License will continue to apply to the part which is the covered work,
+but the special requirements of the GNU Affero General Public License,
+section 13, concerning interaction through a network will apply to the
+combination as such.
+
+  14. Revised Versions of this License.
+
+  The Free Software Foundation may publish revised and/or new versions of
+the GNU General Public License from time to time.  Such new versions will
+be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+  Each version is given a distinguishing version number.  If the
+Program specifies that a certain numbered version of the GNU General
+Public License "or any later version" applies to it, you have the
+option of following the terms and conditions either of that numbered
+version or of any later version published by the Free Software
+Foundation.  If the Program does not specify a version number of the
+GNU General Public License, you may choose any version ever published
+by the Free Software Foundation.
+
+  If the Program specifies that a proxy can decide which future
+versions of the GNU General Public License can be used, that proxy's
+public statement of acceptance of a version permanently authorizes you
+to choose that version for the Program.
+
+  Later license versions may give you additional or different
+permissions.  However, no additional obligations are imposed on any
+author or copyright holder as a result of your choosing to follow a
+later version.
+
+  15. Disclaimer of Warranty.
+
+  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
+APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
+HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
+OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
+IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
+ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. Limitation of Liability.
+
+  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
+THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
+GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
+USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
+DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
+PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
+EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGES.
+
+  17. Interpretation of Sections 15 and 16.
+
+  If the disclaimer of warranty and limitation of liability provided
+above cannot be given local legal effect according to their terms,
+reviewing courts shall apply local law that most closely approximates
+an absolute waiver of all civil liability in connection with the
+Program, unless a warranty or assumption of liability accompanies a
+copy of the Program in return for a fee.
+
+                     END OF TERMS AND CONDITIONS
```

### Comparing `spinedb_api-0.30.3/COPYING.LESSER` & `spinedb_api-0.30.4/COPYING.LESSER`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,165 +1,165 @@
-                   GNU LESSER GENERAL PUBLIC LICENSE
-                       Version 3, 29 June 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-
-  This version of the GNU Lesser General Public License incorporates
-the terms and conditions of version 3 of the GNU General Public
-License, supplemented by the additional permissions listed below.
-
-  0. Additional Definitions.
-
-  As used herein, "this License" refers to version 3 of the GNU Lesser
-General Public License, and the "GNU GPL" refers to version 3 of the GNU
-General Public License.
-
-  "The Library" refers to a covered work governed by this License,
-other than an Application or a Combined Work as defined below.
-
-  An "Application" is any work that makes use of an interface provided
-by the Library, but which is not otherwise based on the Library.
-Defining a subclass of a class defined by the Library is deemed a mode
-of using an interface provided by the Library.
-
-  A "Combined Work" is a work produced by combining or linking an
-Application with the Library.  The particular version of the Library
-with which the Combined Work was made is also called the "Linked
-Version".
-
-  The "Minimal Corresponding Source" for a Combined Work means the
-Corresponding Source for the Combined Work, excluding any source code
-for portions of the Combined Work that, considered in isolation, are
-based on the Application, and not on the Linked Version.
-
-  The "Corresponding Application Code" for a Combined Work means the
-object code and/or source code for the Application, including any data
-and utility programs needed for reproducing the Combined Work from the
-Application, but excluding the System Libraries of the Combined Work.
-
-  1. Exception to Section 3 of the GNU GPL.
-
-  You may convey a covered work under sections 3 and 4 of this License
-without being bound by section 3 of the GNU GPL.
-
-  2. Conveying Modified Versions.
-
-  If you modify a copy of the Library, and, in your modifications, a
-facility refers to a function or data to be supplied by an Application
-that uses the facility (other than as an argument passed when the
-facility is invoked), then you may convey a copy of the modified
-version:
-
-   a) under this License, provided that you make a good faith effort to
-   ensure that, in the event an Application does not supply the
-   function or data, the facility still operates, and performs
-   whatever part of its purpose remains meaningful, or
-
-   b) under the GNU GPL, with none of the additional permissions of
-   this License applicable to that copy.
-
-  3. Object Code Incorporating Material from Library Header Files.
-
-  The object code form of an Application may incorporate material from
-a header file that is part of the Library.  You may convey such object
-code under terms of your choice, provided that, if the incorporated
-material is not limited to numerical parameters, data structure
-layouts and accessors, or small macros, inline functions and templates
-(ten or fewer lines in length), you do both of the following:
-
-   a) Give prominent notice with each copy of the object code that the
-   Library is used in it and that the Library and its use are
-   covered by this License.
-
-   b) Accompany the object code with a copy of the GNU GPL and this license
-   document.
-
-  4. Combined Works.
-
-  You may convey a Combined Work under terms of your choice that,
-taken together, effectively do not restrict modification of the
-portions of the Library contained in the Combined Work and reverse
-engineering for debugging such modifications, if you also do each of
-the following:
-
-   a) Give prominent notice with each copy of the Combined Work that
-   the Library is used in it and that the Library and its use are
-   covered by this License.
-
-   b) Accompany the Combined Work with a copy of the GNU GPL and this license
-   document.
-
-   c) For a Combined Work that displays copyright notices during
-   execution, include the copyright notice for the Library among
-   these notices, as well as a reference directing the user to the
-   copies of the GNU GPL and this license document.
-
-   d) Do one of the following:
-
-       0) Convey the Minimal Corresponding Source under the terms of this
-       License, and the Corresponding Application Code in a form
-       suitable for, and under terms that permit, the user to
-       recombine or relink the Application with a modified version of
-       the Linked Version to produce a modified Combined Work, in the
-       manner specified by section 6 of the GNU GPL for conveying
-       Corresponding Source.
-
-       1) Use a suitable shared library mechanism for linking with the
-       Library.  A suitable mechanism is one that (a) uses at run time
-       a copy of the Library already present on the user's computer
-       system, and (b) will operate properly with a modified version
-       of the Library that is interface-compatible with the Linked
-       Version.
-
-   e) Provide Installation Information, but only if you would otherwise
-   be required to provide such information under section 6 of the
-   GNU GPL, and only to the extent that such information is
-   necessary to install and execute a modified version of the
-   Combined Work produced by recombining or relinking the
-   Application with a modified version of the Linked Version. (If
-   you use option 4d0, the Installation Information must accompany
-   the Minimal Corresponding Source and Corresponding Application
-   Code. If you use option 4d1, you must provide the Installation
-   Information in the manner specified by section 6 of the GNU GPL
-   for conveying Corresponding Source.)
-
-  5. Combined Libraries.
-
-  You may place library facilities that are a work based on the
-Library side by side in a single library together with other library
-facilities that are not Applications and are not covered by this
-License, and convey such a combined library under terms of your
-choice, if you do both of the following:
-
-   a) Accompany the combined library with a copy of the same work based
-   on the Library, uncombined with any other library facilities,
-   conveyed under the terms of this License.
-
-   b) Give prominent notice with the combined library that part of it
-   is a work based on the Library, and explaining where to find the
-   accompanying uncombined form of the same work.
-
-  6. Revised Versions of the GNU Lesser General Public License.
-
-  The Free Software Foundation may publish revised and/or new versions
-of the GNU Lesser General Public License from time to time. Such new
-versions will be similar in spirit to the present version, but may
-differ in detail to address new problems or concerns.
-
-  Each version is given a distinguishing version number. If the
-Library as you received it specifies that a certain numbered version
-of the GNU Lesser General Public License "or any later version"
-applies to it, you have the option of following the terms and
-conditions either of that published version or of any later version
-published by the Free Software Foundation. If the Library as you
-received it does not specify a version number of the GNU Lesser
-General Public License, you may choose any version of the GNU Lesser
-General Public License ever published by the Free Software Foundation.
-
-  If the Library as you received it specifies that a proxy can decide
-whether future versions of the GNU Lesser General Public License shall
-apply, that proxy's public statement of acceptance of any version is
-permanent authorization for you to choose that version for the
-Library.
+                   GNU LESSER GENERAL PUBLIC LICENSE
+                       Version 3, 29 June 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+
+  This version of the GNU Lesser General Public License incorporates
+the terms and conditions of version 3 of the GNU General Public
+License, supplemented by the additional permissions listed below.
+
+  0. Additional Definitions.
+
+  As used herein, "this License" refers to version 3 of the GNU Lesser
+General Public License, and the "GNU GPL" refers to version 3 of the GNU
+General Public License.
+
+  "The Library" refers to a covered work governed by this License,
+other than an Application or a Combined Work as defined below.
+
+  An "Application" is any work that makes use of an interface provided
+by the Library, but which is not otherwise based on the Library.
+Defining a subclass of a class defined by the Library is deemed a mode
+of using an interface provided by the Library.
+
+  A "Combined Work" is a work produced by combining or linking an
+Application with the Library.  The particular version of the Library
+with which the Combined Work was made is also called the "Linked
+Version".
+
+  The "Minimal Corresponding Source" for a Combined Work means the
+Corresponding Source for the Combined Work, excluding any source code
+for portions of the Combined Work that, considered in isolation, are
+based on the Application, and not on the Linked Version.
+
+  The "Corresponding Application Code" for a Combined Work means the
+object code and/or source code for the Application, including any data
+and utility programs needed for reproducing the Combined Work from the
+Application, but excluding the System Libraries of the Combined Work.
+
+  1. Exception to Section 3 of the GNU GPL.
+
+  You may convey a covered work under sections 3 and 4 of this License
+without being bound by section 3 of the GNU GPL.
+
+  2. Conveying Modified Versions.
+
+  If you modify a copy of the Library, and, in your modifications, a
+facility refers to a function or data to be supplied by an Application
+that uses the facility (other than as an argument passed when the
+facility is invoked), then you may convey a copy of the modified
+version:
+
+   a) under this License, provided that you make a good faith effort to
+   ensure that, in the event an Application does not supply the
+   function or data, the facility still operates, and performs
+   whatever part of its purpose remains meaningful, or
+
+   b) under the GNU GPL, with none of the additional permissions of
+   this License applicable to that copy.
+
+  3. Object Code Incorporating Material from Library Header Files.
+
+  The object code form of an Application may incorporate material from
+a header file that is part of the Library.  You may convey such object
+code under terms of your choice, provided that, if the incorporated
+material is not limited to numerical parameters, data structure
+layouts and accessors, or small macros, inline functions and templates
+(ten or fewer lines in length), you do both of the following:
+
+   a) Give prominent notice with each copy of the object code that the
+   Library is used in it and that the Library and its use are
+   covered by this License.
+
+   b) Accompany the object code with a copy of the GNU GPL and this license
+   document.
+
+  4. Combined Works.
+
+  You may convey a Combined Work under terms of your choice that,
+taken together, effectively do not restrict modification of the
+portions of the Library contained in the Combined Work and reverse
+engineering for debugging such modifications, if you also do each of
+the following:
+
+   a) Give prominent notice with each copy of the Combined Work that
+   the Library is used in it and that the Library and its use are
+   covered by this License.
+
+   b) Accompany the Combined Work with a copy of the GNU GPL and this license
+   document.
+
+   c) For a Combined Work that displays copyright notices during
+   execution, include the copyright notice for the Library among
+   these notices, as well as a reference directing the user to the
+   copies of the GNU GPL and this license document.
+
+   d) Do one of the following:
+
+       0) Convey the Minimal Corresponding Source under the terms of this
+       License, and the Corresponding Application Code in a form
+       suitable for, and under terms that permit, the user to
+       recombine or relink the Application with a modified version of
+       the Linked Version to produce a modified Combined Work, in the
+       manner specified by section 6 of the GNU GPL for conveying
+       Corresponding Source.
+
+       1) Use a suitable shared library mechanism for linking with the
+       Library.  A suitable mechanism is one that (a) uses at run time
+       a copy of the Library already present on the user's computer
+       system, and (b) will operate properly with a modified version
+       of the Library that is interface-compatible with the Linked
+       Version.
+
+   e) Provide Installation Information, but only if you would otherwise
+   be required to provide such information under section 6 of the
+   GNU GPL, and only to the extent that such information is
+   necessary to install and execute a modified version of the
+   Combined Work produced by recombining or relinking the
+   Application with a modified version of the Linked Version. (If
+   you use option 4d0, the Installation Information must accompany
+   the Minimal Corresponding Source and Corresponding Application
+   Code. If you use option 4d1, you must provide the Installation
+   Information in the manner specified by section 6 of the GNU GPL
+   for conveying Corresponding Source.)
+
+  5. Combined Libraries.
+
+  You may place library facilities that are a work based on the
+Library side by side in a single library together with other library
+facilities that are not Applications and are not covered by this
+License, and convey such a combined library under terms of your
+choice, if you do both of the following:
+
+   a) Accompany the combined library with a copy of the same work based
+   on the Library, uncombined with any other library facilities,
+   conveyed under the terms of this License.
+
+   b) Give prominent notice with the combined library that part of it
+   is a work based on the Library, and explaining where to find the
+   accompanying uncombined form of the same work.
+
+  6. Revised Versions of the GNU Lesser General Public License.
+
+  The Free Software Foundation may publish revised and/or new versions
+of the GNU Lesser General Public License from time to time. Such new
+versions will be similar in spirit to the present version, but may
+differ in detail to address new problems or concerns.
+
+  Each version is given a distinguishing version number. If the
+Library as you received it specifies that a certain numbered version
+of the GNU Lesser General Public License "or any later version"
+applies to it, you have the option of following the terms and
+conditions either of that published version or of any later version
+published by the Free Software Foundation. If the Library as you
+received it does not specify a version number of the GNU Lesser
+General Public License, you may choose any version of the GNU Lesser
+General Public License ever published by the Free Software Foundation.
+
+  If the Library as you received it specifies that a proxy can decide
+whether future versions of the GNU Lesser General Public License shall
+apply, that proxy's public statement of acceptance of any version is
+permanent authorization for you to choose that version for the
+Library.
```

### Comparing `spinedb_api-0.30.3/PKG-INFO` & `spinedb_api-0.30.4/PKG-INFO`

 * *Files 21% similar despite different names*

```diff
@@ -1,104 +1,104 @@
-Metadata-Version: 2.1
-Name: spinedb_api
-Version: 0.30.3
-Summary: An API to talk to Spine databases.
-Author-email: Spine Project consortium <spine_info@vtt.fi>
-License: LGPL-3.0-or-later
-Project-URL: Repository, https://github.com/spine-tools/Spine-Database-API
-Keywords: energy system modelling,workflow,optimisation,database
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
-Classifier: Operating System :: OS Independent
-Requires-Python: <3.12,>=3.8.1
-Description-Content-Type: text/markdown
-License-File: COPYING
-License-File: COPYING.LESSER
-Requires-Dist: sqlalchemy<1.4,>=1.3
-Requires-Dist: alembic>=1.7
-Requires-Dist: faker>=8.1.2
-Requires-Dist: datapackage>=1.15.2
-Requires-Dist: python-dateutil>=2.8.1
-Requires-Dist: numpy>=1.20.2
-Requires-Dist: scipy>=1.7.1
-Requires-Dist: openpyxl!=3.1.1,>=3.0.7
-Requires-Dist: gdx2py>=2.1.1
-Requires-Dist: ijson>=3.1.4
-Requires-Dist: chardet>=4.0.0
-Requires-Dist: pymysql>=1.0.2
-Requires-Dist: psycopg2
-Requires-Dist: cx_Oracle
-Provides-Extra: dev
-Requires-Dist: coverage[toml]; extra == "dev"
-
-# Spine Database API
-
-[![Documentation Status](https://readthedocs.org/projects/spine-database-api/badge/?version=latest)](https://spine-database-api.readthedocs.io/en/latest/?badge=latest)
-[![Unit tests](https://github.com/spine-tools/Spine-Database-API/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/Spine-Database-API/actions?query=workflow%3A"Unit+tests")
-[![codecov](https://codecov.io/gh/spine-tools/Spine-Database-API/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/Spine-Database-API)
-[![PyPI version](https://badge.fury.io/py/spinedb-api.svg)](https://badge.fury.io/py/spinedb-api)
-
-A Python package to access and manipulate Spine databases in a customary, unified way.
-
-## License
-
-Spine Database API is released under the GNU Lesser General Public License (LGPL) license. All accompanying
-documentation and manual are released under the Creative Commons BY-SA 4.0 license.
-
-## Getting started
-
-### Installation
-
-To install the package run:
-
-    $ pip install spinedb_api
-
-To upgrade to the most recent version, run:
-
-    $ pip install --upgrade spinedb_api
-
-You can also specify a branch, or a tag, for instance:
-
-    $ pip install spinedb_api==0.12.1
-
-To install the latest development version use the Git repository url:
-
-    $ pip install --upgrade git+https://github.com/spine-tools/Spine-Database-API.git
-
-
-## Building the documentation
-
-Source files for the documentation can be found in `docs/source` directory. In order to 
-build the HTML docs, you need to install the additional documentation building requirements
-by running:
-
-    $ pip install -r dev-requirements.txt 
-
-This installs Sphinx (among other things), which is required in building the documentation.
-When Sphinx is installed, you can build the HTML pages from the source files by running:
-
-    > docs\make.bat html
-    
-or
-
-    $ pushd docs
-    $ make html
-    $ popd
-    
-depending on your operating system.        
- 
-After running the build, the index page can be found in `docs/build/html/index.html`.
-
-&nbsp;
-<hr>
-<center>
-<table width=500px frame="none">
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
-</table>
-</center>
+Metadata-Version: 2.1
+Name: spinedb_api
+Version: 0.30.4
+Summary: An API to talk to Spine databases.
+Author-email: Spine Project consortium <spine_info@vtt.fi>
+License: LGPL-3.0-or-later
+Project-URL: Repository, https://github.com/spine-tools/Spine-Database-API
+Keywords: energy system modelling,workflow,optimisation,database
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
+Classifier: Operating System :: OS Independent
+Requires-Python: <3.12,>=3.8.1
+Description-Content-Type: text/markdown
+License-File: COPYING
+License-File: COPYING.LESSER
+Requires-Dist: sqlalchemy<1.4,>=1.3
+Requires-Dist: alembic>=1.7
+Requires-Dist: faker>=8.1.2
+Requires-Dist: datapackage>=1.15.2
+Requires-Dist: python-dateutil>=2.8.1
+Requires-Dist: numpy>=1.20.2
+Requires-Dist: scipy>=1.7.1
+Requires-Dist: openpyxl!=3.1.1,>=3.0.7
+Requires-Dist: gdx2py>=2.1.1
+Requires-Dist: ijson>=3.1.4
+Requires-Dist: chardet>=4.0.0
+Requires-Dist: pymysql>=1.0.2
+Requires-Dist: psycopg2
+Requires-Dist: cx_Oracle
+Provides-Extra: dev
+Requires-Dist: coverage[toml]; extra == "dev"
+
+# Spine Database API
+
+[![Documentation Status](https://readthedocs.org/projects/spine-database-api/badge/?version=latest)](https://spine-database-api.readthedocs.io/en/latest/?badge=latest)
+[![Unit tests](https://github.com/spine-tools/Spine-Database-API/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/Spine-Database-API/actions?query=workflow%3A"Unit+tests")
+[![codecov](https://codecov.io/gh/spine-tools/Spine-Database-API/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/Spine-Database-API)
+[![PyPI version](https://badge.fury.io/py/spinedb-api.svg)](https://badge.fury.io/py/spinedb-api)
+
+A Python package to access and manipulate Spine databases in a customary, unified way.
+
+## License
+
+Spine Database API is released under the GNU Lesser General Public License (LGPL) license. All accompanying
+documentation and manual are released under the Creative Commons BY-SA 4.0 license.
+
+## Getting started
+
+### Installation
+
+To install the package run:
+
+    $ pip install spinedb_api
+
+To upgrade to the most recent version, run:
+
+    $ pip install --upgrade spinedb_api
+
+You can also specify a branch, or a tag, for instance:
+
+    $ pip install spinedb_api==0.12.1
+
+To install the latest development version use the Git repository url:
+
+    $ pip install --upgrade git+https://github.com/spine-tools/Spine-Database-API.git
+
+
+## Building the documentation
+
+Source files for the documentation can be found in `docs/source` directory. In order to 
+build the HTML docs, you need to install the additional documentation building requirements
+by running:
+
+    $ pip install -r dev-requirements.txt 
+
+This installs Sphinx (among other things), which is required in building the documentation.
+When Sphinx is installed, you can build the HTML pages from the source files by running:
+
+    > docs\make.bat html
+    
+or
+
+    $ pushd docs
+    $ make html
+    $ popd
+    
+depending on your operating system.        
+ 
+After running the build, the index page can be found in `docs/build/html/index.html`.
+
+&nbsp;
+<hr>
+<center>
+<table width=500px frame="none">
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
+</table>
+</center>
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: spinedb_api Version: 0.30.3 Summary: An API to talk
+Metadata-Version: 2.1 Name: spinedb_api Version: 0.30.4 Summary: An API to talk
 to Spine databases. Author-email: Spine Project consortium
 vtt.fi> License: LGPL-3.0-or-later Project-URL: Repository, https://github.com/
 spine-tools/Spine-Database-API Keywords: energy system
 modelling,workflow,optimisation,database Classifier: Programming Language ::
 Python :: 3 Classifier: License :: OSI Approved :: GNU Lesser General Public
 License v3 (LGPLv3) Classifier: Operating System :: OS Independent Requires-
 Python: <3.12,>=3.8.1 Description-Content-Type: text/markdown License-File:
```

### Comparing `spinedb_api-0.30.3/README.md` & `spinedb_api-0.30.4/README.md`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,72 +1,72 @@
-# Spine Database API
-
-[![Documentation Status](https://readthedocs.org/projects/spine-database-api/badge/?version=latest)](https://spine-database-api.readthedocs.io/en/latest/?badge=latest)
-[![Unit tests](https://github.com/spine-tools/Spine-Database-API/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/Spine-Database-API/actions?query=workflow%3A"Unit+tests")
-[![codecov](https://codecov.io/gh/spine-tools/Spine-Database-API/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/Spine-Database-API)
-[![PyPI version](https://badge.fury.io/py/spinedb-api.svg)](https://badge.fury.io/py/spinedb-api)
-
-A Python package to access and manipulate Spine databases in a customary, unified way.
-
-## License
-
-Spine Database API is released under the GNU Lesser General Public License (LGPL) license. All accompanying
-documentation and manual are released under the Creative Commons BY-SA 4.0 license.
-
-## Getting started
-
-### Installation
-
-To install the package run:
-
-    $ pip install spinedb_api
-
-To upgrade to the most recent version, run:
-
-    $ pip install --upgrade spinedb_api
-
-You can also specify a branch, or a tag, for instance:
-
-    $ pip install spinedb_api==0.12.1
-
-To install the latest development version use the Git repository url:
-
-    $ pip install --upgrade git+https://github.com/spine-tools/Spine-Database-API.git
-
-
-## Building the documentation
-
-Source files for the documentation can be found in `docs/source` directory. In order to 
-build the HTML docs, you need to install the additional documentation building requirements
-by running:
-
-    $ pip install -r dev-requirements.txt 
-
-This installs Sphinx (among other things), which is required in building the documentation.
-When Sphinx is installed, you can build the HTML pages from the source files by running:
-
-    > docs\make.bat html
-    
-or
-
-    $ pushd docs
-    $ make html
-    $ popd
-    
-depending on your operating system.        
- 
-After running the build, the index page can be found in `docs/build/html/index.html`.
-
-&nbsp;
-<hr>
-<center>
-<table width=500px frame="none">
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
-</table>
-</center>
+# Spine Database API
+
+[![Documentation Status](https://readthedocs.org/projects/spine-database-api/badge/?version=latest)](https://spine-database-api.readthedocs.io/en/latest/?badge=latest)
+[![Unit tests](https://github.com/spine-tools/Spine-Database-API/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/Spine-Database-API/actions?query=workflow%3A"Unit+tests")
+[![codecov](https://codecov.io/gh/spine-tools/Spine-Database-API/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/Spine-Database-API)
+[![PyPI version](https://badge.fury.io/py/spinedb-api.svg)](https://badge.fury.io/py/spinedb-api)
+
+A Python package to access and manipulate Spine databases in a customary, unified way.
+
+## License
+
+Spine Database API is released under the GNU Lesser General Public License (LGPL) license. All accompanying
+documentation and manual are released under the Creative Commons BY-SA 4.0 license.
+
+## Getting started
+
+### Installation
+
+To install the package run:
+
+    $ pip install spinedb_api
+
+To upgrade to the most recent version, run:
+
+    $ pip install --upgrade spinedb_api
+
+You can also specify a branch, or a tag, for instance:
+
+    $ pip install spinedb_api==0.12.1
+
+To install the latest development version use the Git repository url:
+
+    $ pip install --upgrade git+https://github.com/spine-tools/Spine-Database-API.git
+
+
+## Building the documentation
+
+Source files for the documentation can be found in `docs/source` directory. In order to 
+build the HTML docs, you need to install the additional documentation building requirements
+by running:
+
+    $ pip install -r dev-requirements.txt 
+
+This installs Sphinx (among other things), which is required in building the documentation.
+When Sphinx is installed, you can build the HTML pages from the source files by running:
+
+    > docs\make.bat html
+    
+or
+
+    $ pushd docs
+    $ make html
+    $ popd
+    
+depending on your operating system.        
+ 
+After running the build, the index page can be found in `docs/build/html/index.html`.
+
+&nbsp;
+<hr>
+<center>
+<table width=500px frame="none">
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
+</table>
+</center>
```

### Comparing `spinedb_api-0.30.3/deploy-key.enc` & `spinedb_api-0.30.4/deploy-key.enc`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.3/docs/Makefile` & `spinedb_api-0.30.4/docs/Makefile`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# Minimal makefile for Sphinx documentation
-#
-
-# You can set these variables from the command line.
-SPHINXOPTS    =
-SPHINXBUILD   = sphinx-build
-SOURCEDIR     = source
-BUILDDIR      = build
-
-# Put it first so that "make" without argument is like "make help".
-help:
-	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
-
-.PHONY: help Makefile
-
-# Catch-all target: route all unknown targets to Sphinx using the new
-# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
-%: Makefile
+# Minimal makefile for Sphinx documentation
+#
+
+# You can set these variables from the command line.
+SPHINXOPTS    =
+SPHINXBUILD   = sphinx-build
+SOURCEDIR     = source
+BUILDDIR      = build
+
+# Put it first so that "make" without argument is like "make help".
+help:
+	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
+
+.PHONY: help Makefile
+
+# Catch-all target: route all unknown targets to Sphinx using the new
+# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
+%: Makefile
 	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
```

### Comparing `spinedb_api-0.30.3/docs/make.bat` & `spinedb_api-0.30.4/docs/make.bat`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-@ECHO OFF
-
-pushd %~dp0
-
-REM Command file for Sphinx documentation
-
-if "%SPHINXBUILD%" == "" (
-	set SPHINXBUILD=sphinx-build
-)
-set SOURCEDIR=source
-set BUILDDIR=build
-
-if "%1" == "" goto help
-
-%SPHINXBUILD% >NUL 2>NUL
-if errorlevel 9009 (
-	echo.
-	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
-	echo.installed, then set the SPHINXBUILD environment variable to point
-	echo.to the full path of the 'sphinx-build' executable. Alternatively you
-	echo.may add the Sphinx directory to PATH.
-	echo.
-	echo.If you don't have Sphinx installed, grab it from
-	echo.http://sphinx-doc.org/
-	exit /b 1
-)
-
-%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS%
-goto end
-
-:help
-%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS%
-
-:end
-popd
+@ECHO OFF
+
+pushd %~dp0
+
+REM Command file for Sphinx documentation
+
+if "%SPHINXBUILD%" == "" (
+	set SPHINXBUILD=sphinx-build
+)
+set SOURCEDIR=source
+set BUILDDIR=build
+
+if "%1" == "" goto help
+
+%SPHINXBUILD% >NUL 2>NUL
+if errorlevel 9009 (
+	echo.
+	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
+	echo.installed, then set the SPHINXBUILD environment variable to point
+	echo.to the full path of the 'sphinx-build' executable. Alternatively you
+	echo.may add the Sphinx directory to PATH.
+	echo.
+	echo.If you don't have Sphinx installed, grab it from
+	echo.http://sphinx-doc.org/
+	exit /b 1
+)
+
+%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS%
+goto end
+
+:help
+%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS%
+
+:end
+popd
```

### Comparing `spinedb_api-0.30.3/docs/source/front_matter.rst` & `spinedb_api-0.30.4/docs/source/front_matter.rst`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-..  spinedb_api tutorial
-    Created: 18.6.2018
-
-.. _SQLAlchemy: http://www.sqlalchemy.org/
-
-
-***************
-Front matter
-***************
-
-Information about the Spine database API project.
-
-.. contents::
-   :local:
-
-
-Project homepage
-----------------
-
-Spine database API is hosted on GitHub at https://github.com/spine-tools/Spine-Database-API
-under the Spine project.
-
-
-.. _installation:
-
-Installation
-------------
-
-Install released versions of Spine database API from the project repository with pip or a similar tool::
-
-  pip install git+https://github.com/spine-tools/Spine-Database-API.git
-
-
-Dependencies
-------------
-
-Spine database API's install process will ensure that SQLAlchemy_ is installed,
-in addition to other dependencies. Spine database API will work with SQLAlchemy as of version 1.3.0.
-
-
-Bugs
-----
-
-Bugs and feature enhancements to Spine database API should be reported on the
-`GitHub issue tracker <https://github.com/spine-tools/Spine-Database-API/issues>`_.
+..  spinedb_api tutorial
+    Created: 18.6.2018
+
+.. _SQLAlchemy: http://www.sqlalchemy.org/
+
+
+***************
+Front matter
+***************
+
+Information about the Spine database API project.
+
+.. contents::
+   :local:
+
+
+Project homepage
+----------------
+
+Spine database API is hosted on GitHub at https://github.com/spine-tools/Spine-Database-API
+under the Spine project.
+
+
+.. _installation:
+
+Installation
+------------
+
+Install released versions of Spine database API from the project repository with pip or a similar tool::
+
+  pip install git+https://github.com/spine-tools/Spine-Database-API.git
+
+
+Dependencies
+------------
+
+Spine database API's install process will ensure that SQLAlchemy_ is installed,
+in addition to other dependencies. Spine database API will work with SQLAlchemy as of version 1.3.0.
+
+
+Bugs
+----
+
+Bugs and feature enhancements to Spine database API should be reported on the
+`GitHub issue tracker <https://github.com/spine-tools/Spine-Database-API/issues>`_.
```

### Comparing `spinedb_api-0.30.3/docs/source/img/spinetoolbox_on_wht.svg` & `spinedb_api-0.30.4/docs/source/img/spinetoolbox_on_wht.svg`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.3/docs/source/metadata_description.rst` & `spinedb_api-0.30.4/docs/source/metadata_description.rst`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,105 +1,105 @@
-********************
-Metadata description
-********************
-
-This is the metadata description for Spine, edited from `<https://frictionlessdata.io/specs/data-package/>`_.
-
-Required properties
--------------------
-
-``title``
-    One sentence description for the data.
-
-``sources``
-    The raw sources of the data. Each source must have a ``title`` property and optionally a ``path`` property.
-
-.. code-block::
-    :caption: Example
-
-    "sources": [{
-      "title": "World Bank and OECD",
-      "path": "http://data.worldbank.org/indicator/NY.GDP.MKTP.CD"
-    }]
-
-``contributors``
-    The people or organisations who contributed to the data.
-    Must include ``title`` and may include ``path``, ``email``, ``role`` and ``organization``.
-    ``role`` is one of ``author``, ``publisher``, ``maintainer``, ``wrangler``, or ``contributor``.
-
-.. code-block::
-    :caption: Example
-
-    "contributors": [{
-      "title": "Joe Bloggs",
-      "email": "joe@bloggs.com",
-      "path": "http://www.bloggs.com",
-      "role": "author"
-    }]
-
-``created``
-    The date this data was created or put together, in ISO8601 format (YYYY-MM-DDTHH:MM).
-
-Optional properties
--------------------
-
-``description``
-    A description of the data. Describe here how the data was collected, how it was processed etc.
-    The description *must* be markdown formatted –
-    this also allows for simple plain text as plain text is itself valid markdown.
-    The first paragraph (up to the first double line break) should be usable as summary information for the package.
-
-``spine_results_metadata``
-    Property contains :ref:`results metadata description<results-metadata-description>`.
-
-``keywords``
-    An array of keywords.
-
-``homepage``
-    A URL for the home on the web that is related to this data package.
-
-``name``
-    Name of the data package, url-usable, all-lowercase string.
-
-``id``
-    Globally unique id, such as UUID or DOI.
-
-``licenses``
-    Licences that apply to the data.
-    Each item must have a name property (Open Definition license ID) or a path property and may contain title.
-
-.. code-block::
-    :caption: Example
-
-    "licenses": [{
-      "name": "ODC-PDDL-1.0",
-      "path": "http://opendatacommons.org/licenses/pddl/",
-      "title": "Open Data Commons Public Domain Dedication and License v1.0"
-    }]
-
-``temporal``
-    Temporal properties of the data (if applicable).
-
-.. code-block::
-    :caption: Example using DCMI Period Encoding Scheme
-
-    "temporal": {
-      "start": "2000-01-01",
-      "end": "2000-12-31",
-      "name": "The first year of the 21st century"
-    }
-
-``spatial``
-    Spatial properties of the data (if applicable).
-
-.. code-block::
-    :caption: Example using DCMI Point Encoding Scheme
-
-    "spatial": {
-      "east": 23.766667,
-      "north": 61.5,
-      "projection": "geographic coordinates (WGS 84)",
-      "name": "Tampere, Finland"
-    }
-
-``unitOfMeasurement``
-    Unit of measurement. Can also be embedded in description.
+********************
+Metadata description
+********************
+
+This is the metadata description for Spine, edited from `<https://frictionlessdata.io/specs/data-package/>`_.
+
+Required properties
+-------------------
+
+``title``
+    One sentence description for the data.
+
+``sources``
+    The raw sources of the data. Each source must have a ``title`` property and optionally a ``path`` property.
+
+.. code-block::
+    :caption: Example
+
+    "sources": [{
+      "title": "World Bank and OECD",
+      "path": "http://data.worldbank.org/indicator/NY.GDP.MKTP.CD"
+    }]
+
+``contributors``
+    The people or organisations who contributed to the data.
+    Must include ``title`` and may include ``path``, ``email``, ``role`` and ``organization``.
+    ``role`` is one of ``author``, ``publisher``, ``maintainer``, ``wrangler``, or ``contributor``.
+
+.. code-block::
+    :caption: Example
+
+    "contributors": [{
+      "title": "Joe Bloggs",
+      "email": "joe@bloggs.com",
+      "path": "http://www.bloggs.com",
+      "role": "author"
+    }]
+
+``created``
+    The date this data was created or put together, in ISO8601 format (YYYY-MM-DDTHH:MM).
+
+Optional properties
+-------------------
+
+``description``
+    A description of the data. Describe here how the data was collected, how it was processed etc.
+    The description *must* be markdown formatted –
+    this also allows for simple plain text as plain text is itself valid markdown.
+    The first paragraph (up to the first double line break) should be usable as summary information for the package.
+
+``spine_results_metadata``
+    Property contains :ref:`results metadata description<results-metadata-description>`.
+
+``keywords``
+    An array of keywords.
+
+``homepage``
+    A URL for the home on the web that is related to this data package.
+
+``name``
+    Name of the data package, url-usable, all-lowercase string.
+
+``id``
+    Globally unique id, such as UUID or DOI.
+
+``licenses``
+    Licences that apply to the data.
+    Each item must have a name property (Open Definition license ID) or a path property and may contain title.
+
+.. code-block::
+    :caption: Example
+
+    "licenses": [{
+      "name": "ODC-PDDL-1.0",
+      "path": "http://opendatacommons.org/licenses/pddl/",
+      "title": "Open Data Commons Public Domain Dedication and License v1.0"
+    }]
+
+``temporal``
+    Temporal properties of the data (if applicable).
+
+.. code-block::
+    :caption: Example using DCMI Period Encoding Scheme
+
+    "temporal": {
+      "start": "2000-01-01",
+      "end": "2000-12-31",
+      "name": "The first year of the 21st century"
+    }
+
+``spatial``
+    Spatial properties of the data (if applicable).
+
+.. code-block::
+    :caption: Example using DCMI Point Encoding Scheme
+
+    "spatial": {
+      "east": 23.766667,
+      "north": 61.5,
+      "projection": "geographic coordinates (WGS 84)",
+      "name": "Tampere, Finland"
+    }
+
+``unitOfMeasurement``
+    Unit of measurement. Can also be embedded in description.
```

### Comparing `spinedb_api-0.30.3/docs/source/parameter_value_format.rst` & `spinedb_api-0.30.4/docs/source/parameter_value_format.rst`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,333 +1,333 @@
-**********************
-Parameter value format
-**********************
-
-Parameter values are specified using JSON in the ``value`` field of the ``parameter_value`` table.
-This document describes the JSON specification for parameter values of special type
-(namely, date-time, duration, time-pattern, time-series, array, and map.)
-
-A value of special type is a JSON object with two mandatory properties, ``type`` and ``data``:
-
-- ``type`` indicates the value *type* and must be a JSON string
-  (either ``date_time``, ``duration``, ``dictionary``, ``time_pattern``, ``time_series``, ``array``, or ``map``).
-- ``data`` specifies the value *itself* and must be a JSON object in accordance with ``type`` as explained below.
-
-Date-time
----------
-
-If the ``type`` property is ``date_time``, then the ``data`` property specifies a date/time
-and must be a JSON string in the `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ format.
-
-Example
-~~~~~~~
-
-.. code-block:: json
-
-   {
-     "type": "date_time",
-     "data": "2019-06-01T22:15:00+01:00"
-   }
-
-
-Duration
---------
-
-If the  ``type`` property is ``duration``, then the ``data`` property specifies an extension of time
-where the accepted values are the following:
-
-- The number of time-units, specified as a 'verbose' JSON string.
-  The format is ``x unit``, where ``x`` is an integer
-  and ``unit`` is either ``year``, ``month``, ``day``, ``hour``, ``minute``, or ``second``
-  (either singular or plural).
-- The number of time-units, specified as a 'compact' JSON string.
-  The format is ``xU``, where ``x`` is an integer
-  and ``U`` is either ``Y`` (for year), ``M`` (for month), ``D`` (for day),
-  ``h`` (for hour), ``m`` (for minute), or ``s`` (for second).
-- The number of *minutes*, specified as a JSON integer.
-
-.. note::
-
-   The array version of Duration is deprecated and no longer supported.
-   Use the Array type for variable durations.
-
-Examples
-~~~~~~~~
-
-Verbose string:
-
-.. code-block:: json
-
-   {
-     "type": "duration",
-     "data": "1 hour"
-   }
-
-Compact string:
-
-.. code-block:: json
-
-   {
-     "type": "duration",
-     "data": "1h"
-   }
-
-Integer:
-
-.. code-block:: json
-
-   {
-     "type": "duration",
-     "data": 60
-   }
-
-Time-pattern
-------------
-
-If the ``type`` property is ``time_pattern``, then the ``data`` property specifies *time-patterned data*.
-This is data that varies *periodically* in time taking specific *values* at specific *time-periods* (such as summer and winter).
-Values must be JSON numbers, whereas time-periods must be JSON strings
-where the accepted values are the following:
-
-- An interval of time in a given time-unit.
-  The format is ``Ua-b``, where ``U`` is either ``Y`` (for year), ``M`` (for month), ``D`` (for day), ``WD`` (for weekday),
-  ``h`` (for hour), ``m`` (for minute), or ``s`` (for second);
-  and ``a`` and ``b`` are two integers corresponding to the lower and upper bound, respectively.
-- An intersection of intervals.
-  The format is ``s1;s2;...``,
-  where ``s1``, ``s2``, ..., are intervals as described above.
-- A union of ranges.
-  The format is ``r1,r2,...``,
-  where ``r1``, ``r2``, ..., are either intervals or intersections of intervals as described above.
-
-The ``data`` property must be a JSON object mapping time periods to values.
-
-Example
-~~~~~~~
-
-The following corresponds to a parameter which takes the value ``300`` in months 1 to 4 *and* 9 to 12,
-and the value ``221.5`` in months 5 to 8.
-
-.. code-block:: json
-
-   {
-     "type": "time_pattern",
-     "data": {
-       "M1-4,M9-12": 300,
-       "M5-8": 221.5
-     }
-   }
-
-Time-series
------------
-
-If the ``type`` property is ``time_series``, then the ``data`` property specifies time-series data.
-This is data that varies *arbitrarily* in time taking specific *values* at specific *time-stamps*.
-Values must be JSON numbers,
-whereas time-stamps must be JSON strings in the `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ format.
-
-Accepted values for the ``data`` property are the following:
-
-- A JSON object mapping time-stamps to values.
-- A two-column JSON array listing tuples of the form [time-stamp, value].
-- A (one-column) JSON array of values.
-  In this case it is assumed that the time-series begins at the first hour of *any* year,
-  has a resolution of one hour, and repeats cyclically until the *end* of time.
-
-In case of time-series, the specification may have one additional property, ``index``.
-``index`` must be a JSON object with the following properties, all of them optional:
-
-- ``start``: the *first* time-stamp, used in case ``data`` is a one-column array (ignored otherwise).
-  It must be a JSON string in the `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ format.
-  The default is ``0001-01-01T00:00:00``.
-- ``resolution``: the 'time between stamps', used in case ``data`` is a one-column array (ignored otherwise).
-  Accepted values are the same as for the ``data`` property of [duration](#duration) values.
-  The default is ``1 hour``.
-  If ``resolution`` is itself an array, then it is either trunk or repeated so as to fit ``data``.
-- ``ignore_year``: a JSON boolean to indicate whether or not the time-series should apply to *any* year.
-  The default is ``false``, unless ``data`` is a one-column array and ``start`` is not given.
-- ``repeat``: a JSON boolean whether or not the time-series should repeat cyclically until the *end* of time.
-  The default is ``false``, unless ``data`` is a one-column array and ``start`` is not given.
-
-Examples
-~~~~~~~~
-
-Dictionary:
-
-.. code-block:: json
-
-   {
-     "type": "time_series",
-     "data": {
-       "2019-01-01T00:00": 1,
-       "2019-01-01T01:30": 5,
-       "2019-01-01T02:00": 8
-     }
-   }
-
-Two-column array:
-
-.. code-block:: json
-
-   {
-
-     "type": "time_series",
-     "data": [
-       ["2019-01-01T00:00", 1],
-       ["2019-01-01T00:30", 2],
-       ["2019-01-01T02:00", 8]
-     ]
-   }
-
-One-column array with implicit (default) indices:
-
-.. code-block:: json
-
-   {
-     "type": "time_series",
-     "data": [1, 2, 3, 5, 8]
-   }
-
-One-column array with explicit (custom) indices:
-
-.. code-block:: json
-
-   {
-     "type": "time_series",
-     "data": [1, 2, 3, 5, 8],
-     "index": {
-       "start": "2019-01-01T00:00",
-       "resolution": "30 minutes",
-       "ignore_year": false,
-       "repeat": true
-     }
-   }
-
-Array
------
-
-If the ``type`` property is ``array``, then the ``data`` property specifies a one dimensional array.
-This is a list of values with zero based indexing.
-All values are of the same type which is specified by an optional ``value_type`` property.
-If specified, ``value_type`` must be one of the following: ``float``, ``str``, ``duration``, or ``date_time``.
-If omitted, ``value_type`` defaults to ``float``
-
-The ``data`` property must be a JSON list. The elements depent on ``value_type``:
-
-- If ``value_type`` is ``float`` then all elements in ``data`` must be JSON numbers.
-- If ``value_type`` is ``str`` then all elements in ``data`` must be JSON strings.
-- If ``value_type`` is ``duration`` then all elements in ``data`` must be single extensions of time.
-- If ``value_type`` is ``date_time`` then all elements in ``data`` must be JSON strings
-  in the `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ format.
-
-Examples
-~~~~~~~~
-
-An array of numbers:
-
-.. code-block:: json
-
-   {
-     "type": "array",
-     "data": [2.3, 23.0, 5.0]
-   }
-
-An array of durations:
-
-.. code-block:: json
-
-   {
-     "type": "array",
-     "value_type": "duration",
-     "data": ["3 months", "2Y", "4 minutes"]
-   }
-
-Map
----
-
-If the ``type`` property is ``map``, then the ``data`` property specifies indexed array data.
-An additional ``index_type`` specifies the type of the index and must be one of the following:
-``float``, ``str``, ``duration``, or ``date_time``.
-
-The ``data`` property can be a JSON mapping with the following properties:
-
-- Every key in the map must be a scalar of the same type as given by ``index_type``:
-
-  * floats are represented by JSON numbers, e.g. ``5.5``
-  * strings are represented by JSON strings, e.g. ``"key_1"``
-  * durations are represented by duration strings, e.g. ``"1 hour"``.
-    Note that *variable* durations are not supported
-  * datetimes are represented by ISO8601 time stamps, e.g. ``"2020-01-01T12:00"``
-
-- Every value in the map can be
-
-  * a float, e.g. ``5.5``
-  * a duration, e.g. ``{"type": "duration", "data": "3 days"}``
-  * a datetime, e.g. ``{"type": "date_time", "data": "2020-01-01T12:00"``}
-  * a map, e.g. ``{"type": "map", "index_type": "str", "data":{"a": 2, "b": 3}}``
-  * any of the following: time-series, array, time-pattern
-
-Optionally, the ``data`` property can be a two-column JSON array
-where the first element is the key and the second the value.
-
-Examples
-~~~~~~~~
-
-Dictionary:
-
-.. code-block:: json
-
-   {
-     "type": "map",
-     "index_type": "date_time",
-     "data": {
-       "2010-01-01T00:00": {
-         "type": "map",
-         "index_type": "duration",
-         "data": [["1D", -1.0], ["1D", -1.5]]
-       },
-       "2010-02-01-T00:00": {
-         "type": "map",
-         "index_type": "duration",
-         "data": [["1 month", 2.3], ["2 months", 2.5]]
-       }
-     }
-   }
-
-Two-column array:
-
-.. code-block:: json
-
-   {
-     "type": "map",
-     "index_type": "str",
-     "data": [["cell_1", 1.0], ["cell_2", 2.0], ["cell_3", 3.0]]
-   }
-
-Stochastic time series corresponding to the table below:
-
-================ ================ =================== =====
-Forecast time    Target time      Stochastic scenario Value
-================ ================ =================== =====
-2020-04-17T08:00 2020-04-17T08:00 0                   23.0
-2020-04-17T08:00 2020-04-17T09:00 0                   24.0
-2020-04-17T08:00 2020-04-17T10:00 0                   25.0
-2020-04-17T08:00 2020-04-17T08:00 1                   5.5
-2020-04-17T08:00 2020-04-17T09:00 1                   6.6
-2020-04-17T08:00 2020-04-17T10:00 1                   7.7
-================ ================ =================== =====
-
-.. code-block:: json
-
-   {
-     "type": "map",
-     "index_type": "date_time",
-     "data": [
-       ["2020-04-17T08:00",
-        {"type": "map", "index_type": "date_time", "data": [
-          ["2020-04-17T08:00", {"type": "map", "index_type": "float", "data": [[0, 23.0], [1, 5.5]]}],
-          ["2020-04-17T09:00", {"type": "map", "index_type": "float", "data": [[0, 24.0], [1, 6.6]]}],
-          ["2020-04-17T10:00", {"type": "map", "index_type": "float", "data": [[0, 25.0], [1, 7.7]]}]
-        ]}
-       ]
-     ]
-   }
+**********************
+Parameter value format
+**********************
+
+Parameter values are specified using JSON in the ``value`` field of the ``parameter_value`` table.
+This document describes the JSON specification for parameter values of special type
+(namely, date-time, duration, time-pattern, time-series, array, and map.)
+
+A value of special type is a JSON object with two mandatory properties, ``type`` and ``data``:
+
+- ``type`` indicates the value *type* and must be a JSON string
+  (either ``date_time``, ``duration``, ``dictionary``, ``time_pattern``, ``time_series``, ``array``, or ``map``).
+- ``data`` specifies the value *itself* and must be a JSON object in accordance with ``type`` as explained below.
+
+Date-time
+---------
+
+If the ``type`` property is ``date_time``, then the ``data`` property specifies a date/time
+and must be a JSON string in the `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ format.
+
+Example
+~~~~~~~
+
+.. code-block:: json
+
+   {
+     "type": "date_time",
+     "data": "2019-06-01T22:15:00+01:00"
+   }
+
+
+Duration
+--------
+
+If the  ``type`` property is ``duration``, then the ``data`` property specifies an extension of time
+where the accepted values are the following:
+
+- The number of time-units, specified as a 'verbose' JSON string.
+  The format is ``x unit``, where ``x`` is an integer
+  and ``unit`` is either ``year``, ``month``, ``day``, ``hour``, ``minute``, or ``second``
+  (either singular or plural).
+- The number of time-units, specified as a 'compact' JSON string.
+  The format is ``xU``, where ``x`` is an integer
+  and ``U`` is either ``Y`` (for year), ``M`` (for month), ``D`` (for day),
+  ``h`` (for hour), ``m`` (for minute), or ``s`` (for second).
+- The number of *minutes*, specified as a JSON integer.
+
+.. note::
+
+   The array version of Duration is deprecated and no longer supported.
+   Use the Array type for variable durations.
+
+Examples
+~~~~~~~~
+
+Verbose string:
+
+.. code-block:: json
+
+   {
+     "type": "duration",
+     "data": "1 hour"
+   }
+
+Compact string:
+
+.. code-block:: json
+
+   {
+     "type": "duration",
+     "data": "1h"
+   }
+
+Integer:
+
+.. code-block:: json
+
+   {
+     "type": "duration",
+     "data": 60
+   }
+
+Time-pattern
+------------
+
+If the ``type`` property is ``time_pattern``, then the ``data`` property specifies *time-patterned data*.
+This is data that varies *periodically* in time taking specific *values* at specific *time-periods* (such as summer and winter).
+Values must be JSON numbers, whereas time-periods must be JSON strings
+where the accepted values are the following:
+
+- An interval of time in a given time-unit.
+  The format is ``Ua-b``, where ``U`` is either ``Y`` (for year), ``M`` (for month), ``D`` (for day), ``WD`` (for weekday),
+  ``h`` (for hour), ``m`` (for minute), or ``s`` (for second);
+  and ``a`` and ``b`` are two integers corresponding to the lower and upper bound, respectively.
+- An intersection of intervals.
+  The format is ``s1;s2;...``,
+  where ``s1``, ``s2``, ..., are intervals as described above.
+- A union of ranges.
+  The format is ``r1,r2,...``,
+  where ``r1``, ``r2``, ..., are either intervals or intersections of intervals as described above.
+
+The ``data`` property must be a JSON object mapping time periods to values.
+
+Example
+~~~~~~~
+
+The following corresponds to a parameter which takes the value ``300`` in months 1 to 4 *and* 9 to 12,
+and the value ``221.5`` in months 5 to 8.
+
+.. code-block:: json
+
+   {
+     "type": "time_pattern",
+     "data": {
+       "M1-4,M9-12": 300,
+       "M5-8": 221.5
+     }
+   }
+
+Time-series
+-----------
+
+If the ``type`` property is ``time_series``, then the ``data`` property specifies time-series data.
+This is data that varies *arbitrarily* in time taking specific *values* at specific *time-stamps*.
+Values must be JSON numbers,
+whereas time-stamps must be JSON strings in the `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ format.
+
+Accepted values for the ``data`` property are the following:
+
+- A JSON object mapping time-stamps to values.
+- A two-column JSON array listing tuples of the form [time-stamp, value].
+- A (one-column) JSON array of values.
+  In this case it is assumed that the time-series begins at the first hour of *any* year,
+  has a resolution of one hour, and repeats cyclically until the *end* of time.
+
+In case of time-series, the specification may have one additional property, ``index``.
+``index`` must be a JSON object with the following properties, all of them optional:
+
+- ``start``: the *first* time-stamp, used in case ``data`` is a one-column array (ignored otherwise).
+  It must be a JSON string in the `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ format.
+  The default is ``0001-01-01T00:00:00``.
+- ``resolution``: the 'time between stamps', used in case ``data`` is a one-column array (ignored otherwise).
+  Accepted values are the same as for the ``data`` property of [duration](#duration) values.
+  The default is ``1 hour``.
+  If ``resolution`` is itself an array, then it is either trunk or repeated so as to fit ``data``.
+- ``ignore_year``: a JSON boolean to indicate whether or not the time-series should apply to *any* year.
+  The default is ``false``, unless ``data`` is a one-column array and ``start`` is not given.
+- ``repeat``: a JSON boolean whether or not the time-series should repeat cyclically until the *end* of time.
+  The default is ``false``, unless ``data`` is a one-column array and ``start`` is not given.
+
+Examples
+~~~~~~~~
+
+Dictionary:
+
+.. code-block:: json
+
+   {
+     "type": "time_series",
+     "data": {
+       "2019-01-01T00:00": 1,
+       "2019-01-01T01:30": 5,
+       "2019-01-01T02:00": 8
+     }
+   }
+
+Two-column array:
+
+.. code-block:: json
+
+   {
+
+     "type": "time_series",
+     "data": [
+       ["2019-01-01T00:00", 1],
+       ["2019-01-01T00:30", 2],
+       ["2019-01-01T02:00", 8]
+     ]
+   }
+
+One-column array with implicit (default) indices:
+
+.. code-block:: json
+
+   {
+     "type": "time_series",
+     "data": [1, 2, 3, 5, 8]
+   }
+
+One-column array with explicit (custom) indices:
+
+.. code-block:: json
+
+   {
+     "type": "time_series",
+     "data": [1, 2, 3, 5, 8],
+     "index": {
+       "start": "2019-01-01T00:00",
+       "resolution": "30 minutes",
+       "ignore_year": false,
+       "repeat": true
+     }
+   }
+
+Array
+-----
+
+If the ``type`` property is ``array``, then the ``data`` property specifies a one dimensional array.
+This is a list of values with zero based indexing.
+All values are of the same type which is specified by an optional ``value_type`` property.
+If specified, ``value_type`` must be one of the following: ``float``, ``str``, ``duration``, or ``date_time``.
+If omitted, ``value_type`` defaults to ``float``
+
+The ``data`` property must be a JSON list. The elements depent on ``value_type``:
+
+- If ``value_type`` is ``float`` then all elements in ``data`` must be JSON numbers.
+- If ``value_type`` is ``str`` then all elements in ``data`` must be JSON strings.
+- If ``value_type`` is ``duration`` then all elements in ``data`` must be single extensions of time.
+- If ``value_type`` is ``date_time`` then all elements in ``data`` must be JSON strings
+  in the `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ format.
+
+Examples
+~~~~~~~~
+
+An array of numbers:
+
+.. code-block:: json
+
+   {
+     "type": "array",
+     "data": [2.3, 23.0, 5.0]
+   }
+
+An array of durations:
+
+.. code-block:: json
+
+   {
+     "type": "array",
+     "value_type": "duration",
+     "data": ["3 months", "2Y", "4 minutes"]
+   }
+
+Map
+---
+
+If the ``type`` property is ``map``, then the ``data`` property specifies indexed array data.
+An additional ``index_type`` specifies the type of the index and must be one of the following:
+``float``, ``str``, ``duration``, or ``date_time``.
+
+The ``data`` property can be a JSON mapping with the following properties:
+
+- Every key in the map must be a scalar of the same type as given by ``index_type``:
+
+  * floats are represented by JSON numbers, e.g. ``5.5``
+  * strings are represented by JSON strings, e.g. ``"key_1"``
+  * durations are represented by duration strings, e.g. ``"1 hour"``.
+    Note that *variable* durations are not supported
+  * datetimes are represented by ISO8601 time stamps, e.g. ``"2020-01-01T12:00"``
+
+- Every value in the map can be
+
+  * a float, e.g. ``5.5``
+  * a duration, e.g. ``{"type": "duration", "data": "3 days"}``
+  * a datetime, e.g. ``{"type": "date_time", "data": "2020-01-01T12:00"``}
+  * a map, e.g. ``{"type": "map", "index_type": "str", "data":{"a": 2, "b": 3}}``
+  * any of the following: time-series, array, time-pattern
+
+Optionally, the ``data`` property can be a two-column JSON array
+where the first element is the key and the second the value.
+
+Examples
+~~~~~~~~
+
+Dictionary:
+
+.. code-block:: json
+
+   {
+     "type": "map",
+     "index_type": "date_time",
+     "data": {
+       "2010-01-01T00:00": {
+         "type": "map",
+         "index_type": "duration",
+         "data": [["1D", -1.0], ["1D", -1.5]]
+       },
+       "2010-02-01-T00:00": {
+         "type": "map",
+         "index_type": "duration",
+         "data": [["1 month", 2.3], ["2 months", 2.5]]
+       }
+     }
+   }
+
+Two-column array:
+
+.. code-block:: json
+
+   {
+     "type": "map",
+     "index_type": "str",
+     "data": [["cell_1", 1.0], ["cell_2", 2.0], ["cell_3", 3.0]]
+   }
+
+Stochastic time series corresponding to the table below:
+
+================ ================ =================== =====
+Forecast time    Target time      Stochastic scenario Value
+================ ================ =================== =====
+2020-04-17T08:00 2020-04-17T08:00 0                   23.0
+2020-04-17T08:00 2020-04-17T09:00 0                   24.0
+2020-04-17T08:00 2020-04-17T10:00 0                   25.0
+2020-04-17T08:00 2020-04-17T08:00 1                   5.5
+2020-04-17T08:00 2020-04-17T09:00 1                   6.6
+2020-04-17T08:00 2020-04-17T10:00 1                   7.7
+================ ================ =================== =====
+
+.. code-block:: json
+
+   {
+     "type": "map",
+     "index_type": "date_time",
+     "data": [
+       ["2020-04-17T08:00",
+        {"type": "map", "index_type": "date_time", "data": [
+          ["2020-04-17T08:00", {"type": "map", "index_type": "float", "data": [[0, 23.0], [1, 5.5]]}],
+          ["2020-04-17T09:00", {"type": "map", "index_type": "float", "data": [[0, 24.0], [1, 6.6]]}],
+          ["2020-04-17T10:00", {"type": "map", "index_type": "float", "data": [[0, 25.0], [1, 7.7]]}]
+        ]}
+       ]
+     ]
+   }
```

### Comparing `spinedb_api-0.30.3/docs/source/tutorial.rst` & `spinedb_api-0.30.4/docs/source/tutorial.rst`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,77 +1,77 @@
-..  spinedb_api tutorial
-    Created: 18.6.2018
-
-.. _SQLAlchemy: http://www.sqlalchemy.org/
-
-
-********
-Tutorial
-********
-
-Spine database API provides for the creation and management of
-Spine databases, using SQLAlchemy_ as the underlying engine.
-This tutorial will provide a full introduction to the usage of this package.
-
-To begin, make sure Spine database API is installed as described at :ref:`installation`.
-
-
-Creation
---------
-
-Usage of Spine database API starts with the creation of a Spine database.
-
-Mapping
--------
-
-Next step is the creation of a *Database Mapping*,
-a Python object that provides means of interacting with the database.
-Spine database API provides two classes of mapping:
-
-- :class:`.DatabaseMapping`, just for *querying* the database (i.e., run ``SELECT`` statements).
-- :class:`.DiffDatabaseMapping`, for both querying and *modifying* the database.
-
-The differences between these two will become more apparent as we go through this tutorial.
-However, it is important to note that everything you can do with a :class:`.DatabaseMapping`,
-you can also do with a :class:`.DiffDatabaseMapping`.
-
-To create a :class:`.DatabaseMapping`, we just pass the database URL to the class constructor::
-
-    from spinedb_api import DatabaseMapping
-
-    url = "sqlite:///spine.db"
-
-    db_map = DatabaseMapping(url)
-
-The URL should be formatted following the RFC-1738 standard, so it basically
-works with :func:`sqlalchemy.create_engine` as described
-`here <https://docs.sqlalchemy.org/en/13/core/engines.html?highlight=database%20urls#database-urls>`_.
-
-.. note::
-
-  Currently supported database backends are only SQLite and MySQL. More will be added later.
-
-Querying
---------
-
-The database mapping object provides two mechanisms for querying the database.
-The first is for running *standard*, general-purpose queries
-such as selecting all records from the ``object_class`` table.
-The second is for performing *custom* queries that one may need for a particular purpose.
-
-Standard querying
-=================
-
-To perform standard querying, we chose among the methods of the :class:`~.DatabaseMappingQueryMixin` class,
-the one that bets suits our purpose. E.g.::
-
-    TODO
-
-Custom querying
-===============
-
-TODO
-
-Inserting
----------
-
-TODO
+..  spinedb_api tutorial
+    Created: 18.6.2018
+
+.. _SQLAlchemy: http://www.sqlalchemy.org/
+
+
+********
+Tutorial
+********
+
+Spine database API provides for the creation and management of
+Spine databases, using SQLAlchemy_ as the underlying engine.
+This tutorial will provide a full introduction to the usage of this package.
+
+To begin, make sure Spine database API is installed as described at :ref:`installation`.
+
+
+Creation
+--------
+
+Usage of Spine database API starts with the creation of a Spine database.
+
+Mapping
+-------
+
+Next step is the creation of a *Database Mapping*,
+a Python object that provides means of interacting with the database.
+Spine database API provides two classes of mapping:
+
+- :class:`.DatabaseMapping`, just for *querying* the database (i.e., run ``SELECT`` statements).
+- :class:`.DiffDatabaseMapping`, for both querying and *modifying* the database.
+
+The differences between these two will become more apparent as we go through this tutorial.
+However, it is important to note that everything you can do with a :class:`.DatabaseMapping`,
+you can also do with a :class:`.DiffDatabaseMapping`.
+
+To create a :class:`.DatabaseMapping`, we just pass the database URL to the class constructor::
+
+    from spinedb_api import DatabaseMapping
+
+    url = "sqlite:///spine.db"
+
+    db_map = DatabaseMapping(url)
+
+The URL should be formatted following the RFC-1738 standard, so it basically
+works with :func:`sqlalchemy.create_engine` as described
+`here <https://docs.sqlalchemy.org/en/13/core/engines.html?highlight=database%20urls#database-urls>`_.
+
+.. note::
+
+  Currently supported database backends are only SQLite and MySQL. More will be added later.
+
+Querying
+--------
+
+The database mapping object provides two mechanisms for querying the database.
+The first is for running *standard*, general-purpose queries
+such as selecting all records from the ``object_class`` table.
+The second is for performing *custom* queries that one may need for a particular purpose.
+
+Standard querying
+=================
+
+To perform standard querying, we chose among the methods of the :class:`~.DatabaseMappingQueryMixin` class,
+the one that bets suits our purpose. E.g.::
+
+    TODO
+
+Custom querying
+===============
+
+TODO
+
+Inserting
+---------
+
+TODO
```

### Comparing `spinedb_api-0.30.3/fig/eu-emblem-low-res.jpg` & `spinedb_api-0.30.4/fig/eu-emblem-low-res.jpg`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.3/pylintrc` & `spinedb_api-0.30.4/pylintrc`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.3/pyproject.toml` & `spinedb_api-0.30.4/pyproject.toml`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,70 +1,70 @@
-[project]
-name = "spinedb_api"
-dynamic = ["version"]
-authors = [{name = "Spine Project consortium", email = "spine_info@vtt.fi"}]
-license = {text = "LGPL-3.0-or-later"}
-description = "An API to talk to Spine databases."
-keywords = ["energy system modelling", "workflow", "optimisation", "database"]
-readme = {file = "README.md", content-type = "text/markdown"}
-classifiers = [
-    "Programming Language :: Python :: 3",
-    "License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)",
-    "Operating System :: OS Independent",
-]
-requires-python = ">=3.8.1, <3.12"
-dependencies = [
-    # v1.4 does not pass tests
-    "sqlalchemy >=1.3, <1.4",
-    "alembic >=1.7",
-    "faker >=8.1.2",
-    "datapackage >=1.15.2",
-    "python-dateutil >=2.8.1",
-    # v1.22 requires Python 3.8 or later
-    "numpy >=1.20.2",
-    "scipy >=1.7.1",
-    "openpyxl >=3.0.7, !=3.1.1",
-    "gdx2py >=2.1.1",
-    "ijson >=3.1.4",
-    "chardet >=4.0.0",
-    "pymysql >=1.0.2",
-    "psycopg2",
-    "cx_Oracle",
-]
-
-[project.urls]
-Repository = "https://github.com/spine-tools/Spine-Database-API"
-
-[project.optional-dependencies]
-dev = ["coverage[toml]"]
-
-[build-system]
-requires = ["setuptools>=64", "setuptools_scm[toml]>=6.2", "wheel", "build"]
-build-backend = "setuptools.build_meta"
-
-[tool.setuptools_scm]
-write_to = "spinedb_api/version.py"
-version_scheme = "release-branch-semver"
-
-[tool.setuptools]
-zip-safe = false
-include-package-data = true
-
-[tool.setuptools.packages.find]
-exclude = [
-    "bin*",
-    "docs*",
-    "fig*",
-    "tests*",
-]
-
-[tool.coverage.run]
-source = ["spinedb_api"]
-branch = true
-
-[tool.coverage.report]
-ignore_errors = true
-
-[tool.black]
-line-length = 120
-skip-string-normalization = true
-exclude = '\.git'
+[project]
+name = "spinedb_api"
+dynamic = ["version"]
+authors = [{name = "Spine Project consortium", email = "spine_info@vtt.fi"}]
+license = {text = "LGPL-3.0-or-later"}
+description = "An API to talk to Spine databases."
+keywords = ["energy system modelling", "workflow", "optimisation", "database"]
+readme = {file = "README.md", content-type = "text/markdown"}
+classifiers = [
+    "Programming Language :: Python :: 3",
+    "License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)",
+    "Operating System :: OS Independent",
+]
+requires-python = ">=3.8.1, <3.12"
+dependencies = [
+    # v1.4 does not pass tests
+    "sqlalchemy >=1.3, <1.4",
+    "alembic >=1.7",
+    "faker >=8.1.2",
+    "datapackage >=1.15.2",
+    "python-dateutil >=2.8.1",
+    # v1.22 requires Python 3.8 or later
+    "numpy >=1.20.2",
+    "scipy >=1.7.1",
+    "openpyxl >=3.0.7, !=3.1.1",
+    "gdx2py >=2.1.1",
+    "ijson >=3.1.4",
+    "chardet >=4.0.0",
+    "pymysql >=1.0.2",
+    "psycopg2",
+    "cx_Oracle",
+]
+
+[project.urls]
+Repository = "https://github.com/spine-tools/Spine-Database-API"
+
+[project.optional-dependencies]
+dev = ["coverage[toml]"]
+
+[build-system]
+requires = ["setuptools>=64", "setuptools_scm[toml]>=6.2", "wheel", "build"]
+build-backend = "setuptools.build_meta"
+
+[tool.setuptools_scm]
+write_to = "spinedb_api/version.py"
+version_scheme = "release-branch-semver"
+
+[tool.setuptools]
+zip-safe = false
+include-package-data = true
+
+[tool.setuptools.packages.find]
+exclude = [
+    "bin*",
+    "docs*",
+    "fig*",
+    "tests*",
+]
+
+[tool.coverage.run]
+source = ["spinedb_api"]
+branch = true
+
+[tool.coverage.report]
+ignore_errors = true
+
+[tool.black]
+line-length = 120
+skip-string-normalization = true
+exclude = '\.git'
```

### Comparing `spinedb_api-0.30.3/spinedb_api/__init__.py` & `spinedb_api-0.30.4/spinedb_api/__init__.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,131 +1,131 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-from .db_mapping import DatabaseMapping
-from .diff_db_mapping import DiffDatabaseMapping
-from .exception import (
-    SpineDBAPIError,
-    SpineIntegrityError,
-    SpineDBVersionError,
-    SpineTableNotFoundError,
-    RecordNotFoundError,
-    ParameterValueError,
-    ParameterValueFormatError,
-    InvalidMapping,
-)
-from .helpers import (
-    naming_convention,
-    create_spine_metadata,
-    SUPPORTED_DIALECTS,
-    create_new_spine_database,
-    copy_database,
-    is_unlocked,
-    is_head,
-    is_empty,
-    forward_sweep,
-    Asterisk,
-)
-from .check_functions import (
-    check_alternative,
-    check_scenario,
-    check_scenario_alternative,
-    check_object_class,
-    check_object,
-    check_wide_relationship_class,
-    check_wide_relationship,
-    check_parameter_definition,
-    check_parameter_value,
-    check_parameter_value_list,
-)
-from .import_functions import (
-    import_alternatives,
-    import_data,
-    import_object_classes,
-    import_objects,
-    import_object_parameters,
-    import_object_parameter_values,
-    import_parameter_value_lists,
-    import_relationship_classes,
-    import_relationship_parameter_values,
-    import_relationship_parameters,
-    import_relationships,
-    import_scenarios,
-    import_scenario_alternatives,
-    import_tools,
-    import_features,
-    import_tool_features,
-    import_tool_feature_methods,
-    import_metadata,
-    import_object_metadata,
-    import_relationship_metadata,
-    import_object_parameter_value_metadata,
-    import_relationship_parameter_value_metadata,
-    get_data_for_import,
-)
-from .export_functions import (
-    export_alternatives,
-    export_data,
-    export_object_classes,
-    export_object_groups,
-    export_object_parameters,
-    export_object_parameter_values,
-    export_objects,
-    export_relationship_classes,
-    export_relationship_parameter_values,
-    export_relationship_parameters,
-    export_relationships,
-    export_scenario_alternatives,
-    export_scenarios,
-    export_tools,
-    export_features,
-    export_tool_features,
-    export_tool_feature_methods,
-)
-from .import_mapping.import_mapping_compat import import_mapping_from_dict
-from .import_mapping.generator import get_mapped_data
-from .parameter_value import (
-    convert_containers_to_maps,
-    convert_leaf_maps_to_specialized_containers,
-    convert_map_to_dict,
-    convert_map_to_table,
-    duration_to_relativedelta,
-    relativedelta_to_duration,
-    from_database,
-    to_database,
-    Array,
-    DateTime,
-    Duration,
-    IndexedNumberArray,
-    IndexedValue,
-    Map,
-    TimePattern,
-    TimeSeries,
-    TimeSeriesFixedResolution,
-    TimeSeriesVariableResolution,
-    ListValueRef,
-)
-from .filters.alternative_filter import apply_alternative_filter_to_parameter_value_sq
-from .filters.scenario_filter import apply_scenario_filter_to_subqueries
-from .filters.tool_filter import apply_tool_filter_to_entity_sq
-from .filters.execution_filter import apply_execution_filter
-from .filters.renamer import apply_renaming_to_parameter_definition_sq, apply_renaming_to_entity_class_sq
-from .filters.tools import (
-    append_filter_config,
-    apply_filter_stack,
-    clear_filter_configs,
-    config_to_shorthand,
-    load_filters,
-    pop_filter_configs,
-    name_from_dict,
-)
-from .version import __version__, __version_tuple__
-
-name = "spinedb_api"
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+from .db_mapping import DatabaseMapping
+from .diff_db_mapping import DiffDatabaseMapping
+from .exception import (
+    SpineDBAPIError,
+    SpineIntegrityError,
+    SpineDBVersionError,
+    SpineTableNotFoundError,
+    RecordNotFoundError,
+    ParameterValueError,
+    ParameterValueFormatError,
+    InvalidMapping,
+)
+from .helpers import (
+    naming_convention,
+    create_spine_metadata,
+    SUPPORTED_DIALECTS,
+    create_new_spine_database,
+    copy_database,
+    is_unlocked,
+    is_head,
+    is_empty,
+    forward_sweep,
+    Asterisk,
+)
+from .check_functions import (
+    check_alternative,
+    check_scenario,
+    check_scenario_alternative,
+    check_object_class,
+    check_object,
+    check_wide_relationship_class,
+    check_wide_relationship,
+    check_parameter_definition,
+    check_parameter_value,
+    check_parameter_value_list,
+)
+from .import_functions import (
+    import_alternatives,
+    import_data,
+    import_object_classes,
+    import_objects,
+    import_object_parameters,
+    import_object_parameter_values,
+    import_parameter_value_lists,
+    import_relationship_classes,
+    import_relationship_parameter_values,
+    import_relationship_parameters,
+    import_relationships,
+    import_scenarios,
+    import_scenario_alternatives,
+    import_tools,
+    import_features,
+    import_tool_features,
+    import_tool_feature_methods,
+    import_metadata,
+    import_object_metadata,
+    import_relationship_metadata,
+    import_object_parameter_value_metadata,
+    import_relationship_parameter_value_metadata,
+    get_data_for_import,
+)
+from .export_functions import (
+    export_alternatives,
+    export_data,
+    export_object_classes,
+    export_object_groups,
+    export_object_parameters,
+    export_object_parameter_values,
+    export_objects,
+    export_relationship_classes,
+    export_relationship_parameter_values,
+    export_relationship_parameters,
+    export_relationships,
+    export_scenario_alternatives,
+    export_scenarios,
+    export_tools,
+    export_features,
+    export_tool_features,
+    export_tool_feature_methods,
+)
+from .import_mapping.import_mapping_compat import import_mapping_from_dict
+from .import_mapping.generator import get_mapped_data
+from .parameter_value import (
+    convert_containers_to_maps,
+    convert_leaf_maps_to_specialized_containers,
+    convert_map_to_dict,
+    convert_map_to_table,
+    duration_to_relativedelta,
+    relativedelta_to_duration,
+    from_database,
+    to_database,
+    Array,
+    DateTime,
+    Duration,
+    IndexedNumberArray,
+    IndexedValue,
+    Map,
+    TimePattern,
+    TimeSeries,
+    TimeSeriesFixedResolution,
+    TimeSeriesVariableResolution,
+    ListValueRef,
+)
+from .filters.alternative_filter import apply_alternative_filter_to_parameter_value_sq
+from .filters.scenario_filter import apply_scenario_filter_to_subqueries
+from .filters.tool_filter import apply_tool_filter_to_entity_sq
+from .filters.execution_filter import apply_execution_filter
+from .filters.renamer import apply_renaming_to_parameter_definition_sq, apply_renaming_to_entity_class_sq
+from .filters.tools import (
+    append_filter_config,
+    apply_filter_stack,
+    clear_filter_configs,
+    config_to_shorthand,
+    load_filters,
+    pop_filter_configs,
+    name_from_dict,
+)
+from .version import __version__, __version_tuple__
+
+name = "spinedb_api"
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/env.py` & `spinedb_api-0.30.4/spinedb_api/alembic/env.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,73 +1,73 @@
-from __future__ import with_statement
-
-from logging.config import fileConfig
-
-from sqlalchemy import engine_from_config
-from sqlalchemy import pool
-
-from alembic import context
-
-# this is the Alembic Config object, which provides
-# access to the values within the .ini file in use.
-config = context.config
-
-# Interpret the config file for Python logging.
-# This line sets up loggers basically.
-fileConfig(config.config_file_name)
-
-# add your model's MetaData object here
-# for 'autogenerate' support
-import sys
-
-sys.path = ['', '..'] + sys.path[1:]
-from spinedb_api.helpers import create_spine_metadata
-
-target_metadata = create_spine_metadata()
-
-# other values from the config, defined by the needs of env.py,
-# can be acquired:
-# my_important_option = config.get_main_option("my_important_option")
-# ... etc.
-
-
-def run_migrations_offline():
-    """Run migrations in 'offline' mode.
-
-    This configures the context with just a URL
-    and not an Engine, though an Engine is acceptable
-    here as well.  By skipping the Engine creation
-    we don't even need a DBAPI to be available.
-
-    Calls to context.execute() here emit the given string to the
-    script output.
-
-    """
-    url = config.get_main_option("sqlalchemy.url")
-    context.configure(url=url, target_metadata=target_metadata, literal_binds=True)
-
-    with context.begin_transaction():
-        context.run_migrations()
-
-
-def run_migrations_online():
-    """Run migrations in 'online' mode.
-
-    In this scenario we need to create an Engine
-    and associate a connection with the context.
-
-    """
-    connectable = engine_from_config(
-        config.get_section(config.config_ini_section), prefix="sqlalchemy.", poolclass=pool.NullPool
-    )
-
-    with connectable.connect() as connection:
-        context.configure(connection=connection, target_metadata=target_metadata)
-
-        with context.begin_transaction():
-            context.run_migrations()
-
-
-if context.is_offline_mode():
-    run_migrations_offline()
-else:
-    run_migrations_online()
+from __future__ import with_statement
+
+from logging.config import fileConfig
+
+from sqlalchemy import engine_from_config
+from sqlalchemy import pool
+
+from alembic import context
+
+# this is the Alembic Config object, which provides
+# access to the values within the .ini file in use.
+config = context.config
+
+# Interpret the config file for Python logging.
+# This line sets up loggers basically.
+fileConfig(config.config_file_name)
+
+# add your model's MetaData object here
+# for 'autogenerate' support
+import sys
+
+sys.path = ['', '..'] + sys.path[1:]
+from spinedb_api.helpers import create_spine_metadata
+
+target_metadata = create_spine_metadata()
+
+# other values from the config, defined by the needs of env.py,
+# can be acquired:
+# my_important_option = config.get_main_option("my_important_option")
+# ... etc.
+
+
+def run_migrations_offline():
+    """Run migrations in 'offline' mode.
+
+    This configures the context with just a URL
+    and not an Engine, though an Engine is acceptable
+    here as well.  By skipping the Engine creation
+    we don't even need a DBAPI to be available.
+
+    Calls to context.execute() here emit the given string to the
+    script output.
+
+    """
+    url = config.get_main_option("sqlalchemy.url")
+    context.configure(url=url, target_metadata=target_metadata, literal_binds=True)
+
+    with context.begin_transaction():
+        context.run_migrations()
+
+
+def run_migrations_online():
+    """Run migrations in 'online' mode.
+
+    In this scenario we need to create an Engine
+    and associate a connection with the context.
+
+    """
+    connectable = engine_from_config(
+        config.get_section(config.config_ini_section), prefix="sqlalchemy.", poolclass=pool.NullPool
+    )
+
+    with connectable.connect() as connection:
+        context.configure(connection=connection, target_metadata=target_metadata)
+
+        with context.begin_transaction():
+            context.run_migrations()
+
+
+if context.is_offline_mode():
+    run_migrations_offline()
+else:
+    run_migrations_online()
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/070a0eb89e88_drop_category_tables.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/070a0eb89e88_drop_category_tables.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-"""drop category tables
-
-Revision ID: 070a0eb89e88
-Revises: bba1e2ef5153
-Create Date: 2019-09-20 13:04:52.423483
-
-"""
-from alembic import op
-import sqlalchemy as sa
-
-
-# revision identifiers, used by Alembic.
-revision = "070a0eb89e88"
-down_revision = "bba1e2ef5153"
-branch_labels = None
-depends_on = None
-
-
-def upgrade():
-    with op.batch_alter_table("object_class") as batch_op:
-        batch_op.drop_column("category_id")
-        batch_op.create_check_constraint("type_id", "`type_id` = 1")
-    with op.batch_alter_table("object") as batch_op:
-        batch_op.drop_column("category_id")
-        batch_op.create_check_constraint("type_id", "`type_id` = 1")
-    op.drop_table("object_class_category")
-    op.drop_table("object_category")
-    # Sneak some patches in:
-    # 1. DEFAULT NULL for parameter_tag.description
-    with op.batch_alter_table("parameter_tag") as batch_op:
-        batch_op.alter_column("description", server_default=sa.null())
-    # 2. Rename parameter_definition constraints. Basically remove and readd them so they get the correct name
-    insp = sa.inspect(op.get_bind())
-    parameter_definition_pk_name = insp.get_pk_constraint("parameter_definition")["name"]
-    parameter_definition_fk_names = [x["name"] for x in insp.get_foreign_keys("parameter_definition")]
-    with op.batch_alter_table("parameter_definition") as batch_op:
-        if parameter_definition_pk_name == "pk_parameter":
-            batch_op.drop_constraint("pk_parameter", type_="primary")
-            batch_op.create_primary_key(None, ["id"])
-        if "fk_parameter_commit_id_commit" in parameter_definition_fk_names:
-            batch_op.drop_constraint("fk_parameter_commit_id_commit", type_="foreignkey")
-            batch_op.create_foreign_key(None, "commit", ["commit_id"], ["id"])
-
-
-def downgrade():
-    pass
+"""drop category tables
+
+Revision ID: 070a0eb89e88
+Revises: bba1e2ef5153
+Create Date: 2019-09-20 13:04:52.423483
+
+"""
+from alembic import op
+import sqlalchemy as sa
+
+
+# revision identifiers, used by Alembic.
+revision = "070a0eb89e88"
+down_revision = "bba1e2ef5153"
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    with op.batch_alter_table("object_class") as batch_op:
+        batch_op.drop_column("category_id")
+        batch_op.create_check_constraint("type_id", "`type_id` = 1")
+    with op.batch_alter_table("object") as batch_op:
+        batch_op.drop_column("category_id")
+        batch_op.create_check_constraint("type_id", "`type_id` = 1")
+    op.drop_table("object_class_category")
+    op.drop_table("object_category")
+    # Sneak some patches in:
+    # 1. DEFAULT NULL for parameter_tag.description
+    with op.batch_alter_table("parameter_tag") as batch_op:
+        batch_op.alter_column("description", server_default=sa.null())
+    # 2. Rename parameter_definition constraints. Basically remove and readd them so they get the correct name
+    insp = sa.inspect(op.get_bind())
+    parameter_definition_pk_name = insp.get_pk_constraint("parameter_definition")["name"]
+    parameter_definition_fk_names = [x["name"] for x in insp.get_foreign_keys("parameter_definition")]
+    with op.batch_alter_table("parameter_definition") as batch_op:
+        if parameter_definition_pk_name == "pk_parameter":
+            batch_op.drop_constraint("pk_parameter", type_="primary")
+            batch_op.create_primary_key(None, ["id"])
+        if "fk_parameter_commit_id_commit" in parameter_definition_fk_names:
+            batch_op.drop_constraint("fk_parameter_commit_id_commit", type_="foreignkey")
+            batch_op.create_foreign_key(None, "commit", ["commit_id"], ["id"])
+
+
+def downgrade():
+    pass
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/1892adebc00f_create_metadata_tables.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/51fd7b69acf7_add_parameter_tag_and_parameter_value_list.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,71 +1,66 @@
-"""create metadata tables
-
-Revision ID: 1892adebc00f
-Revises: defbda3bf2b5
-Create Date: 2020-10-05 14:51:13.787685
-
-"""
-from alembic import op
-import sqlalchemy as sa
-
-
-# revision identifiers, used by Alembic.
-revision = '1892adebc00f'
-down_revision = 'defbda3bf2b5'
-branch_labels = None
-depends_on = None
-
-
-def upgrade():
-    m = sa.MetaData(op.get_bind())
-    m.reflect()
-    if "next_id" in m.tables:
-        with op.batch_alter_table("next_id") as batch_op:
-            batch_op.add_column(sa.Column("metadata_id", sa.Integer, server_default=sa.null()))
-            batch_op.add_column(sa.Column("parameter_value_metadata_id", sa.Integer, server_default=sa.null()))
-            batch_op.add_column(sa.Column("entity_metadata_id", sa.Integer, server_default=sa.null()))
-    op.create_table(
-        "metadata",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("name", sa.String(155), nullable=False),
-        sa.Column("value", sa.String(255), nullable=False),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("name", "value"),
-    )
-    op.create_table(
-        "parameter_value_metadata",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column(
-            "parameter_value_id",
-            sa.Integer,
-            sa.ForeignKey("parameter_value.id", onupdate="CASCADE", ondelete="CASCADE"),
-            nullable=False,
-        ),
-        sa.Column(
-            "metadata_id",
-            sa.Integer,
-            sa.ForeignKey("metadata.id", onupdate="CASCADE", ondelete="CASCADE"),
-            nullable=False,
-        ),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("parameter_value_id", "metadata_id"),
-    )
-    op.create_table(
-        "entity_metadata",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column(
-            "entity_id", sa.Integer, sa.ForeignKey("entity.id", onupdate="CASCADE", ondelete="CASCADE"), nullable=False
-        ),
-        sa.Column(
-            "metadata_id",
-            sa.Integer,
-            sa.ForeignKey("metadata.id", onupdate="CASCADE", ondelete="CASCADE"),
-            nullable=False,
-        ),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("entity_id", "metadata_id"),
-    )
-
-
-def downgrade():
-    pass
+"""Add parameter_tag, parameter_definition_tag, and parameter_value_list
+
+Revision ID: 51fd7b69acf7
+Revises: 8c19c53d5701
+Create Date: 2019-01-25 15:47:05.100028
+
+"""
+from alembic import op
+import sqlalchemy as sa
+
+
+# revision identifiers, used by Alembic.
+revision = "51fd7b69acf7"
+down_revision = "8c19c53d5701"
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    m = sa.MetaData(op.get_bind())
+    m.reflect()
+    if "next_id" in m.tables:
+        with op.batch_alter_table("next_id") as batch_op:
+            batch_op.add_column(sa.Column("parameter_tag_id", sa.Integer))
+            batch_op.add_column(sa.Column("parameter_value_list_id", sa.Integer))
+            batch_op.add_column(sa.Column("parameter_definition_tag_id", sa.Integer))
+    op.create_table(
+        "parameter_tag",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("tag", sa.String(155), nullable=False, unique=True),
+        sa.Column("description", sa.Unicode(255)),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+    )
+    op.create_table(
+        "parameter_definition_tag",
+        sa.Column("id", sa.Integer, nullable=False, primary_key=True),
+        sa.Column("parameter_definition_id", sa.Integer, sa.ForeignKey("parameter_definition.id"), nullable=False),
+        sa.Column("parameter_tag_id", sa.Integer, sa.ForeignKey("parameter_tag.id"), nullable=False),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+        sa.UniqueConstraint("parameter_definition_id", "parameter_tag_id"),
+    )
+    op.create_table(
+        "parameter_value_list",
+        sa.Column("id", sa.Integer, primary_key=True, nullable=False),
+        sa.Column("name", sa.String(155), nullable=False),
+        sa.Column("value_index", sa.Integer, primary_key=True, nullable=False),
+        sa.Column("value", sa.Unicode(255), nullable=False),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+    )
+    with op.batch_alter_table("parameter_definition") as batch_op:
+        batch_op.add_column(sa.Column("parameter_value_list_id", sa.Integer))
+
+
+def downgrade():
+    try:
+        with op.batch_alter_table("next_id") as batch_op:
+            batch_op.drop_column("parameter_tag_id")
+            batch_op.drop_column("parameter_value_list_id")
+            batch_op.drop_column("parameter_definition_tag_id")
+    except (sa.exc.NoSuchTableError, sa.exc.OperationalError):
+        pass
+    with op.batch_alter_table("parameter_definition") as batch_op:
+        batch_op.drop_column("parameter_value_list_id")
+    op.drop_table("parameter_value_list")
+    op.drop_table("parameter_definition_tag")
+    op.drop_table("parameter_tag")
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/1e4997105288_separate_type_from_value.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/1e4997105288_separate_type_from_value.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-"""Separate type from value
-
-Revision ID: 1e4997105288
-Revises: fbb540efbf15
-Create Date: 2021-05-26 16:00:49.244440
-
-"""
-
-import json
-from alembic import op
-import sqlalchemy as sa
-from sqlalchemy.ext.automap import automap_base
-from sqlalchemy.orm import sessionmaker
-
-
-# revision identifiers, used by Alembic.
-revision = '1e4997105288'
-down_revision = 'fbb540efbf15'
-branch_labels = None
-depends_on = None
-
-LONGTEXT_LENGTH = 2 ** 32 - 1
-
-
-def upgrade():
-    conn = op.get_bind()
-    Session = sessionmaker(bind=conn)
-    session = Session()
-    Base = automap_base()
-    Base.prepare(conn, reflect=True)
-    # Get items to update
-    pd_items = _get_items(session, Base, "parameter_definition")
-    pv_items = _get_items(session, Base, "parameter_value")
-    pvl_items = _get_pvl_items(session, Base)
-    # Alter tables
-    with op.batch_alter_table("parameter_definition") as batch_op:
-        batch_op.drop_column('data_type')
-        batch_op.drop_column("default_value")
-        batch_op.add_column(sa.Column("default_value", sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()))
-        batch_op.add_column(sa.Column('default_type', sa.String(length=255), nullable=True))
-    with op.batch_alter_table("parameter_value") as batch_op:
-        batch_op.drop_column("value")
-        batch_op.add_column(sa.Column("value", sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()))
-        batch_op.add_column(sa.Column('type', sa.String(length=255), nullable=True))
-    with op.batch_alter_table("parameter_value_list") as batch_op:
-        batch_op.drop_column("value")
-        batch_op.add_column(sa.Column("value", sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()))
-    # Do update items
-    Base = automap_base()
-    Base.prepare(conn, reflect=True)
-    session.bulk_update_mappings(Base.classes.parameter_definition, pd_items)
-    session.bulk_update_mappings(Base.classes.parameter_value, pv_items)
-    session.bulk_update_mappings(Base.classes.parameter_value_list, pvl_items)
-    session.commit()
-
-
-def _get_items(session, Base, tablename):
-    fields = {"parameter_definition": ("default_value", "default_type"), "parameter_value": ("value", "type")}[
-        tablename
-    ]
-    items = []
-    for row in session.query(getattr(Base.classes, tablename)):
-        value = getattr(row, fields[0], None)
-        if value is None:
-            continue
-        parsed_value = json.loads(value)
-        type_ = parsed_value.pop("type", None) if isinstance(parsed_value, dict) else None
-        value = bytes(json.dumps(parsed_value), "UTF8")
-        item = dict(zip(fields, (value, type_)))
-        item["id"] = row.id
-        items.append(item)
-    return items
-
-
-def _get_pvl_items(session, Base):
-    return [
-        {"id": row.id, "value_index": row.value_index, "value": bytes(row.value, "UTF8")}
-        for row in session.query(Base.classes.parameter_value_list)
-    ]
-
-
-def downgrade():
-    pass
+"""Separate type from value
+
+Revision ID: 1e4997105288
+Revises: fbb540efbf15
+Create Date: 2021-05-26 16:00:49.244440
+
+"""
+
+import json
+from alembic import op
+import sqlalchemy as sa
+from sqlalchemy.ext.automap import automap_base
+from sqlalchemy.orm import sessionmaker
+
+
+# revision identifiers, used by Alembic.
+revision = '1e4997105288'
+down_revision = 'fbb540efbf15'
+branch_labels = None
+depends_on = None
+
+LONGTEXT_LENGTH = 2 ** 32 - 1
+
+
+def upgrade():
+    conn = op.get_bind()
+    Session = sessionmaker(bind=conn)
+    session = Session()
+    Base = automap_base()
+    Base.prepare(conn, reflect=True)
+    # Get items to update
+    pd_items = _get_items(session, Base, "parameter_definition")
+    pv_items = _get_items(session, Base, "parameter_value")
+    pvl_items = _get_pvl_items(session, Base)
+    # Alter tables
+    with op.batch_alter_table("parameter_definition") as batch_op:
+        batch_op.drop_column('data_type')
+        batch_op.drop_column("default_value")
+        batch_op.add_column(sa.Column("default_value", sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()))
+        batch_op.add_column(sa.Column('default_type', sa.String(length=255), nullable=True))
+    with op.batch_alter_table("parameter_value") as batch_op:
+        batch_op.drop_column("value")
+        batch_op.add_column(sa.Column("value", sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()))
+        batch_op.add_column(sa.Column('type', sa.String(length=255), nullable=True))
+    with op.batch_alter_table("parameter_value_list") as batch_op:
+        batch_op.drop_column("value")
+        batch_op.add_column(sa.Column("value", sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()))
+    # Do update items
+    Base = automap_base()
+    Base.prepare(conn, reflect=True)
+    session.bulk_update_mappings(Base.classes.parameter_definition, pd_items)
+    session.bulk_update_mappings(Base.classes.parameter_value, pv_items)
+    session.bulk_update_mappings(Base.classes.parameter_value_list, pvl_items)
+    session.commit()
+
+
+def _get_items(session, Base, tablename):
+    fields = {"parameter_definition": ("default_value", "default_type"), "parameter_value": ("value", "type")}[
+        tablename
+    ]
+    items = []
+    for row in session.query(getattr(Base.classes, tablename)):
+        value = getattr(row, fields[0], None)
+        if value is None:
+            continue
+        parsed_value = json.loads(value)
+        type_ = parsed_value.pop("type", None) if isinstance(parsed_value, dict) else None
+        value = bytes(json.dumps(parsed_value), "UTF8")
+        item = dict(zip(fields, (value, type_)))
+        item["id"] = row.id
+        items.append(item)
+    return items
+
+
+def _get_pvl_items(session, Base):
+    return [
+        {"id": row.id, "value_index": row.value_index, "value": bytes(row.value, "UTF8")}
+        for row in session.query(Base.classes.parameter_value_list)
+    ]
+
+
+def downgrade():
+    pass
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/39e860a11b05_add_alternatives_and_scenarios.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/39e860a11b05_add_alternatives_and_scenarios.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,151 +1,151 @@
-"""add alternatives and scenarios
-
-Revision ID: 39e860a11b05
-Revises: 9da58d2def22
-Create Date: 2020-03-05 14:04:00.854936
-
-"""
-from datetime import datetime, timezone
-from alembic import op
-import sqlalchemy as sa
-from sqlalchemy.ext.automap import automap_base
-from sqlalchemy.orm import sessionmaker
-
-
-# revision identifiers, used by Alembic.
-revision = '39e860a11b05'
-down_revision = '9da58d2def22'
-branch_labels = None
-depends_on = None
-
-
-def create_new_tables():
-    op.create_table(
-        "alternative",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("name", sa.Text, nullable=False),
-        sa.Column("description", sa.Text, nullable=True),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("name"),
-    )
-    op.create_table(
-        "scenario",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("name", sa.Text, nullable=False),
-        sa.Column("description", sa.Text, nullable=True),
-        sa.Column("active", sa.Boolean(name="active"), server_default=sa.false(), nullable=False),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("name"),
-    )
-    op.create_table(
-        "scenario_alternative",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("scenario_id", sa.Integer, nullable=False),
-        sa.Column("alternative_id", sa.Integer, nullable=False),
-        sa.Column("rank", sa.Integer, nullable=False),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("scenario_id", "rank"),
-        sa.UniqueConstraint("scenario_id", "alternative_id"),
-        sa.ForeignKeyConstraint(("scenario_id",), ("scenario.id",), onupdate="CASCADE", ondelete="CASCADE"),
-        sa.ForeignKeyConstraint(("alternative_id",), ("alternative.id",), onupdate="CASCADE", ondelete="CASCADE"),
-    )
-
-
-def add_upgrade_comment_to_commits(session, Base):
-    commit = Base.classes.commit(
-        comment="Upgrade database: add scenarios and alternatives.", user="alembic", date=datetime.now(timezone.utc)
-    )
-    session.add(commit)
-    session.commit()
-    return commit.id
-
-
-def add_base_alternative(commit_id, session, Base):
-    alternative = Base.classes.alternative(name="Base", description="Base alternative", commit_id=commit_id)
-    session.add(alternative)
-    session.commit()
-
-
-def commit_ids_for_types(upgrade_commit_id, session, Base):
-    for entity_type in session.query(Base.classes.entity_type).all():
-        entity_type.commit_id = upgrade_commit_id
-    for entity_class_type in session.query(Base.classes.entity_class_type).all():
-        entity_class_type.commit_id = upgrade_commit_id
-    session.commit()
-
-
-def alter_tables_after_update():
-    inspector = sa.inspect(op.get_bind())
-    parameter_value_uq_names = [x["name"] for x in inspector.get_unique_constraints("parameter_value")]
-    with op.batch_alter_table("parameter_value") as batch_op:
-        batch_op.add_column(sa.Column("alternative_id", sa.Integer, nullable=True))
-        batch_op.create_unique_constraint(None, ["parameter_definition_id", "entity_id", "alternative_id"])
-        if "uq_parameter_value_parameter_definition_identity_id" in parameter_value_uq_names:
-            batch_op.drop_constraint("uq_parameter_value_parameter_definition_identity_id")
-
-    op.execute("UPDATE parameter_value SET alternative_id = 1")
-    with op.batch_alter_table("parameter_value") as batch_op:
-        batch_op.alter_column('alternative_id', nullable=False)
-        batch_op.create_foreign_key(
-            None, "alternative", ("alternative_id",), ("id",), onupdate="CASCADE", ondelete="CASCADE"
-        )
-
-    m = sa.MetaData(op.get_bind())
-    m.reflect()
-    if "next_id" in m.tables:
-        with op.batch_alter_table("next_id") as batch_op:
-            batch_op.add_column(sa.Column("alternative_id", sa.Integer, server_default=sa.null()))
-            batch_op.add_column(sa.Column("scenario_id", sa.Integer, server_default=sa.null()))
-            batch_op.add_column(sa.Column("scenario_alternative_id", sa.Integer, server_default=sa.null()))
-        user = "alembic"
-        date = datetime.utcnow()
-        conn = op.get_bind()
-        conn.execute(
-            """
-            UPDATE next_id
-            SET
-                user = :user,
-                date = :date,
-                alternative_id = 2,
-                scenario_id = 1,
-                scenario_alternative_id = 1
-            """,
-            user=user,
-            date=date,
-        )
-
-    with op.batch_alter_table("entity_type") as batch_op:
-        batch_op.alter_column('commit_id', nullable=False)
-    with op.batch_alter_table("entity_class_type") as batch_op:
-        batch_op.alter_column('commit_id', nullable=False)
-
-
-def upgrade():
-    create_new_tables()
-    Session = sessionmaker(bind=op.get_bind())
-    session = Session()
-    Base = automap_base()
-    Base.prepare(op.get_bind(), reflect=True)
-    upgrade_commit_id = add_upgrade_comment_to_commits(session, Base)
-    add_base_alternative(upgrade_commit_id, session, Base)
-    commit_ids_for_types(upgrade_commit_id, session, Base)
-    alter_tables_after_update()
-
-
-def downgrade():
-    with op.batch_alter_table("parameter_value") as batch_op:
-        batch_op.drop_column("alternative_id")
-    op.delete_table("scenario_alternative")
-    op.delete_table("scenario")
-    op.delete_table("alternative")
-    m = sa.MetaData(op.get_bind())
-    m.reflect()
-    if "next_id" in m.tables:
-        with op.batch_alter_table("next_id") as batch_op:
-            batch_op.drop_column("alternative_id")
-            batch_op.drop_column("scenario_id")
-            batch_op.drop_column("scenario_alternative_id")
-    with op.batch_alter_table("entity_type") as batch_op:
-        batch_op.alter_column('commit_id', nullable=True)
-    with op.batch_alter_table("entity_class_type") as batch_op:
-        batch_op.alter_column('commit_id', nullable=True)
+"""add alternatives and scenarios
+
+Revision ID: 39e860a11b05
+Revises: 9da58d2def22
+Create Date: 2020-03-05 14:04:00.854936
+
+"""
+from datetime import datetime, timezone
+from alembic import op
+import sqlalchemy as sa
+from sqlalchemy.ext.automap import automap_base
+from sqlalchemy.orm import sessionmaker
+
+
+# revision identifiers, used by Alembic.
+revision = '39e860a11b05'
+down_revision = '9da58d2def22'
+branch_labels = None
+depends_on = None
+
+
+def create_new_tables():
+    op.create_table(
+        "alternative",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("name", sa.Text, nullable=False),
+        sa.Column("description", sa.Text, nullable=True),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+        sa.UniqueConstraint("name"),
+    )
+    op.create_table(
+        "scenario",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("name", sa.Text, nullable=False),
+        sa.Column("description", sa.Text, nullable=True),
+        sa.Column("active", sa.Boolean(name="active"), server_default=sa.false(), nullable=False),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+        sa.UniqueConstraint("name"),
+    )
+    op.create_table(
+        "scenario_alternative",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("scenario_id", sa.Integer, nullable=False),
+        sa.Column("alternative_id", sa.Integer, nullable=False),
+        sa.Column("rank", sa.Integer, nullable=False),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+        sa.UniqueConstraint("scenario_id", "rank"),
+        sa.UniqueConstraint("scenario_id", "alternative_id"),
+        sa.ForeignKeyConstraint(("scenario_id",), ("scenario.id",), onupdate="CASCADE", ondelete="CASCADE"),
+        sa.ForeignKeyConstraint(("alternative_id",), ("alternative.id",), onupdate="CASCADE", ondelete="CASCADE"),
+    )
+
+
+def add_upgrade_comment_to_commits(session, Base):
+    commit = Base.classes.commit(
+        comment="Upgrade database: add scenarios and alternatives.", user="alembic", date=datetime.now(timezone.utc)
+    )
+    session.add(commit)
+    session.commit()
+    return commit.id
+
+
+def add_base_alternative(commit_id, session, Base):
+    alternative = Base.classes.alternative(name="Base", description="Base alternative", commit_id=commit_id)
+    session.add(alternative)
+    session.commit()
+
+
+def commit_ids_for_types(upgrade_commit_id, session, Base):
+    for entity_type in session.query(Base.classes.entity_type).all():
+        entity_type.commit_id = upgrade_commit_id
+    for entity_class_type in session.query(Base.classes.entity_class_type).all():
+        entity_class_type.commit_id = upgrade_commit_id
+    session.commit()
+
+
+def alter_tables_after_update():
+    inspector = sa.inspect(op.get_bind())
+    parameter_value_uq_names = [x["name"] for x in inspector.get_unique_constraints("parameter_value")]
+    with op.batch_alter_table("parameter_value") as batch_op:
+        batch_op.add_column(sa.Column("alternative_id", sa.Integer, nullable=True))
+        batch_op.create_unique_constraint(None, ["parameter_definition_id", "entity_id", "alternative_id"])
+        if "uq_parameter_value_parameter_definition_identity_id" in parameter_value_uq_names:
+            batch_op.drop_constraint("uq_parameter_value_parameter_definition_identity_id")
+
+    op.execute("UPDATE parameter_value SET alternative_id = 1")
+    with op.batch_alter_table("parameter_value") as batch_op:
+        batch_op.alter_column('alternative_id', nullable=False)
+        batch_op.create_foreign_key(
+            None, "alternative", ("alternative_id",), ("id",), onupdate="CASCADE", ondelete="CASCADE"
+        )
+
+    m = sa.MetaData(op.get_bind())
+    m.reflect()
+    if "next_id" in m.tables:
+        with op.batch_alter_table("next_id") as batch_op:
+            batch_op.add_column(sa.Column("alternative_id", sa.Integer, server_default=sa.null()))
+            batch_op.add_column(sa.Column("scenario_id", sa.Integer, server_default=sa.null()))
+            batch_op.add_column(sa.Column("scenario_alternative_id", sa.Integer, server_default=sa.null()))
+        user = "alembic"
+        date = datetime.utcnow()
+        conn = op.get_bind()
+        conn.execute(
+            """
+            UPDATE next_id
+            SET
+                user = :user,
+                date = :date,
+                alternative_id = 2,
+                scenario_id = 1,
+                scenario_alternative_id = 1
+            """,
+            user=user,
+            date=date,
+        )
+
+    with op.batch_alter_table("entity_type") as batch_op:
+        batch_op.alter_column('commit_id', nullable=False)
+    with op.batch_alter_table("entity_class_type") as batch_op:
+        batch_op.alter_column('commit_id', nullable=False)
+
+
+def upgrade():
+    create_new_tables()
+    Session = sessionmaker(bind=op.get_bind())
+    session = Session()
+    Base = automap_base()
+    Base.prepare(op.get_bind(), reflect=True)
+    upgrade_commit_id = add_upgrade_comment_to_commits(session, Base)
+    add_base_alternative(upgrade_commit_id, session, Base)
+    commit_ids_for_types(upgrade_commit_id, session, Base)
+    alter_tables_after_update()
+
+
+def downgrade():
+    with op.batch_alter_table("parameter_value") as batch_op:
+        batch_op.drop_column("alternative_id")
+    op.delete_table("scenario_alternative")
+    op.delete_table("scenario")
+    op.delete_table("alternative")
+    m = sa.MetaData(op.get_bind())
+    m.reflect()
+    if "next_id" in m.tables:
+        with op.batch_alter_table("next_id") as batch_op:
+            batch_op.drop_column("alternative_id")
+            batch_op.drop_column("scenario_id")
+            batch_op.drop_column("scenario_alternative_id")
+    with op.batch_alter_table("entity_type") as batch_op:
+        batch_op.alter_column('commit_id', nullable=True)
+    with op.batch_alter_table("entity_class_type") as batch_op:
+        batch_op.alter_column('commit_id', nullable=True)
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/51fd7b69acf7_add_parameter_tag_and_parameter_value_list.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/8c19c53d5701_rename_parameter_to_parameter_definition.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,66 +1,56 @@
-"""Add parameter_tag, parameter_definition_tag, and parameter_value_list
-
-Revision ID: 51fd7b69acf7
-Revises: 8c19c53d5701
-Create Date: 2019-01-25 15:47:05.100028
-
-"""
-from alembic import op
-import sqlalchemy as sa
-
-
-# revision identifiers, used by Alembic.
-revision = "51fd7b69acf7"
-down_revision = "8c19c53d5701"
-branch_labels = None
-depends_on = None
-
-
-def upgrade():
-    m = sa.MetaData(op.get_bind())
-    m.reflect()
-    if "next_id" in m.tables:
-        with op.batch_alter_table("next_id") as batch_op:
-            batch_op.add_column(sa.Column("parameter_tag_id", sa.Integer))
-            batch_op.add_column(sa.Column("parameter_value_list_id", sa.Integer))
-            batch_op.add_column(sa.Column("parameter_definition_tag_id", sa.Integer))
-    op.create_table(
-        "parameter_tag",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("tag", sa.String(155), nullable=False, unique=True),
-        sa.Column("description", sa.Unicode(255)),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-    )
-    op.create_table(
-        "parameter_definition_tag",
-        sa.Column("id", sa.Integer, nullable=False, primary_key=True),
-        sa.Column("parameter_definition_id", sa.Integer, sa.ForeignKey("parameter_definition.id"), nullable=False),
-        sa.Column("parameter_tag_id", sa.Integer, sa.ForeignKey("parameter_tag.id"), nullable=False),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("parameter_definition_id", "parameter_tag_id"),
-    )
-    op.create_table(
-        "parameter_value_list",
-        sa.Column("id", sa.Integer, primary_key=True, nullable=False),
-        sa.Column("name", sa.String(155), nullable=False),
-        sa.Column("value_index", sa.Integer, primary_key=True, nullable=False),
-        sa.Column("value", sa.Unicode(255), nullable=False),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-    )
-    with op.batch_alter_table("parameter_definition") as batch_op:
-        batch_op.add_column(sa.Column("parameter_value_list_id", sa.Integer))
-
-
-def downgrade():
-    try:
-        with op.batch_alter_table("next_id") as batch_op:
-            batch_op.drop_column("parameter_tag_id")
-            batch_op.drop_column("parameter_value_list_id")
-            batch_op.drop_column("parameter_definition_tag_id")
-    except (sa.exc.NoSuchTableError, sa.exc.OperationalError):
-        pass
-    with op.batch_alter_table("parameter_definition") as batch_op:
-        batch_op.drop_column("parameter_value_list_id")
-    op.drop_table("parameter_value_list")
-    op.drop_table("parameter_definition_tag")
-    op.drop_table("parameter_tag")
+"""rename parameter to parameter_definition
+
+Revision ID: 8c19c53d5701
+Revises:
+Create Date: 2019-01-24 16:47:21.493240
+
+"""
+from alembic import op
+import sqlalchemy as sa
+
+naming_convention = {
+    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
+    "uq": "uq_%(table_name)s_%(column_0N_name)s",
+}
+
+# revision identifiers, used by Alembic.
+revision = "8c19c53d5701"
+down_revision = None
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    m = sa.MetaData(op.get_bind())
+    m.reflect()
+    if "next_id" in m.tables:
+        with op.batch_alter_table("next_id") as batch_op:
+            batch_op.alter_column("parameter_id", new_column_name="parameter_definition_id", type_=sa.Integer)
+    with op.batch_alter_table("parameter_value", naming_convention=naming_convention) as batch_op:
+        batch_op.alter_column("parameter_id", new_column_name="parameter_definition_id", type_=sa.Integer)
+        batch_op.drop_constraint("fk_parameter_value_parameter_id_parameter", type_="foreignkey")
+    with op.batch_alter_table("parameter", naming_convention=naming_convention) as batch_op:
+        batch_op.drop_constraint("uq_parameter_name", type_="unique")
+        batch_op.create_unique_constraint("uq_parameter_definition_name", ["name"])
+    op.rename_table("parameter", "parameter_definition")
+    with op.batch_alter_table("parameter_value", naming_convention=naming_convention) as batch_op:
+        batch_op.create_foreign_key(
+            "fk_parameter_value_parameter_definition_id_parameter_definition",
+            "parameter_definition",
+            ["parameter_definition_id"],
+            ["id"],
+        )
+
+
+def downgrade():
+    m = sa.MetaData(op.get_bind())
+    m.reflect()
+    if "next_id" in m.tables:
+        with op.batch_alter_table("next_id") as batch_op:
+            batch_op.alter_column("parameter_definition_id", new_column_name="parameter_id")
+    with op.batch_alter_table("parameter_value") as batch_op:
+        batch_op.alter_column("parameter_definition_id", new_column_name="parameter_id", type_=sa.Integer)
+        batch_op.drop_constraint("fk_parameter_value_parameter_definition_id_parameter_definition", type_="foreignkey")
+    op.rename_table("parameter_definition", "parameter")
+    with op.batch_alter_table("parameter_value", naming_convention=naming_convention) as batch_op:
+        batch_op.create_foreign_key("fk_parameter_value_parameter_id_parameter", "parameter", ["parameter_id"], ["id"])
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/738d494a08ac_fix_foreign_key_constraints_in_object_.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/fd542cebf699_drop_on_update_clauses_from_object_and_.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,43 +1,41 @@
-"""fix foreign key constraints in object and relationship tables
-
-Revision ID: 738d494a08ac
-Revises: 1e4997105288
-Create Date: 2022-01-05 09:18:48.858784
-
-"""
-from alembic import op
-from spinedb_api.helpers import naming_convention
-
-
-# revision identifiers, used by Alembic.
-revision = '738d494a08ac'
-down_revision = '1e4997105288'
-branch_labels = None
-depends_on = None
-
-
-def upgrade():
-    with op.batch_alter_table("object", naming_convention=naming_convention) as batch_op:
-        batch_op.drop_constraint('fk_object_entity_id_entity', type_='foreignkey')
-        batch_op.create_foreign_key(
-            op.f('fk_object_entity_id_entity'),
-            'entity',
-            ['entity_id', 'type_id'],
-            ['id', 'type_id'],
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        )
-    with op.batch_alter_table("relationship", naming_convention=naming_convention) as batch_op:
-        batch_op.drop_constraint('fk_relationship_entity_id_entity', type_='foreignkey')
-        batch_op.create_foreign_key(
-            op.f('fk_relationship_entity_id_entity'),
-            'entity',
-            ['entity_id', 'type_id'],
-            ['id', 'type_id'],
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        )
-
-
-def downgrade():
-    pass
+"""drop on update clauses from object and relationship foreign key constraints
+
+Revision ID: fd542cebf699
+Revises: 738d494a08ac
+Create Date: 2022-01-05 11:00:31.154790
+
+"""
+from alembic import op
+from spinedb_api.helpers import naming_convention
+
+
+# revision identifiers, used by Alembic.
+revision = 'fd542cebf699'
+down_revision = '738d494a08ac'
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    with op.batch_alter_table("object", naming_convention=naming_convention) as batch_op:
+        batch_op.drop_constraint('fk_object_entity_id_entity', type_='foreignkey')
+        batch_op.create_foreign_key(
+            op.f('fk_object_entity_id_entity'),
+            'entity',
+            ['entity_id', 'type_id'],
+            ['id', 'type_id'],
+            ondelete="CASCADE",
+        )
+    with op.batch_alter_table("relationship", naming_convention=naming_convention) as batch_op:
+        batch_op.drop_constraint('fk_relationship_entity_id_entity', type_='foreignkey')
+        batch_op.create_foreign_key(
+            op.f('fk_relationship_entity_id_entity'),
+            'entity',
+            ['entity_id', 'type_id'],
+            ['id', 'type_id'],
+            ondelete="CASCADE",
+        )
+
+
+def downgrade():
+    pass
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/989fccf80441_replace_values_with_reference_to_list_.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/989fccf80441_replace_values_with_reference_to_list_.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,73 +1,73 @@
-"""Replace values with reference to list values
-
-Revision ID: 989fccf80441
-Revises: 0c7d199ae915
-Create Date: 2022-03-14 11:33:13.777028
-
-"""
-from alembic import op
-from sqlalchemy import MetaData
-from sqlalchemy.sql.expression import bindparam
-from sqlalchemy.orm import sessionmaker
-from spinedb_api.check_functions import (
-    replace_default_values_with_list_references,
-    replace_parameter_values_with_list_references,
-)
-from spinedb_api.parameter_value import from_database
-from spinedb_api.helpers import group_concat
-from spinedb_api.exception import SpineIntegrityError
-
-
-# revision identifiers, used by Alembic.
-revision = '989fccf80441'
-down_revision = '0c7d199ae915'
-branch_labels = None
-depends_on = None
-
-
-def upgrade():
-    conn = op.get_bind()
-    Session = sessionmaker(bind=conn)
-    session = Session()
-    meta = MetaData(conn)
-    meta.reflect()
-    list_value = meta.tables["list_value"]
-    parameter_definition = meta.tables["parameter_definition"]
-    parameter_value = meta.tables["parameter_value"]
-    parameter_definitions = {x.id: x._asdict() for x in session.query(parameter_definition)}
-    parameter_values = {x.id: x._asdict() for x in session.query(parameter_value)}
-    parameter_value_lists = {
-        x.parameter_value_list_id: x.value_id_list
-        for x in session.query(
-            list_value.c.parameter_value_list_id,
-            group_concat(list_value.c.id, list_value.c.index).label("value_id_list"),
-        ).group_by(list_value.c.parameter_value_list_id)
-    }
-    list_values = {x.id: from_database(x.value, x.type) for x in session.query(list_value)}
-    pdefs = []
-    for pdef in parameter_definitions.values():
-        try:
-            if replace_default_values_with_list_references(pdef, parameter_value_lists, list_values):
-                pdefs.append(pdef)
-        except SpineIntegrityError:
-            pass
-    pvals = []
-    for pval in parameter_values.values():
-        try:
-            if replace_parameter_values_with_list_references(
-                pval, parameter_definitions, parameter_value_lists, list_values
-            ):
-                pvals.append(pval)
-        except SpineIntegrityError:
-            pass
-    for table, items in ((parameter_definition, pdefs), (parameter_value, pvals)):
-        if not items:
-            continue
-        upd = table.update()
-        upd = upd.where(table.c.id == bindparam("id"))
-        upd = upd.values({key: bindparam(key) for key in table.columns.keys()})
-        conn.execute(upd, items)
-
-
-def downgrade():
-    pass
+"""Replace values with reference to list values
+
+Revision ID: 989fccf80441
+Revises: 0c7d199ae915
+Create Date: 2022-03-14 11:33:13.777028
+
+"""
+from alembic import op
+from sqlalchemy import MetaData
+from sqlalchemy.sql.expression import bindparam
+from sqlalchemy.orm import sessionmaker
+from spinedb_api.check_functions import (
+    replace_default_values_with_list_references,
+    replace_parameter_values_with_list_references,
+)
+from spinedb_api.parameter_value import from_database
+from spinedb_api.helpers import group_concat
+from spinedb_api.exception import SpineIntegrityError
+
+
+# revision identifiers, used by Alembic.
+revision = '989fccf80441'
+down_revision = '0c7d199ae915'
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    conn = op.get_bind()
+    Session = sessionmaker(bind=conn)
+    session = Session()
+    meta = MetaData(conn)
+    meta.reflect()
+    list_value = meta.tables["list_value"]
+    parameter_definition = meta.tables["parameter_definition"]
+    parameter_value = meta.tables["parameter_value"]
+    parameter_definitions = {x.id: x._asdict() for x in session.query(parameter_definition)}
+    parameter_values = {x.id: x._asdict() for x in session.query(parameter_value)}
+    parameter_value_lists = {
+        x.parameter_value_list_id: x.value_id_list
+        for x in session.query(
+            list_value.c.parameter_value_list_id,
+            group_concat(list_value.c.id, list_value.c.index).label("value_id_list"),
+        ).group_by(list_value.c.parameter_value_list_id)
+    }
+    list_values = {x.id: from_database(x.value, x.type) for x in session.query(list_value)}
+    pdefs = []
+    for pdef in parameter_definitions.values():
+        try:
+            if replace_default_values_with_list_references(pdef, parameter_value_lists, list_values):
+                pdefs.append(pdef)
+        except SpineIntegrityError:
+            pass
+    pvals = []
+    for pval in parameter_values.values():
+        try:
+            if replace_parameter_values_with_list_references(
+                pval, parameter_definitions, parameter_value_lists, list_values
+            ):
+                pvals.append(pval)
+        except SpineIntegrityError:
+            pass
+    for table, items in ((parameter_definition, pdefs), (parameter_value, pvals)):
+        if not items:
+            continue
+        upd = table.update()
+        upd = upd.where(table.c.id == bindparam("id"))
+        upd = upd.values({key: bindparam(key) for key in table.columns.keys()})
+        conn.execute(upd, items)
+
+
+def downgrade():
+    pass
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/9da58d2def22_create_entity_group_table.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/9da58d2def22_create_entity_group_table.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,42 +1,42 @@
-"""create entity_group_table
-
-Revision ID: 9da58d2def22
-Revises: 070a0eb89e88
-Create Date: 2020-06-09 21:31:07.912724
-
-"""
-from alembic import op
-import sqlalchemy as sa
-
-
-# revision identifiers, used by Alembic.
-revision = '9da58d2def22'
-down_revision = '070a0eb89e88'
-branch_labels = None
-depends_on = None
-
-
-def upgrade():
-    m = sa.MetaData(op.get_bind())
-    m.reflect()
-    if "next_id" in m.tables:
-        with op.batch_alter_table("next_id") as batch_op:
-            batch_op.add_column(sa.Column("entity_group_id", sa.Integer))
-    op.create_table(
-        "entity_group",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("entity_id", sa.Integer, nullable=False),
-        sa.Column("entity_class_id", sa.Integer, nullable=False),
-        sa.Column("member_id", sa.Integer, nullable=False),
-        sa.UniqueConstraint("entity_id", "member_id"),
-        sa.ForeignKeyConstraint(
-            ("entity_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
-        ),
-        sa.ForeignKeyConstraint(
-            ("member_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
-        ),
-    )
-
-
-def downgrade():
-    pass
+"""create entity_group_table
+
+Revision ID: 9da58d2def22
+Revises: 070a0eb89e88
+Create Date: 2020-06-09 21:31:07.912724
+
+"""
+from alembic import op
+import sqlalchemy as sa
+
+
+# revision identifiers, used by Alembic.
+revision = '9da58d2def22'
+down_revision = '070a0eb89e88'
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    m = sa.MetaData(op.get_bind())
+    m.reflect()
+    if "next_id" in m.tables:
+        with op.batch_alter_table("next_id") as batch_op:
+            batch_op.add_column(sa.Column("entity_group_id", sa.Integer))
+    op.create_table(
+        "entity_group",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("entity_id", sa.Integer, nullable=False),
+        sa.Column("entity_class_id", sa.Integer, nullable=False),
+        sa.Column("member_id", sa.Integer, nullable=False),
+        sa.UniqueConstraint("entity_id", "member_id"),
+        sa.ForeignKeyConstraint(
+            ("entity_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
+        ),
+        sa.ForeignKeyConstraint(
+            ("member_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
+        ),
+    )
+
+
+def downgrade():
+    pass
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/bba1e2ef5153_move_to_entity_based_design.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/bba1e2ef5153_move_to_entity_based_design.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,473 +1,473 @@
-"""move to entity based design
-
-Revision ID: bba1e2ef5153
-Revises: bf255c179bce
-Create Date: 2019-09-17 13:38:53.437119
-
-"""
-from datetime import datetime
-from alembic import op
-import sqlalchemy as sa
-
-
-# revision identifiers, used by Alembic.
-revision = "bba1e2ef5153"
-down_revision = "bf255c179bce"
-branch_labels = None
-depends_on = None
-
-
-def create_new_tables():
-    op.create_table(
-        "entity_class_type",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("name", sa.Unicode(255), nullable=False, unique=True),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-    )
-    op.create_table(
-        "entity_class",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column(
-            "type_id",
-            sa.Integer,
-            sa.ForeignKey("entity_class_type.id", onupdate="CASCADE", ondelete="CASCADE"),
-            nullable=False,
-        ),
-        sa.Column("name", sa.Unicode(255), nullable=False, unique=True),
-        sa.Column("description", sa.Unicode(255), server_default=sa.null()),
-        sa.Column("display_order", sa.Integer, server_default="99"),
-        sa.Column("display_icon", sa.BigInteger, server_default=sa.null()),
-        sa.Column("hidden", sa.Integer, server_default="0"),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-    )
-    op.create_table(
-        "temp_relationship_class",
-        sa.Column("entity_class_id", sa.Integer),
-        sa.Column("type_id", sa.Integer, nullable=False),
-    )
-    op.create_table(
-        "relationship_entity_class",
-        sa.Column("entity_class_id", sa.Integer, primary_key=True),
-        sa.Column("dimension", sa.Integer, primary_key=True),
-        sa.Column("member_class_id", sa.Integer, nullable=False),
-        sa.Column("member_class_type_id", sa.Integer, nullable=False),
-        sa.ForeignKeyConstraint(
-            ("member_class_id", "member_class_type_id"),
-            ("entity_class.id", "entity_class.type_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    op.create_table(
-        "entity_type",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("name", sa.Unicode(255), nullable=False, unique=True),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-    )
-    op.create_table(
-        "entity",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("type_id", sa.Integer, sa.ForeignKey("entity_type.id", onupdate="CASCADE", ondelete="CASCADE")),
-        sa.Column("class_id", sa.Integer, sa.ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE")),
-        sa.Column("name", sa.Unicode(255), nullable=False),
-        sa.Column("description", sa.String(255), server_default=sa.null()),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("class_id", "name"),
-    )
-    op.create_table(
-        "temp_relationship",
-        sa.Column("entity_id", sa.Integer),
-        sa.Column("entity_class_id", sa.Integer, nullable=False),
-        sa.Column("type_id", sa.Integer, nullable=False),
-    )
-    op.create_table(
-        "relationship_entity",
-        sa.Column("entity_id", sa.Integer, primary_key=True),
-        sa.Column("entity_class_id", sa.Integer, nullable=False),
-        sa.Column("dimension", sa.Integer, primary_key=True),
-        sa.Column("member_id", sa.Integer, nullable=False),
-        sa.Column("member_class_id", sa.Integer, nullable=False),
-        sa.ForeignKeyConstraint(
-            ("member_id", "member_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
-        ),
-        sa.ForeignKeyConstraint(
-            ("entity_class_id", "dimension", "member_class_id"),
-            (
-                "relationship_entity_class.entity_class_id",
-                "relationship_entity_class.dimension",
-                "relationship_entity_class.member_class_id",
-            ),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-
-
-def insert_into_new_tables():
-    # Easy ones
-    op.execute("""INSERT INTO entity_class_type (id, name) VALUES (1, "object")""")
-    op.execute("""INSERT INTO entity_class_type (id, name) VALUES (2, "relationship")""")
-    op.execute("""INSERT INTO entity_type (id, name) VALUES (1, "object")""")
-    op.execute("""INSERT INTO entity_type (id, name) VALUES (2, "relationship")""")
-    # More difficult ones
-    conn = op.get_bind()
-    meta = sa.MetaData(conn)
-    meta.reflect()
-    # entity class level
-    entity_classes = [
-        {
-            "type_id": 1,
-            "name": r["name"],
-            "description": r["description"],
-            "display_order": r["display_order"],
-            "display_icon": r["display_icon"],
-            "hidden": r["hidden"],
-            "commit_id": r["commit_id"],
-        }
-        for r in conn.execute(
-            "SELECT name, description, display_order, display_icon, hidden, commit_id FROM object_class"
-        )
-    ] + [
-        {
-            "type_id": 2,
-            "name": r["name"],
-            "description": None,
-            "display_order": None,
-            "display_icon": None,
-            "hidden": r["hidden"],
-            "commit_id": r["commit_id"],
-        }
-        for r in conn.execute("SELECT name, hidden, commit_id FROM relationship_class GROUP BY name")
-    ]
-    op.bulk_insert(meta.tables["entity_class"], entity_classes)
-    # Id mappings
-    obj_cls_to_ent_cls = {
-        r["object_class_id"]: r["entity_class_id"]
-        for r in conn.execute(
-            """
-            SELECT object_class.id AS object_class_id, entity_class.id AS entity_class_id
-            FROM object_class, entity_class
-            WHERE entity_class.type_id = 1
-            AND object_class.name = entity_class.name
-            """
-        )
-    }
-    rel_cls_to_ent_cls = {
-        r["relationship_class_id"]: r["entity_class_id"]
-        for r in conn.execute(
-            """
-            SELECT relationship_class.id AS relationship_class_id, entity_class.id AS entity_class_id
-            FROM relationship_class, entity_class
-            WHERE entity_class.type_id = 2
-            AND relationship_class.name = entity_class.name
-			GROUP BY relationship_class_id, entity_class_id
-            """
-        )
-    }
-    temp_relationship_classes = [
-        {"entity_class_id": r["id"], "type_id": 2, "commit_id": r["commit_id"]}
-        for r in conn.execute("SELECT id, commit_id FROM entity_class WHERE type_id = 2")
-    ]
-    op.bulk_insert(meta.tables["temp_relationship_class"], temp_relationship_classes)
-    relationship_entity_classes = [
-        {
-            "entity_class_id": rel_cls_to_ent_cls[r["id"]],
-            "dimension": r["dimension"],
-            "member_class_id": obj_cls_to_ent_cls[r["object_class_id"]],
-            "member_class_type_id": 1,
-            "commit_id": r["commit_id"],
-        }
-        for r in conn.execute("SELECT id, dimension, object_class_id, commit_id FROM relationship_class")
-    ]
-    op.bulk_insert(meta.tables["relationship_entity_class"], relationship_entity_classes)
-    # entity level
-    entities = [
-        {"type_id": 1, "class_id": obj_cls_to_ent_cls[r["class_id"]], "name": r["name"], "commit_id": r["commit_id"]}
-        for r in conn.execute("SELECT class_id, name, commit_id FROM object")
-    ] + [
-        {"type_id": 2, "class_id": rel_cls_to_ent_cls[r["class_id"]], "name": r["name"], "commit_id": r["commit_id"]}
-        for r in conn.execute("SELECT class_id, name, commit_id FROM relationship GROUP BY class_id, name")
-    ]
-    op.bulk_insert(meta.tables["entity"], entities)
-    # Id mappings
-    obj_to_ent = {
-        r["object_id"]: r["entity_id"]
-        for r in conn.execute(
-            """
-            SELECT object.id AS object_id, entity.id AS entity_id
-            FROM object, entity
-            WHERE entity.type_id = 1
-            AND object.name = entity.name
-            """
-        )
-    }
-    rel_to_ent = {
-        r["relationship_id"]: r["entity_id"]
-        for r in conn.execute(
-            """
-            SELECT relationship.id AS relationship_id, entity.id AS entity_id
-            FROM relationship, entity
-            WHERE entity.type_id = 2
-            AND relationship.name = entity.name
-			GROUP BY relationship_id, entity_id
-            """
-        )
-    }
-    temp_relationships = [
-        {"entity_id": r["id"], "entity_class_id": r["class_id"], "type_id": 2, "commit_id": r["commit_id"]}
-        for r in conn.execute("SELECT id, class_id, commit_id FROM entity WHERE type_id = 2")
-    ]
-    op.bulk_insert(meta.tables["temp_relationship"], temp_relationships)
-    relationship_entities = [
-        {
-            "entity_id": rel_to_ent[r["id"]],
-            "entity_class_id": rel_cls_to_ent_cls[r["class_id"]],
-            "dimension": r["dimension"],
-            "member_id": obj_to_ent[r["object_id"]],
-            "member_class_id": obj_cls_to_ent_cls[r["object_class_id"]],
-            "commit_id": r["commit_id"],
-        }
-        for r in conn.execute(
-            """
-            SELECT r.id, r.class_id, r.dimension, o.class_id AS object_class_id, r.object_id, r.commit_id
-            FROM relationship AS r, object AS o
-            WHERE r.object_id = o.id
-            """
-        )
-    ]
-    op.bulk_insert(meta.tables["relationship_entity"], relationship_entities)
-    # Return metadata and id mappings
-    return (meta, obj_cls_to_ent_cls, rel_cls_to_ent_cls, obj_to_ent, rel_to_ent)
-
-
-def alter_tables_before_update(meta):
-    with op.batch_alter_table("object_class") as batch_op:
-        batch_op.add_column(sa.Column("entity_class_id", sa.Integer))
-        batch_op.add_column(sa.Column("type_id", sa.Integer))
-        batch_op.create_foreign_key(
-            None,
-            "entity_class",
-            ["entity_class_id", "type_id"],
-            ["id", "type_id"],
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        )
-    with op.batch_alter_table("object") as batch_op:
-        batch_op.add_column(sa.Column("entity_id", sa.Integer))
-        batch_op.add_column(sa.Column("type_id", sa.Integer))
-        batch_op.create_foreign_key(
-            None, "entity", ["entity_id", "type_id"], ["id", "type_id"], onupdate="CASCADE", ondelete="CASCADE"
-        )
-    with op.batch_alter_table("parameter_definition") as batch_op:
-        batch_op.add_column(sa.Column("entity_class_id", sa.Integer))
-        batch_op.drop_constraint("uq_parameter_definition_name_class_id", type_="unique")
-        batch_op.create_foreign_key(
-            None, "entity_class", ["entity_class_id"], ["id"], onupdate="CASCADE", ondelete="CASCADE"
-        )
-        batch_op.create_unique_constraint(None, ["name", "entity_class_id"])
-    with op.batch_alter_table("parameter_value") as batch_op:
-        batch_op.alter_column("parameter_definition_id", nullable=False)
-        batch_op.add_column(sa.Column("entity_id", sa.Integer))
-        batch_op.add_column(sa.Column("entity_class_id", sa.Integer))
-        batch_op.create_unique_constraint(None, ["parameter_definition_id", "entity_id"])
-        batch_op.create_foreign_key(
-            None, "entity", ["entity_id", "entity_class_id"], ["id", "class_id"], onupdate="CASCADE", ondelete="CASCADE"
-        )
-    # Can you believe some dbs still have the `parameter` table after revision 8c19c53d5701 ???
-    if "parameter" in sa.inspect(op.get_bind()).get_table_names():
-        op.drop_table("parameter")
-    if "next_id" not in meta.tables:
-        return
-    with op.batch_alter_table("next_id") as batch_op:
-        batch_op.drop_column("object_class_id")
-        batch_op.drop_column("object_id")
-        batch_op.drop_column("relationship_class_id")
-        batch_op.drop_column("relationship_id")
-        batch_op.add_column(sa.Column("entity_class_type_id", sa.Integer, server_default=sa.null()))
-        batch_op.add_column(sa.Column("entity_class_id", sa.Integer, server_default=sa.null()))
-        batch_op.add_column(sa.Column("entity_type_id", sa.Integer, server_default=sa.null()))
-        batch_op.add_column(sa.Column("entity_id", sa.Integer, server_default=sa.null()))
-
-
-def update_tables(meta, obj_cls_to_ent_cls, rel_cls_to_ent_cls, obj_to_ent, rel_to_ent):
-    conn = op.get_bind()
-    ent_to_ent_cls = {r["id"]: r["class_id"] for r in conn.execute("SELECT id, class_id FROM entity")}
-    for object_class_id, entity_class_id in obj_cls_to_ent_cls.items():
-        conn.execute(
-            "UPDATE object_class SET entity_class_id = :entity_class_id, type_id = 1 WHERE id = :object_class_id",
-            entity_class_id=entity_class_id,
-            object_class_id=object_class_id,
-        )
-        conn.execute(
-            """
-            UPDATE parameter_definition SET entity_class_id = :entity_class_id
-            WHERE object_class_id = :object_class_id
-            """,
-            entity_class_id=entity_class_id,
-            object_class_id=object_class_id,
-        )
-    for relationship_class_id, entity_class_id in rel_cls_to_ent_cls.items():
-        conn.execute(
-            """
-            UPDATE parameter_definition SET entity_class_id = :entity_class_id
-            WHERE relationship_class_id = :relationship_class_id
-            """,
-            entity_class_id=entity_class_id,
-            relationship_class_id=relationship_class_id,
-        )
-    for object_id, entity_id in obj_to_ent.items():
-        conn.execute(
-            "UPDATE object SET entity_id = :entity_id, type_id = 1 WHERE id = :object_id",
-            entity_id=entity_id,
-            object_id=object_id,
-        )
-        entity_class_id = ent_to_ent_cls[entity_id]
-        conn.execute(
-            """
-            UPDATE parameter_value SET entity_id = :entity_id, entity_class_id = :entity_class_id
-            WHERE object_id = :object_id
-            """,
-            entity_id=entity_id,
-            entity_class_id=entity_class_id,
-            object_id=object_id,
-        )
-    for relationship_id, entity_id in rel_to_ent.items():
-        entity_class_id = ent_to_ent_cls[entity_id]
-        conn.execute(
-            """
-            UPDATE parameter_value SET entity_id = :entity_id, entity_class_id = :entity_class_id
-            WHERE relationship_id = :relationship_id
-            """,
-            entity_id=entity_id,
-            entity_class_id=entity_class_id,
-            relationship_id=relationship_id,
-        )
-    # Clean our potential mess.
-    # E.g., I've seen parameter definitions with an invalid relationship_class_id for some reason...!
-    conn.execute("DELETE FROM parameter_definition WHERE entity_class_id IS NULL")
-    conn.execute("DELETE FROM parameter_value WHERE entity_class_id IS NULL OR entity_id IS NULL")
-    if "next_id" not in meta.tables:
-        return
-    row = conn.execute("SELECT MAX(id) FROM entity_class").fetchone()
-    entity_class_id = row[0] + 1 if row else 1
-    row = conn.execute("SELECT MAX(id) FROM entity").fetchone()
-    entity_id = row[0] + 1 if row else 1
-    user = "alembic"
-    date = datetime.utcnow()
-    conn.execute(
-        """
-        UPDATE next_id
-        SET
-            user = :user,
-            date = :date,
-            entity_class_type_id = 3,
-            entity_type_id = 3,
-            entity_class_id = :entity_class_id,
-            entity_id = :entity_id
-        """,
-        user=user,
-        date=date,
-        entity_class_id=entity_class_id,
-        entity_id=entity_id,
-    )
-
-
-def alter_tables_after_update(meta):
-    with op.batch_alter_table("object_class") as batch_op:
-        batch_op.drop_column("id")
-        batch_op.drop_column("name")
-        batch_op.drop_column("description")
-        batch_op.drop_column("display_order")
-        batch_op.drop_column("display_icon")
-        batch_op.drop_column("hidden")
-        batch_op.drop_column("commit_id")
-        batch_op.alter_column("type_id", nullable=False)
-        batch_op.create_check_constraint("type_id", "`type_id` = 1")
-        batch_op.create_primary_key(None, ["entity_class_id"])
-    with op.batch_alter_table("object") as batch_op:
-        batch_op.drop_column("class_id")
-        batch_op.drop_column("id")
-        batch_op.drop_column("name")
-        batch_op.drop_column("description")
-        batch_op.drop_column("commit_id")
-        batch_op.alter_column("type_id", nullable=False)
-        batch_op.create_check_constraint("type_id", "`type_id` = 1")
-        batch_op.create_primary_key(None, ["entity_id"])
-    op.drop_table("relationship_class")
-    op.drop_table("relationship")
-    op.rename_table("temp_relationship_class", "relationship_class")
-    op.rename_table("temp_relationship", "relationship")
-    with op.batch_alter_table("relationship_class") as batch_op:
-        batch_op.create_check_constraint("type_id", "`type_id` = 2")
-        batch_op.create_primary_key(None, ["entity_class_id"])
-        batch_op.create_foreign_key(
-            None,
-            "entity_class",
-            ("entity_class_id", "type_id"),
-            ("id", "type_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        )
-    with op.batch_alter_table("relationship") as batch_op:
-        batch_op.create_check_constraint("type_id", "`type_id` = 2")
-        batch_op.create_primary_key(None, ["entity_id"])
-        batch_op.create_foreign_key(
-            None, "entity", ("entity_id", "type_id"), ("id", "type_id"), onupdate="CASCADE", ondelete="CASCADE"
-        )
-    with op.batch_alter_table("relationship_entity_class") as batch_op:
-        batch_op.create_foreign_key(
-            None, "relationship_class", ["entity_class_id"], ["entity_class_id"], onupdate="CASCADE", ondelete="CASCADE"
-        )
-        batch_op.create_check_constraint("member_class_type_id", "`member_class_type_id` != 2")
-    with op.batch_alter_table("relationship_entity") as batch_op:
-        batch_op.create_foreign_key(
-            None,
-            "relationship",
-            ["entity_id", "entity_class_id"],
-            ["entity_id", "entity_class_id"],
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        )
-    with op.batch_alter_table("parameter_definition") as batch_op:
-        batch_op.drop_constraint("ck_parameter_obj_or_rel_class_id_is_not_null")
-        batch_op.drop_column("object_class_id")
-        batch_op.drop_column("relationship_class_id")
-        batch_op.alter_column("entity_class_id", nullable=False)
-        dummy_relationship_class = next(
-            (x.name for x in meta.tables["parameter_definition"].c if x.name.startswith("dummy_relationship_class")),
-            None,
-        )
-        if dummy_relationship_class:
-            batch_op.drop_column(dummy_relationship_class)
-    with op.batch_alter_table("parameter_value") as batch_op:
-        batch_op.drop_constraint("ck_parameter_value_obj_or_rel_id_is_not_null")
-        batch_op.drop_column("object_id")
-        batch_op.drop_column("relationship_id")
-        batch_op.alter_column("entity_class_id", nullable=False)
-        batch_op.alter_column("entity_id", nullable=False)
-        dummy_relationship = next(
-            (x.name for x in meta.tables["parameter_value"].c if x.name.startswith("dummy_relationship")), None
-        )
-        if dummy_relationship:
-            batch_op.drop_column(dummy_relationship)
-        batch_op.create_foreign_key(
-            None,
-            "parameter_definition",
-            ["parameter_definition_id", "entity_class_id"],
-            ["id", "entity_class_id"],
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        )
-
-
-def upgrade():
-    create_new_tables()
-    meta, obj_cls_to_ent_cls, rel_cls_to_ent_cls, obj_to_ent, rel_to_ent = insert_into_new_tables()
-    alter_tables_before_update(meta)
-    update_tables(meta, obj_cls_to_ent_cls, rel_cls_to_ent_cls, obj_to_ent, rel_to_ent)
-    alter_tables_after_update(meta)
-
-
-def downgrade():
-    # TODO: try and do this???
-    pass
+"""move to entity based design
+
+Revision ID: bba1e2ef5153
+Revises: bf255c179bce
+Create Date: 2019-09-17 13:38:53.437119
+
+"""
+from datetime import datetime
+from alembic import op
+import sqlalchemy as sa
+
+
+# revision identifiers, used by Alembic.
+revision = "bba1e2ef5153"
+down_revision = "bf255c179bce"
+branch_labels = None
+depends_on = None
+
+
+def create_new_tables():
+    op.create_table(
+        "entity_class_type",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("name", sa.Unicode(255), nullable=False, unique=True),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+    )
+    op.create_table(
+        "entity_class",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column(
+            "type_id",
+            sa.Integer,
+            sa.ForeignKey("entity_class_type.id", onupdate="CASCADE", ondelete="CASCADE"),
+            nullable=False,
+        ),
+        sa.Column("name", sa.Unicode(255), nullable=False, unique=True),
+        sa.Column("description", sa.Unicode(255), server_default=sa.null()),
+        sa.Column("display_order", sa.Integer, server_default="99"),
+        sa.Column("display_icon", sa.BigInteger, server_default=sa.null()),
+        sa.Column("hidden", sa.Integer, server_default="0"),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+    )
+    op.create_table(
+        "temp_relationship_class",
+        sa.Column("entity_class_id", sa.Integer),
+        sa.Column("type_id", sa.Integer, nullable=False),
+    )
+    op.create_table(
+        "relationship_entity_class",
+        sa.Column("entity_class_id", sa.Integer, primary_key=True),
+        sa.Column("dimension", sa.Integer, primary_key=True),
+        sa.Column("member_class_id", sa.Integer, nullable=False),
+        sa.Column("member_class_type_id", sa.Integer, nullable=False),
+        sa.ForeignKeyConstraint(
+            ("member_class_id", "member_class_type_id"),
+            ("entity_class.id", "entity_class.type_id"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+    op.create_table(
+        "entity_type",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("name", sa.Unicode(255), nullable=False, unique=True),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+    )
+    op.create_table(
+        "entity",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("type_id", sa.Integer, sa.ForeignKey("entity_type.id", onupdate="CASCADE", ondelete="CASCADE")),
+        sa.Column("class_id", sa.Integer, sa.ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE")),
+        sa.Column("name", sa.Unicode(255), nullable=False),
+        sa.Column("description", sa.String(255), server_default=sa.null()),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+        sa.UniqueConstraint("class_id", "name"),
+    )
+    op.create_table(
+        "temp_relationship",
+        sa.Column("entity_id", sa.Integer),
+        sa.Column("entity_class_id", sa.Integer, nullable=False),
+        sa.Column("type_id", sa.Integer, nullable=False),
+    )
+    op.create_table(
+        "relationship_entity",
+        sa.Column("entity_id", sa.Integer, primary_key=True),
+        sa.Column("entity_class_id", sa.Integer, nullable=False),
+        sa.Column("dimension", sa.Integer, primary_key=True),
+        sa.Column("member_id", sa.Integer, nullable=False),
+        sa.Column("member_class_id", sa.Integer, nullable=False),
+        sa.ForeignKeyConstraint(
+            ("member_id", "member_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
+        ),
+        sa.ForeignKeyConstraint(
+            ("entity_class_id", "dimension", "member_class_id"),
+            (
+                "relationship_entity_class.entity_class_id",
+                "relationship_entity_class.dimension",
+                "relationship_entity_class.member_class_id",
+            ),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+
+
+def insert_into_new_tables():
+    # Easy ones
+    op.execute("""INSERT INTO entity_class_type (id, name) VALUES (1, "object")""")
+    op.execute("""INSERT INTO entity_class_type (id, name) VALUES (2, "relationship")""")
+    op.execute("""INSERT INTO entity_type (id, name) VALUES (1, "object")""")
+    op.execute("""INSERT INTO entity_type (id, name) VALUES (2, "relationship")""")
+    # More difficult ones
+    conn = op.get_bind()
+    meta = sa.MetaData(conn)
+    meta.reflect()
+    # entity class level
+    entity_classes = [
+        {
+            "type_id": 1,
+            "name": r["name"],
+            "description": r["description"],
+            "display_order": r["display_order"],
+            "display_icon": r["display_icon"],
+            "hidden": r["hidden"],
+            "commit_id": r["commit_id"],
+        }
+        for r in conn.execute(
+            "SELECT name, description, display_order, display_icon, hidden, commit_id FROM object_class"
+        )
+    ] + [
+        {
+            "type_id": 2,
+            "name": r["name"],
+            "description": None,
+            "display_order": None,
+            "display_icon": None,
+            "hidden": r["hidden"],
+            "commit_id": r["commit_id"],
+        }
+        for r in conn.execute("SELECT name, hidden, commit_id FROM relationship_class GROUP BY name")
+    ]
+    op.bulk_insert(meta.tables["entity_class"], entity_classes)
+    # Id mappings
+    obj_cls_to_ent_cls = {
+        r["object_class_id"]: r["entity_class_id"]
+        for r in conn.execute(
+            """
+            SELECT object_class.id AS object_class_id, entity_class.id AS entity_class_id
+            FROM object_class, entity_class
+            WHERE entity_class.type_id = 1
+            AND object_class.name = entity_class.name
+            """
+        )
+    }
+    rel_cls_to_ent_cls = {
+        r["relationship_class_id"]: r["entity_class_id"]
+        for r in conn.execute(
+            """
+            SELECT relationship_class.id AS relationship_class_id, entity_class.id AS entity_class_id
+            FROM relationship_class, entity_class
+            WHERE entity_class.type_id = 2
+            AND relationship_class.name = entity_class.name
+			GROUP BY relationship_class_id, entity_class_id
+            """
+        )
+    }
+    temp_relationship_classes = [
+        {"entity_class_id": r["id"], "type_id": 2, "commit_id": r["commit_id"]}
+        for r in conn.execute("SELECT id, commit_id FROM entity_class WHERE type_id = 2")
+    ]
+    op.bulk_insert(meta.tables["temp_relationship_class"], temp_relationship_classes)
+    relationship_entity_classes = [
+        {
+            "entity_class_id": rel_cls_to_ent_cls[r["id"]],
+            "dimension": r["dimension"],
+            "member_class_id": obj_cls_to_ent_cls[r["object_class_id"]],
+            "member_class_type_id": 1,
+            "commit_id": r["commit_id"],
+        }
+        for r in conn.execute("SELECT id, dimension, object_class_id, commit_id FROM relationship_class")
+    ]
+    op.bulk_insert(meta.tables["relationship_entity_class"], relationship_entity_classes)
+    # entity level
+    entities = [
+        {"type_id": 1, "class_id": obj_cls_to_ent_cls[r["class_id"]], "name": r["name"], "commit_id": r["commit_id"]}
+        for r in conn.execute("SELECT class_id, name, commit_id FROM object")
+    ] + [
+        {"type_id": 2, "class_id": rel_cls_to_ent_cls[r["class_id"]], "name": r["name"], "commit_id": r["commit_id"]}
+        for r in conn.execute("SELECT class_id, name, commit_id FROM relationship GROUP BY class_id, name")
+    ]
+    op.bulk_insert(meta.tables["entity"], entities)
+    # Id mappings
+    obj_to_ent = {
+        r["object_id"]: r["entity_id"]
+        for r in conn.execute(
+            """
+            SELECT object.id AS object_id, entity.id AS entity_id
+            FROM object, entity
+            WHERE entity.type_id = 1
+            AND object.name = entity.name
+            """
+        )
+    }
+    rel_to_ent = {
+        r["relationship_id"]: r["entity_id"]
+        for r in conn.execute(
+            """
+            SELECT relationship.id AS relationship_id, entity.id AS entity_id
+            FROM relationship, entity
+            WHERE entity.type_id = 2
+            AND relationship.name = entity.name
+			GROUP BY relationship_id, entity_id
+            """
+        )
+    }
+    temp_relationships = [
+        {"entity_id": r["id"], "entity_class_id": r["class_id"], "type_id": 2, "commit_id": r["commit_id"]}
+        for r in conn.execute("SELECT id, class_id, commit_id FROM entity WHERE type_id = 2")
+    ]
+    op.bulk_insert(meta.tables["temp_relationship"], temp_relationships)
+    relationship_entities = [
+        {
+            "entity_id": rel_to_ent[r["id"]],
+            "entity_class_id": rel_cls_to_ent_cls[r["class_id"]],
+            "dimension": r["dimension"],
+            "member_id": obj_to_ent[r["object_id"]],
+            "member_class_id": obj_cls_to_ent_cls[r["object_class_id"]],
+            "commit_id": r["commit_id"],
+        }
+        for r in conn.execute(
+            """
+            SELECT r.id, r.class_id, r.dimension, o.class_id AS object_class_id, r.object_id, r.commit_id
+            FROM relationship AS r, object AS o
+            WHERE r.object_id = o.id
+            """
+        )
+    ]
+    op.bulk_insert(meta.tables["relationship_entity"], relationship_entities)
+    # Return metadata and id mappings
+    return (meta, obj_cls_to_ent_cls, rel_cls_to_ent_cls, obj_to_ent, rel_to_ent)
+
+
+def alter_tables_before_update(meta):
+    with op.batch_alter_table("object_class") as batch_op:
+        batch_op.add_column(sa.Column("entity_class_id", sa.Integer))
+        batch_op.add_column(sa.Column("type_id", sa.Integer))
+        batch_op.create_foreign_key(
+            None,
+            "entity_class",
+            ["entity_class_id", "type_id"],
+            ["id", "type_id"],
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        )
+    with op.batch_alter_table("object") as batch_op:
+        batch_op.add_column(sa.Column("entity_id", sa.Integer))
+        batch_op.add_column(sa.Column("type_id", sa.Integer))
+        batch_op.create_foreign_key(
+            None, "entity", ["entity_id", "type_id"], ["id", "type_id"], onupdate="CASCADE", ondelete="CASCADE"
+        )
+    with op.batch_alter_table("parameter_definition") as batch_op:
+        batch_op.add_column(sa.Column("entity_class_id", sa.Integer))
+        batch_op.drop_constraint("uq_parameter_definition_name_class_id", type_="unique")
+        batch_op.create_foreign_key(
+            None, "entity_class", ["entity_class_id"], ["id"], onupdate="CASCADE", ondelete="CASCADE"
+        )
+        batch_op.create_unique_constraint(None, ["name", "entity_class_id"])
+    with op.batch_alter_table("parameter_value") as batch_op:
+        batch_op.alter_column("parameter_definition_id", nullable=False)
+        batch_op.add_column(sa.Column("entity_id", sa.Integer))
+        batch_op.add_column(sa.Column("entity_class_id", sa.Integer))
+        batch_op.create_unique_constraint(None, ["parameter_definition_id", "entity_id"])
+        batch_op.create_foreign_key(
+            None, "entity", ["entity_id", "entity_class_id"], ["id", "class_id"], onupdate="CASCADE", ondelete="CASCADE"
+        )
+    # Can you believe some dbs still have the `parameter` table after revision 8c19c53d5701 ???
+    if "parameter" in sa.inspect(op.get_bind()).get_table_names():
+        op.drop_table("parameter")
+    if "next_id" not in meta.tables:
+        return
+    with op.batch_alter_table("next_id") as batch_op:
+        batch_op.drop_column("object_class_id")
+        batch_op.drop_column("object_id")
+        batch_op.drop_column("relationship_class_id")
+        batch_op.drop_column("relationship_id")
+        batch_op.add_column(sa.Column("entity_class_type_id", sa.Integer, server_default=sa.null()))
+        batch_op.add_column(sa.Column("entity_class_id", sa.Integer, server_default=sa.null()))
+        batch_op.add_column(sa.Column("entity_type_id", sa.Integer, server_default=sa.null()))
+        batch_op.add_column(sa.Column("entity_id", sa.Integer, server_default=sa.null()))
+
+
+def update_tables(meta, obj_cls_to_ent_cls, rel_cls_to_ent_cls, obj_to_ent, rel_to_ent):
+    conn = op.get_bind()
+    ent_to_ent_cls = {r["id"]: r["class_id"] for r in conn.execute("SELECT id, class_id FROM entity")}
+    for object_class_id, entity_class_id in obj_cls_to_ent_cls.items():
+        conn.execute(
+            "UPDATE object_class SET entity_class_id = :entity_class_id, type_id = 1 WHERE id = :object_class_id",
+            entity_class_id=entity_class_id,
+            object_class_id=object_class_id,
+        )
+        conn.execute(
+            """
+            UPDATE parameter_definition SET entity_class_id = :entity_class_id
+            WHERE object_class_id = :object_class_id
+            """,
+            entity_class_id=entity_class_id,
+            object_class_id=object_class_id,
+        )
+    for relationship_class_id, entity_class_id in rel_cls_to_ent_cls.items():
+        conn.execute(
+            """
+            UPDATE parameter_definition SET entity_class_id = :entity_class_id
+            WHERE relationship_class_id = :relationship_class_id
+            """,
+            entity_class_id=entity_class_id,
+            relationship_class_id=relationship_class_id,
+        )
+    for object_id, entity_id in obj_to_ent.items():
+        conn.execute(
+            "UPDATE object SET entity_id = :entity_id, type_id = 1 WHERE id = :object_id",
+            entity_id=entity_id,
+            object_id=object_id,
+        )
+        entity_class_id = ent_to_ent_cls[entity_id]
+        conn.execute(
+            """
+            UPDATE parameter_value SET entity_id = :entity_id, entity_class_id = :entity_class_id
+            WHERE object_id = :object_id
+            """,
+            entity_id=entity_id,
+            entity_class_id=entity_class_id,
+            object_id=object_id,
+        )
+    for relationship_id, entity_id in rel_to_ent.items():
+        entity_class_id = ent_to_ent_cls[entity_id]
+        conn.execute(
+            """
+            UPDATE parameter_value SET entity_id = :entity_id, entity_class_id = :entity_class_id
+            WHERE relationship_id = :relationship_id
+            """,
+            entity_id=entity_id,
+            entity_class_id=entity_class_id,
+            relationship_id=relationship_id,
+        )
+    # Clean our potential mess.
+    # E.g., I've seen parameter definitions with an invalid relationship_class_id for some reason...!
+    conn.execute("DELETE FROM parameter_definition WHERE entity_class_id IS NULL")
+    conn.execute("DELETE FROM parameter_value WHERE entity_class_id IS NULL OR entity_id IS NULL")
+    if "next_id" not in meta.tables:
+        return
+    row = conn.execute("SELECT MAX(id) FROM entity_class").fetchone()
+    entity_class_id = row[0] + 1 if row else 1
+    row = conn.execute("SELECT MAX(id) FROM entity").fetchone()
+    entity_id = row[0] + 1 if row else 1
+    user = "alembic"
+    date = datetime.utcnow()
+    conn.execute(
+        """
+        UPDATE next_id
+        SET
+            user = :user,
+            date = :date,
+            entity_class_type_id = 3,
+            entity_type_id = 3,
+            entity_class_id = :entity_class_id,
+            entity_id = :entity_id
+        """,
+        user=user,
+        date=date,
+        entity_class_id=entity_class_id,
+        entity_id=entity_id,
+    )
+
+
+def alter_tables_after_update(meta):
+    with op.batch_alter_table("object_class") as batch_op:
+        batch_op.drop_column("id")
+        batch_op.drop_column("name")
+        batch_op.drop_column("description")
+        batch_op.drop_column("display_order")
+        batch_op.drop_column("display_icon")
+        batch_op.drop_column("hidden")
+        batch_op.drop_column("commit_id")
+        batch_op.alter_column("type_id", nullable=False)
+        batch_op.create_check_constraint("type_id", "`type_id` = 1")
+        batch_op.create_primary_key(None, ["entity_class_id"])
+    with op.batch_alter_table("object") as batch_op:
+        batch_op.drop_column("class_id")
+        batch_op.drop_column("id")
+        batch_op.drop_column("name")
+        batch_op.drop_column("description")
+        batch_op.drop_column("commit_id")
+        batch_op.alter_column("type_id", nullable=False)
+        batch_op.create_check_constraint("type_id", "`type_id` = 1")
+        batch_op.create_primary_key(None, ["entity_id"])
+    op.drop_table("relationship_class")
+    op.drop_table("relationship")
+    op.rename_table("temp_relationship_class", "relationship_class")
+    op.rename_table("temp_relationship", "relationship")
+    with op.batch_alter_table("relationship_class") as batch_op:
+        batch_op.create_check_constraint("type_id", "`type_id` = 2")
+        batch_op.create_primary_key(None, ["entity_class_id"])
+        batch_op.create_foreign_key(
+            None,
+            "entity_class",
+            ("entity_class_id", "type_id"),
+            ("id", "type_id"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        )
+    with op.batch_alter_table("relationship") as batch_op:
+        batch_op.create_check_constraint("type_id", "`type_id` = 2")
+        batch_op.create_primary_key(None, ["entity_id"])
+        batch_op.create_foreign_key(
+            None, "entity", ("entity_id", "type_id"), ("id", "type_id"), onupdate="CASCADE", ondelete="CASCADE"
+        )
+    with op.batch_alter_table("relationship_entity_class") as batch_op:
+        batch_op.create_foreign_key(
+            None, "relationship_class", ["entity_class_id"], ["entity_class_id"], onupdate="CASCADE", ondelete="CASCADE"
+        )
+        batch_op.create_check_constraint("member_class_type_id", "`member_class_type_id` != 2")
+    with op.batch_alter_table("relationship_entity") as batch_op:
+        batch_op.create_foreign_key(
+            None,
+            "relationship",
+            ["entity_id", "entity_class_id"],
+            ["entity_id", "entity_class_id"],
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        )
+    with op.batch_alter_table("parameter_definition") as batch_op:
+        batch_op.drop_constraint("ck_parameter_obj_or_rel_class_id_is_not_null")
+        batch_op.drop_column("object_class_id")
+        batch_op.drop_column("relationship_class_id")
+        batch_op.alter_column("entity_class_id", nullable=False)
+        dummy_relationship_class = next(
+            (x.name for x in meta.tables["parameter_definition"].c if x.name.startswith("dummy_relationship_class")),
+            None,
+        )
+        if dummy_relationship_class:
+            batch_op.drop_column(dummy_relationship_class)
+    with op.batch_alter_table("parameter_value") as batch_op:
+        batch_op.drop_constraint("ck_parameter_value_obj_or_rel_id_is_not_null")
+        batch_op.drop_column("object_id")
+        batch_op.drop_column("relationship_id")
+        batch_op.alter_column("entity_class_id", nullable=False)
+        batch_op.alter_column("entity_id", nullable=False)
+        dummy_relationship = next(
+            (x.name for x in meta.tables["parameter_value"].c if x.name.startswith("dummy_relationship")), None
+        )
+        if dummy_relationship:
+            batch_op.drop_column(dummy_relationship)
+        batch_op.create_foreign_key(
+            None,
+            "parameter_definition",
+            ["parameter_definition_id", "entity_class_id"],
+            ["id", "entity_class_id"],
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        )
+
+
+def upgrade():
+    create_new_tables()
+    meta, obj_cls_to_ent_cls, rel_cls_to_ent_cls, obj_to_ent, rel_to_ent = insert_into_new_tables()
+    alter_tables_before_update(meta)
+    update_tables(meta, obj_cls_to_ent_cls, rel_cls_to_ent_cls, obj_to_ent, rel_to_ent)
+    alter_tables_after_update(meta)
+
+
+def downgrade():
+    # TODO: try and do this???
+    pass
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/bf255c179bce_get_rid_of_unused_fields_in_parameter_.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/bf255c179bce_get_rid_of_unused_fields_in_parameter_.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,82 +1,82 @@
-"""get rid of unused fields in parameter tables and update unique constraints
-
-Revision ID: bf255c179bce
-Revises: 51fd7b69acf7
-Create Date: 2019-03-26 15:34:26.550171
-
-"""
-from alembic import op
-import sqlalchemy as sa
-
-naming_convention = {
-    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
-    "uq": "uq_%(table_name)s_%(column_0N_name)s",
-}
-
-
-# revision identifiers, used by Alembic.
-revision = "bf255c179bce"
-down_revision = "51fd7b69acf7"
-branch_labels = None
-depends_on = None
-
-
-def upgrade():
-    with op.batch_alter_table("parameter_definition") as batch_op:
-        batch_op.drop_column("can_have_time_series", type_=sa.Integer)
-        batch_op.drop_column("can_have_time_pattern", type_=sa.Integer)
-        batch_op.drop_column("can_be_stochastic", type_=sa.Integer)
-        batch_op.drop_column("is_mandatory", type_=sa.Integer)
-        batch_op.drop_column("precision", type_=sa.Integer)
-        batch_op.drop_column("unit", type_=sa.String(155))
-        batch_op.drop_column("minimum_value", type_=sa.Float)
-        batch_op.drop_column("maximum_value", type_=sa.Float)
-    # Move value to json
-    op.execute("UPDATE parameter_value SET json = value WHERE json IS NULL and value IS NOT NULL")
-    with op.batch_alter_table("parameter_value") as batch_op:
-        batch_op.drop_column("index", type_=sa.Integer)
-        batch_op.drop_column("value", type_=sa.String(155))
-        batch_op.drop_column("expression", type_=sa.String(155))
-        batch_op.drop_column("time_pattern", type_=sa.String(155))
-        batch_op.drop_column("time_series_id", type_=sa.String(155))
-        batch_op.drop_column("stochastic_model_id", type_=sa.String(155))
-        batch_op.alter_column("json", new_column_name="value", type_=sa.String(155))
-    # Update primary keys
-    with op.batch_alter_table("object", naming_convention=naming_convention) as batch_op:
-        batch_op.drop_constraint("uq_object_name", type_="unique")
-        batch_op.create_unique_constraint("uq_object_name_class_id", ["name", "class_id"])
-    with op.batch_alter_table("relationship", naming_convention=naming_convention) as batch_op:
-        batch_op.create_unique_constraint("uq_relationship_name_class_id_dimension", ["name", "class_id", "dimension"])
-    with op.batch_alter_table("parameter_definition", naming_convention=naming_convention) as batch_op:
-        batch_op.drop_constraint("uq_parameter_definition_name", type_="unique")
-        batch_op.create_unique_constraint(
-            "uq_parameter_definition_name_class_id", ["name", "object_class_id", "relationship_class_id"]
-        )
-
-
-def downgrade():
-    with op.batch_alter_table("parameter_definition") as batch_op:
-        batch_op.add_column(sa.Column("can_have_time_series", sa.Integer, default=0))
-        batch_op.add_column(sa.Column("can_have_time_pattern", sa.Integer, default=1))
-        batch_op.add_column(sa.Column("can_be_stochastic", sa.Integer, default=0))
-        batch_op.add_column(sa.Column("is_mandatory", sa.Integer, default=0))
-        batch_op.add_column(sa.Column("precision", sa.Integer, default=2))
-        batch_op.add_column(sa.Column("unit", sa.String(155)))
-        batch_op.add_column(sa.Column("minimum_value", sa.Float))
-        batch_op.add_column(sa.Column("maximum_value", sa.Float))
-    with op.batch_alter_table("parameter_value") as batch_op:
-        batch_op.add_column(sa.Column("index", sa.Integer, default=1))
-        batch_op.add_column(sa.Column("json", sa.String(255)))
-        batch_op.add_column(sa.Column("expression", sa.String(255)))
-        batch_op.add_column(sa.Column("time_pattern", sa.String(155)))
-        batch_op.add_column(sa.Column("time_series_id", sa.Integer))
-        batch_op.add_column(sa.Column("stochastic_model_id", sa.Integer))
-    # Update primary keys
-    with op.batch_alter_table("object") as batch_op:
-        batch_op.drop_constraint("uq_object_name_class_id")
-        batch_op.create_unique_constraint("uq_object_name", ["name"])
-    with op.batch_alter_table("relationship") as batch_op:
-        batch_op.drop_constraint("uq_relationship_name_class_id_dimension")
-    with op.batch_alter_table("parameter_definition") as batch_op:
-        batch_op.drop_constraint("uq_parameter_definition_name_class_id")
-        batch_op.create_unique_constraint("uq_parameter_definition_name", ["name"])
+"""get rid of unused fields in parameter tables and update unique constraints
+
+Revision ID: bf255c179bce
+Revises: 51fd7b69acf7
+Create Date: 2019-03-26 15:34:26.550171
+
+"""
+from alembic import op
+import sqlalchemy as sa
+
+naming_convention = {
+    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
+    "uq": "uq_%(table_name)s_%(column_0N_name)s",
+}
+
+
+# revision identifiers, used by Alembic.
+revision = "bf255c179bce"
+down_revision = "51fd7b69acf7"
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    with op.batch_alter_table("parameter_definition") as batch_op:
+        batch_op.drop_column("can_have_time_series", type_=sa.Integer)
+        batch_op.drop_column("can_have_time_pattern", type_=sa.Integer)
+        batch_op.drop_column("can_be_stochastic", type_=sa.Integer)
+        batch_op.drop_column("is_mandatory", type_=sa.Integer)
+        batch_op.drop_column("precision", type_=sa.Integer)
+        batch_op.drop_column("unit", type_=sa.String(155))
+        batch_op.drop_column("minimum_value", type_=sa.Float)
+        batch_op.drop_column("maximum_value", type_=sa.Float)
+    # Move value to json
+    op.execute("UPDATE parameter_value SET json = value WHERE json IS NULL and value IS NOT NULL")
+    with op.batch_alter_table("parameter_value") as batch_op:
+        batch_op.drop_column("index", type_=sa.Integer)
+        batch_op.drop_column("value", type_=sa.String(155))
+        batch_op.drop_column("expression", type_=sa.String(155))
+        batch_op.drop_column("time_pattern", type_=sa.String(155))
+        batch_op.drop_column("time_series_id", type_=sa.String(155))
+        batch_op.drop_column("stochastic_model_id", type_=sa.String(155))
+        batch_op.alter_column("json", new_column_name="value", type_=sa.String(155))
+    # Update primary keys
+    with op.batch_alter_table("object", naming_convention=naming_convention) as batch_op:
+        batch_op.drop_constraint("uq_object_name", type_="unique")
+        batch_op.create_unique_constraint("uq_object_name_class_id", ["name", "class_id"])
+    with op.batch_alter_table("relationship", naming_convention=naming_convention) as batch_op:
+        batch_op.create_unique_constraint("uq_relationship_name_class_id_dimension", ["name", "class_id", "dimension"])
+    with op.batch_alter_table("parameter_definition", naming_convention=naming_convention) as batch_op:
+        batch_op.drop_constraint("uq_parameter_definition_name", type_="unique")
+        batch_op.create_unique_constraint(
+            "uq_parameter_definition_name_class_id", ["name", "object_class_id", "relationship_class_id"]
+        )
+
+
+def downgrade():
+    with op.batch_alter_table("parameter_definition") as batch_op:
+        batch_op.add_column(sa.Column("can_have_time_series", sa.Integer, default=0))
+        batch_op.add_column(sa.Column("can_have_time_pattern", sa.Integer, default=1))
+        batch_op.add_column(sa.Column("can_be_stochastic", sa.Integer, default=0))
+        batch_op.add_column(sa.Column("is_mandatory", sa.Integer, default=0))
+        batch_op.add_column(sa.Column("precision", sa.Integer, default=2))
+        batch_op.add_column(sa.Column("unit", sa.String(155)))
+        batch_op.add_column(sa.Column("minimum_value", sa.Float))
+        batch_op.add_column(sa.Column("maximum_value", sa.Float))
+    with op.batch_alter_table("parameter_value") as batch_op:
+        batch_op.add_column(sa.Column("index", sa.Integer, default=1))
+        batch_op.add_column(sa.Column("json", sa.String(255)))
+        batch_op.add_column(sa.Column("expression", sa.String(255)))
+        batch_op.add_column(sa.Column("time_pattern", sa.String(155)))
+        batch_op.add_column(sa.Column("time_series_id", sa.Integer))
+        batch_op.add_column(sa.Column("stochastic_model_id", sa.Integer))
+    # Update primary keys
+    with op.batch_alter_table("object") as batch_op:
+        batch_op.drop_constraint("uq_object_name_class_id")
+        batch_op.create_unique_constraint("uq_object_name", ["name"])
+    with op.batch_alter_table("relationship") as batch_op:
+        batch_op.drop_constraint("uq_relationship_name_class_id_dimension")
+    with op.batch_alter_table("parameter_definition") as batch_op:
+        batch_op.drop_constraint("uq_parameter_definition_name_class_id")
+        batch_op.create_unique_constraint("uq_parameter_definition_name", ["name"])
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic/versions/defbda3bf2b5_add_tool_feature_tables.py` & `spinedb_api-0.30.4/spinedb_api/alembic/versions/defbda3bf2b5_add_tool_feature_tables.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,90 +1,90 @@
-"""add tool feature tables
-
-Revision ID: defbda3bf2b5
-Revises: 39e860a11b05
-Create Date: 2020-09-01 20:12:57.300147
-
-"""
-from alembic import op
-import sqlalchemy as sa
-
-
-# revision identifiers, used by Alembic.
-revision = 'defbda3bf2b5'
-down_revision = '39e860a11b05'
-branch_labels = None
-depends_on = None
-
-
-def upgrade():
-    m = sa.MetaData(op.get_bind())
-    m.reflect()
-    if "next_id" in m.tables:
-        with op.batch_alter_table("next_id") as batch_op:
-            batch_op.add_column(sa.Column("tool_id", sa.Integer, server_default=sa.null()))
-            batch_op.add_column(sa.Column("feature_id", sa.Integer, server_default=sa.null()))
-            batch_op.add_column(sa.Column("tool_feature_id", sa.Integer, server_default=sa.null()))
-            batch_op.add_column(sa.Column("tool_feature_method_id", sa.Integer, server_default=sa.null()))
-    op.create_table(
-        "tool",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("name", sa.String(155), nullable=False),
-        sa.Column("description", sa.String(255), server_default=sa.null()),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-    )
-    op.create_table(
-        "feature",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("parameter_definition_id", sa.Integer, nullable=False),
-        sa.Column("parameter_value_list_id", sa.Integer, nullable=False),
-        sa.Column("description", sa.String(255), server_default=sa.null()),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("parameter_definition_id", "parameter_value_list_id"),
-        sa.ForeignKeyConstraint(
-            ("parameter_definition_id", "parameter_value_list_id"),
-            ("parameter_definition.id", "parameter_definition.parameter_value_list_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    op.create_table(
-        "tool_feature",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("tool_id", sa.Integer, sa.ForeignKey("tool.id")),
-        sa.Column("feature_id", sa.Integer, nullable=False),
-        sa.Column("parameter_value_list_id", sa.Integer, nullable=False),
-        sa.Column("required", sa.Boolean(name="required"), server_default=sa.false(), nullable=False),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("tool_id", "feature_id"),
-        sa.ForeignKeyConstraint(
-            ("feature_id", "parameter_value_list_id"),
-            ("feature.id", "feature.parameter_value_list_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    op.create_table(
-        "tool_feature_method",
-        sa.Column("id", sa.Integer, primary_key=True),
-        sa.Column("tool_feature_id", sa.Integer, nullable=False),
-        sa.Column("parameter_value_list_id", sa.Integer, nullable=False),
-        sa.Column("method_index", sa.Integer),
-        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint("tool_feature_id", "method_index"),
-        sa.ForeignKeyConstraint(
-            ("tool_feature_id", "parameter_value_list_id"),
-            ("tool_feature.id", "tool_feature.parameter_value_list_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-        sa.ForeignKeyConstraint(
-            ("parameter_value_list_id", "method_index"),
-            ("parameter_value_list.id", "parameter_value_list.value_index"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-
-
-def downgrade():
-    pass
+"""add tool feature tables
+
+Revision ID: defbda3bf2b5
+Revises: 39e860a11b05
+Create Date: 2020-09-01 20:12:57.300147
+
+"""
+from alembic import op
+import sqlalchemy as sa
+
+
+# revision identifiers, used by Alembic.
+revision = 'defbda3bf2b5'
+down_revision = '39e860a11b05'
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    m = sa.MetaData(op.get_bind())
+    m.reflect()
+    if "next_id" in m.tables:
+        with op.batch_alter_table("next_id") as batch_op:
+            batch_op.add_column(sa.Column("tool_id", sa.Integer, server_default=sa.null()))
+            batch_op.add_column(sa.Column("feature_id", sa.Integer, server_default=sa.null()))
+            batch_op.add_column(sa.Column("tool_feature_id", sa.Integer, server_default=sa.null()))
+            batch_op.add_column(sa.Column("tool_feature_method_id", sa.Integer, server_default=sa.null()))
+    op.create_table(
+        "tool",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("name", sa.String(155), nullable=False),
+        sa.Column("description", sa.String(255), server_default=sa.null()),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+    )
+    op.create_table(
+        "feature",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("parameter_definition_id", sa.Integer, nullable=False),
+        sa.Column("parameter_value_list_id", sa.Integer, nullable=False),
+        sa.Column("description", sa.String(255), server_default=sa.null()),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+        sa.UniqueConstraint("parameter_definition_id", "parameter_value_list_id"),
+        sa.ForeignKeyConstraint(
+            ("parameter_definition_id", "parameter_value_list_id"),
+            ("parameter_definition.id", "parameter_definition.parameter_value_list_id"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+    op.create_table(
+        "tool_feature",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("tool_id", sa.Integer, sa.ForeignKey("tool.id")),
+        sa.Column("feature_id", sa.Integer, nullable=False),
+        sa.Column("parameter_value_list_id", sa.Integer, nullable=False),
+        sa.Column("required", sa.Boolean(name="required"), server_default=sa.false(), nullable=False),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+        sa.UniqueConstraint("tool_id", "feature_id"),
+        sa.ForeignKeyConstraint(
+            ("feature_id", "parameter_value_list_id"),
+            ("feature.id", "feature.parameter_value_list_id"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+    op.create_table(
+        "tool_feature_method",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("tool_feature_id", sa.Integer, nullable=False),
+        sa.Column("parameter_value_list_id", sa.Integer, nullable=False),
+        sa.Column("method_index", sa.Integer),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+        sa.UniqueConstraint("tool_feature_id", "method_index"),
+        sa.ForeignKeyConstraint(
+            ("tool_feature_id", "parameter_value_list_id"),
+            ("tool_feature.id", "tool_feature.parameter_value_list_id"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+        sa.ForeignKeyConstraint(
+            ("parameter_value_list_id", "method_index"),
+            ("parameter_value_list.id", "parameter_value_list.value_index"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+
+
+def downgrade():
+    pass
```

### Comparing `spinedb_api-0.30.3/spinedb_api/alembic.ini` & `spinedb_api-0.30.4/spinedb_api/alembic.ini`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,74 +1,74 @@
-# A generic, single database configuration.
-
-[alembic]
-# path to migration scripts
-script_location = alembic
-
-# template used to generate migration files
-# file_template = %%(rev)s_%%(slug)s
-
-# timezone to use when rendering the date
-# within the migration file as well as the filename.
-# string value is passed to dateutil.tz.gettz()
-# leave blank for localtime
-# timezone =
-
-# max length of characters to apply to the
-# "slug" field
-#truncate_slug_length = 40
-
-# set to 'true' to run the environment during
-# the 'revision' command, regardless of autogenerate
-# revision_environment = false
-
-# set to 'true' to allow .pyc and .pyo files without
-# a source .py file to be detected as revisions in the
-# versions/ directory
-# sourceless = false
-
-# version location specification; this defaults
-# to alembic/versions.  When using multiple version
-# directories, initial revisions must be specified with --version-path
-# version_locations = %(here)s/bar %(here)s/bat alembic/versions
-
-# the output encoding used when revision files
-# are written from script.py.mako
-# output_encoding = utf-8
-
-sqlalchemy.url = sqlite:////home/manuelma/Codes/spine/model/examples/data/testsystem2_v2_multiD.sqlite
-
-
-# Logging configuration
-[loggers]
-keys = root,sqlalchemy,alembic
-
-[handlers]
-keys = console
-
-[formatters]
-keys = generic
-
-[logger_root]
-level = WARN
-handlers = console
-qualname =
-
-[logger_sqlalchemy]
-level = WARN
-handlers =
-qualname = sqlalchemy.engine
-
-[logger_alembic]
-level = INFO
-handlers =
-qualname = alembic
-
-[handler_console]
-class = StreamHandler
-args = (sys.stderr,)
-level = NOTSET
-formatter = generic
-
-[formatter_generic]
-format = %(levelname)-5.5s [%(name)s] %(message)s
-datefmt = %H:%M:%S
+# A generic, single database configuration.
+
+[alembic]
+# path to migration scripts
+script_location = alembic
+
+# template used to generate migration files
+# file_template = %%(rev)s_%%(slug)s
+
+# timezone to use when rendering the date
+# within the migration file as well as the filename.
+# string value is passed to dateutil.tz.gettz()
+# leave blank for localtime
+# timezone =
+
+# max length of characters to apply to the
+# "slug" field
+#truncate_slug_length = 40
+
+# set to 'true' to run the environment during
+# the 'revision' command, regardless of autogenerate
+# revision_environment = false
+
+# set to 'true' to allow .pyc and .pyo files without
+# a source .py file to be detected as revisions in the
+# versions/ directory
+# sourceless = false
+
+# version location specification; this defaults
+# to alembic/versions.  When using multiple version
+# directories, initial revisions must be specified with --version-path
+# version_locations = %(here)s/bar %(here)s/bat alembic/versions
+
+# the output encoding used when revision files
+# are written from script.py.mako
+# output_encoding = utf-8
+
+sqlalchemy.url = sqlite:////home/manuelma/Codes/spine/model/examples/data/testsystem2_v2_multiD.sqlite
+
+
+# Logging configuration
+[loggers]
+keys = root,sqlalchemy,alembic
+
+[handlers]
+keys = console
+
+[formatters]
+keys = generic
+
+[logger_root]
+level = WARN
+handlers = console
+qualname =
+
+[logger_sqlalchemy]
+level = WARN
+handlers =
+qualname = sqlalchemy.engine
+
+[logger_alembic]
+level = INFO
+handlers =
+qualname = alembic
+
+[handler_console]
+class = StreamHandler
+args = (sys.stderr,)
+level = NOTSET
+formatter = generic
+
+[formatter_generic]
+format = %(levelname)-5.5s [%(name)s] %(message)s
+datefmt = %H:%M:%S
```

### Comparing `spinedb_api-0.30.3/spinedb_api/check_functions.py` & `spinedb_api-0.30.4/spinedb_api/check_functions.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,643 +1,643 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""Functions for checking whether inserting data into a Spine database leads
-to the violation of integrity constraints.
-
-"""
-
-from .parameter_value import dump_db_value, from_database, ParameterValueFormatError
-from .exception import SpineIntegrityError
-
-# NOTE: We parse each parameter value or default value before accepting it. Is it too much?
-
-
-def check_alternative(item, current_items):
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError("Missing alternative name.")
-    if name in current_items:
-        raise SpineIntegrityError(f"There can't be more than one alternative called '{name}'.", id=current_items[name])
-
-
-def check_scenario(item, current_items):
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError("Missing scenario name.")
-    if name in current_items:
-        raise SpineIntegrityError(f"There can't be more than one scenario called '{name}'.", id=current_items[name])
-
-
-def check_scenario_alternative(item, ids_by_alt_id, ids_by_rank, scenario_names, alternative_names):
-    """
-    Checks if given scenario alternative violates a database's integrity.
-
-    Args:
-        item (dict: a scenario alternative item for checking; must contain the following fields:
-            - "scenario_id": scenario's id
-            - "alternative_id": alternative's id
-            - "rank": alternative's rank within the scenario
-        ids_by_alt_id (dict): a mapping from (scenario id, alternative id) tuples to scenario_alternative ids
-            already in the database
-        ids_by_rank (dict): a mapping from (scenario id, rank) tuples to scenario_alternative ranks already in the database
-        scenario_names (Iterable): the names of existing scenarios in the database keyed by id
-        alternative_names (Iterable): the names of existing alternatives in the database keyed by id
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        scen_id = item["scenario_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing scenario identifier.")
-    try:
-        alt_id = item["alternative_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing alternative identifier.")
-    try:
-        rank = item["rank"]
-    except KeyError:
-        raise SpineIntegrityError("Missing scenario alternative rank.")
-    scen_name = scenario_names.get(scen_id)
-    if scen_name is None:
-        raise SpineIntegrityError(f"Scenario with id {scen_id} does not have a name.")
-    alt_name = alternative_names.get(alt_id)
-    if alt_name is None:
-        raise SpineIntegrityError(f"Alternative with id {alt_id} does not have a name.")
-    dup_id = ids_by_alt_id.get((scen_id, alt_id))
-    if dup_id is not None:
-        raise SpineIntegrityError(f"Alternative {alt_name} already exists in scenario {scen_name}.", id=dup_id)
-    dup_id = ids_by_rank.get((scen_id, rank))
-    if dup_id is not None:
-        raise SpineIntegrityError(
-            f"Rank {rank} already exists in scenario {scen_name}. Cannot give the same rank for "
-            f"alternative {alt_name}.",
-            id=dup_id,
-        )
-
-
-def check_object_class(item, current_items, object_class_type):
-    """Check whether the insertion of an object class item
-    results in the violation of an integrity constraint.
-
-    Args:
-        item (dict): An object class item to be checked.
-        current_items (dict): A dictionary mapping names to ids of object classes already in the database.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the object class name. Probably a bug, please report."
-        )
-    if not name:
-        raise SpineIntegrityError("Object class name is an empty string and therefore not valid")
-    if "type_id" in item and item["type_id"] != object_class_type:
-        raise SpineIntegrityError(
-            f"Object class '{name}' does not have a type_id of an object class.", id=current_items[name]
-        )
-    if name in current_items:
-        raise SpineIntegrityError(f"There can't be more than one object class called '{name}'.", id=current_items[name])
-
-
-def check_object(item, current_items, object_class_ids, object_entity_type):
-    """Check whether the insertion of an object item
-    results in the violation of an integrity constraint.
-
-    Args:
-        item (dict): An object item to be checked.
-        current_items (dict): A dictionary mapping tuples (class_id, name) to ids of objects already in the database.
-        object_class_ids (list): A list of object class ids in the database.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the object name. Probably a bug, please report."
-        )
-    if not name:
-        raise SpineIntegrityError("Object name is an empty string and therefore not valid")
-    if "type_id" in item and item["type_id"] != object_entity_type:
-        raise SpineIntegrityError(f"Object '{name}' does not have entity type of and object", id=current_items[name])
-    try:
-        class_id = item["class_id"]
-    except KeyError:
-        raise SpineIntegrityError(f"Object '{name}' does not have an object class id.")
-    if class_id not in object_class_ids:
-        raise SpineIntegrityError(f"Object class id for object '{name}' not found.")
-    if (class_id, name) in current_items:
-        raise SpineIntegrityError(
-            f"There's already an object called '{name}' in the same object class.", id=current_items[class_id, name]
-        )
-
-
-def check_wide_relationship_class(wide_item, current_items, object_class_ids, relationship_class_type):
-    """Check whether the insertion of a relationship class item
-    results in the violation of an integrity constraint.
-
-    Args:
-        wide_item (dict): A wide relationship class item to be checked.
-        current_items (dict): A dictionary mapping names to ids of relationship classes already in the database.
-        object_class_ids (list): A list of object class ids in the database.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        name = wide_item["name"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the relationship class name. "
-            "Probably a bug, please report."
-        )
-    if not name:
-        raise SpineIntegrityError(f"Name '{name}' is not valid")
-    try:
-        given_object_class_id_list = wide_item["object_class_id_list"]
-    except KeyError:
-        raise SpineIntegrityError(
-            f"Python KeyError: There is no dictionary keys for the object class ids of relationship class '{name}'. "
-            "Probably a bug, please report."
-        )
-    if not given_object_class_id_list:
-        raise SpineIntegrityError(f"At least one object class is needed for the relationship class '{name}'.")
-    if not all(id_ in object_class_ids for id_ in given_object_class_id_list):
-        raise SpineIntegrityError(
-            f"At least one of the object class ids of the relationship class '{name}' is not in the database."
-        )
-    if "type_id" in wide_item and wide_item["type_id"] != relationship_class_type:
-        raise SpineIntegrityError(f"Relationship class '{name}' must have correct type_id .", id=current_items[name])
-    if name in current_items:
-        raise SpineIntegrityError(
-            f"There can't be more than one relationship class with the name '{name}'.", id=current_items[name]
-        )
-
-
-def check_wide_relationship(
-    wide_item, current_items_by_name, current_items_by_obj_lst, relationship_classes, objects, relationship_entity_type
-):
-    """Check whether the insertion of a relationship item
-    results in the violation of an integrity constraint.
-
-    Args:
-        wide_item (dict): A wide relationship item to be checked.
-        current_items_by_name (dict): A dictionary mapping tuples (class_id, name) to ids of
-            relationships already in the database.
-        current_items_by_obj_lst (dict): A dictionary mapping tuples (class_id, object_name_list) to ids
-            of relationships already in the database.
-        relationship_classes (dict): A dictionary of wide relationship class items in the database keyed by id.
-        objects (dict): A dictionary of object items in the database keyed by id.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-
-    try:
-        name = wide_item["name"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the relationship name. Probably a bug, please report."
-        )
-    if not name:
-        raise SpineIntegrityError("Relationship name is an empty string, which is not valid")
-    try:
-        class_id = wide_item["class_id"]
-    except KeyError:
-        raise SpineIntegrityError(
-            f"Python KeyError: There is no dictionary key for the relationship class id of relationship '{name}'. "
-            "Probably a bug, please report"
-        )
-    if "type_id" in wide_item and wide_item["type_id"] != relationship_entity_type:
-        raise SpineIntegrityError(
-            f"Relationship '{name}' does not have entity type of a relationship.",
-            id=current_items_by_name[class_id, name],
-        )
-    if (class_id, name) in current_items_by_name:
-        raise SpineIntegrityError(
-            f"There's already a relationship called '{name}' in the same class.",
-            id=current_items_by_name[class_id, name],
-        )
-    try:
-        object_class_id_list = relationship_classes[class_id]["object_class_id_list"]
-    except KeyError:
-        raise SpineIntegrityError(f"There is no object class id list for relationship '{name}'")
-    try:
-        object_id_list = tuple(wide_item["object_id_list"])
-    except KeyError:
-        raise SpineIntegrityError(f"There is no object id list for relationship '{name}'")
-    try:
-        given_object_class_id_list = tuple(objects[id]["class_id"] for id in object_id_list)
-    except KeyError:
-        raise SpineIntegrityError(f"Some of the objects in relationship '{name}' are invalid.")
-    if given_object_class_id_list != object_class_id_list:
-        object_name_list = [objects[id]["name"] for id in object_id_list]
-        relationship_class_name = relationship_classes[class_id]["name"]
-        raise SpineIntegrityError(
-            f"Incorrect objects '{object_name_list}' for relationship class '{relationship_class_name}'."
-        )
-    if (class_id, object_id_list) in current_items_by_obj_lst:
-        object_name_list = [objects[id]["name"] for id in object_id_list]
-        relationship_class_name = relationship_classes[class_id]["name"]
-        raise SpineIntegrityError(
-            "There's already a relationship between objects {} in class {}.".format(
-                object_name_list, relationship_class_name
-            ),
-            id=current_items_by_obj_lst[class_id, object_id_list],
-        )
-
-
-def check_entity_group(item, current_items, entities):
-    """Check whether the insertion of an entity group item
-    results in the violation of an integrity constraint.
-
-    Args:
-        item (dict): An entity group item to be checked.
-        current_items (dict): A dictionary mapping tuples (entity_id, member_id) to ids of entity groups
-            already in the database.
-        entities (dict): A dictionary mapping entity class ids, to entity ids, to entity items already in the db
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        entity_id = item["entity_id"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the entity id of entity group. "
-            "Probably a bug, please report."
-        )
-    try:
-        member_id = item["member_id"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the member id of an entity group. "
-            "Probably a bug, please report."
-        )
-    try:
-        entity_class_id = item["entity_class_id"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the entity class id of entity group. "
-            "Probably a bug, please report."
-        )
-    ents = entities.get(entity_class_id)
-    if ents is None:
-        raise SpineIntegrityError("Entity class not found for entity group.")
-    entity = ents.get(entity_id)
-    if not entity:
-        raise SpineIntegrityError("No entity id for the entity group.")
-    member = ents.get(member_id)
-    if not member:
-        raise SpineIntegrityError("Entity group has no members.")
-    if (entity_id, member_id) in current_items:
-        raise SpineIntegrityError(
-            "{0} is already a member in {1}.".format(member["name"], entity["name"]),
-            id=current_items[entity_id, member_id],
-        )
-
-
-def check_parameter_definition(item, current_items, entity_class_ids, parameter_value_lists, list_values):
-    """Check whether the insertion of a parameter definition item
-    results in the violation of an integrity constraint.
-
-    Args:
-        item (dict): A parameter definition item to be checked.
-        current_items (dict): A dictionary mapping tuples (entity_class_id, name) to ids of parameter definitions
-            already in the database.
-        entity_class_ids (Iterable): A set of entity class ids in the database.
-        parameter_value_lists (dict): A dictionary of value-lists in the database keyed by id.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    name = item.get("name")
-    if not name:
-        raise SpineIntegrityError("No name provided for a parameter definition.")
-    entity_class_id = item.get("entity_class_id")
-    if not entity_class_id:
-        raise SpineIntegrityError(f"Missing entity class id for parameter definition '{name}'.")
-    if entity_class_id not in entity_class_ids:
-        raise SpineIntegrityError(
-            f"Entity class id for parameter definition '{name}' not found in the entity class "
-            "ids of the current database."
-        )
-    if (entity_class_id, name) in current_items:
-        raise SpineIntegrityError(
-            "There's already a parameter called {0} in entity class with id {1}.".format(name, entity_class_id),
-            id=current_items[entity_class_id, name],
-        )
-    replace_default_values_with_list_references(item, parameter_value_lists, list_values)
-
-
-def check_parameter_value(
-    item, current_items, parameter_definitions, entities, parameter_value_lists, list_values, alternatives
-):
-    """Check whether the insertion of a parameter value item results in the violation of an integrity constraint.
-
-    Args:
-        item (dict): A parameter value item to be checked.
-        current_items (dict): A dictionary mapping tuples (entity_id, parameter_definition_id)
-            to ids of parameter values already in the database.
-        parameter_definitions (dict): A dictionary of parameter definition items in the database keyed by id.
-        entities (dict): A dictionary of entity items already in the database keyed by id.
-        parameter_value_lists (dict): A dictionary of value-lists in the database keyed by id.
-        list_values (dict): A dictionary of list-values in the database keyed by id.
-        alternatives (set)
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        parameter_definition_id = item["parameter_definition_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter identifier.")
-    try:
-        parameter_definition = parameter_definitions[parameter_definition_id]
-    except KeyError:
-        raise SpineIntegrityError("Parameter not found.")
-    alt_id = item.get("alternative_id")
-    if alt_id not in alternatives:
-        raise SpineIntegrityError("Alternative not found.")
-    entity_id = item.get("entity_id")
-    if not entity_id:
-        raise SpineIntegrityError("Missing object or relationship identifier.")
-    try:
-        entity_class_id = entities[entity_id]["class_id"]
-    except KeyError:
-        raise SpineIntegrityError("Entity not found")
-    if entity_class_id != parameter_definition["entity_class_id"]:
-        entity_name = entities[entity_id]["name"]
-        parameter_name = parameter_definition["name"]
-        raise SpineIntegrityError("Incorrect entity '{}' for parameter '{}'.".format(entity_name, parameter_name))
-    if (entity_id, parameter_definition_id, alt_id) in current_items:
-        entity_name = entities[entity_id]["name"]
-        parameter_name = parameter_definition["name"]
-        raise SpineIntegrityError(
-            "The value of parameter '{}' for entity '{}' is already specified.".format(parameter_name, entity_name),
-            id=current_items[entity_id, parameter_definition_id, alt_id],
-        )
-    replace_parameter_values_with_list_references(item, parameter_definitions, parameter_value_lists, list_values)
-
-
-def replace_default_values_with_list_references(item, parameter_value_lists, list_values):
-    parameter_value_list_id = item.get("parameter_value_list_id")
-    return _replace_values_with_list_references(
-        "parameter_definition", item, parameter_value_list_id, parameter_value_lists, list_values
-    )
-
-
-def replace_parameter_values_with_list_references(item, parameter_definitions, parameter_value_lists, list_values):
-    parameter_definition_id = item["parameter_definition_id"]
-    parameter_definition = parameter_definitions[parameter_definition_id]
-    parameter_value_list_id = parameter_definition["parameter_value_list_id"]
-    return _replace_values_with_list_references(
-        "parameter_value", item, parameter_value_list_id, parameter_value_lists, list_values
-    )
-
-
-def _replace_values_with_list_references(item_type, item, parameter_value_list_id, parameter_value_lists, list_values):
-    if parameter_value_list_id is None:
-        return False
-    if parameter_value_list_id not in parameter_value_lists:
-        raise SpineIntegrityError("Parameter value list not found.")
-    value_id_list = parameter_value_lists[parameter_value_list_id]
-    if value_id_list is None:
-        raise SpineIntegrityError("Parameter value list is empty!")
-    value_key, type_key = {
-        "parameter_value": ("value", "type"),
-        "parameter_definition": ("default_value", "default_type"),
-    }[item_type]
-    value = dict.get(item, value_key)
-    value_type = dict.get(item, type_key)
-    try:
-        parsed_value = from_database(value, value_type)
-    except ParameterValueFormatError as err:
-        raise SpineIntegrityError(f"Invalid {value_key} '{value}': {err}") from None
-    if parsed_value is None:
-        return False
-    list_value_id = next((id_ for id_ in value_id_list if list_values.get(id_) == parsed_value), None)
-    if list_value_id is None:
-        valid_values = ", ".join(f"{dump_db_value(list_values.get(id_))[0].decode('utf8')!r}" for id_ in value_id_list)
-        raise SpineIntegrityError(
-            f"Invalid {value_key} '{parsed_value}' - it should be one from the parameter value list: {valid_values}."
-        )
-    item[value_key] = str(list_value_id).encode("UTF8")
-    item[type_key] = "list_value_ref"
-    item["list_value_id"] = list_value_id
-    return True
-
-
-def check_parameter_value_list(item, current_items):
-    """Check whether the insertion of a parameter value-list item results in the violation of an integrity constraint.
-
-    Args:
-        item (dict): A parameter value-list item to be checked.
-        current_items (dict): A dictionary mapping names to ids of parameter value-lists already in the database.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter value list name.")
-    if name in current_items:
-        raise SpineIntegrityError(
-            "There can't be more than one parameter value_list called '{}'.".format(name), id=current_items[name]
-        )
-
-
-def check_list_value(item, list_names_by_id, list_value_ids_by_index, list_value_ids_by_value):
-    """Check whether the insertion of a list value item results in the violation of an integrity constraint.
-
-    Args:
-        item (dict): A list value item to be checked.
-        list_names_by_id (dict): Mapping parameter value list ids to names.
-        list_value_ids_by_index (dict): Mapping tuples (list id, index) to ids of existing list values.
-        list_value_ids_by_value (dict): Mapping tuples (list id, type, value) to ids of existing list values.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    keys = {"parameter_value_list_id", "index", "value", "type"}
-    missing_keys = keys - item.keys()
-    if missing_keys:
-        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
-    list_id = item["parameter_value_list_id"]
-    list_name = list_names_by_id.get(list_id)
-    if list_name is None:
-        raise SpineIntegrityError("Unknown parameter value list identifier.")
-    index = item["index"]
-    type_ = item["type"]
-    value = item["value"]
-    dup_id = list_value_ids_by_index.get((list_id, index))
-    if dup_id is not None:
-        raise SpineIntegrityError(f"'{list_name}' already has the index '{index}'.", id=dup_id)
-    dup_id = list_value_ids_by_value.get((list_id, type_, value))
-    if dup_id is not None:
-        raise SpineIntegrityError(f"'{list_name}' already has the value '{from_database(value, type_)}'.", id=dup_id)
-
-
-def check_tool(item, current_items):
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError("Missing tool name.")
-    if name in current_items:
-        raise SpineIntegrityError(f"There can't be more than one tool called '{name}'.", id=current_items[name])
-
-
-def check_feature(item, current_items, parameter_definitions):
-    try:
-        parameter_definition_id = item["parameter_definition_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter identifier.")
-    try:
-        parameter_value_list_id = item["parameter_value_list_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter value list identifier.")
-    try:
-        parameter_definition = parameter_definitions[parameter_definition_id]
-    except KeyError:
-        raise SpineIntegrityError("Parameter not found.")
-    if parameter_value_list_id is None:
-        raise SpineIntegrityError(f"Parameter '{parameter_definition['name']}' doesn't have a value list.")
-    if parameter_value_list_id != parameter_definition["parameter_value_list_id"]:
-        raise SpineIntegrityError("Parameter definition and value list don't match.")
-    if parameter_definition_id in current_items:
-        raise SpineIntegrityError(
-            f"There's already a feature defined for parameter '{parameter_definition['name']}'.",
-            id=current_items[parameter_definition_id],
-        )
-
-
-def check_tool_feature(item, current_items, tools, features):
-    try:
-        tool_id = item["tool_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing tool identifier.")
-    try:
-        feature_id = item["feature_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing feature identifier.")
-    try:
-        parameter_value_list_id = item["parameter_value_list_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter value list identifier.")
-    try:
-        tool = tools[tool_id]
-    except KeyError:
-        raise SpineIntegrityError("Tool not found.")
-    try:
-        feature = features[feature_id]
-    except KeyError:
-        raise SpineIntegrityError("Feature not found.")
-    dup_id = current_items.get((tool_id, feature_id))
-    if dup_id is not None:
-        raise SpineIntegrityError(f"Tool '{tool['name']}' already has feature '{feature['name']}'.", id=dup_id)
-    if parameter_value_list_id != feature["parameter_value_list_id"]:
-        raise SpineIntegrityError("Feature and parameter value list don't match.")
-
-
-def check_tool_feature_method(item, current_items, tool_features, parameter_value_lists):
-    try:
-        tool_feature_id = item["tool_feature_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing tool feature identifier.")
-    try:
-        parameter_value_list_id = item["parameter_value_list_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter value list identifier.")
-    try:
-        method_index = item["method_index"]
-    except KeyError:
-        raise SpineIntegrityError("Missing method index.")
-    try:
-        tool_feature = tool_features[tool_feature_id]
-    except KeyError:
-        raise SpineIntegrityError("Tool feature not found.")
-    try:
-        parameter_value_list = parameter_value_lists[parameter_value_list_id]
-    except KeyError:
-        raise SpineIntegrityError("Parameter value list not found.")
-    dup_id = current_items.get((tool_feature_id, method_index))
-    if dup_id is not None:
-        raise SpineIntegrityError("Tool feature already has the given method.", id=dup_id)
-    if parameter_value_list_id != tool_feature["parameter_value_list_id"]:
-        raise SpineIntegrityError("Feature and parameter value list don't match.")
-    if method_index not in parameter_value_list["value_index_list"]:
-        raise SpineIntegrityError("Invalid method for tool feature.")
-
-
-def check_metadata(item, metadata):
-    """Check whether the entity metadata item violates an integrity constraint.
-
-    Args:
-        item (dict): An entity metadata item to be checked.
-        metadata (dict): Mapping from metadata name and value to metadata id.
-
-    Raises:
-        SpineIntegrityError: if the item violates an integrity constraint.
-    """
-    keys = {"name", "value"}
-    missing_keys = keys - item.keys()
-    if missing_keys:
-        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
-
-
-def check_entity_metadata(item, entities, metadata):
-    """Check whether the entity metadata item violates an integrity constraint.
-
-    Args:
-        item (dict): An entity metadata item to be checked.
-        entities (set of int): Available entity ids.
-        metadata (set of int): Available metadata ids.
-
-    Raises:
-        SpineIntegrityError: if the item violates an integrity constraint.
-    """
-    keys = {"entity_id", "metadata_id"}
-    missing_keys = keys - item.keys()
-    if missing_keys:
-        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
-    if item["entity_id"] not in entities:
-        raise SpineIntegrityError("Unknown entity identifier.")
-    if item["metadata_id"] not in metadata:
-        raise SpineIntegrityError("Unknown metadata identifier.")
-
-
-def check_parameter_value_metadata(item, values, metadata):
-    """Check whether the parameter value metadata item violates an integrity constraint.
-
-    Args:
-        item (dict): An entity metadata item to be checked.
-        values (set of int): Available parameter value ids.
-        metadata (set of int): Available metadata ids.
-
-    Raises:
-        SpineIntegrityError: if the item violates an integrity constraint.
-    """
-    keys = {"parameter_value_id", "metadata_id"}
-    missing_keys = keys - item.keys()
-    if missing_keys:
-        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
-    if item["parameter_value_id"] not in values:
-        raise SpineIntegrityError("Unknown parameter value identifier.")
-    if item["metadata_id"] not in metadata:
-        raise SpineIntegrityError("Unknown metadata identifier.")
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""Functions for checking whether inserting data into a Spine database leads
+to the violation of integrity constraints.
+
+"""
+
+from .parameter_value import dump_db_value, from_database, ParameterValueFormatError
+from .exception import SpineIntegrityError
+
+# NOTE: We parse each parameter value or default value before accepting it. Is it too much?
+
+
+def check_alternative(item, current_items):
+    try:
+        name = item["name"]
+    except KeyError:
+        raise SpineIntegrityError("Missing alternative name.")
+    if name in current_items:
+        raise SpineIntegrityError(f"There can't be more than one alternative called '{name}'.", id=current_items[name])
+
+
+def check_scenario(item, current_items):
+    try:
+        name = item["name"]
+    except KeyError:
+        raise SpineIntegrityError("Missing scenario name.")
+    if name in current_items:
+        raise SpineIntegrityError(f"There can't be more than one scenario called '{name}'.", id=current_items[name])
+
+
+def check_scenario_alternative(item, ids_by_alt_id, ids_by_rank, scenario_names, alternative_names):
+    """
+    Checks if given scenario alternative violates a database's integrity.
+
+    Args:
+        item (dict: a scenario alternative item for checking; must contain the following fields:
+            - "scenario_id": scenario's id
+            - "alternative_id": alternative's id
+            - "rank": alternative's rank within the scenario
+        ids_by_alt_id (dict): a mapping from (scenario id, alternative id) tuples to scenario_alternative ids
+            already in the database
+        ids_by_rank (dict): a mapping from (scenario id, rank) tuples to scenario_alternative ranks already in the database
+        scenario_names (Iterable): the names of existing scenarios in the database keyed by id
+        alternative_names (Iterable): the names of existing alternatives in the database keyed by id
+
+    Raises:
+        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    """
+    try:
+        scen_id = item["scenario_id"]
+    except KeyError:
+        raise SpineIntegrityError("Missing scenario identifier.")
+    try:
+        alt_id = item["alternative_id"]
+    except KeyError:
+        raise SpineIntegrityError("Missing alternative identifier.")
+    try:
+        rank = item["rank"]
+    except KeyError:
+        raise SpineIntegrityError("Missing scenario alternative rank.")
+    scen_name = scenario_names.get(scen_id)
+    if scen_name is None:
+        raise SpineIntegrityError(f"Scenario with id {scen_id} does not have a name.")
+    alt_name = alternative_names.get(alt_id)
+    if alt_name is None:
+        raise SpineIntegrityError(f"Alternative with id {alt_id} does not have a name.")
+    dup_id = ids_by_alt_id.get((scen_id, alt_id))
+    if dup_id is not None:
+        raise SpineIntegrityError(f"Alternative {alt_name} already exists in scenario {scen_name}.", id=dup_id)
+    dup_id = ids_by_rank.get((scen_id, rank))
+    if dup_id is not None:
+        raise SpineIntegrityError(
+            f"Rank {rank} already exists in scenario {scen_name}. Cannot give the same rank for "
+            f"alternative {alt_name}.",
+            id=dup_id,
+        )
+
+
+def check_object_class(item, current_items, object_class_type):
+    """Check whether the insertion of an object class item
+    results in the violation of an integrity constraint.
+
+    Args:
+        item (dict): An object class item to be checked.
+        current_items (dict): A dictionary mapping names to ids of object classes already in the database.
+
+    Raises:
+        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    """
+    try:
+        name = item["name"]
+    except KeyError:
+        raise SpineIntegrityError(
+            "Python KeyError: There is no dictionary key for the object class name. Probably a bug, please report."
+        )
+    if not name:
+        raise SpineIntegrityError("Object class name is an empty string and therefore not valid")
+    if "type_id" in item and item["type_id"] != object_class_type:
+        raise SpineIntegrityError(
+            f"Object class '{name}' does not have a type_id of an object class.", id=current_items[name]
+        )
+    if name in current_items:
+        raise SpineIntegrityError(f"There can't be more than one object class called '{name}'.", id=current_items[name])
+
+
+def check_object(item, current_items, object_class_ids, object_entity_type):
+    """Check whether the insertion of an object item
+    results in the violation of an integrity constraint.
+
+    Args:
+        item (dict): An object item to be checked.
+        current_items (dict): A dictionary mapping tuples (class_id, name) to ids of objects already in the database.
+        object_class_ids (list): A list of object class ids in the database.
+
+    Raises:
+        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    """
+    try:
+        name = item["name"]
+    except KeyError:
+        raise SpineIntegrityError(
+            "Python KeyError: There is no dictionary key for the object name. Probably a bug, please report."
+        )
+    if not name:
+        raise SpineIntegrityError("Object name is an empty string and therefore not valid")
+    if "type_id" in item and item["type_id"] != object_entity_type:
+        raise SpineIntegrityError(f"Object '{name}' does not have entity type of and object", id=current_items[name])
+    try:
+        class_id = item["class_id"]
+    except KeyError:
+        raise SpineIntegrityError(f"Object '{name}' does not have an object class id.")
+    if class_id not in object_class_ids:
+        raise SpineIntegrityError(f"Object class id for object '{name}' not found.")
+    if (class_id, name) in current_items:
+        raise SpineIntegrityError(
+            f"There's already an object called '{name}' in the same object class.", id=current_items[class_id, name]
+        )
+
+
+def check_wide_relationship_class(wide_item, current_items, object_class_ids, relationship_class_type):
+    """Check whether the insertion of a relationship class item
+    results in the violation of an integrity constraint.
+
+    Args:
+        wide_item (dict): A wide relationship class item to be checked.
+        current_items (dict): A dictionary mapping names to ids of relationship classes already in the database.
+        object_class_ids (list): A list of object class ids in the database.
+
+    Raises:
+        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    """
+    try:
+        name = wide_item["name"]
+    except KeyError:
+        raise SpineIntegrityError(
+            "Python KeyError: There is no dictionary key for the relationship class name. "
+            "Probably a bug, please report."
+        )
+    if not name:
+        raise SpineIntegrityError(f"Name '{name}' is not valid")
+    try:
+        given_object_class_id_list = wide_item["object_class_id_list"]
+    except KeyError:
+        raise SpineIntegrityError(
+            f"Python KeyError: There is no dictionary keys for the object class ids of relationship class '{name}'. "
+            "Probably a bug, please report."
+        )
+    if not given_object_class_id_list:
+        raise SpineIntegrityError(f"At least one object class is needed for the relationship class '{name}'.")
+    if not all(id_ in object_class_ids for id_ in given_object_class_id_list):
+        raise SpineIntegrityError(
+            f"At least one of the object class ids of the relationship class '{name}' is not in the database."
+        )
+    if "type_id" in wide_item and wide_item["type_id"] != relationship_class_type:
+        raise SpineIntegrityError(f"Relationship class '{name}' must have correct type_id .", id=current_items[name])
+    if name in current_items:
+        raise SpineIntegrityError(
+            f"There can't be more than one relationship class with the name '{name}'.", id=current_items[name]
+        )
+
+
+def check_wide_relationship(
+    wide_item, current_items_by_name, current_items_by_obj_lst, relationship_classes, objects, relationship_entity_type
+):
+    """Check whether the insertion of a relationship item
+    results in the violation of an integrity constraint.
+
+    Args:
+        wide_item (dict): A wide relationship item to be checked.
+        current_items_by_name (dict): A dictionary mapping tuples (class_id, name) to ids of
+            relationships already in the database.
+        current_items_by_obj_lst (dict): A dictionary mapping tuples (class_id, object_name_list) to ids
+            of relationships already in the database.
+        relationship_classes (dict): A dictionary of wide relationship class items in the database keyed by id.
+        objects (dict): A dictionary of object items in the database keyed by id.
+
+    Raises:
+        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    """
+
+    try:
+        name = wide_item["name"]
+    except KeyError:
+        raise SpineIntegrityError(
+            "Python KeyError: There is no dictionary key for the relationship name. Probably a bug, please report."
+        )
+    if not name:
+        raise SpineIntegrityError("Relationship name is an empty string, which is not valid")
+    try:
+        class_id = wide_item["class_id"]
+    except KeyError:
+        raise SpineIntegrityError(
+            f"Python KeyError: There is no dictionary key for the relationship class id of relationship '{name}'. "
+            "Probably a bug, please report"
+        )
+    if "type_id" in wide_item and wide_item["type_id"] != relationship_entity_type:
+        raise SpineIntegrityError(
+            f"Relationship '{name}' does not have entity type of a relationship.",
+            id=current_items_by_name[class_id, name],
+        )
+    if (class_id, name) in current_items_by_name:
+        raise SpineIntegrityError(
+            f"There's already a relationship called '{name}' in the same class.",
+            id=current_items_by_name[class_id, name],
+        )
+    try:
+        object_class_id_list = relationship_classes[class_id]["object_class_id_list"]
+    except KeyError:
+        raise SpineIntegrityError(f"There is no object class id list for relationship '{name}'")
+    try:
+        object_id_list = tuple(wide_item["object_id_list"])
+    except KeyError:
+        raise SpineIntegrityError(f"There is no object id list for relationship '{name}'")
+    try:
+        given_object_class_id_list = tuple(objects[id]["class_id"] for id in object_id_list)
+    except KeyError:
+        raise SpineIntegrityError(f"Some of the objects in relationship '{name}' are invalid.")
+    if given_object_class_id_list != object_class_id_list:
+        object_name_list = [objects[id]["name"] for id in object_id_list]
+        relationship_class_name = relationship_classes[class_id]["name"]
+        raise SpineIntegrityError(
+            f"Incorrect objects '{object_name_list}' for relationship class '{relationship_class_name}'."
+        )
+    if (class_id, object_id_list) in current_items_by_obj_lst:
+        object_name_list = [objects[id]["name"] for id in object_id_list]
+        relationship_class_name = relationship_classes[class_id]["name"]
+        raise SpineIntegrityError(
+            "There's already a relationship between objects {} in class {}.".format(
+                object_name_list, relationship_class_name
+            ),
+            id=current_items_by_obj_lst[class_id, object_id_list],
+        )
+
+
+def check_entity_group(item, current_items, entities):
+    """Check whether the insertion of an entity group item
+    results in the violation of an integrity constraint.
+
+    Args:
+        item (dict): An entity group item to be checked.
+        current_items (dict): A dictionary mapping tuples (entity_id, member_id) to ids of entity groups
+            already in the database.
+        entities (dict): A dictionary mapping entity class ids, to entity ids, to entity items already in the db
+
+    Raises:
+        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    """
+    try:
+        entity_id = item["entity_id"]
+    except KeyError:
+        raise SpineIntegrityError(
+            "Python KeyError: There is no dictionary key for the entity id of entity group. "
+            "Probably a bug, please report."
+        )
+    try:
+        member_id = item["member_id"]
+    except KeyError:
+        raise SpineIntegrityError(
+            "Python KeyError: There is no dictionary key for the member id of an entity group. "
+            "Probably a bug, please report."
+        )
+    try:
+        entity_class_id = item["entity_class_id"]
+    except KeyError:
+        raise SpineIntegrityError(
+            "Python KeyError: There is no dictionary key for the entity class id of entity group. "
+            "Probably a bug, please report."
+        )
+    ents = entities.get(entity_class_id)
+    if ents is None:
+        raise SpineIntegrityError("Entity class not found for entity group.")
+    entity = ents.get(entity_id)
+    if not entity:
+        raise SpineIntegrityError("No entity id for the entity group.")
+    member = ents.get(member_id)
+    if not member:
+        raise SpineIntegrityError("Entity group has no members.")
+    if (entity_id, member_id) in current_items:
+        raise SpineIntegrityError(
+            "{0} is already a member in {1}.".format(member["name"], entity["name"]),
+            id=current_items[entity_id, member_id],
+        )
+
+
+def check_parameter_definition(item, current_items, entity_class_ids, parameter_value_lists, list_values):
+    """Check whether the insertion of a parameter definition item
+    results in the violation of an integrity constraint.
+
+    Args:
+        item (dict): A parameter definition item to be checked.
+        current_items (dict): A dictionary mapping tuples (entity_class_id, name) to ids of parameter definitions
+            already in the database.
+        entity_class_ids (Iterable): A set of entity class ids in the database.
+        parameter_value_lists (dict): A dictionary of value-lists in the database keyed by id.
+
+    Raises:
+        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    """
+    name = item.get("name")
+    if not name:
+        raise SpineIntegrityError("No name provided for a parameter definition.")
+    entity_class_id = item.get("entity_class_id")
+    if not entity_class_id:
+        raise SpineIntegrityError(f"Missing entity class id for parameter definition '{name}'.")
+    if entity_class_id not in entity_class_ids:
+        raise SpineIntegrityError(
+            f"Entity class id for parameter definition '{name}' not found in the entity class "
+            "ids of the current database."
+        )
+    if (entity_class_id, name) in current_items:
+        raise SpineIntegrityError(
+            "There's already a parameter called {0} in entity class with id {1}.".format(name, entity_class_id),
+            id=current_items[entity_class_id, name],
+        )
+    replace_default_values_with_list_references(item, parameter_value_lists, list_values)
+
+
+def check_parameter_value(
+    item, current_items, parameter_definitions, entities, parameter_value_lists, list_values, alternatives
+):
+    """Check whether the insertion of a parameter value item results in the violation of an integrity constraint.
+
+    Args:
+        item (dict): A parameter value item to be checked.
+        current_items (dict): A dictionary mapping tuples (entity_id, parameter_definition_id)
+            to ids of parameter values already in the database.
+        parameter_definitions (dict): A dictionary of parameter definition items in the database keyed by id.
+        entities (dict): A dictionary of entity items already in the database keyed by id.
+        parameter_value_lists (dict): A dictionary of value-lists in the database keyed by id.
+        list_values (dict): A dictionary of list-values in the database keyed by id.
+        alternatives (set)
+
+    Raises:
+        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    """
+    try:
+        parameter_definition_id = item["parameter_definition_id"]
+    except KeyError:
+        raise SpineIntegrityError("Missing parameter identifier.")
+    try:
+        parameter_definition = parameter_definitions[parameter_definition_id]
+    except KeyError:
+        raise SpineIntegrityError("Parameter not found.")
+    alt_id = item.get("alternative_id")
+    if alt_id not in alternatives:
+        raise SpineIntegrityError("Alternative not found.")
+    entity_id = item.get("entity_id")
+    if not entity_id:
+        raise SpineIntegrityError("Missing object or relationship identifier.")
+    try:
+        entity_class_id = entities[entity_id]["class_id"]
+    except KeyError:
+        raise SpineIntegrityError("Entity not found")
+    if entity_class_id != parameter_definition["entity_class_id"]:
+        entity_name = entities[entity_id]["name"]
+        parameter_name = parameter_definition["name"]
+        raise SpineIntegrityError("Incorrect entity '{}' for parameter '{}'.".format(entity_name, parameter_name))
+    if (entity_id, parameter_definition_id, alt_id) in current_items:
+        entity_name = entities[entity_id]["name"]
+        parameter_name = parameter_definition["name"]
+        raise SpineIntegrityError(
+            "The value of parameter '{}' for entity '{}' is already specified.".format(parameter_name, entity_name),
+            id=current_items[entity_id, parameter_definition_id, alt_id],
+        )
+    replace_parameter_values_with_list_references(item, parameter_definitions, parameter_value_lists, list_values)
+
+
+def replace_default_values_with_list_references(item, parameter_value_lists, list_values):
+    parameter_value_list_id = item.get("parameter_value_list_id")
+    return _replace_values_with_list_references(
+        "parameter_definition", item, parameter_value_list_id, parameter_value_lists, list_values
+    )
+
+
+def replace_parameter_values_with_list_references(item, parameter_definitions, parameter_value_lists, list_values):
+    parameter_definition_id = item["parameter_definition_id"]
+    parameter_definition = parameter_definitions[parameter_definition_id]
+    parameter_value_list_id = parameter_definition["parameter_value_list_id"]
+    return _replace_values_with_list_references(
+        "parameter_value", item, parameter_value_list_id, parameter_value_lists, list_values
+    )
+
+
+def _replace_values_with_list_references(item_type, item, parameter_value_list_id, parameter_value_lists, list_values):
+    if parameter_value_list_id is None:
+        return False
+    if parameter_value_list_id not in parameter_value_lists:
+        raise SpineIntegrityError("Parameter value list not found.")
+    value_id_list = parameter_value_lists[parameter_value_list_id]
+    if value_id_list is None:
+        raise SpineIntegrityError("Parameter value list is empty!")
+    value_key, type_key = {
+        "parameter_value": ("value", "type"),
+        "parameter_definition": ("default_value", "default_type"),
+    }[item_type]
+    value = dict.get(item, value_key)
+    value_type = dict.get(item, type_key)
+    try:
+        parsed_value = from_database(value, value_type)
+    except ParameterValueFormatError as err:
+        raise SpineIntegrityError(f"Invalid {value_key} '{value}': {err}") from None
+    if parsed_value is None:
+        return False
+    list_value_id = next((id_ for id_ in value_id_list if list_values.get(id_) == parsed_value), None)
+    if list_value_id is None:
+        valid_values = ", ".join(f"{dump_db_value(list_values.get(id_))[0].decode('utf8')!r}" for id_ in value_id_list)
+        raise SpineIntegrityError(
+            f"Invalid {value_key} '{parsed_value}' - it should be one from the parameter value list: {valid_values}."
+        )
+    item[value_key] = str(list_value_id).encode("UTF8")
+    item[type_key] = "list_value_ref"
+    item["list_value_id"] = list_value_id
+    return True
+
+
+def check_parameter_value_list(item, current_items):
+    """Check whether the insertion of a parameter value-list item results in the violation of an integrity constraint.
+
+    Args:
+        item (dict): A parameter value-list item to be checked.
+        current_items (dict): A dictionary mapping names to ids of parameter value-lists already in the database.
+
+    Raises:
+        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    """
+    try:
+        name = item["name"]
+    except KeyError:
+        raise SpineIntegrityError("Missing parameter value list name.")
+    if name in current_items:
+        raise SpineIntegrityError(
+            "There can't be more than one parameter value_list called '{}'.".format(name), id=current_items[name]
+        )
+
+
+def check_list_value(item, list_names_by_id, list_value_ids_by_index, list_value_ids_by_value):
+    """Check whether the insertion of a list value item results in the violation of an integrity constraint.
+
+    Args:
+        item (dict): A list value item to be checked.
+        list_names_by_id (dict): Mapping parameter value list ids to names.
+        list_value_ids_by_index (dict): Mapping tuples (list id, index) to ids of existing list values.
+        list_value_ids_by_value (dict): Mapping tuples (list id, type, value) to ids of existing list values.
+
+    Raises:
+        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    """
+    keys = {"parameter_value_list_id", "index", "value", "type"}
+    missing_keys = keys - item.keys()
+    if missing_keys:
+        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
+    list_id = item["parameter_value_list_id"]
+    list_name = list_names_by_id.get(list_id)
+    if list_name is None:
+        raise SpineIntegrityError("Unknown parameter value list identifier.")
+    index = item["index"]
+    type_ = item["type"]
+    value = item["value"]
+    dup_id = list_value_ids_by_index.get((list_id, index))
+    if dup_id is not None:
+        raise SpineIntegrityError(f"'{list_name}' already has the index '{index}'.", id=dup_id)
+    dup_id = list_value_ids_by_value.get((list_id, type_, value))
+    if dup_id is not None:
+        raise SpineIntegrityError(f"'{list_name}' already has the value '{from_database(value, type_)}'.", id=dup_id)
+
+
+def check_tool(item, current_items):
+    try:
+        name = item["name"]
+    except KeyError:
+        raise SpineIntegrityError("Missing tool name.")
+    if name in current_items:
+        raise SpineIntegrityError(f"There can't be more than one tool called '{name}'.", id=current_items[name])
+
+
+def check_feature(item, current_items, parameter_definitions):
+    try:
+        parameter_definition_id = item["parameter_definition_id"]
+    except KeyError:
+        raise SpineIntegrityError("Missing parameter identifier.")
+    try:
+        parameter_value_list_id = item["parameter_value_list_id"]
+    except KeyError:
+        raise SpineIntegrityError("Missing parameter value list identifier.")
+    try:
+        parameter_definition = parameter_definitions[parameter_definition_id]
+    except KeyError:
+        raise SpineIntegrityError("Parameter not found.")
+    if parameter_value_list_id is None:
+        raise SpineIntegrityError(f"Parameter '{parameter_definition['name']}' doesn't have a value list.")
+    if parameter_value_list_id != parameter_definition["parameter_value_list_id"]:
+        raise SpineIntegrityError("Parameter definition and value list don't match.")
+    if parameter_definition_id in current_items:
+        raise SpineIntegrityError(
+            f"There's already a feature defined for parameter '{parameter_definition['name']}'.",
+            id=current_items[parameter_definition_id],
+        )
+
+
+def check_tool_feature(item, current_items, tools, features):
+    try:
+        tool_id = item["tool_id"]
+    except KeyError:
+        raise SpineIntegrityError("Missing tool identifier.")
+    try:
+        feature_id = item["feature_id"]
+    except KeyError:
+        raise SpineIntegrityError("Missing feature identifier.")
+    try:
+        parameter_value_list_id = item["parameter_value_list_id"]
+    except KeyError:
+        raise SpineIntegrityError("Missing parameter value list identifier.")
+    try:
+        tool = tools[tool_id]
+    except KeyError:
+        raise SpineIntegrityError("Tool not found.")
+    try:
+        feature = features[feature_id]
+    except KeyError:
+        raise SpineIntegrityError("Feature not found.")
+    dup_id = current_items.get((tool_id, feature_id))
+    if dup_id is not None:
+        raise SpineIntegrityError(f"Tool '{tool['name']}' already has feature '{feature['name']}'.", id=dup_id)
+    if parameter_value_list_id != feature["parameter_value_list_id"]:
+        raise SpineIntegrityError("Feature and parameter value list don't match.")
+
+
+def check_tool_feature_method(item, current_items, tool_features, parameter_value_lists):
+    try:
+        tool_feature_id = item["tool_feature_id"]
+    except KeyError:
+        raise SpineIntegrityError("Missing tool feature identifier.")
+    try:
+        parameter_value_list_id = item["parameter_value_list_id"]
+    except KeyError:
+        raise SpineIntegrityError("Missing parameter value list identifier.")
+    try:
+        method_index = item["method_index"]
+    except KeyError:
+        raise SpineIntegrityError("Missing method index.")
+    try:
+        tool_feature = tool_features[tool_feature_id]
+    except KeyError:
+        raise SpineIntegrityError("Tool feature not found.")
+    try:
+        parameter_value_list = parameter_value_lists[parameter_value_list_id]
+    except KeyError:
+        raise SpineIntegrityError("Parameter value list not found.")
+    dup_id = current_items.get((tool_feature_id, method_index))
+    if dup_id is not None:
+        raise SpineIntegrityError("Tool feature already has the given method.", id=dup_id)
+    if parameter_value_list_id != tool_feature["parameter_value_list_id"]:
+        raise SpineIntegrityError("Feature and parameter value list don't match.")
+    if method_index not in parameter_value_list["value_index_list"]:
+        raise SpineIntegrityError("Invalid method for tool feature.")
+
+
+def check_metadata(item, metadata):
+    """Check whether the entity metadata item violates an integrity constraint.
+
+    Args:
+        item (dict): An entity metadata item to be checked.
+        metadata (dict): Mapping from metadata name and value to metadata id.
+
+    Raises:
+        SpineIntegrityError: if the item violates an integrity constraint.
+    """
+    keys = {"name", "value"}
+    missing_keys = keys - item.keys()
+    if missing_keys:
+        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
+
+
+def check_entity_metadata(item, entities, metadata):
+    """Check whether the entity metadata item violates an integrity constraint.
+
+    Args:
+        item (dict): An entity metadata item to be checked.
+        entities (set of int): Available entity ids.
+        metadata (set of int): Available metadata ids.
+
+    Raises:
+        SpineIntegrityError: if the item violates an integrity constraint.
+    """
+    keys = {"entity_id", "metadata_id"}
+    missing_keys = keys - item.keys()
+    if missing_keys:
+        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
+    if item["entity_id"] not in entities:
+        raise SpineIntegrityError("Unknown entity identifier.")
+    if item["metadata_id"] not in metadata:
+        raise SpineIntegrityError("Unknown metadata identifier.")
+
+
+def check_parameter_value_metadata(item, values, metadata):
+    """Check whether the parameter value metadata item violates an integrity constraint.
+
+    Args:
+        item (dict): An entity metadata item to be checked.
+        values (set of int): Available parameter value ids.
+        metadata (set of int): Available metadata ids.
+
+    Raises:
+        SpineIntegrityError: if the item violates an integrity constraint.
+    """
+    keys = {"parameter_value_id", "metadata_id"}
+    missing_keys = keys - item.keys()
+    if missing_keys:
+        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
+    if item["parameter_value_id"] not in values:
+        raise SpineIntegrityError("Unknown parameter value identifier.")
+    if item["metadata_id"] not in metadata:
+        raise SpineIntegrityError("Unknown metadata identifier.")
```

### Comparing `spinedb_api-0.30.3/spinedb_api/db_cache.py` & `spinedb_api-0.30.4/spinedb_api/db_cache.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,665 +1,665 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-DB cache utility.
-
-"""
-from contextlib import suppress
-from operator import itemgetter
-
-
-class DBCache(dict):
-    def __init__(self, advance_query, *args, **kwargs):
-        """
-        A dictionary that maps table names to ids to items. Used to store and retrieve database contents.
-
-        Args:
-            advance_query (function): A function to call when references aren't found.
-                It receives a table name (a.k.a item type) and should bring more items of that type into this cache.
-        """
-        super().__init__(*args, **kwargs)
-        self._advance_query = advance_query
-
-    def table_cache(self, item_type):
-        return self.setdefault(item_type, TableCache(self, item_type))
-
-    def get_item(self, item_type, id_):
-        table_cache = self.get(item_type, {})
-        item = table_cache.get(id_)
-        if item is None:
-            return {}
-        return item
-
-    def fetch_ref(self, item_type, id_):
-        while self._advance_query(item_type):
-            with suppress(KeyError):
-                return self[item_type][id_]
-        # It is possible that fetching was completed between deciding to call this function
-        # and starting the while loop above resulting in self._advance_query() to return False immediately.
-        # Therefore, we should try one last time if the ref is available.
-        with suppress(KeyError):
-            return self[item_type][id_]
-        return None
-
-    def make_item(self, item_type, item):
-        """Returns a cache item.
-
-        Args:
-            item_type (str): the item type, equal to a table name
-            item (dict): the 'db item' to use as base
-
-        Returns:
-            CacheItem
-        """
-        factory = {
-            "object_class": ObjectClassItem,
-            "object": ObjectItem,
-            "relationship_class": RelationshipClassItem,
-            "relationship": RelationshipItem,
-            "parameter_definition": ParameterDefinitionItem,
-            "parameter_value": ParameterValueItem,
-            "entity_group": EntityGroupItem,
-            "scenario": ScenarioItem,
-            "scenario_alternative": ScenarioAlternativeItem,
-            "feature": FeatureItem,
-            "tool_feature": ToolFeatureItem,
-            "tool_feature_method": ToolFeatureMethodItem,
-            "parameter_value_list": ParameterValueListItem,
-        }.get(item_type, CacheItem)
-        return factory(self, item_type, **item)
-
-
-class TableCache(dict):
-    def __init__(self, db_cache, item_type, *args, **kwargs):
-        """
-        Args:
-            db_cache (DBCache): the DB cache where this table cache belongs.
-            item_type (str): the item type, equal to a table name
-        """
-        super().__init__(*args, **kwargs)
-        self._db_cache = db_cache
-        self._item_type = item_type
-
-    def values(self):
-        return (x for x in super().values() if x.is_valid())
-
-    def add_item(self, item, keep_existing=False):
-        if keep_existing:
-            existing_item = self.get(item["id"])
-            if existing_item is not None:
-                return existing_item
-        self[item["id"]] = new_item = self._db_cache.make_item(self._item_type, item)
-        return new_item
-
-    def update_item(self, item):
-        current_item = self[item["id"]]
-        current_item.update(item)
-        current_item.cascade_update()
-
-    def remove_item(self, id_):
-        """Removes item and its referrers from the cache.
-
-        Args:
-            id_ (int): item's database id
-
-        Returns:
-            list of CacheItem: removed items
-        """
-        current_item = self.get(id_)
-        if current_item:
-            return current_item.cascade_remove()
-        return []
-
-
-class CacheItem(dict):
-    """A dictionary that behaves kinda like a row from a query result.
-
-    It is used to store items in a cache, so we can access them as if they were rows from a query result.
-    This is mainly because we want to use the cache as a replacement for db queries in some methods.
-    """
-
-    def __init__(self, db_cache, item_type, *args, **kwargs):
-        """
-        Args:
-            db_cache (DBCache): the DB cache where this item belongs.
-        """
-        super().__init__(*args, **kwargs)
-        self._db_cache = db_cache
-        self._item_type = item_type
-        self._referrers = {}
-        self._weak_referrers = {}
-        self.readd_callbacks = set()
-        self.update_callbacks = set()
-        self.remove_callbacks = set()
-        self._to_remove = False
-        self._removed = False
-        self._corrupted = False
-        self._valid = None
-
-    @property
-    def item_type(self):
-        return self._item_type
-
-    @property
-    def key(self):
-        if dict.get(self, "id") is None:
-            return None
-        return (self._item_type, self["id"])
-
-    @property
-    def referrers(self):
-        return self._referrers
-
-    def __getattr__(self, name):
-        """Overridden method to return the dictionary key named after the attribute, or None if it doesn't exist."""
-        return self.get(name)
-
-    def __repr__(self):
-        return f"{self._item_type}{self._extended()}"
-
-    def _extended(self):
-        return {**self, **{key: self[key] for key in self._reference_keys()}}
-
-    def _asdict(self):
-        return dict(**self)
-
-    def _reference_keys(self):
-        return ()
-
-    def _get_ref(self, ref_type, ref_id, source_key):
-        ref = self._db_cache.get_item(ref_type, ref_id)
-        if not ref:
-            if source_key not in self._reference_keys():
-                return {}
-            ref = self._db_cache.fetch_ref(ref_type, ref_id)
-            if not ref:
-                self._corrupted = True
-                return {}
-        return self._handle_ref(ref, source_key)
-
-    def _handle_ref(self, ref, source_key):
-        if source_key in self._reference_keys():
-            ref.add_referrer(self)
-            if ref.is_removed():
-                self._to_remove = True
-        else:
-            ref.add_weak_referrer(self)
-            if ref.is_removed():
-                return {}
-        return ref
-
-    def get(self, key, default=None):
-        try:
-            return self[key]
-        except KeyError:
-            return default
-
-    def copy(self):
-        return type(self)(self._db_cache, self._item_type, **self)
-
-    def deepcopy(self):
-        """Makes a deep copy of the item.
-
-        Returns:
-            CacheItem: copied item
-        """
-        copy = self.copy()
-        self._copy_internal_state(copy)
-        return copy
-
-    def _copy_internal_state(self, other):
-        """Copies item's internal state to other cache item.
-
-        Args:
-            other (CacheItem): target item
-        """
-        other._referrers = {key: item.deepcopy() for key, item in self._referrers.items()}
-        other._weak_referrers = {key: item.deepcopy() for key, item in self._weak_referrers.items()}
-        other.readd_callbacks = set(self.readd_callbacks)
-        other.update_callbacks = set(self.update_callbacks)
-        other.remove_callbacks = set(self.remove_callbacks)
-        other._to_remove = self._to_remove
-        other._removed = self._removed
-        other._corrupted = self._corrupted
-        other._valid = self._valid
-
-    def is_valid(self):
-        if self._valid is not None:
-            return self._valid
-        if self._removed or self._corrupted:
-            return False
-        self._to_remove = False
-        self._corrupted = False
-        for key in self._reference_keys():
-            _ = self[key]
-        if self._to_remove:
-            self.cascade_remove()
-        self._valid = not self._removed and not self._corrupted
-        return self._valid
-
-    def is_removed(self):
-        return self._removed
-
-    def add_referrer(self, referrer):
-        if referrer.key is None:
-            return
-        self._referrers[referrer.key] = self._weak_referrers.pop(referrer.key, referrer)
-
-    def add_weak_referrer(self, referrer):
-        if referrer.key is None:
-            return
-        if referrer.key not in self._referrers:
-            self._weak_referrers[referrer.key] = referrer
-
-    def readd(self):
-        """Adds item back to cache without adding its referrers."""
-        if not self._removed:
-            return
-        self._removed = False
-        self._to_remove = False
-        self._call_readd_callbacks()
-
-    def cascade_readd(self):
-        if not self._removed:
-            return
-        self._removed = False
-        self._to_remove = False
-        for referrer in self._referrers.values():
-            referrer.cascade_readd()
-        for weak_referrer in self._weak_referrers.values():
-            weak_referrer.call_update_callbacks()
-        self._call_readd_callbacks()
-
-    def _call_readd_callbacks(self):
-        """Calls readd callbacks and removes obsolete ones."""
-        obsolete = set()
-        for callback in self.readd_callbacks:
-            if not callback(self):
-                obsolete.add(callback)
-        self.readd_callbacks -= obsolete
-
-    def cascade_remove(self):
-        """Sets item and its referrers as removed.
-
-        Calls necessary callbacks on weak referrers too.
-
-        Returns:
-            list of CacheItem: removed items
-        """
-        if self._removed:
-            return []
-        self._removed = True
-        self._to_remove = False
-        self._valid = None
-        obsolete = set()
-        for callback in self.remove_callbacks:
-            if not callback(self):
-                obsolete.add(callback)
-        self.remove_callbacks -= obsolete
-        removed_items = [self]
-        for referrer in self._referrers.values():
-            removed_items += referrer.cascade_remove()
-        for weak_referrer in self._weak_referrers.values():
-            weak_referrer.call_update_callbacks()
-        return removed_items
-
-    def cascade_update(self):
-        self.call_update_callbacks()
-        for weak_referrer in self._weak_referrers.values():
-            weak_referrer.call_update_callbacks()
-        for referrer in self._referrers.values():
-            referrer.cascade_update()
-
-    def call_update_callbacks(self):
-        self.pop("parsed_value", None)
-        obsolete = set()
-        for callback in self.update_callbacks:
-            if not callback(self):
-                obsolete.add(callback)
-        self.update_callbacks -= obsolete
-
-
-class DisplayIconMixin:
-    def __getitem__(self, key):
-        if key == "display_icon":
-            return dict.get(self, "display_icon")
-        return super().__getitem__(key)
-
-
-class DescriptionMixin:
-    def __getitem__(self, key):
-        if key == "description":
-            return dict.get(self, "description")
-        return super().__getitem__(key)
-
-
-class ObjectClassItem(DisplayIconMixin, DescriptionMixin, CacheItem):
-    pass
-
-
-class ObjectItem(DescriptionMixin, CacheItem):
-    def __getitem__(self, key):
-        if key == "class_name":
-            return self._get_ref("object_class", self["class_id"], key).get("name")
-        return super().__getitem__(key)
-
-    def _reference_keys(self):
-        return super()._reference_keys() + ("class_name",)
-
-
-class ObjectClassIdListMixin:
-    def __init__(self, *args, **kwargs):
-        object_class_id_list = kwargs["object_class_id_list"]
-        if isinstance(object_class_id_list, str):
-            object_class_id_list = (int(id_) for id_ in object_class_id_list.split(","))
-        kwargs["object_class_id_list"] = tuple(object_class_id_list)
-        super().__init__(*args, **kwargs)
-
-    def __getitem__(self, key):
-        if key == "object_class_name_list":
-            return tuple(self._get_ref("object_class", id_, key).get("name") for id_ in self["object_class_id_list"])
-        return super().__getitem__(key)
-
-    def _reference_keys(self):
-        return super()._reference_keys() + ("object_class_name_list",)
-
-
-class RelationshipClassItem(DisplayIconMixin, ObjectClassIdListMixin, DescriptionMixin, CacheItem):
-    pass
-
-
-class RelationshipItem(ObjectClassIdListMixin, CacheItem):
-    def __init__(self, db_cache, *args, **kwargs):
-        if "object_class_id_list" not in kwargs:
-            kwargs["object_class_id_list"] = db_cache.get_item("relationship_class", kwargs["class_id"]).get(
-                "object_class_id_list", ()
-            )
-        object_id_list = kwargs.get("object_id_list", ())
-        if isinstance(object_id_list, str):
-            object_id_list = (int(id_) for id_ in object_id_list.split(","))
-        kwargs["object_id_list"] = tuple(object_id_list)
-        super().__init__(db_cache, *args, **kwargs)
-
-    def __getitem__(self, key):
-        if key == "class_name":
-            return self._get_ref("relationship_class", self["class_id"], key).get("name")
-        if key == "object_name_list":
-            return tuple(self._get_ref("object", id_, key).get("name") for id_ in self["object_id_list"])
-        return super().__getitem__(key)
-
-    def _reference_keys(self):
-        return super()._reference_keys() + ("class_name", "object_name_list")
-
-
-class ParameterMixin:
-    def __init__(self, *args, **kwargs):
-        if "entity_class_id" not in kwargs:
-            kwargs["entity_class_id"] = kwargs.get("object_class_id") or kwargs.get("relationship_class_id")
-        super().__init__(*args, **kwargs)
-
-    def __getitem__(self, key):
-        if key in ("object_class_id", "relationship_class_id"):
-            return dict.get(self, key)
-        if key == "object_class_name":
-            if self["object_class_id"] is None:
-                return None
-            return self._get_ref("object_class", self["object_class_id"], key).get("name")
-        if key == "relationship_class_name":
-            if self["relationship_class_id"] is None:
-                return None
-            return self._get_ref("relationship_class", self["relationship_class_id"], key).get("name")
-        if key in ("object_class_id_list", "object_class_name_list"):
-            if self["relationship_class_id"] is None:
-                return None
-            return self._get_ref("relationship_class", self["relationship_class_id"], key).get(key)
-        if key == "entity_class_name":
-            return self["relationship_class_name"] if self["object_class_id"] is None else self["object_class_name"]
-        if key == "parameter_value_list_id":
-            return dict.get(self, key)
-        return super().__getitem__(key)
-
-    def _reference_keys(self):
-        keys = super()._reference_keys()
-        if self["object_class_id"]:
-            keys += ("object_class_name",)
-        elif self["relationship_class_id"]:
-            keys += ("relationship_class_name", "object_class_id_list", "object_class_name_list")
-        return keys
-
-
-class ParameterDefinitionItem(DescriptionMixin, ParameterMixin, CacheItem):
-    def __init__(self, *args, **kwargs):
-        if kwargs.get("list_value_id") is None:
-            kwargs["list_value_id"] = (
-                int(kwargs["default_value"]) if kwargs.get("default_type") == "list_value_ref" else None
-            )
-        super().__init__(*args, **kwargs)
-
-    def __getitem__(self, key):
-        if key == "parameter_name":
-            return super().__getitem__("name")
-        if key == "value_list_id":
-            return super().__getitem__("parameter_value_list_id")
-        if key == "value_list_name":
-            return self._get_ref("parameter_value_list", self["value_list_id"], key).get("name")
-        if key in ("default_value", "default_type"):
-            if self["list_value_id"] is not None:
-                return self._get_ref("list_value", self["list_value_id"], key).get(key.split("_")[1])
-            return dict.get(self, key)
-        return super().__getitem__(key)
-
-
-class ParameterValueItem(ParameterMixin, CacheItem):
-    def __init__(self, *args, **kwargs):
-        if "entity_id" not in kwargs:
-            kwargs["entity_id"] = kwargs.get("object_id") or kwargs.get("relationship_id")
-        if kwargs.get("list_value_id") is None:
-            kwargs["list_value_id"] = int(kwargs["value"]) if kwargs.get("type") == "list_value_ref" else None
-        super().__init__(*args, **kwargs)
-
-    def __getitem__(self, key):
-        if key in ("object_id", "relationship_id"):
-            return dict.get(self, key)
-        if key == "parameter_id":
-            return super().__getitem__("parameter_definition_id")
-        if key == "parameter_name":
-            return self._get_ref("parameter_definition", self["parameter_definition_id"], key).get("name")
-        if key == "object_name":
-            if self["object_id"] is None:
-                return None
-            return self._get_ref("object", self["object_id"], key).get("name")
-        if key in ("object_id_list", "object_name_list"):
-            if self["relationship_id"] is None:
-                return None
-            return self._get_ref("relationship", self["relationship_id"], key).get(key)
-        if key == "alternative_name":
-            return self._get_ref("alternative", self["alternative_id"], key).get("name")
-        if key in ("value", "type") and self["list_value_id"] is not None:
-            return self._get_ref("list_value", self["list_value_id"], key).get(key)
-        return super().__getitem__(key)
-
-    def _reference_keys(self):
-        keys = super()._reference_keys() + ("parameter_name", "alternative_name")
-        if self["object_id"]:
-            keys += ("object_name",)
-        elif self["relationship_id"]:
-            keys += ("object_id_list", "object_name_list")
-        return keys
-
-
-class EntityGroupItem(CacheItem):
-    def __getitem__(self, key):
-        if key == "class_id":
-            return self["entity_class_id"]
-        if key == "group_id":
-            return self["entity_id"]
-        if key == "class_name":
-            return (
-                self._get_ref("object_class", self["entity_class_id"], key)
-                or self._get_ref("relationship_class", self["entity_class_id"], key)
-            ).get("name")
-        if key == "group_name":
-            return (
-                self._get_ref("object", self["entity_id"], key) or self._get_ref("relationship", self["entity_id"], key)
-            ).get("name")
-        if key == "member_name":
-            return (
-                self._get_ref("object", self["member_id"], key) or self._get_ref("relationship", self["member_id"], key)
-            ).get("name")
-        if key == "object_class_id":
-            return self._get_ref("object_class", self["entity_class_id"], key).get("id")
-        if key == "relationship_class_id":
-            return self._get_ref("relationship_class", self["entity_class_id"], key).get("id")
-        return super().__getitem__(key)
-
-    def _reference_keys(self):
-        return super()._reference_keys() + ("class_name", "group_name", "member_name")
-
-
-class ScenarioItem(CacheItem):
-    @property
-    def _sorted_scen_alts(self):
-        return sorted(
-            (x for x in self._db_cache.get("scenario_alternative", {}).values() if x["scenario_id"] == self["id"]),
-            key=itemgetter("rank"),
-        )
-
-    def __getitem__(self, key):
-        if key == "active":
-            return dict.get(self, "active", False)
-        if key == "alternative_id_list":
-            return tuple(x.get("alternative_id") for x in self._sorted_scen_alts)
-        if key == "alternative_name_list":
-            return tuple(x.get("alternative_name") for x in self._sorted_scen_alts)
-        return super().__getitem__(key)
-
-
-class ScenarioAlternativeItem(CacheItem):
-    def __getitem__(self, key):
-        if key == "scenario_name":
-            return self._get_ref("scenario", self["scenario_id"], key).get("name")
-        if key == "alternative_name":
-            return self._get_ref("alternative", self["alternative_id"], key).get("name")
-        scen_key = {
-            "before_alternative_id": "alternative_id_list",
-            "before_alternative_name": "alternative_name_list",
-        }.get(key)
-        if scen_key is not None:
-            scenario = self._get_ref("scenario", self["scenario_id"], key)
-            try:
-                return scenario[scen_key][self["rank"]]
-            except IndexError:
-                return None
-        return super().__getitem__(key)
-
-    def _reference_keys(self):
-        return super()._reference_keys() + ("scenario_name", "alternative_name")
-
-
-class FeatureItem(CacheItem):
-    def __getitem__(self, key):
-        if key == "parameter_definition_name":
-            return self._get_ref("parameter_definition", self["parameter_definition_id"], key).get("name")
-        if key in ("entity_class_id", "entity_class_name"):
-            return self._get_ref("parameter_definition", self["parameter_definition_id"], key).get(key)
-        if key == "parameter_value_list_name":
-            return self._get_ref("parameter_value_list", self["parameter_value_list_id"], key).get("name")
-        return super().__getitem__(key)
-
-    def _reference_keys(self):
-        return super()._reference_keys() + (
-            "entity_class_id",
-            "entity_class_name",
-            "parameter_definition_name",
-            "parameter_value_list_name",
-        )
-
-
-class ToolFeatureItem(CacheItem):
-    def __getitem__(self, key):
-        if key in ("entity_class_id", "entity_class_name", "parameter_definition_id", "parameter_definition_name"):
-            return self._get_ref("feature", self["feature_id"], key).get(key)
-        if key == "tool_name":
-            return self._get_ref("tool", self["tool_id"], key).get("name")
-        if key == "parameter_value_list_name":
-            return self._get_ref("parameter_value_list", self["parameter_value_list_id"], key).get("name")
-        if key == "required":
-            return dict.get(self, "required", False)
-        return super().__getitem__(key)
-
-    def _reference_keys(self):
-        return super()._reference_keys() + (
-            "tool_name",
-            "entity_class_id",
-            "entity_class_name",
-            "parameter_definition_id",
-            "parameter_definition_name",
-            "parameter_value_list_name",
-        )
-
-
-class ToolFeatureMethodItem(CacheItem):
-    def __getitem__(self, key):
-        if key in (
-            "tool_id",
-            "tool_name",
-            "feature_id",
-            "entity_class_id",
-            "entity_class_name",
-            "parameter_definition_id",
-            "parameter_definition_name",
-            "parameter_value_list_id",
-            "parameter_value_list_name",
-        ):
-            return self._get_ref("tool_feature", self["tool_feature_id"], key).get(key)
-        if key == "method":
-            value_list = self._get_ref("parameter_value_list", self["parameter_value_list_id"], key)
-            if not value_list:
-                return None
-            try:
-                list_value_id = value_list["value_id_list"][self["method_index"]]
-                return self._get_ref("list_value", list_value_id, key).get("value")
-            except IndexError:
-                return None
-        return super().__getitem__(key)
-
-    def _reference_keys(self):
-        return super()._reference_keys() + (
-            "tool_id",
-            "tool_name",
-            "feature_id",
-            "entity_class_id",
-            "entity_class_name",
-            "parameter_definition_id",
-            "parameter_definition_name",
-            "parameter_value_list_id",
-            "parameter_value_list_name",
-            "method",
-        )
-
-
-class ParameterValueListItem(CacheItem):
-    def _sorted_list_values(self, key):
-        return sorted(
-            (
-                self._get_ref("list_value", x["id"], key)
-                for x in self._db_cache.get("list_value", {}).values()
-                if x["parameter_value_list_id"] == self["id"]
-            ),
-            key=itemgetter("index"),
-        )
-
-    def __getitem__(self, key):
-        if key == "value_index_list":
-            return tuple(x.get("index") for x in self._sorted_list_values(key))
-        if key == "value_id_list":
-            return tuple(x.get("id") for x in self._sorted_list_values(key))
-        return super().__getitem__(key)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+DB cache utility.
+
+"""
+from contextlib import suppress
+from operator import itemgetter
+
+
+class DBCache(dict):
+    def __init__(self, advance_query, *args, **kwargs):
+        """
+        A dictionary that maps table names to ids to items. Used to store and retrieve database contents.
+
+        Args:
+            advance_query (function): A function to call when references aren't found.
+                It receives a table name (a.k.a item type) and should bring more items of that type into this cache.
+        """
+        super().__init__(*args, **kwargs)
+        self._advance_query = advance_query
+
+    def table_cache(self, item_type):
+        return self.setdefault(item_type, TableCache(self, item_type))
+
+    def get_item(self, item_type, id_):
+        table_cache = self.get(item_type, {})
+        item = table_cache.get(id_)
+        if item is None:
+            return {}
+        return item
+
+    def fetch_ref(self, item_type, id_):
+        while self._advance_query(item_type):
+            with suppress(KeyError):
+                return self[item_type][id_]
+        # It is possible that fetching was completed between deciding to call this function
+        # and starting the while loop above resulting in self._advance_query() to return False immediately.
+        # Therefore, we should try one last time if the ref is available.
+        with suppress(KeyError):
+            return self[item_type][id_]
+        return None
+
+    def make_item(self, item_type, item):
+        """Returns a cache item.
+
+        Args:
+            item_type (str): the item type, equal to a table name
+            item (dict): the 'db item' to use as base
+
+        Returns:
+            CacheItem
+        """
+        factory = {
+            "object_class": ObjectClassItem,
+            "object": ObjectItem,
+            "relationship_class": RelationshipClassItem,
+            "relationship": RelationshipItem,
+            "parameter_definition": ParameterDefinitionItem,
+            "parameter_value": ParameterValueItem,
+            "entity_group": EntityGroupItem,
+            "scenario": ScenarioItem,
+            "scenario_alternative": ScenarioAlternativeItem,
+            "feature": FeatureItem,
+            "tool_feature": ToolFeatureItem,
+            "tool_feature_method": ToolFeatureMethodItem,
+            "parameter_value_list": ParameterValueListItem,
+        }.get(item_type, CacheItem)
+        return factory(self, item_type, **item)
+
+
+class TableCache(dict):
+    def __init__(self, db_cache, item_type, *args, **kwargs):
+        """
+        Args:
+            db_cache (DBCache): the DB cache where this table cache belongs.
+            item_type (str): the item type, equal to a table name
+        """
+        super().__init__(*args, **kwargs)
+        self._db_cache = db_cache
+        self._item_type = item_type
+
+    def values(self):
+        return (x for x in super().values() if x.is_valid())
+
+    def add_item(self, item, keep_existing=False):
+        if keep_existing:
+            existing_item = self.get(item["id"])
+            if existing_item is not None:
+                return existing_item
+        self[item["id"]] = new_item = self._db_cache.make_item(self._item_type, item)
+        return new_item
+
+    def update_item(self, item):
+        current_item = self[item["id"]]
+        current_item.update(item)
+        current_item.cascade_update()
+
+    def remove_item(self, id_):
+        """Removes item and its referrers from the cache.
+
+        Args:
+            id_ (int): item's database id
+
+        Returns:
+            list of CacheItem: removed items
+        """
+        current_item = self.get(id_)
+        if current_item:
+            return current_item.cascade_remove()
+        return []
+
+
+class CacheItem(dict):
+    """A dictionary that behaves kinda like a row from a query result.
+
+    It is used to store items in a cache, so we can access them as if they were rows from a query result.
+    This is mainly because we want to use the cache as a replacement for db queries in some methods.
+    """
+
+    def __init__(self, db_cache, item_type, *args, **kwargs):
+        """
+        Args:
+            db_cache (DBCache): the DB cache where this item belongs.
+        """
+        super().__init__(*args, **kwargs)
+        self._db_cache = db_cache
+        self._item_type = item_type
+        self._referrers = {}
+        self._weak_referrers = {}
+        self.readd_callbacks = set()
+        self.update_callbacks = set()
+        self.remove_callbacks = set()
+        self._to_remove = False
+        self._removed = False
+        self._corrupted = False
+        self._valid = None
+
+    @property
+    def item_type(self):
+        return self._item_type
+
+    @property
+    def key(self):
+        if dict.get(self, "id") is None:
+            return None
+        return (self._item_type, self["id"])
+
+    @property
+    def referrers(self):
+        return self._referrers
+
+    def __getattr__(self, name):
+        """Overridden method to return the dictionary key named after the attribute, or None if it doesn't exist."""
+        return self.get(name)
+
+    def __repr__(self):
+        return f"{self._item_type}{self._extended()}"
+
+    def _extended(self):
+        return {**self, **{key: self[key] for key in self._reference_keys()}}
+
+    def _asdict(self):
+        return dict(**self)
+
+    def _reference_keys(self):
+        return ()
+
+    def _get_ref(self, ref_type, ref_id, source_key):
+        ref = self._db_cache.get_item(ref_type, ref_id)
+        if not ref:
+            if source_key not in self._reference_keys():
+                return {}
+            ref = self._db_cache.fetch_ref(ref_type, ref_id)
+            if not ref:
+                self._corrupted = True
+                return {}
+        return self._handle_ref(ref, source_key)
+
+    def _handle_ref(self, ref, source_key):
+        if source_key in self._reference_keys():
+            ref.add_referrer(self)
+            if ref.is_removed():
+                self._to_remove = True
+        else:
+            ref.add_weak_referrer(self)
+            if ref.is_removed():
+                return {}
+        return ref
+
+    def get(self, key, default=None):
+        try:
+            return self[key]
+        except KeyError:
+            return default
+
+    def copy(self):
+        return type(self)(self._db_cache, self._item_type, **self)
+
+    def deepcopy(self):
+        """Makes a deep copy of the item.
+
+        Returns:
+            CacheItem: copied item
+        """
+        copy = self.copy()
+        self._copy_internal_state(copy)
+        return copy
+
+    def _copy_internal_state(self, other):
+        """Copies item's internal state to other cache item.
+
+        Args:
+            other (CacheItem): target item
+        """
+        other._referrers = {key: item.deepcopy() for key, item in self._referrers.items()}
+        other._weak_referrers = {key: item.deepcopy() for key, item in self._weak_referrers.items()}
+        other.readd_callbacks = set(self.readd_callbacks)
+        other.update_callbacks = set(self.update_callbacks)
+        other.remove_callbacks = set(self.remove_callbacks)
+        other._to_remove = self._to_remove
+        other._removed = self._removed
+        other._corrupted = self._corrupted
+        other._valid = self._valid
+
+    def is_valid(self):
+        if self._valid is not None:
+            return self._valid
+        if self._removed or self._corrupted:
+            return False
+        self._to_remove = False
+        self._corrupted = False
+        for key in self._reference_keys():
+            _ = self[key]
+        if self._to_remove:
+            self.cascade_remove()
+        self._valid = not self._removed and not self._corrupted
+        return self._valid
+
+    def is_removed(self):
+        return self._removed
+
+    def add_referrer(self, referrer):
+        if referrer.key is None:
+            return
+        self._referrers[referrer.key] = self._weak_referrers.pop(referrer.key, referrer)
+
+    def add_weak_referrer(self, referrer):
+        if referrer.key is None:
+            return
+        if referrer.key not in self._referrers:
+            self._weak_referrers[referrer.key] = referrer
+
+    def readd(self):
+        """Adds item back to cache without adding its referrers."""
+        if not self._removed:
+            return
+        self._removed = False
+        self._to_remove = False
+        self._call_readd_callbacks()
+
+    def cascade_readd(self):
+        if not self._removed:
+            return
+        self._removed = False
+        self._to_remove = False
+        for referrer in self._referrers.values():
+            referrer.cascade_readd()
+        for weak_referrer in self._weak_referrers.values():
+            weak_referrer.call_update_callbacks()
+        self._call_readd_callbacks()
+
+    def _call_readd_callbacks(self):
+        """Calls readd callbacks and removes obsolete ones."""
+        obsolete = set()
+        for callback in self.readd_callbacks:
+            if not callback(self):
+                obsolete.add(callback)
+        self.readd_callbacks -= obsolete
+
+    def cascade_remove(self):
+        """Sets item and its referrers as removed.
+
+        Calls necessary callbacks on weak referrers too.
+
+        Returns:
+            list of CacheItem: removed items
+        """
+        if self._removed:
+            return []
+        self._removed = True
+        self._to_remove = False
+        self._valid = None
+        obsolete = set()
+        for callback in self.remove_callbacks:
+            if not callback(self):
+                obsolete.add(callback)
+        self.remove_callbacks -= obsolete
+        removed_items = [self]
+        for referrer in self._referrers.values():
+            removed_items += referrer.cascade_remove()
+        for weak_referrer in self._weak_referrers.values():
+            weak_referrer.call_update_callbacks()
+        return removed_items
+
+    def cascade_update(self):
+        self.call_update_callbacks()
+        for weak_referrer in self._weak_referrers.values():
+            weak_referrer.call_update_callbacks()
+        for referrer in self._referrers.values():
+            referrer.cascade_update()
+
+    def call_update_callbacks(self):
+        self.pop("parsed_value", None)
+        obsolete = set()
+        for callback in self.update_callbacks:
+            if not callback(self):
+                obsolete.add(callback)
+        self.update_callbacks -= obsolete
+
+
+class DisplayIconMixin:
+    def __getitem__(self, key):
+        if key == "display_icon":
+            return dict.get(self, "display_icon")
+        return super().__getitem__(key)
+
+
+class DescriptionMixin:
+    def __getitem__(self, key):
+        if key == "description":
+            return dict.get(self, "description")
+        return super().__getitem__(key)
+
+
+class ObjectClassItem(DisplayIconMixin, DescriptionMixin, CacheItem):
+    pass
+
+
+class ObjectItem(DescriptionMixin, CacheItem):
+    def __getitem__(self, key):
+        if key == "class_name":
+            return self._get_ref("object_class", self["class_id"], key).get("name")
+        return super().__getitem__(key)
+
+    def _reference_keys(self):
+        return super()._reference_keys() + ("class_name",)
+
+
+class ObjectClassIdListMixin:
+    def __init__(self, *args, **kwargs):
+        object_class_id_list = kwargs["object_class_id_list"]
+        if isinstance(object_class_id_list, str):
+            object_class_id_list = (int(id_) for id_ in object_class_id_list.split(","))
+        kwargs["object_class_id_list"] = tuple(object_class_id_list)
+        super().__init__(*args, **kwargs)
+
+    def __getitem__(self, key):
+        if key == "object_class_name_list":
+            return tuple(self._get_ref("object_class", id_, key).get("name") for id_ in self["object_class_id_list"])
+        return super().__getitem__(key)
+
+    def _reference_keys(self):
+        return super()._reference_keys() + ("object_class_name_list",)
+
+
+class RelationshipClassItem(DisplayIconMixin, ObjectClassIdListMixin, DescriptionMixin, CacheItem):
+    pass
+
+
+class RelationshipItem(ObjectClassIdListMixin, CacheItem):
+    def __init__(self, db_cache, *args, **kwargs):
+        if "object_class_id_list" not in kwargs:
+            kwargs["object_class_id_list"] = db_cache.get_item("relationship_class", kwargs["class_id"]).get(
+                "object_class_id_list", ()
+            )
+        object_id_list = kwargs.get("object_id_list", ())
+        if isinstance(object_id_list, str):
+            object_id_list = (int(id_) for id_ in object_id_list.split(","))
+        kwargs["object_id_list"] = tuple(object_id_list)
+        super().__init__(db_cache, *args, **kwargs)
+
+    def __getitem__(self, key):
+        if key == "class_name":
+            return self._get_ref("relationship_class", self["class_id"], key).get("name")
+        if key == "object_name_list":
+            return tuple(self._get_ref("object", id_, key).get("name") for id_ in self["object_id_list"])
+        return super().__getitem__(key)
+
+    def _reference_keys(self):
+        return super()._reference_keys() + ("class_name", "object_name_list")
+
+
+class ParameterMixin:
+    def __init__(self, *args, **kwargs):
+        if "entity_class_id" not in kwargs:
+            kwargs["entity_class_id"] = kwargs.get("object_class_id") or kwargs.get("relationship_class_id")
+        super().__init__(*args, **kwargs)
+
+    def __getitem__(self, key):
+        if key in ("object_class_id", "relationship_class_id"):
+            return dict.get(self, key)
+        if key == "object_class_name":
+            if self["object_class_id"] is None:
+                return None
+            return self._get_ref("object_class", self["object_class_id"], key).get("name")
+        if key == "relationship_class_name":
+            if self["relationship_class_id"] is None:
+                return None
+            return self._get_ref("relationship_class", self["relationship_class_id"], key).get("name")
+        if key in ("object_class_id_list", "object_class_name_list"):
+            if self["relationship_class_id"] is None:
+                return None
+            return self._get_ref("relationship_class", self["relationship_class_id"], key).get(key)
+        if key == "entity_class_name":
+            return self["relationship_class_name"] if self["object_class_id"] is None else self["object_class_name"]
+        if key == "parameter_value_list_id":
+            return dict.get(self, key)
+        return super().__getitem__(key)
+
+    def _reference_keys(self):
+        keys = super()._reference_keys()
+        if self["object_class_id"]:
+            keys += ("object_class_name",)
+        elif self["relationship_class_id"]:
+            keys += ("relationship_class_name", "object_class_id_list", "object_class_name_list")
+        return keys
+
+
+class ParameterDefinitionItem(DescriptionMixin, ParameterMixin, CacheItem):
+    def __init__(self, *args, **kwargs):
+        if kwargs.get("list_value_id") is None:
+            kwargs["list_value_id"] = (
+                int(kwargs["default_value"]) if kwargs.get("default_type") == "list_value_ref" else None
+            )
+        super().__init__(*args, **kwargs)
+
+    def __getitem__(self, key):
+        if key == "parameter_name":
+            return super().__getitem__("name")
+        if key == "value_list_id":
+            return super().__getitem__("parameter_value_list_id")
+        if key == "value_list_name":
+            return self._get_ref("parameter_value_list", self["value_list_id"], key).get("name")
+        if key in ("default_value", "default_type"):
+            if self["list_value_id"] is not None:
+                return self._get_ref("list_value", self["list_value_id"], key).get(key.split("_")[1])
+            return dict.get(self, key)
+        return super().__getitem__(key)
+
+
+class ParameterValueItem(ParameterMixin, CacheItem):
+    def __init__(self, *args, **kwargs):
+        if "entity_id" not in kwargs:
+            kwargs["entity_id"] = kwargs.get("object_id") or kwargs.get("relationship_id")
+        if kwargs.get("list_value_id") is None:
+            kwargs["list_value_id"] = int(kwargs["value"]) if kwargs.get("type") == "list_value_ref" else None
+        super().__init__(*args, **kwargs)
+
+    def __getitem__(self, key):
+        if key in ("object_id", "relationship_id"):
+            return dict.get(self, key)
+        if key == "parameter_id":
+            return super().__getitem__("parameter_definition_id")
+        if key == "parameter_name":
+            return self._get_ref("parameter_definition", self["parameter_definition_id"], key).get("name")
+        if key == "object_name":
+            if self["object_id"] is None:
+                return None
+            return self._get_ref("object", self["object_id"], key).get("name")
+        if key in ("object_id_list", "object_name_list"):
+            if self["relationship_id"] is None:
+                return None
+            return self._get_ref("relationship", self["relationship_id"], key).get(key)
+        if key == "alternative_name":
+            return self._get_ref("alternative", self["alternative_id"], key).get("name")
+        if key in ("value", "type") and self["list_value_id"] is not None:
+            return self._get_ref("list_value", self["list_value_id"], key).get(key)
+        return super().__getitem__(key)
+
+    def _reference_keys(self):
+        keys = super()._reference_keys() + ("parameter_name", "alternative_name")
+        if self["object_id"]:
+            keys += ("object_name",)
+        elif self["relationship_id"]:
+            keys += ("object_id_list", "object_name_list")
+        return keys
+
+
+class EntityGroupItem(CacheItem):
+    def __getitem__(self, key):
+        if key == "class_id":
+            return self["entity_class_id"]
+        if key == "group_id":
+            return self["entity_id"]
+        if key == "class_name":
+            return (
+                self._get_ref("object_class", self["entity_class_id"], key)
+                or self._get_ref("relationship_class", self["entity_class_id"], key)
+            ).get("name")
+        if key == "group_name":
+            return (
+                self._get_ref("object", self["entity_id"], key) or self._get_ref("relationship", self["entity_id"], key)
+            ).get("name")
+        if key == "member_name":
+            return (
+                self._get_ref("object", self["member_id"], key) or self._get_ref("relationship", self["member_id"], key)
+            ).get("name")
+        if key == "object_class_id":
+            return self._get_ref("object_class", self["entity_class_id"], key).get("id")
+        if key == "relationship_class_id":
+            return self._get_ref("relationship_class", self["entity_class_id"], key).get("id")
+        return super().__getitem__(key)
+
+    def _reference_keys(self):
+        return super()._reference_keys() + ("class_name", "group_name", "member_name")
+
+
+class ScenarioItem(CacheItem):
+    @property
+    def _sorted_scen_alts(self):
+        return sorted(
+            (x for x in self._db_cache.get("scenario_alternative", {}).values() if x["scenario_id"] == self["id"]),
+            key=itemgetter("rank"),
+        )
+
+    def __getitem__(self, key):
+        if key == "active":
+            return dict.get(self, "active", False)
+        if key == "alternative_id_list":
+            return tuple(x.get("alternative_id") for x in self._sorted_scen_alts)
+        if key == "alternative_name_list":
+            return tuple(x.get("alternative_name") for x in self._sorted_scen_alts)
+        return super().__getitem__(key)
+
+
+class ScenarioAlternativeItem(CacheItem):
+    def __getitem__(self, key):
+        if key == "scenario_name":
+            return self._get_ref("scenario", self["scenario_id"], key).get("name")
+        if key == "alternative_name":
+            return self._get_ref("alternative", self["alternative_id"], key).get("name")
+        scen_key = {
+            "before_alternative_id": "alternative_id_list",
+            "before_alternative_name": "alternative_name_list",
+        }.get(key)
+        if scen_key is not None:
+            scenario = self._get_ref("scenario", self["scenario_id"], key)
+            try:
+                return scenario[scen_key][self["rank"]]
+            except IndexError:
+                return None
+        return super().__getitem__(key)
+
+    def _reference_keys(self):
+        return super()._reference_keys() + ("scenario_name", "alternative_name")
+
+
+class FeatureItem(CacheItem):
+    def __getitem__(self, key):
+        if key == "parameter_definition_name":
+            return self._get_ref("parameter_definition", self["parameter_definition_id"], key).get("name")
+        if key in ("entity_class_id", "entity_class_name"):
+            return self._get_ref("parameter_definition", self["parameter_definition_id"], key).get(key)
+        if key == "parameter_value_list_name":
+            return self._get_ref("parameter_value_list", self["parameter_value_list_id"], key).get("name")
+        return super().__getitem__(key)
+
+    def _reference_keys(self):
+        return super()._reference_keys() + (
+            "entity_class_id",
+            "entity_class_name",
+            "parameter_definition_name",
+            "parameter_value_list_name",
+        )
+
+
+class ToolFeatureItem(CacheItem):
+    def __getitem__(self, key):
+        if key in ("entity_class_id", "entity_class_name", "parameter_definition_id", "parameter_definition_name"):
+            return self._get_ref("feature", self["feature_id"], key).get(key)
+        if key == "tool_name":
+            return self._get_ref("tool", self["tool_id"], key).get("name")
+        if key == "parameter_value_list_name":
+            return self._get_ref("parameter_value_list", self["parameter_value_list_id"], key).get("name")
+        if key == "required":
+            return dict.get(self, "required", False)
+        return super().__getitem__(key)
+
+    def _reference_keys(self):
+        return super()._reference_keys() + (
+            "tool_name",
+            "entity_class_id",
+            "entity_class_name",
+            "parameter_definition_id",
+            "parameter_definition_name",
+            "parameter_value_list_name",
+        )
+
+
+class ToolFeatureMethodItem(CacheItem):
+    def __getitem__(self, key):
+        if key in (
+            "tool_id",
+            "tool_name",
+            "feature_id",
+            "entity_class_id",
+            "entity_class_name",
+            "parameter_definition_id",
+            "parameter_definition_name",
+            "parameter_value_list_id",
+            "parameter_value_list_name",
+        ):
+            return self._get_ref("tool_feature", self["tool_feature_id"], key).get(key)
+        if key == "method":
+            value_list = self._get_ref("parameter_value_list", self["parameter_value_list_id"], key)
+            if not value_list:
+                return None
+            try:
+                list_value_id = value_list["value_id_list"][self["method_index"]]
+                return self._get_ref("list_value", list_value_id, key).get("value")
+            except IndexError:
+                return None
+        return super().__getitem__(key)
+
+    def _reference_keys(self):
+        return super()._reference_keys() + (
+            "tool_id",
+            "tool_name",
+            "feature_id",
+            "entity_class_id",
+            "entity_class_name",
+            "parameter_definition_id",
+            "parameter_definition_name",
+            "parameter_value_list_id",
+            "parameter_value_list_name",
+            "method",
+        )
+
+
+class ParameterValueListItem(CacheItem):
+    def _sorted_list_values(self, key):
+        return sorted(
+            (
+                self._get_ref("list_value", x["id"], key)
+                for x in self._db_cache.get("list_value", {}).values()
+                if x["parameter_value_list_id"] == self["id"]
+            ),
+            key=itemgetter("index"),
+        )
+
+    def __getitem__(self, key):
+        if key == "value_index_list":
+            return tuple(x.get("index") for x in self._sorted_list_values(key))
+        if key == "value_id_list":
+            return tuple(x.get("id") for x in self._sorted_list_values(key))
+        return super().__getitem__(key)
```

### Comparing `spinedb_api-0.30.3/spinedb_api/db_mapping.py` & `spinedb_api-0.30.4/spinedb_api/db_mapping.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Provides :class:`.DatabaseMapping`.
-
-"""
-
-from .db_mapping_query_mixin import DatabaseMappingQueryMixin
-from .db_mapping_base import DatabaseMappingBase
-from .db_mapping_add_mixin import DatabaseMappingAddMixin
-from .db_mapping_check_mixin import DatabaseMappingCheckMixin
-from .db_mapping_update_mixin import DatabaseMappingUpdateMixin
-from .db_mapping_remove_mixin import DatabaseMappingRemoveMixin
-from .db_mapping_commit_mixin import DatabaseMappingCommitMixin
-from .filters.tools import apply_filter_stack, load_filters
-
-
-class DatabaseMapping(
-    DatabaseMappingQueryMixin,
-    DatabaseMappingCheckMixin,
-    DatabaseMappingAddMixin,
-    DatabaseMappingUpdateMixin,
-    DatabaseMappingRemoveMixin,
-    DatabaseMappingCommitMixin,
-    DatabaseMappingBase,
-):
-    """A basic read-write database mapping.
-
-    :param str db_url: A database URL in RFC-1738 format pointing to the database to be mapped.
-    :param str username: A user name. If ``None``, it gets replaced by the string ``"anon"``.
-    :param bool upgrade: Whether or not the db at the given URL should be upgraded to the most recent version.
-    """
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._init_type_attributes()
-        if self._filter_configs is not None:
-            stack = load_filters(self._filter_configs)
-            apply_filter_stack(self, stack)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Provides :class:`.DatabaseMapping`.
+
+"""
+
+from .db_mapping_query_mixin import DatabaseMappingQueryMixin
+from .db_mapping_base import DatabaseMappingBase
+from .db_mapping_add_mixin import DatabaseMappingAddMixin
+from .db_mapping_check_mixin import DatabaseMappingCheckMixin
+from .db_mapping_update_mixin import DatabaseMappingUpdateMixin
+from .db_mapping_remove_mixin import DatabaseMappingRemoveMixin
+from .db_mapping_commit_mixin import DatabaseMappingCommitMixin
+from .filters.tools import apply_filter_stack, load_filters
+
+
+class DatabaseMapping(
+    DatabaseMappingQueryMixin,
+    DatabaseMappingCheckMixin,
+    DatabaseMappingAddMixin,
+    DatabaseMappingUpdateMixin,
+    DatabaseMappingRemoveMixin,
+    DatabaseMappingCommitMixin,
+    DatabaseMappingBase,
+):
+    """A basic read-write database mapping.
+
+    :param str db_url: A database URL in RFC-1738 format pointing to the database to be mapped.
+    :param str username: A user name. If ``None``, it gets replaced by the string ``"anon"``.
+    :param bool upgrade: Whether or not the db at the given URL should be upgraded to the most recent version.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._init_type_attributes()
+        if self._filter_configs is not None:
+            stack = load_filters(self._filter_configs)
+            apply_filter_stack(self, stack)
```

### Comparing `spinedb_api-0.30.3/spinedb_api/db_mapping_add_mixin.py` & `spinedb_api-0.30.4/spinedb_api/db_mapping_add_mixin.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,580 +1,580 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""Provides :class:`.DatabaseMappingAddMixin`.
-
-"""
-# TODO: improve docstrings
-
-from datetime import datetime
-from sqlalchemy import func, Table, Column, Integer, String, null, select
-from sqlalchemy.exc import DBAPIError
-from .exception import SpineDBAPIError, SpineIntegrityError
-from .helpers import get_relationship_entity_class_items, get_relationship_entity_items
-
-
-class DatabaseMappingAddMixin:
-    """Provides methods to perform ``INSERT`` operations over a Spine db."""
-
-    def __init__(self, *args, **kwargs):
-        """Initialize class."""
-        super().__init__(*args, **kwargs)
-        self._next_id = self._metadata.tables.get("next_id")
-        if self._next_id is None:
-            self._next_id = Table(
-                "next_id",
-                self._metadata,
-                Column("user", String(155), primary_key=True),
-                Column("date", String(155), primary_key=True),
-                Column("entity_id", Integer, server_default=null()),
-                Column("entity_class_id", Integer, server_default=null()),
-                Column("entity_group_id", Integer, server_default=null()),
-                Column("parameter_definition_id", Integer, server_default=null()),
-                Column("parameter_value_id", Integer, server_default=null()),
-                Column("parameter_value_list_id", Integer, server_default=null()),
-                Column("list_value_id", Integer, server_default=null()),
-                Column("alternative_id", Integer, server_default=null()),
-                Column("scenario_id", Integer, server_default=null()),
-                Column("scenario_alternative_id", Integer, server_default=null()),
-                Column("tool_id", Integer, server_default=null()),
-                Column("feature_id", Integer, server_default=null()),
-                Column("tool_feature_id", Integer, server_default=null()),
-                Column("tool_feature_method_id", Integer, server_default=null()),
-                Column("metadata_id", Integer, server_default=null()),
-                Column("parameter_value_metadata_id", Integer, server_default=null()),
-                Column("entity_metadata_id", Integer, server_default=null()),
-            )
-            try:
-                self._next_id.create(self.connection)
-            except DBAPIError:
-                # Some other concurrent process must have beaten us to create the table
-                self._next_id = Table("next_id", self._metadata, autoload=True)
-
-    def _add_commit_id_and_ids(self, tablename, *items):
-        if not items:
-            return [], set()
-        ids = self._reserve_ids(tablename, len(items))
-        commit_id = self._make_commit_id()
-        for id_, item in zip(ids, items):
-            item["commit_id"] = commit_id
-            item["id"] = id_
-
-    def _reserve_ids(self, tablename, count):
-        if self.committing:
-            return self._do_reserve_ids(self.connection, tablename, count)
-        with self.engine.begin() as connection:
-            return self._do_reserve_ids(connection, tablename, count)
-
-    def _do_reserve_ids(self, connection, tablename, count):
-        fieldname = {
-            "object_class": "entity_class_id",
-            "object": "entity_id",
-            "relationship_class": "entity_class_id",
-            "relationship": "entity_id",
-            "entity_group": "entity_group_id",
-            "parameter_definition": "parameter_definition_id",
-            "parameter_value": "parameter_value_id",
-            "parameter_value_list": "parameter_value_list_id",
-            "list_value": "list_value_id",
-            "alternative": "alternative_id",
-            "scenario": "scenario_id",
-            "scenario_alternative": "scenario_alternative_id",
-            "tool": "tool_id",
-            "feature": "feature_id",
-            "tool_feature": "tool_feature_id",
-            "tool_feature_method": "tool_feature_method_id",
-            "metadata": "metadata_id",
-            "parameter_value_metadata": "parameter_value_metadata_id",
-            "entity_metadata": "entity_metadata_id",
-        }[tablename]
-        select_next_id = select([self._next_id])
-        next_id_row = connection.execute(select_next_id).first()
-        if next_id_row is None:
-            next_id = None
-            stmt = self._next_id.insert()
-        else:
-            next_id = getattr(next_id_row, fieldname)
-            stmt = self._next_id.update()
-        if next_id is None:
-            table = self._metadata.tables[tablename]
-            id_col = self.table_ids.get(tablename, "id")
-            select_max_id = select([func.max(getattr(table.c, id_col))])
-            max_id = connection.execute(select_max_id).scalar()
-            next_id = max_id + 1 if max_id else 1
-        new_next_id = next_id + count
-        connection.execute(stmt, {"user": self.username, "date": datetime.utcnow(), fieldname: new_next_id})
-        return range(next_id, new_next_id)
-
-    def _readd_items(self, tablename, *items):
-        """Add known items to database."""
-        self._make_commit_id()
-        for _ in self._do_add_items(tablename, *items):
-            pass
-
-    def add_items(
-        self,
-        tablename,
-        *items,
-        check=True,
-        strict=False,
-        return_dups=False,
-        return_items=False,
-        cache=None,
-        readd=False,
-    ):
-        """Add items to db.
-
-        Args:
-            tablename (str)
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be inserted.
-            check (bool): Whether or not to check integrity
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if the insertion of one of the items violates an integrity constraint.
-            return_dups (bool): Whether or not already existing and duplicated entries should also be returned.
-            return_items (bool): Return full items rather than just ids
-            cache (dict, optional): A dict mapping table names to a list of dictionary items, to use as db replacement
-                for queries
-            readd (bool): Readds items directly
-
-        Returns:
-            set: ids or items successfully added
-            list(SpineIntegrityError): found violations
-        """
-        if readd:
-            try:
-                self._readd_items(tablename, *items)
-                return items if return_items else {x["id"] for x in items}, []
-            except SpineDBAPIError as e:
-                return set(), [e]
-
-        if check:
-            checked_items, intgr_error_log = self.check_items(
-                tablename, *items, for_update=False, strict=strict, cache=cache
-            )
-        else:
-            checked_items, intgr_error_log = list(items), []
-        try:
-            ids = self._add_items(tablename, *checked_items)
-        except DBAPIError as e:
-            intgr_error_log.append(SpineIntegrityError(f"Fail to add items: {e.orig.args}"))
-            return set(), intgr_error_log
-        if return_items:
-            return checked_items, intgr_error_log
-        if return_dups:
-            ids.update(set(x.id for x in intgr_error_log if x.id))
-        return ids, intgr_error_log
-
-    def _add_items(self, tablename, *items):
-        """Add items to database without checking integrity.
-
-        Args:
-            tablename (str)
-            items (Iterable): list of dictionaries which correspond to the instances to add
-            strict (bool): if True SpineIntegrityError are raised. Otherwise
-                they are caught and returned as a log
-
-        Returns:
-            ids (set): added instances' ids
-        """
-        self._add_commit_id_and_ids(tablename, *items)
-        for _ in self._do_add_items(tablename, *items):
-            pass
-        return {item["id"] for item in items}
-
-    def _get_table_for_insert(self, tablename):
-        """
-        Returns the table name where to perform insertion.
-
-        Subclasses can override this method to insert to another table instead (e.g., diff...)
-
-        Args:
-            tablename (str): target database table name
-
-        Yields:
-            str: database table name
-        """
-        return self._metadata.tables[tablename]
-
-    def _do_add_items(self, tablename, *items_to_add):
-        if not self.committing:
-            return
-        items_to_add = tuple(self._items_with_type_id(tablename, *items_to_add))
-        try:
-            for tablename_, items_to_add_ in self._items_to_add_per_table(tablename, items_to_add):
-                table = self._get_table_for_insert(tablename_)
-                self._checked_execute(table.insert(), [{**item} for item in items_to_add_])
-                yield tablename_
-        except DBAPIError as e:
-            msg = f"DBAPIError while inserting {tablename} items: {e.orig.args}"
-            raise SpineDBAPIError(msg) from e
-
-    def _items_to_add_per_table(self, tablename, items_to_add):
-        """
-        Yields tuples of string tablename, list of items to insert. Needed because some insert queries
-        actually need to insert records to more than one table.
-
-        Args:
-            tablename (str): target database table name
-            items_to_add (list): items to add
-
-        Yields:
-            tuple: database table name, items to add
-        """
-        if tablename == "object_class":
-            oc_items_to_add = list()
-            append_oc_items_to_add = oc_items_to_add.append
-            for item in items_to_add:
-                append_oc_items_to_add({"entity_class_id": item["id"], "type_id": self.object_class_type})
-            yield ("entity_class", items_to_add)
-            yield ("object_class", oc_items_to_add)
-        elif tablename == "object":
-            o_items_to_add = list()
-            append_o_items_to_add = o_items_to_add.append
-            for item in items_to_add:
-                append_o_items_to_add({"entity_id": item["id"], "type_id": item["type_id"]})
-            yield ("entity", items_to_add)
-            yield ("object", o_items_to_add)
-        elif tablename == "relationship_class":
-            rc_items_to_add = list()
-            rec_items_to_add = list()
-            for item in items_to_add:
-                rc_items_to_add.append({"entity_class_id": item["id"], "type_id": self.relationship_class_type})
-                rec_items_to_add += get_relationship_entity_class_items(item, self.object_class_type)
-            yield ("entity_class", items_to_add)
-            yield ("relationship_class", rc_items_to_add)
-            yield ("relationship_entity_class", rec_items_to_add)
-        elif tablename == "relationship":
-            re_items_to_add = list()
-            r_items_to_add = list()
-            for item in items_to_add:
-                r_items_to_add.append(
-                    {
-                        "entity_id": item["id"],
-                        "entity_class_id": item["class_id"],
-                        "type_id": self.relationship_entity_type,
-                    }
-                )
-                re_items_to_add += get_relationship_entity_items(
-                    item, self.relationship_entity_type, self.object_entity_type
-                )
-            yield ("entity", items_to_add)
-            yield ("relationship", r_items_to_add)
-            yield ("relationship_entity", re_items_to_add)
-        elif tablename == "parameter_definition":
-            for item in items_to_add:
-                item["entity_class_id"] = (
-                    item.get("object_class_id") or item.get("relationship_class_id") or item.get("entity_class_id")
-                )
-            yield ("parameter_definition", items_to_add)
-        elif tablename == "parameter_value":
-            for item in items_to_add:
-                item["entity_id"] = item.get("object_id") or item.get("relationship_id") or item.get("entity_id")
-                item["entity_class_id"] = (
-                    item.get("object_class_id") or item.get("relationship_class_id") or item.get("entity_class_id")
-                )
-            yield ("parameter_value", items_to_add)
-        else:
-            yield (tablename, items_to_add)
-
-    def add_object_classes(self, *items, **kwargs):
-        return self.add_items("object_class", *items, **kwargs)
-
-    def add_objects(self, *items, **kwargs):
-        return self.add_items("object", *items, **kwargs)
-
-    def add_wide_relationship_classes(self, *items, **kwargs):
-        return self.add_items("relationship_class", *items, **kwargs)
-
-    def add_wide_relationships(self, *items, **kwargs):
-        return self.add_items("relationship", *items, **kwargs)
-
-    def add_parameter_definitions(self, *items, **kwargs):
-        return self.add_items("parameter_definition", *items, **kwargs)
-
-    def add_parameter_values(self, *items, **kwargs):
-        return self.add_items("parameter_value", *items, **kwargs)
-
-    def add_parameter_value_lists(self, *items, **kwargs):
-        return self.add_items("parameter_value_list", *items, **kwargs)
-
-    def add_list_values(self, *items, **kwargs):
-        return self.add_items("list_value", *items, **kwargs)
-
-    def add_features(self, *items, **kwargs):
-        return self.add_items("feature", *items, **kwargs)
-
-    def add_tools(self, *items, **kwargs):
-        return self.add_items("tool", *items, **kwargs)
-
-    def add_tool_features(self, *items, **kwargs):
-        return self.add_items("tool_feature", *items, **kwargs)
-
-    def add_tool_feature_methods(self, *items, **kwargs):
-        return self.add_items("tool_feature_method", *items, **kwargs)
-
-    def add_alternatives(self, *items, **kwargs):
-        return self.add_items("alternative", *items, **kwargs)
-
-    def add_scenarios(self, *items, **kwargs):
-        return self.add_items("scenario", *items, **kwargs)
-
-    def add_scenario_alternatives(self, *items, **kwargs):
-        return self.add_items("scenario_alternative", *items, **kwargs)
-
-    def add_entity_groups(self, *items, **kwargs):
-        return self.add_items("entity_group", *items, **kwargs)
-
-    def add_metadata(self, *items, **kwargs):
-        return self.add_items("metadata", *items, **kwargs)
-
-    def add_entity_metadata(self, *items, **kwargs):
-        return self.add_items("entity_metadata", *items, **kwargs)
-
-    def add_parameter_value_metadata(self, *items, **kwargs):
-        return self.add_items("parameter_value_metadata", *items, **kwargs)
-
-    def _get_or_add_metadata_ids_for_items(self, *items, check, strict, cache):
-        metadata_ids = {}
-        for entry in cache.get("metadata", {}).values():
-            metadata_ids.setdefault(entry.name, {})[entry.value] = entry.id
-        metadata_to_add = []
-        items_missing_metadata_ids = {}
-        for item in items:
-            existing_values = metadata_ids.get(item["metadata_name"])
-            existing_id = existing_values.get(item["metadata_value"]) if existing_values is not None else None
-            if existing_values is None or existing_id is None:
-                metadata_to_add.append({"name": item["metadata_name"], "value": item["metadata_value"]})
-                items_missing_metadata_ids.setdefault(item["metadata_name"], {})[item["metadata_value"]] = item
-            else:
-                item["metadata_id"] = existing_id
-        added_metadata, errors = self.add_items(
-            "metadata", *metadata_to_add, check=check, strict=strict, return_items=True, cache=cache
-        )
-        for x in added_metadata:
-            cache.table_cache("metadata").add_item(x)
-        if errors:
-            return added_metadata, errors
-        new_metadata_ids = {}
-        for added in added_metadata:
-            new_metadata_ids.setdefault(added["name"], {})[added["value"]] = added["id"]
-        for metadata_name, value_to_item in items_missing_metadata_ids.items():
-            for metadata_value, item in value_to_item.items():
-                item["metadata_id"] = new_metadata_ids[metadata_name][metadata_value]
-        return added_metadata, errors
-
-    def _add_ext_item_metadata(self, table_name, *items, check=True, strict=False, return_items=False, cache=None):
-        # Note, that even though return_items can be False, it doesn't make much sense here because we'll be mixing
-        # metadata and entity metadata ids.
-        if cache is None:
-            cache = self.make_cache({table_name}, include_ancestors=True)
-        added_metadata, metadata_errors = self._get_or_add_metadata_ids_for_items(
-            *items, check=check, strict=strict, cache=cache
-        )
-        if metadata_errors:
-            if not return_items:
-                return added_metadata, metadata_errors
-            return {i["id"] for i in added_metadata}, metadata_errors
-        added_item_metadata, item_errors = self.add_items(
-            table_name, *items, check=check, strict=strict, return_items=True, cache=cache
-        )
-        errors = metadata_errors + item_errors
-        if not return_items:
-            return {i["id"] for i in added_metadata + added_item_metadata}, errors
-        return added_metadata + added_item_metadata, errors
-
-    def add_ext_entity_metadata(self, *items, check=True, strict=False, return_items=False, cache=None, readd=False):
-        return self._add_ext_item_metadata(
-            "entity_metadata", *items, check=check, strict=strict, return_items=return_items, cache=cache
-        )
-
-    def add_ext_parameter_value_metadata(
-        self, *items, check=True, strict=False, return_items=False, cache=None, readd=False
-    ):
-        return self._add_ext_item_metadata(
-            "parameter_value_metadata", *items, check=check, strict=strict, return_items=return_items, cache=cache
-        )
-
-    def _add_object_classes(self, *items):
-        return self._add_items("object_class", *items)
-
-    def _add_objects(self, *items):
-        return self._add_items("object", *items)
-
-    def _add_wide_relationship_classes(self, *items):
-        return self._add_items("relationship_class", *items)
-
-    def _add_wide_relationships(self, *items):
-        return self._add_items("relationship", *items)
-
-    def _add_parameter_definitions(self, *items):
-        return self._add_items("parameter_definition", *items)
-
-    def _add_parameter_values(self, *items):
-        return self._add_items("parameter_value", *items)
-
-    def _add_parameter_value_lists(self, *items):
-        return self._add_items("parameter_value_list", *items)
-
-    def _add_list_values(self, *items):
-        return self._add_items("list_value", *items)
-
-    def _add_features(self, *items):
-        return self._add_items("feature", *items)
-
-    def _add_tools(self, *items):
-        return self._add_items("tool", *items)
-
-    def _add_tool_features(self, *items):
-        return self._add_items("tool_feature", *items)
-
-    def _add_tool_feature_methods(self, *items):
-        return self._add_items("tool_feature_method", *items)
-
-    def _add_alternatives(self, *items):
-        return self._add_items("alternative", *items)
-
-    def _add_scenarios(self, *items):
-        return self._add_items("scenario", *items)
-
-    def _add_scenario_alternatives(self, *items):
-        return self._add_items("scenario_alternative", *items)
-
-    def _add_entity_groups(self, *items):
-        return self._add_items("entity_group", *items)
-
-    def _add_metadata(self, *items):
-        return self._add_items("metadata", *items)
-
-    def _add_parameter_value_metadata(self, *items):
-        return self._add_items("parameter_value_metadata", *items)
-
-    def _add_entity_metadata(self, *items):
-        return self._add_items("entity_metadata", *items)
-
-    def add_object_class(self, **kwargs):
-        """Stage an object class item for insertion.
-
-        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-
-        :returns:
-            - **new_item** -- The item successfully staged for insertion.
-
-        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
-        """
-        sq = self.object_class_sq
-        ids, _ = self.add_object_classes(kwargs, strict=True)
-        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
-
-    def add_object(self, **kwargs):
-        """Stage an object item for insertion.
-
-        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-
-        :returns:
-            - **new_item** -- The item successfully staged for insertion.
-
-        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
-        """
-        sq = self.object_sq
-        ids, _ = self.add_objects(kwargs, strict=True)
-        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
-
-    def add_wide_relationship_class(self, **kwargs):
-        """Stage a relationship class item for insertion.
-
-        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-
-        :returns:
-            - **new_item** -- The item successfully staged for insertion.
-
-        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
-        """
-        sq = self.wide_relationship_class_sq
-        ids, _ = self.add_wide_relationship_classes(kwargs, strict=True)
-        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
-
-    def add_wide_relationship(self, **kwargs):
-        """Stage a relationship item for insertion.
-
-        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-
-        :returns:
-            - **new_item** -- The item successfully staged for insertion.
-
-        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
-        """
-        sq = self.wide_relationship_sq
-        ids, _ = self.add_wide_relationships(kwargs, strict=True)
-        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
-
-    def add_parameter_definition(self, **kwargs):
-        """Stage a parameter definition item for insertion.
-
-        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-
-        :returns:
-            - **new_item** -- The item successfully staged for insertion.
-
-        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
-        """
-        sq = self.parameter_definition_sq
-        ids, _ = self.add_parameter_definitions(kwargs, strict=True)
-        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
-
-    def add_parameter_value(self, **kwargs):
-        """Stage a parameter value item for insertion.
-
-        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-
-        :returns:
-            - **new_item** -- The item successfully staged for insertion.
-
-        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
-        """
-        sq = self.parameter_value_sq
-        ids, _ = self.add_parameter_values(kwargs, strict=True)
-        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
-
-    def get_or_add_object_class(self, **kwargs):
-        """Stage an object class item for insertion if it doesn't already exists in the db.
-
-        :returns:
-            - **item** -- The item successfully staged for insertion or already existing.
-
-        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
-        """
-        sq = self.object_class_sq
-        ids, _ = self.add_object_classes(kwargs, return_dups=True)
-        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
-
-    def get_or_add_object(self, **kwargs):
-        """Stage an object item for insertion if it doesn't already exists in the db.
-
-        :returns:
-            - **item** -- The item successfully staged for insertion or already existing.
-
-        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
-        """
-        sq = self.object_sq
-        ids, _ = self.add_objects(kwargs, return_dups=True)
-        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
-
-    def get_or_add_parameter_definition(self, **kwargs):
-        """Stage a parameter definition item for insertion if it doesn't already exists in the db.
-
-        :returns:
-            - **item** -- The item successfully staged for insertion or already existing.
-
-        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
-        """
-        sq = self.parameter_definition_sq
-        ids, _ = self.add_parameter_definitions(kwargs, return_dups=True)
-        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""Provides :class:`.DatabaseMappingAddMixin`.
+
+"""
+# TODO: improve docstrings
+
+from datetime import datetime
+from sqlalchemy import func, Table, Column, Integer, String, null, select
+from sqlalchemy.exc import DBAPIError
+from .exception import SpineDBAPIError, SpineIntegrityError
+from .helpers import get_relationship_entity_class_items, get_relationship_entity_items
+
+
+class DatabaseMappingAddMixin:
+    """Provides methods to perform ``INSERT`` operations over a Spine db."""
+
+    def __init__(self, *args, **kwargs):
+        """Initialize class."""
+        super().__init__(*args, **kwargs)
+        self._next_id = self._metadata.tables.get("next_id")
+        if self._next_id is None:
+            self._next_id = Table(
+                "next_id",
+                self._metadata,
+                Column("user", String(155), primary_key=True),
+                Column("date", String(155), primary_key=True),
+                Column("entity_id", Integer, server_default=null()),
+                Column("entity_class_id", Integer, server_default=null()),
+                Column("entity_group_id", Integer, server_default=null()),
+                Column("parameter_definition_id", Integer, server_default=null()),
+                Column("parameter_value_id", Integer, server_default=null()),
+                Column("parameter_value_list_id", Integer, server_default=null()),
+                Column("list_value_id", Integer, server_default=null()),
+                Column("alternative_id", Integer, server_default=null()),
+                Column("scenario_id", Integer, server_default=null()),
+                Column("scenario_alternative_id", Integer, server_default=null()),
+                Column("tool_id", Integer, server_default=null()),
+                Column("feature_id", Integer, server_default=null()),
+                Column("tool_feature_id", Integer, server_default=null()),
+                Column("tool_feature_method_id", Integer, server_default=null()),
+                Column("metadata_id", Integer, server_default=null()),
+                Column("parameter_value_metadata_id", Integer, server_default=null()),
+                Column("entity_metadata_id", Integer, server_default=null()),
+            )
+            try:
+                self._next_id.create(self.connection)
+            except DBAPIError:
+                # Some other concurrent process must have beaten us to create the table
+                self._next_id = Table("next_id", self._metadata, autoload=True)
+
+    def _add_commit_id_and_ids(self, tablename, *items):
+        if not items:
+            return [], set()
+        ids = self._reserve_ids(tablename, len(items))
+        commit_id = self._make_commit_id()
+        for id_, item in zip(ids, items):
+            item["commit_id"] = commit_id
+            item["id"] = id_
+
+    def _reserve_ids(self, tablename, count):
+        if self.committing:
+            return self._do_reserve_ids(self.connection, tablename, count)
+        with self.engine.begin() as connection:
+            return self._do_reserve_ids(connection, tablename, count)
+
+    def _do_reserve_ids(self, connection, tablename, count):
+        fieldname = {
+            "object_class": "entity_class_id",
+            "object": "entity_id",
+            "relationship_class": "entity_class_id",
+            "relationship": "entity_id",
+            "entity_group": "entity_group_id",
+            "parameter_definition": "parameter_definition_id",
+            "parameter_value": "parameter_value_id",
+            "parameter_value_list": "parameter_value_list_id",
+            "list_value": "list_value_id",
+            "alternative": "alternative_id",
+            "scenario": "scenario_id",
+            "scenario_alternative": "scenario_alternative_id",
+            "tool": "tool_id",
+            "feature": "feature_id",
+            "tool_feature": "tool_feature_id",
+            "tool_feature_method": "tool_feature_method_id",
+            "metadata": "metadata_id",
+            "parameter_value_metadata": "parameter_value_metadata_id",
+            "entity_metadata": "entity_metadata_id",
+        }[tablename]
+        select_next_id = select([self._next_id])
+        next_id_row = connection.execute(select_next_id).first()
+        if next_id_row is None:
+            next_id = None
+            stmt = self._next_id.insert()
+        else:
+            next_id = getattr(next_id_row, fieldname)
+            stmt = self._next_id.update()
+        if next_id is None:
+            table = self._metadata.tables[tablename]
+            id_col = self.table_ids.get(tablename, "id")
+            select_max_id = select([func.max(getattr(table.c, id_col))])
+            max_id = connection.execute(select_max_id).scalar()
+            next_id = max_id + 1 if max_id else 1
+        new_next_id = next_id + count
+        connection.execute(stmt, {"user": self.username, "date": datetime.utcnow(), fieldname: new_next_id})
+        return range(next_id, new_next_id)
+
+    def _readd_items(self, tablename, *items):
+        """Add known items to database."""
+        self._make_commit_id()
+        for _ in self._do_add_items(tablename, *items):
+            pass
+
+    def add_items(
+        self,
+        tablename,
+        *items,
+        check=True,
+        strict=False,
+        return_dups=False,
+        return_items=False,
+        cache=None,
+        readd=False,
+    ):
+        """Add items to db.
+
+        Args:
+            tablename (str)
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be inserted.
+            check (bool): Whether or not to check integrity
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if the insertion of one of the items violates an integrity constraint.
+            return_dups (bool): Whether or not already existing and duplicated entries should also be returned.
+            return_items (bool): Return full items rather than just ids
+            cache (dict, optional): A dict mapping table names to a list of dictionary items, to use as db replacement
+                for queries
+            readd (bool): Readds items directly
+
+        Returns:
+            set: ids or items successfully added
+            list(SpineIntegrityError): found violations
+        """
+        if readd:
+            try:
+                self._readd_items(tablename, *items)
+                return items if return_items else {x["id"] for x in items}, []
+            except SpineDBAPIError as e:
+                return set(), [e]
+
+        if check:
+            checked_items, intgr_error_log = self.check_items(
+                tablename, *items, for_update=False, strict=strict, cache=cache
+            )
+        else:
+            checked_items, intgr_error_log = list(items), []
+        try:
+            ids = self._add_items(tablename, *checked_items)
+        except DBAPIError as e:
+            intgr_error_log.append(SpineIntegrityError(f"Fail to add items: {e.orig.args}"))
+            return set(), intgr_error_log
+        if return_items:
+            return checked_items, intgr_error_log
+        if return_dups:
+            ids.update(set(x.id for x in intgr_error_log if x.id))
+        return ids, intgr_error_log
+
+    def _add_items(self, tablename, *items):
+        """Add items to database without checking integrity.
+
+        Args:
+            tablename (str)
+            items (Iterable): list of dictionaries which correspond to the instances to add
+            strict (bool): if True SpineIntegrityError are raised. Otherwise
+                they are caught and returned as a log
+
+        Returns:
+            ids (set): added instances' ids
+        """
+        self._add_commit_id_and_ids(tablename, *items)
+        for _ in self._do_add_items(tablename, *items):
+            pass
+        return {item["id"] for item in items}
+
+    def _get_table_for_insert(self, tablename):
+        """
+        Returns the table name where to perform insertion.
+
+        Subclasses can override this method to insert to another table instead (e.g., diff...)
+
+        Args:
+            tablename (str): target database table name
+
+        Yields:
+            str: database table name
+        """
+        return self._metadata.tables[tablename]
+
+    def _do_add_items(self, tablename, *items_to_add):
+        if not self.committing:
+            return
+        items_to_add = tuple(self._items_with_type_id(tablename, *items_to_add))
+        try:
+            for tablename_, items_to_add_ in self._items_to_add_per_table(tablename, items_to_add):
+                table = self._get_table_for_insert(tablename_)
+                self._checked_execute(table.insert(), [{**item} for item in items_to_add_])
+                yield tablename_
+        except DBAPIError as e:
+            msg = f"DBAPIError while inserting {tablename} items: {e.orig.args}"
+            raise SpineDBAPIError(msg) from e
+
+    def _items_to_add_per_table(self, tablename, items_to_add):
+        """
+        Yields tuples of string tablename, list of items to insert. Needed because some insert queries
+        actually need to insert records to more than one table.
+
+        Args:
+            tablename (str): target database table name
+            items_to_add (list): items to add
+
+        Yields:
+            tuple: database table name, items to add
+        """
+        if tablename == "object_class":
+            oc_items_to_add = list()
+            append_oc_items_to_add = oc_items_to_add.append
+            for item in items_to_add:
+                append_oc_items_to_add({"entity_class_id": item["id"], "type_id": self.object_class_type})
+            yield ("entity_class", items_to_add)
+            yield ("object_class", oc_items_to_add)
+        elif tablename == "object":
+            o_items_to_add = list()
+            append_o_items_to_add = o_items_to_add.append
+            for item in items_to_add:
+                append_o_items_to_add({"entity_id": item["id"], "type_id": item["type_id"]})
+            yield ("entity", items_to_add)
+            yield ("object", o_items_to_add)
+        elif tablename == "relationship_class":
+            rc_items_to_add = list()
+            rec_items_to_add = list()
+            for item in items_to_add:
+                rc_items_to_add.append({"entity_class_id": item["id"], "type_id": self.relationship_class_type})
+                rec_items_to_add += get_relationship_entity_class_items(item, self.object_class_type)
+            yield ("entity_class", items_to_add)
+            yield ("relationship_class", rc_items_to_add)
+            yield ("relationship_entity_class", rec_items_to_add)
+        elif tablename == "relationship":
+            re_items_to_add = list()
+            r_items_to_add = list()
+            for item in items_to_add:
+                r_items_to_add.append(
+                    {
+                        "entity_id": item["id"],
+                        "entity_class_id": item["class_id"],
+                        "type_id": self.relationship_entity_type,
+                    }
+                )
+                re_items_to_add += get_relationship_entity_items(
+                    item, self.relationship_entity_type, self.object_entity_type
+                )
+            yield ("entity", items_to_add)
+            yield ("relationship", r_items_to_add)
+            yield ("relationship_entity", re_items_to_add)
+        elif tablename == "parameter_definition":
+            for item in items_to_add:
+                item["entity_class_id"] = (
+                    item.get("object_class_id") or item.get("relationship_class_id") or item.get("entity_class_id")
+                )
+            yield ("parameter_definition", items_to_add)
+        elif tablename == "parameter_value":
+            for item in items_to_add:
+                item["entity_id"] = item.get("object_id") or item.get("relationship_id") or item.get("entity_id")
+                item["entity_class_id"] = (
+                    item.get("object_class_id") or item.get("relationship_class_id") or item.get("entity_class_id")
+                )
+            yield ("parameter_value", items_to_add)
+        else:
+            yield (tablename, items_to_add)
+
+    def add_object_classes(self, *items, **kwargs):
+        return self.add_items("object_class", *items, **kwargs)
+
+    def add_objects(self, *items, **kwargs):
+        return self.add_items("object", *items, **kwargs)
+
+    def add_wide_relationship_classes(self, *items, **kwargs):
+        return self.add_items("relationship_class", *items, **kwargs)
+
+    def add_wide_relationships(self, *items, **kwargs):
+        return self.add_items("relationship", *items, **kwargs)
+
+    def add_parameter_definitions(self, *items, **kwargs):
+        return self.add_items("parameter_definition", *items, **kwargs)
+
+    def add_parameter_values(self, *items, **kwargs):
+        return self.add_items("parameter_value", *items, **kwargs)
+
+    def add_parameter_value_lists(self, *items, **kwargs):
+        return self.add_items("parameter_value_list", *items, **kwargs)
+
+    def add_list_values(self, *items, **kwargs):
+        return self.add_items("list_value", *items, **kwargs)
+
+    def add_features(self, *items, **kwargs):
+        return self.add_items("feature", *items, **kwargs)
+
+    def add_tools(self, *items, **kwargs):
+        return self.add_items("tool", *items, **kwargs)
+
+    def add_tool_features(self, *items, **kwargs):
+        return self.add_items("tool_feature", *items, **kwargs)
+
+    def add_tool_feature_methods(self, *items, **kwargs):
+        return self.add_items("tool_feature_method", *items, **kwargs)
+
+    def add_alternatives(self, *items, **kwargs):
+        return self.add_items("alternative", *items, **kwargs)
+
+    def add_scenarios(self, *items, **kwargs):
+        return self.add_items("scenario", *items, **kwargs)
+
+    def add_scenario_alternatives(self, *items, **kwargs):
+        return self.add_items("scenario_alternative", *items, **kwargs)
+
+    def add_entity_groups(self, *items, **kwargs):
+        return self.add_items("entity_group", *items, **kwargs)
+
+    def add_metadata(self, *items, **kwargs):
+        return self.add_items("metadata", *items, **kwargs)
+
+    def add_entity_metadata(self, *items, **kwargs):
+        return self.add_items("entity_metadata", *items, **kwargs)
+
+    def add_parameter_value_metadata(self, *items, **kwargs):
+        return self.add_items("parameter_value_metadata", *items, **kwargs)
+
+    def _get_or_add_metadata_ids_for_items(self, *items, check, strict, cache):
+        metadata_ids = {}
+        for entry in cache.get("metadata", {}).values():
+            metadata_ids.setdefault(entry.name, {})[entry.value] = entry.id
+        metadata_to_add = []
+        items_missing_metadata_ids = {}
+        for item in items:
+            existing_values = metadata_ids.get(item["metadata_name"])
+            existing_id = existing_values.get(item["metadata_value"]) if existing_values is not None else None
+            if existing_values is None or existing_id is None:
+                metadata_to_add.append({"name": item["metadata_name"], "value": item["metadata_value"]})
+                items_missing_metadata_ids.setdefault(item["metadata_name"], {})[item["metadata_value"]] = item
+            else:
+                item["metadata_id"] = existing_id
+        added_metadata, errors = self.add_items(
+            "metadata", *metadata_to_add, check=check, strict=strict, return_items=True, cache=cache
+        )
+        for x in added_metadata:
+            cache.table_cache("metadata").add_item(x)
+        if errors:
+            return added_metadata, errors
+        new_metadata_ids = {}
+        for added in added_metadata:
+            new_metadata_ids.setdefault(added["name"], {})[added["value"]] = added["id"]
+        for metadata_name, value_to_item in items_missing_metadata_ids.items():
+            for metadata_value, item in value_to_item.items():
+                item["metadata_id"] = new_metadata_ids[metadata_name][metadata_value]
+        return added_metadata, errors
+
+    def _add_ext_item_metadata(self, table_name, *items, check=True, strict=False, return_items=False, cache=None):
+        # Note, that even though return_items can be False, it doesn't make much sense here because we'll be mixing
+        # metadata and entity metadata ids.
+        if cache is None:
+            cache = self.make_cache({table_name}, include_ancestors=True)
+        added_metadata, metadata_errors = self._get_or_add_metadata_ids_for_items(
+            *items, check=check, strict=strict, cache=cache
+        )
+        if metadata_errors:
+            if not return_items:
+                return added_metadata, metadata_errors
+            return {i["id"] for i in added_metadata}, metadata_errors
+        added_item_metadata, item_errors = self.add_items(
+            table_name, *items, check=check, strict=strict, return_items=True, cache=cache
+        )
+        errors = metadata_errors + item_errors
+        if not return_items:
+            return {i["id"] for i in added_metadata + added_item_metadata}, errors
+        return added_metadata + added_item_metadata, errors
+
+    def add_ext_entity_metadata(self, *items, check=True, strict=False, return_items=False, cache=None, readd=False):
+        return self._add_ext_item_metadata(
+            "entity_metadata", *items, check=check, strict=strict, return_items=return_items, cache=cache
+        )
+
+    def add_ext_parameter_value_metadata(
+        self, *items, check=True, strict=False, return_items=False, cache=None, readd=False
+    ):
+        return self._add_ext_item_metadata(
+            "parameter_value_metadata", *items, check=check, strict=strict, return_items=return_items, cache=cache
+        )
+
+    def _add_object_classes(self, *items):
+        return self._add_items("object_class", *items)
+
+    def _add_objects(self, *items):
+        return self._add_items("object", *items)
+
+    def _add_wide_relationship_classes(self, *items):
+        return self._add_items("relationship_class", *items)
+
+    def _add_wide_relationships(self, *items):
+        return self._add_items("relationship", *items)
+
+    def _add_parameter_definitions(self, *items):
+        return self._add_items("parameter_definition", *items)
+
+    def _add_parameter_values(self, *items):
+        return self._add_items("parameter_value", *items)
+
+    def _add_parameter_value_lists(self, *items):
+        return self._add_items("parameter_value_list", *items)
+
+    def _add_list_values(self, *items):
+        return self._add_items("list_value", *items)
+
+    def _add_features(self, *items):
+        return self._add_items("feature", *items)
+
+    def _add_tools(self, *items):
+        return self._add_items("tool", *items)
+
+    def _add_tool_features(self, *items):
+        return self._add_items("tool_feature", *items)
+
+    def _add_tool_feature_methods(self, *items):
+        return self._add_items("tool_feature_method", *items)
+
+    def _add_alternatives(self, *items):
+        return self._add_items("alternative", *items)
+
+    def _add_scenarios(self, *items):
+        return self._add_items("scenario", *items)
+
+    def _add_scenario_alternatives(self, *items):
+        return self._add_items("scenario_alternative", *items)
+
+    def _add_entity_groups(self, *items):
+        return self._add_items("entity_group", *items)
+
+    def _add_metadata(self, *items):
+        return self._add_items("metadata", *items)
+
+    def _add_parameter_value_metadata(self, *items):
+        return self._add_items("parameter_value_metadata", *items)
+
+    def _add_entity_metadata(self, *items):
+        return self._add_items("entity_metadata", *items)
+
+    def add_object_class(self, **kwargs):
+        """Stage an object class item for insertion.
+
+        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+
+        :returns:
+            - **new_item** -- The item successfully staged for insertion.
+
+        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
+        """
+        sq = self.object_class_sq
+        ids, _ = self.add_object_classes(kwargs, strict=True)
+        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
+
+    def add_object(self, **kwargs):
+        """Stage an object item for insertion.
+
+        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+
+        :returns:
+            - **new_item** -- The item successfully staged for insertion.
+
+        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
+        """
+        sq = self.object_sq
+        ids, _ = self.add_objects(kwargs, strict=True)
+        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
+
+    def add_wide_relationship_class(self, **kwargs):
+        """Stage a relationship class item for insertion.
+
+        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+
+        :returns:
+            - **new_item** -- The item successfully staged for insertion.
+
+        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
+        """
+        sq = self.wide_relationship_class_sq
+        ids, _ = self.add_wide_relationship_classes(kwargs, strict=True)
+        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
+
+    def add_wide_relationship(self, **kwargs):
+        """Stage a relationship item for insertion.
+
+        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+
+        :returns:
+            - **new_item** -- The item successfully staged for insertion.
+
+        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
+        """
+        sq = self.wide_relationship_sq
+        ids, _ = self.add_wide_relationships(kwargs, strict=True)
+        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
+
+    def add_parameter_definition(self, **kwargs):
+        """Stage a parameter definition item for insertion.
+
+        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+
+        :returns:
+            - **new_item** -- The item successfully staged for insertion.
+
+        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
+        """
+        sq = self.parameter_definition_sq
+        ids, _ = self.add_parameter_definitions(kwargs, strict=True)
+        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
+
+    def add_parameter_value(self, **kwargs):
+        """Stage a parameter value item for insertion.
+
+        :raises SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+
+        :returns:
+            - **new_item** -- The item successfully staged for insertion.
+
+        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
+        """
+        sq = self.parameter_value_sq
+        ids, _ = self.add_parameter_values(kwargs, strict=True)
+        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
+
+    def get_or_add_object_class(self, **kwargs):
+        """Stage an object class item for insertion if it doesn't already exists in the db.
+
+        :returns:
+            - **item** -- The item successfully staged for insertion or already existing.
+
+        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
+        """
+        sq = self.object_class_sq
+        ids, _ = self.add_object_classes(kwargs, return_dups=True)
+        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
+
+    def get_or_add_object(self, **kwargs):
+        """Stage an object item for insertion if it doesn't already exists in the db.
+
+        :returns:
+            - **item** -- The item successfully staged for insertion or already existing.
+
+        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
+        """
+        sq = self.object_sq
+        ids, _ = self.add_objects(kwargs, return_dups=True)
+        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
+
+    def get_or_add_parameter_definition(self, **kwargs):
+        """Stage a parameter definition item for insertion if it doesn't already exists in the db.
+
+        :returns:
+            - **item** -- The item successfully staged for insertion or already existing.
+
+        :rtype: :class:`~sqlalchemy.util.KeyedTuple`
+        """
+        sq = self.parameter_definition_sq
+        ids, _ = self.add_parameter_definitions(kwargs, return_dups=True)
+        return self.query(sq).filter(sq.c.id.in_(ids)).one_or_none()
```

### Comparing `spinedb_api-0.30.3/spinedb_api/db_mapping_base.py` & `spinedb_api-0.30.4/spinedb_api/db_mapping_base.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,2167 +1,2166 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""Provides :class:`.DatabaseMappingBase`.
-
-"""
-# TODO: Finish docstrings
-import uuid
-import hashlib
-import os
-import logging
-import time
-from collections import Counter
-from types import MethodType
-from contextlib import contextmanager
-from sqlalchemy import create_engine, case, MetaData, Table, Column, false, and_, func, inspect, cast, Integer, or_
-from sqlalchemy.sql.expression import label, Alias
-from sqlalchemy.engine.url import make_url, URL
-from sqlalchemy.orm import Session, aliased
-from sqlalchemy.exc import DatabaseError
-from sqlalchemy.event import listen
-from sqlalchemy.pool import NullPool
-from alembic.migration import MigrationContext
-from alembic.environment import EnvironmentContext
-from alembic.script import ScriptDirectory
-from alembic.config import Config
-from alembic.util.exc import CommandError
-from .exception import SpineDBAPIError, SpineDBVersionError
-from .helpers import (
-    _create_first_spine_database,
-    create_new_spine_database,
-    compare_schemas,
-    forward_sweep,
-    group_concat,
-    model_meta,
-    copy_database_bind,
-)
-from .filters.tools import pop_filter_configs
-from .spine_db_client import get_db_url_from_server
-from .db_cache import DBCache
-
-logging.getLogger("alembic").setLevel(logging.CRITICAL)
-
-
-class DatabaseMappingBase:
-    """Base class for all database mappings.
-
-    It provides the :meth:`query` method for custom db querying.
-    """
-
-    _session_kwargs = {}
-    ITEM_TYPES = (
-        "object_class",
-        "relationship_class",
-        "parameter_value_list",
-        "list_value",
-        "parameter_definition",
-        "object",
-        "relationship",
-        "entity_group",
-        "parameter_value",
-        "alternative",
-        "scenario",
-        "scenario_alternative",
-        "feature",
-        "tool",
-        "tool_feature",
-        "tool_feature_method",
-        "metadata",
-        "entity_metadata",
-        "parameter_value_metadata",
-    )
-
-    def __init__(
-        self,
-        db_url,
-        username=None,
-        upgrade=False,
-        codename=None,
-        create=False,
-        apply_filters=True,
-        memory=False,
-        sqlite_timeout=1800,
-        advance_cache_query=None,
-    ):
-        """
-        Args:
-            db_url (str or URL): A URL in RFC-1738 format pointing to the database to be mapped, or to a DB server.
-            username (str, optional): A user name. If ``None``, it gets replaced by the string ``"anon"``.
-            upgrade (bool): Whether or not the db at the given URL should be upgraded to the most recent version.
-            codename (str, optional): A name that uniquely identifies the class instance within a client application.
-            create (bool): Whether or not to create a Spine db at the given URL if it's not already.
-            apply_filters (bool): Whether or not filters in the URL's query part are applied to the database map.
-            memory (bool): Whether or not to use a sqlite memory db as replacement for this DB map.
-        """
-        if advance_cache_query is None:
-            advance_cache_query = self._advance_cache_query
-        # FIXME: We should also check the server memory property and use it here
-        db_url = get_db_url_from_server(db_url)
-        self.db_url = str(db_url)
-        if isinstance(db_url, str):
-            filter_configs, db_url = pop_filter_configs(db_url)
-        elif isinstance(db_url, URL):
-            filter_configs = db_url.query.pop("spinedbfilter", [])
-        else:
-            filter_configs = []
-        self._filter_configs = filter_configs if apply_filters else None
-        self.sa_url = make_url(db_url)
-        self.username = username if username else "anon"
-        self.codename = self._make_codename(codename)
-        self._memory = memory
-        self.committing = True
-        self._memory_dirty = False
-        self._original_engine = self.create_engine(
-            self.sa_url, upgrade=upgrade, create=create, sqlite_timeout=sqlite_timeout
-        )
-        # NOTE: The NullPool is needed to receive the close event (or any events), for some reason
-        self.engine = create_engine("sqlite://", poolclass=NullPool) if self._memory else self._original_engine
-        self.connection = self.engine.connect()
-        if self._memory:
-            copy_database_bind(self.connection, self._original_engine)
-        listen(self.engine, 'close', self._receive_engine_close)
-        self._metadata = MetaData(self.connection)
-        self._metadata.reflect()
-        self._tablenames = [t.name for t in self._metadata.sorted_tables]
-        self.session = Session(self.connection, **self._session_kwargs)
-        self.cache = DBCache(advance_cache_query)
-        # class and entity type id
-        self._object_class_type = None
-        self._relationship_class_type = None
-        self._object_entity_type = None
-        self._relationship_entity_type = None
-        # Subqueries that select everything from each table
-        self._commit_sq = None
-        self._alternative_sq = None
-        self._scenario_sq = None
-        self._scenario_alternative_sq = None
-        self._entity_class_sq = None
-        self._entity_sq = None
-        self._entity_class_type_sq = None
-        self._entity_type_sq = None
-        self._object_class_sq = None
-        self._object_sq = None
-        self._relationship_class_sq = None
-        self._relationship_sq = None
-        self._entity_group_sq = None
-        self._parameter_definition_sq = None
-        self._parameter_value_sq = None
-        self._parameter_value_list_sq = None
-        self._list_value_sq = None
-        self._feature_sq = None
-        self._tool_sq = None
-        self._tool_feature_sq = None
-        self._tool_feature_method_sq = None
-        self._metadata_sq = None
-        self._parameter_value_metadata_sq = None
-        self._entity_metadata_sq = None
-        self._clean_parameter_value_sq = None
-        # Special convenience subqueries that join two or more tables
-        self._ext_parameter_value_list_sq = None
-        self._wide_parameter_value_list_sq = None
-        self._ord_list_value_sq = None
-        self._ext_scenario_sq = None
-        self._wide_scenario_sq = None
-        self._linked_scenario_alternative_sq = None
-        self._ext_linked_scenario_alternative_sq = None
-        self._ext_object_sq = None
-        self._ext_relationship_class_sq = None
-        self._wide_relationship_class_sq = None
-        self._ext_relationship_class_object_parameter_definition_sq = None
-        self._wide_relationship_class_object_parameter_definition_sq = None
-        self._ext_relationship_sq = None
-        self._wide_relationship_sq = None
-        self._ext_entity_group_sq = None
-        self._entity_parameter_definition_sq = None
-        self._object_parameter_definition_sq = None
-        self._relationship_parameter_definition_sq = None
-        self._entity_parameter_value_sq = None
-        self._object_parameter_value_sq = None
-        self._relationship_parameter_value_sq = None
-        self._ext_feature_sq = None
-        self._ext_tool_feature_sq = None
-        self._ext_tool_feature_method_sq = None
-        self._ext_parameter_value_metadata_sq = None
-        self._ext_entity_metadata_sq = None
-        # Import alternative suff
-        self._import_alternative_id = None
-        self._import_alternative_name = None
-        self._table_to_sq_attr = {}
-        # Table primary ids map:
-        self.table_ids = {
-            "relationship_entity_class": "entity_class_id",
-            "object_class": "entity_class_id",
-            "relationship_class": "entity_class_id",
-            "object": "entity_id",
-            "relationship": "entity_id",
-            "relationship_entity": "entity_id",
-        }
-        self.composite_pks = {
-            "relationship_entity": ("entity_id", "dimension"),
-            "relationship_entity_class": ("entity_class_id", "dimension"),
-        }
-        # Subqueries used to populate cache
-        self.cache_sqs = {
-            "entity": "entity_sq",
-            "feature": "feature_sq",
-            "tool": "tool_sq",
-            "tool_feature": "tool_feature_sq",
-            "tool_feature_method": "tool_feature_method_sq",
-            "parameter_value_list": "parameter_value_list_sq",
-            "list_value": "list_value_sq",
-            "alternative": "alternative_sq",
-            "scenario": "scenario_sq",
-            "scenario_alternative": "scenario_alternative_sq",
-            "object_class": "object_class_sq",
-            "object": "object_sq",
-            "relationship_class": "wide_relationship_class_sq",
-            "relationship": "wide_relationship_sq",
-            "entity_group": "entity_group_sq",
-            "parameter_definition": "parameter_definition_sq",
-            "parameter_value": "clean_parameter_value_sq",
-            "metadata": "metadata_sq",
-            "entity_metadata": "ext_entity_metadata_sq",
-            "parameter_value_metadata": "ext_parameter_value_metadata_sq",
-            "commit": "commit_sq",
-        }
-        self.ancestor_tablenames = {
-            "feature": ("parameter_definition",),
-            "tool_feature": ("tool", "feature"),
-            "tool_feature_method": ("tool_feature", "parameter_value_list", "list_value"),
-            "scenario_alternative": ("scenario", "alternative"),
-            "relationship_class": ("object_class",),
-            "object": ("object_class",),
-            "entity_group": ("object_class", "relationship_class", "object", "relationship"),
-            "relationship": ("relationship_class", "object"),
-            "parameter_definition": ("object_class", "relationship_class", "parameter_value_list", "list_value"),
-            "parameter_value": (
-                "alternative",
-                "object_class",
-                "relationship_class",
-                "object",
-                "relationship",
-                "parameter_definition",
-                "parameter_value_list",
-                "list_value",
-            ),
-            "entity_metadata": ("metadata", "object", "object_class", "relationship", "relationship_class"),
-            "parameter_value_metadata": (
-                "metadata",
-                "parameter_value",
-                "parameter_definition",
-                "object",
-                "object_class",
-                "relationship",
-                "relationship_class",
-                "alternative",
-            ),
-            "list_value": ("parameter_value_list",),
-        }
-        self.descendant_tablenames = {
-            tablename: set(self._descendant_tablenames(tablename)) for tablename in self.cache_sqs
-        }
-        self.object_class_type = None
-        self.relationship_class_type = None
-        self.object_entity_type = None
-        self.relationship_entity_type = None
-
-    def _init_type_attributes(self):
-        self.object_class_type = (
-            self.query(self.entity_class_type_sq).filter(self.entity_class_type_sq.c.name == "object").first().id
-        )
-        self.relationship_class_type = (
-            self.query(self.entity_class_type_sq).filter(self.entity_class_type_sq.c.name == "relationship").first().id
-        )
-        self.object_entity_type = (
-            self.query(self.entity_type_sq).filter(self.entity_type_sq.c.name == "object").first().id
-        )
-        self.relationship_entity_type = (
-            self.query(self.entity_type_sq).filter(self.entity_type_sq.c.name == "relationship").first().id
-        )
-
-    def __enter__(self):
-        return self
-
-    def __exit__(self, _exc_type, _exc_val, _exc_tb):
-        self.connection.close()
-
-    @contextmanager
-    def override_committing(self, new_committing):
-        committing = self.committing
-        self.committing = new_committing
-        try:
-            yield None
-        finally:
-            self.committing = committing
-
-    def _descendant_tablenames(self, tablename):
-        child_tablenames = {
-            "alternative": ("parameter_value", "scenario_alternative"),
-            "scenario": ("scenario_alternative",),
-            "object_class": ("object", "relationship_class", "parameter_definition"),
-            "object": ("relationship", "parameter_value", "entity_group", "entity_metadata"),
-            "relationship_class": ("relationship", "parameter_definition"),
-            "relationship": ("parameter_value", "entity_group", "entity_metadata"),
-            "parameter_definition": ("parameter_value", "feature"),
-            "parameter_value_list": ("feature",),
-            "parameter_value": ("parameter_value_metadata", "entity_metadata"),
-            "feature": ("tool_feature",),
-            "tool": ("tool_feature",),
-            "tool_feature": ("tool_feature_method",),
-            "entity_metadata": ("metadata",),
-            "parameter_value_metadata": ("metadata",),
-        }
-        for parent, children in child_tablenames.items():
-            if tablename == parent:
-                for child in children:
-                    yield child
-                    yield from self._descendant_tablenames(child)
-
-    def sorted_tablenames(self):
-        tablenames = list(self.ITEM_TYPES)
-        sorted_tablenames = []
-        while tablenames:
-            tablename = tablenames.pop(0)
-            ancestors = self.ancestor_tablenames.get(tablename)
-            if ancestors is None or all(x in sorted_tablenames for x in ancestors):
-                sorted_tablenames.append(tablename)
-            else:
-                tablenames.append(tablename)
-        return sorted_tablenames
-
-    def commit_id(self):
-        return self._commit_id
-
-    def _make_commit_id(self):
-        return None
-
-    def _check_commit(self, comment):
-        """Raises if commit not possible.
-
-        Args:
-            comment (str): commit message
-        """
-        if not self.has_pending_changes():
-            raise SpineDBAPIError("Nothing to commit.")
-        if not comment:
-            raise SpineDBAPIError("Commit message cannot be empty.")
-
-    def _make_codename(self, codename):
-        if codename:
-            return str(codename)
-        if not self.sa_url.drivername.startswith("sqlite"):
-            return self.sa_url.database
-        if self.sa_url.database is not None:
-            return os.path.basename(self.sa_url.database)
-        hashing = hashlib.sha1()
-        hashing.update(bytes(str(time.time()), "utf-8"))
-        return hashing.hexdigest()
-
-    @staticmethod
-    def create_engine(sa_url, upgrade=False, create=False, sqlite_timeout=1800):
-        """Create engine.
-
-        Args
-            sa_url (URL)
-            upgrade (bool, optional): If True, upgrade the db to the latest version.
-            create (bool, optional): If True, create a new Spine db at the given url if none found.
-
-        Returns
-            Engine
-        """
-        if sa_url.drivername == "sqlite":
-            connect_args = {'timeout': sqlite_timeout}
-        else:
-            connect_args = {}
-        try:
-            engine = create_engine(sa_url, connect_args=connect_args)
-            with engine.connect():
-                pass
-        except Exception as e:
-            raise SpineDBAPIError(
-                f"Could not connect to '{sa_url}': {str(e)}. "
-                f"Please make sure that '{sa_url}' is a valid sqlalchemy URL."
-            ) from None
-        config = Config()
-        config.set_main_option("script_location", "spinedb_api:alembic")
-        script = ScriptDirectory.from_config(config)
-        head = script.get_current_head()
-        with engine.connect() as connection:
-            migration_context = MigrationContext.configure(connection)
-            try:
-                current = migration_context.get_current_revision()
-            except DatabaseError as error:
-                raise SpineDBAPIError(str(error)) from None
-            if current is None:
-                # No revision information. Check that the schema of the given url corresponds to a 'first' Spine db
-                # Otherwise we either raise or create a new Spine db at the url.
-                ref_engine = _create_first_spine_database("sqlite://")
-                if not compare_schemas(engine, ref_engine):
-                    if not create or inspect(engine).get_table_names():
-                        raise SpineDBAPIError(
-                            "Unable to determine db revision. "
-                            f"Please check that\n\n\t{sa_url}\n\nis the URL of a valid Spine db."
-                        )
-                    return create_new_spine_database(sa_url)
-            if current != head:
-                if not upgrade:
-                    try:
-                        script.get_revision(current)  # Check if current revision is part of alembic rev. history
-                    except CommandError:
-                        # Can't find 'current' revision
-                        raise SpineDBVersionError(
-                            url=sa_url, current=current, expected=head, upgrade_available=False
-                        ) from None
-                    raise SpineDBVersionError(url=sa_url, current=current, expected=head)
-
-                # Upgrade function
-                def upgrade_to_head(rev, context):
-                    return script._upgrade_revs("head", rev)
-
-                with EnvironmentContext(
-                    config,
-                    script,
-                    fn=upgrade_to_head,
-                    as_sql=False,
-                    starting_rev=None,
-                    destination_rev="head",
-                    tag=None,
-                ) as environment_context:
-                    environment_context.configure(connection=connection, target_metadata=model_meta)
-                    with environment_context.begin_transaction():
-                        environment_context.run_migrations()
-        return engine
-
-    def _receive_engine_close(self, dbapi_con, _connection_record):
-        if dbapi_con == self.connection.connection.connection and self._memory_dirty:
-            copy_database_bind(self._original_engine, self.connection)
-
-    def reconnect(self):
-        self.connection = self.engine.connect()
-
-    def in_(self, column, values):
-        """Returns an expression equivalent to column.in_(values), that circumvents the
-        'too many sql variables' problem in sqlite."""
-        if not values:
-            return false()
-        if not self.sa_url.drivername.startswith("sqlite"):
-            return column.in_(values)
-        in_value = Table(
-            "in_value_" + str(uuid.uuid4()),
-            MetaData(),
-            Column("value", column.type, primary_key=True),
-            prefixes=['TEMPORARY'],
-        )
-        in_value.create(self.connection, checkfirst=True)
-        python_type = column.type.python_type
-        self._checked_execute(in_value.insert(), [{"value": python_type(val)} for val in set(values)])
-        return column.in_(self.query(in_value.c.value))
-
-    def _get_table_to_sq_attr(self):
-        if not self._table_to_sq_attr:
-            self._table_to_sq_attr = self._make_table_to_sq_attr()
-        return self._table_to_sq_attr
-
-    def _make_table_to_sq_attr(self):
-        """Returns a dict mapping table names to subquery attribute names, involving that table."""
-        # This 'loads' our subquery attributes
-        for attr in dir(self):
-            getattr(self, attr)
-        table_to_sq_attr = {}
-        for attr, val in vars(self).items():
-            if not isinstance(val, Alias):
-                continue
-            tables = set()
-
-            def _func(x):
-                if isinstance(x, Table):
-                    tables.add(x.name)  # pylint: disable=cell-var-from-loop
-
-            forward_sweep(val, _func)
-            # Now `tables` contains all tables related to `val`
-            for table in tables:
-                table_to_sq_attr.setdefault(table, set()).add(attr)
-        return table_to_sq_attr
-
-    def _clear_subqueries(self, *tablenames):
-        """Set to `None` subquery attributes involving the affected tables.
-        This forces the subqueries to be refreshed when the corresponding property is accessed.
-        """
-        attr_names = set(attr for tablename in tablenames for attr in self._get_table_to_sq_attr().get(tablename, []))
-        for attr_name in attr_names:
-            setattr(self, attr_name, None)
-        tablenames = list(tablenames)
-        for tablename in tablenames:
-            if self.cache.pop(tablename, None):
-                self._do_advance_cache_query(tablename)
-
-    def query(self, *args, **kwargs):
-        """Return a sqlalchemy :class:`~sqlalchemy.orm.query.Query` object applied
-        to this :class:`.DatabaseMappingBase`.
-
-        To perform custom ``SELECT`` statements, call this method with one or more of the class documented
-        :class:`~sqlalchemy.sql.expression.Alias` properties. For example, to select the object class with
-        ``id`` equal to 1::
-
-            from spinedb_api import DatabaseMapping
-            url = 'sqlite:///spine.db'
-            ...
-            db_map = DatabaseMapping(url)
-            db_map.query(db_map.object_class_sq).filter_by(id=1).one_or_none()
-
-        To perform more complex queries, just use this method in combination with the SQLAlchemy API.
-        For example, to select all object class names and the names of their objects concatenated in a string::
-
-            from sqlalchemy import func
-
-            db_map.query(
-                db_map.object_class_sq.c.name, func.group_concat(db_map.object_sq.c.name)
-            ).filter(
-                db_map.object_sq.c.class_id == db_map.object_class_sq.c.id
-            ).group_by(db_map.object_class_sq.c.name).all()
-        """
-        return self.session.query(*args, **kwargs)
-
-    def _subquery(self, tablename):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM tablename
-
-        Args:
-            tablename (str): the table to be queried.
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        table = self._metadata.tables[tablename]
-        return self.query(table).subquery()
-
-    @property
-    def alternative_sq(self):
-        if self._alternative_sq is None:
-            self._alternative_sq = self._make_alternative_sq()
-        return self._alternative_sq
-
-    @property
-    def scenario_sq(self):
-        if self._scenario_sq is None:
-            self._scenario_sq = self._make_scenario_sq()
-        return self._scenario_sq
-
-    @property
-    def scenario_alternative_sq(self):
-        if self._scenario_alternative_sq is None:
-            self._scenario_alternative_sq = self._make_scenario_alternative_sq()
-        return self._scenario_alternative_sq
-
-    @property
-    def entity_class_type_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM class_type
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._entity_class_type_sq is None:
-            self._entity_class_type_sq = self._subquery("entity_class_type")
-        return self._entity_class_type_sq
-
-    @property
-    def entity_type_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM class_type
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._entity_type_sq is None:
-            self._entity_type_sq = self._subquery("entity_type")
-        return self._entity_type_sq
-
-    @property
-    def entity_class_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM class
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._entity_class_sq is None:
-            self._entity_class_sq = self._make_entity_class_sq()
-        return self._entity_class_sq
-
-    @property
-    def entity_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM entity
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._entity_sq is None:
-            self._entity_sq = self._make_entity_sq()
-        return self._entity_sq
-
-    @property
-    def object_class_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM object_class
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._object_class_sq is None:
-            object_class_sq = self._subquery("object_class")
-            self._object_class_sq = (
-                self.query(
-                    self.entity_class_sq.c.id.label("id"),
-                    self.entity_class_sq.c.name.label("name"),
-                    self.entity_class_sq.c.description.label("description"),
-                    self.entity_class_sq.c.display_order.label("display_order"),
-                    self.entity_class_sq.c.display_icon.label("display_icon"),
-                    self.entity_class_sq.c.hidden.label("hidden"),
-                    self.entity_class_sq.c.commit_id.label("commit_id"),
-                )
-                .filter(self.entity_class_sq.c.id == object_class_sq.c.entity_class_id)
-                .subquery()
-            )
-        return self._object_class_sq
-
-    @property
-    def object_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM object
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._object_sq is None:
-            object_sq = self._subquery("object")
-            self._object_sq = (
-                self.query(
-                    self.entity_sq.c.id.label("id"),
-                    self.entity_sq.c.class_id.label("class_id"),
-                    self.entity_sq.c.name.label("name"),
-                    self.entity_sq.c.description.label("description"),
-                    self.entity_sq.c.commit_id.label("commit_id"),
-                )
-                .filter(self.entity_sq.c.id == object_sq.c.entity_id)
-                .subquery()
-            )
-        return self._object_sq
-
-    @property
-    def relationship_class_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM relationship_class
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._relationship_class_sq is None:
-            rel_ent_cls_sq = self._subquery("relationship_entity_class")
-            self._relationship_class_sq = (
-                self.query(
-                    rel_ent_cls_sq.c.entity_class_id.label("id"),
-                    rel_ent_cls_sq.c.dimension.label("dimension"),
-                    rel_ent_cls_sq.c.member_class_id.label("object_class_id"),
-                    self.entity_class_sq.c.name.label("name"),
-                    self.entity_class_sq.c.description.label("description"),
-                    self.entity_class_sq.c.display_icon.label("display_icon"),
-                    self.entity_class_sq.c.hidden.label("hidden"),
-                    self.entity_class_sq.c.commit_id.label("commit_id"),
-                )
-                .filter(self.entity_class_sq.c.id == rel_ent_cls_sq.c.entity_class_id)
-                .subquery()
-            )
-        return self._relationship_class_sq
-
-    @property
-    def relationship_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM relationship
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._relationship_sq is None:
-            rel_ent_sq = self._subquery("relationship_entity")
-            self._relationship_sq = (
-                self.query(
-                    rel_ent_sq.c.entity_id.label("id"),
-                    rel_ent_sq.c.dimension.label("dimension"),
-                    rel_ent_sq.c.member_id.label("object_id"),
-                    rel_ent_sq.c.entity_class_id.label("class_id"),
-                    self.entity_sq.c.name.label("name"),
-                    self.entity_sq.c.commit_id.label("commit_id"),
-                )
-                .filter(self.entity_sq.c.id == rel_ent_sq.c.entity_id)
-                .subquery()
-            )
-        return self._relationship_sq
-
-    @property
-    def entity_group_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM entity_group
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._entity_group_sq is None:
-            group_entity = aliased(self.entity_sq)
-            member_entity = aliased(self.entity_sq)
-            entity_group_sq = self._subquery("entity_group")
-            self._entity_group_sq = (
-                self.query(
-                    entity_group_sq.c.id,
-                    entity_group_sq.c.entity_class_id,
-                    group_entity.c.id.label("entity_id"),
-                    member_entity.c.id.label("member_id"),
-                )
-                .join(group_entity, group_entity.c.id == entity_group_sq.c.entity_id)
-                .join(member_entity, member_entity.c.id == entity_group_sq.c.member_id)
-                .subquery()
-            )
-        return self._entity_group_sq
-
-    @property
-    def parameter_definition_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM parameter_definition
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-
-        if self._parameter_definition_sq is None:
-            self._parameter_definition_sq = self._make_parameter_definition_sq()
-        return self._parameter_definition_sq
-
-    @property
-    def parameter_value_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM parameter_value
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._parameter_value_sq is None:
-            self._parameter_value_sq = self._make_parameter_value_sq()
-        return self._parameter_value_sq
-
-    @property
-    def clean_parameter_value_sq(self):
-        """A subquery of the parameter_value table that excludes rows with filtered entities.
-        This yields the correct results whenever there are both a scenario filter that filters some parameter values,
-        and a tool filter that then filters some entities based on the value of some their parameters
-        after the scenario filtering. Mildly insane.
-        """
-        if self._clean_parameter_value_sq is None:
-            self._clean_parameter_value_sq = (
-                self.query(self.parameter_value_sq)
-                .join(self.entity_sq, self.entity_sq.c.id == self.parameter_value_sq.c.entity_id)
-                .subquery()
-            )
-        return self._clean_parameter_value_sq
-
-    @property
-    def parameter_value_list_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT * FROM parameter_value_list
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._parameter_value_list_sq is None:
-            self._parameter_value_list_sq = self._subquery("parameter_value_list")
-        return self._parameter_value_list_sq
-
-    @property
-    def list_value_sq(self):
-        if self._list_value_sq is None:
-            self._list_value_sq = self._subquery("list_value")
-        return self._list_value_sq
-
-    @property
-    def feature_sq(self):
-        if self._feature_sq is None:
-            self._feature_sq = self._subquery("feature")
-        return self._feature_sq
-
-    @property
-    def tool_sq(self):
-        if self._tool_sq is None:
-            self._tool_sq = self._subquery("tool")
-        return self._tool_sq
-
-    @property
-    def tool_feature_sq(self):
-        if self._tool_feature_sq is None:
-            self._tool_feature_sq = self._subquery("tool_feature")
-        return self._tool_feature_sq
-
-    @property
-    def tool_feature_method_sq(self):
-        if self._tool_feature_method_sq is None:
-            self._tool_feature_method_sq = self._subquery("tool_feature_method")
-        return self._tool_feature_method_sq
-
-    @property
-    def metadata_sq(self):
-        if self._metadata_sq is None:
-            self._metadata_sq = self._subquery("metadata")
-        return self._metadata_sq
-
-    @property
-    def parameter_value_metadata_sq(self):
-        if self._parameter_value_metadata_sq is None:
-            self._parameter_value_metadata_sq = self._subquery("parameter_value_metadata")
-        return self._parameter_value_metadata_sq
-
-    @property
-    def entity_metadata_sq(self):
-        if self._entity_metadata_sq is None:
-            self._entity_metadata_sq = self._subquery("entity_metadata")
-        return self._entity_metadata_sq
-
-    @property
-    def commit_sq(self):
-        if self._commit_sq is None:
-            commit_sq = self._subquery("commit")
-            self._commit_sq = self.query(commit_sq).filter(commit_sq.c.comment != "").subquery()
-        return self._commit_sq
-
-    @property
-    def ext_parameter_value_list_sq(self):
-        if self._ext_parameter_value_list_sq is None:
-            self._ext_parameter_value_list_sq = (
-                self.query(
-                    self.parameter_value_list_sq.c.id,
-                    self.parameter_value_list_sq.c.name,
-                    self.parameter_value_list_sq.c.commit_id,
-                    self.list_value_sq.c.id.label("value_id"),
-                    self.list_value_sq.c.index.label("value_index"),
-                ).outerjoin(
-                    self.list_value_sq,
-                    self.list_value_sq.c.parameter_value_list_id == self.parameter_value_list_sq.c.id,
-                )
-            ).subquery()
-        return self._ext_parameter_value_list_sq
-
-    @property
-    def wide_parameter_value_list_sq(self):
-        if self._wide_parameter_value_list_sq is None:
-            self._wide_parameter_value_list_sq = (
-                self.query(
-                    self.ext_parameter_value_list_sq.c.id,
-                    self.ext_parameter_value_list_sq.c.name,
-                    self.ext_parameter_value_list_sq.c.commit_id,
-                    group_concat(
-                        self.ext_parameter_value_list_sq.c.value_id, self.ext_parameter_value_list_sq.c.value_index
-                    ).label("value_id_list"),
-                    group_concat(
-                        self.ext_parameter_value_list_sq.c.value_index, self.ext_parameter_value_list_sq.c.value_index
-                    ).label("value_index_list"),
-                ).group_by(
-                    self.ext_parameter_value_list_sq.c.id,
-                    self.ext_parameter_value_list_sq.c.name,
-                    self.ext_parameter_value_list_sq.c.commit_id,
-                )
-            ).subquery()
-        return self._wide_parameter_value_list_sq
-
-    @property
-    def ord_list_value_sq(self):
-        if self._ord_list_value_sq is None:
-            self._ord_list_value_sq = (
-                self.query(
-                    self.list_value_sq.c.id,
-                    self.list_value_sq.c.parameter_value_list_id,
-                    self.list_value_sq.c.index,
-                    self.list_value_sq.c.value,
-                    self.list_value_sq.c.type,
-                    self.list_value_sq.c.commit_id,
-                )
-                .order_by(self.list_value_sq.c.parameter_value_list_id, self.list_value_sq.c.index)
-                .subquery()
-            )
-        return self._ord_list_value_sq
-
-    @property
-    def ext_scenario_sq(self):
-        if self._ext_scenario_sq is None:
-            self._ext_scenario_sq = (
-                self.query(
-                    self.scenario_sq.c.id.label("id"),
-                    self.scenario_sq.c.name.label("name"),
-                    self.scenario_sq.c.description.label("description"),
-                    self.scenario_sq.c.active.label("active"),
-                    self.scenario_alternative_sq.c.alternative_id.label("alternative_id"),
-                    self.scenario_alternative_sq.c.rank.label("rank"),
-                    self.alternative_sq.c.name.label("alternative_name"),
-                    self.scenario_sq.c.commit_id.label("commit_id"),
-                )
-                .outerjoin(
-                    self.scenario_alternative_sq, self.scenario_alternative_sq.c.scenario_id == self.scenario_sq.c.id
-                )
-                .outerjoin(
-                    self.alternative_sq, self.alternative_sq.c.id == self.scenario_alternative_sq.c.alternative_id
-                )
-                .order_by(self.scenario_sq.c.id, self.scenario_alternative_sq.c.rank)
-                .subquery()
-            )
-        return self._ext_scenario_sq
-
-    @property
-    def wide_scenario_sq(self):
-        if self._wide_scenario_sq is None:
-            self._wide_scenario_sq = (
-                self.query(
-                    self.ext_scenario_sq.c.id.label("id"),
-                    self.ext_scenario_sq.c.name.label("name"),
-                    self.ext_scenario_sq.c.description.label("description"),
-                    self.ext_scenario_sq.c.active.label("active"),
-                    self.ext_scenario_sq.c.commit_id.label("commit_id"),
-                    group_concat(self.ext_scenario_sq.c.alternative_id, self.ext_scenario_sq.c.rank).label(
-                        "alternative_id_list"
-                    ),
-                    group_concat(self.ext_scenario_sq.c.alternative_name, self.ext_scenario_sq.c.rank).label(
-                        "alternative_name_list"
-                    ),
-                )
-                .group_by(
-                    self.ext_scenario_sq.c.id,
-                    self.ext_scenario_sq.c.name,
-                    self.ext_scenario_sq.c.description,
-                    self.ext_scenario_sq.c.active,
-                    self.ext_scenario_sq.c.commit_id,
-                )
-                .subquery()
-            )
-        return self._wide_scenario_sq
-
-    @property
-    def linked_scenario_alternative_sq(self):
-        if self._linked_scenario_alternative_sq is None:
-            scenario_next_alternative = aliased(self.scenario_alternative_sq)
-            self._linked_scenario_alternative_sq = (
-                self.query(
-                    self.scenario_alternative_sq.c.id.label("id"),
-                    self.scenario_alternative_sq.c.scenario_id.label("scenario_id"),
-                    self.scenario_alternative_sq.c.alternative_id.label("alternative_id"),
-                    self.scenario_alternative_sq.c.rank.label("rank"),
-                    scenario_next_alternative.c.alternative_id.label("before_alternative_id"),
-                    scenario_next_alternative.c.rank.label("before_rank"),
-                    self.scenario_alternative_sq.c.commit_id.label("commit_id"),
-                )
-                .outerjoin(
-                    scenario_next_alternative,
-                    and_(
-                        scenario_next_alternative.c.scenario_id == self.scenario_alternative_sq.c.scenario_id,
-                        scenario_next_alternative.c.rank == self.scenario_alternative_sq.c.rank + 1,
-                    ),
-                )
-                .order_by(self.scenario_alternative_sq.c.scenario_id, self.scenario_alternative_sq.c.rank)
-                .subquery()
-            )
-        return self._linked_scenario_alternative_sq
-
-    @property
-    def ext_linked_scenario_alternative_sq(self):
-        if self._ext_linked_scenario_alternative_sq is None:
-            next_alternative = aliased(self.alternative_sq)
-            self._ext_linked_scenario_alternative_sq = (
-                self.query(
-                    self.linked_scenario_alternative_sq.c.id.label("id"),
-                    self.linked_scenario_alternative_sq.c.scenario_id.label("scenario_id"),
-                    self.scenario_sq.c.name.label("scenario_name"),
-                    self.linked_scenario_alternative_sq.c.alternative_id.label("alternative_id"),
-                    self.alternative_sq.c.name.label("alternative_name"),
-                    self.linked_scenario_alternative_sq.c.rank.label("rank"),
-                    self.linked_scenario_alternative_sq.c.before_alternative_id.label("before_alternative_id"),
-                    self.linked_scenario_alternative_sq.c.before_rank.label("before_rank"),
-                    next_alternative.c.name.label("before_alternative_name"),
-                    self.linked_scenario_alternative_sq.c.commit_id.label("commit_id"),
-                )
-                .filter(self.linked_scenario_alternative_sq.c.scenario_id == self.scenario_sq.c.id)
-                .filter(self.alternative_sq.c.id == self.linked_scenario_alternative_sq.c.alternative_id)
-                .outerjoin(
-                    next_alternative,
-                    next_alternative.c.id == self.linked_scenario_alternative_sq.c.before_alternative_id,
-                )
-                .subquery()
-            )
-        return self._ext_linked_scenario_alternative_sq
-
-    @property
-    def ext_object_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT
-                o.id,
-                o.class_id,
-                oc.name AS class_name,
-                o.name,
-                o.description,
-            FROM object AS o, object_class AS oc
-            WHERE o.class_id = oc.id
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._ext_object_sq is None:
-            self._ext_object_sq = (
-                self.query(
-                    self.object_sq.c.id.label("id"),
-                    self.object_sq.c.class_id.label("class_id"),
-                    self.object_class_sq.c.name.label("class_name"),
-                    self.object_sq.c.name.label("name"),
-                    self.object_sq.c.description.label("description"),
-                    self.entity_group_sq.c.entity_id.label("group_id"),
-                    self.object_sq.c.commit_id.label("commit_id"),
-                )
-                .filter(self.object_sq.c.class_id == self.object_class_sq.c.id)
-                .outerjoin(self.entity_group_sq, self.entity_group_sq.c.entity_id == self.object_sq.c.id)
-                .distinct(self.entity_group_sq.c.entity_id)
-                .subquery()
-            )
-        return self._ext_object_sq
-
-    @property
-    def ext_relationship_class_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT
-                rc.id,
-                rc.name,
-                oc.id AS object_class_id,
-                oc.name AS object_class_name
-            FROM relationship_class AS rc, object_class AS oc
-            WHERE rc.object_class_id = oc.id
-            ORDER BY rc.id, rc.dimension
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._ext_relationship_class_sq is None:
-            self._ext_relationship_class_sq = (
-                self.query(
-                    self.relationship_class_sq.c.id.label("id"),
-                    self.relationship_class_sq.c.name.label("name"),
-                    self.relationship_class_sq.c.description.label("description"),
-                    self.relationship_class_sq.c.dimension.label("dimension"),
-                    self.relationship_class_sq.c.display_icon.label("display_icon"),
-                    self.object_class_sq.c.id.label("object_class_id"),
-                    self.object_class_sq.c.name.label("object_class_name"),
-                    self.relationship_class_sq.c.commit_id.label("commit_id"),
-                )
-                .filter(self.relationship_class_sq.c.object_class_id == self.object_class_sq.c.id)
-                .order_by(self.relationship_class_sq.c.id, self.relationship_class_sq.c.dimension)
-                .subquery()
-            )
-        return self._ext_relationship_class_sq
-
-    @property
-    def wide_relationship_class_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT
-                id,
-                name,
-                GROUP_CONCAT(object_class_id) AS object_class_id_list,
-                GROUP_CONCAT(object_class_name) AS object_class_name_list
-            FROM (
-                SELECT
-                    rc.id,
-                    rc.name,
-                    oc.id AS object_class_id,
-                    oc.name AS object_class_name
-                FROM relationship_class AS rc, object_class AS oc
-                WHERE rc.object_class_id = oc.id
-                ORDER BY rc.id, rc.dimension
-            )
-            GROUP BY id, name
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._wide_relationship_class_sq is None:
-            self._wide_relationship_class_sq = (
-                self.query(
-                    self.ext_relationship_class_sq.c.id,
-                    self.ext_relationship_class_sq.c.name,
-                    self.ext_relationship_class_sq.c.description,
-                    self.ext_relationship_class_sq.c.display_icon,
-                    self.ext_relationship_class_sq.c.commit_id,
-                    group_concat(
-                        self.ext_relationship_class_sq.c.object_class_id, self.ext_relationship_class_sq.c.dimension
-                    ).label("object_class_id_list"),
-                    group_concat(
-                        self.ext_relationship_class_sq.c.object_class_name, self.ext_relationship_class_sq.c.dimension
-                    ).label("object_class_name_list"),
-                )
-                .group_by(
-                    self.ext_relationship_class_sq.c.id,
-                    self.ext_relationship_class_sq.c.name,
-                    self.ext_relationship_class_sq.c.description,
-                    self.ext_relationship_class_sq.c.display_icon,
-                    self.ext_relationship_class_sq.c.commit_id,
-                )
-                .subquery()
-            )
-        return self._wide_relationship_class_sq
-
-    @property
-    def ext_relationship_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT
-                r.id,
-                r.class_id,
-                r.name,
-                o.id AS object_id,
-                o.name AS object_name,
-                o.class_id AS object_class_id,
-            FROM relationship as r, object AS o
-            WHERE r.object_id = o.id
-            ORDER BY r.id, r.dimension
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._ext_relationship_sq is None:
-            self._ext_relationship_sq = (
-                self.query(
-                    self.relationship_sq.c.id.label("id"),
-                    self.relationship_sq.c.name.label("name"),
-                    self.relationship_sq.c.class_id.label("class_id"),
-                    self.relationship_sq.c.dimension.label("dimension"),
-                    self.wide_relationship_class_sq.c.name.label("class_name"),
-                    self.ext_object_sq.c.id.label("object_id"),
-                    self.ext_object_sq.c.name.label("object_name"),
-                    self.ext_object_sq.c.class_id.label("object_class_id"),
-                    self.ext_object_sq.c.class_name.label("object_class_name"),
-                    self.relationship_sq.c.commit_id.label("commit_id"),
-                )
-                .filter(self.relationship_sq.c.class_id == self.wide_relationship_class_sq.c.id)
-                .outerjoin(self.ext_object_sq, self.relationship_sq.c.object_id == self.ext_object_sq.c.id)
-                .order_by(self.relationship_sq.c.id, self.relationship_sq.c.dimension)
-                .subquery()
-            )
-        return self._ext_relationship_sq
-
-    @property
-    def wide_relationship_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT
-                id,
-                class_id,
-                class_name,
-                name,
-                GROUP_CONCAT(object_id) AS object_id_list,
-                GROUP_CONCAT(object_name) AS object_name_list
-            FROM (
-                SELECT
-                    r.id,
-                    r.class_id,
-                    r.name,
-                    o.id AS object_id,
-                    o.name AS object_name
-                FROM relationship as r, object AS o
-                WHERE r.object_id = o.id
-                ORDER BY r.id, r.dimension
-            )
-            GROUP BY id, class_id, name
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._wide_relationship_sq is None:
-            self._wide_relationship_sq = (
-                self.query(
-                    self.ext_relationship_sq.c.id,
-                    self.ext_relationship_sq.c.name,
-                    self.ext_relationship_sq.c.class_id,
-                    self.ext_relationship_sq.c.class_name,
-                    self.ext_relationship_sq.c.commit_id,
-                    group_concat(self.ext_relationship_sq.c.object_id, self.ext_relationship_sq.c.dimension).label(
-                        "object_id_list"
-                    ),
-                    group_concat(self.ext_relationship_sq.c.object_name, self.ext_relationship_sq.c.dimension).label(
-                        "object_name_list"
-                    ),
-                    group_concat(
-                        self.ext_relationship_sq.c.object_class_id, self.ext_relationship_sq.c.dimension
-                    ).label("object_class_id_list"),
-                    group_concat(
-                        self.ext_relationship_sq.c.object_class_name, self.ext_relationship_sq.c.dimension
-                    ).label("object_class_name_list"),
-                )
-                .group_by(
-                    self.ext_relationship_sq.c.id,
-                    self.ext_relationship_sq.c.name,
-                    self.ext_relationship_sq.c.class_id,
-                    self.ext_relationship_sq.c.class_name,
-                    self.ext_relationship_sq.c.commit_id,
-                )
-                # dimension count might be higher than object count when objects have been filtered out
-                .having(
-                    func.count(self.ext_relationship_sq.c.dimension) == func.count(self.ext_relationship_sq.c.object_id)
-                )
-                .subquery()
-            )
-        return self._wide_relationship_sq
-
-    @property
-    def ext_entity_group_sq(self):
-        """A subquery of the form:
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._ext_entity_group_sq is None:
-            group_entity = aliased(self.entity_sq)
-            member_entity = aliased(self.entity_sq)
-            self._ext_entity_group_sq = (
-                self.query(
-                    self.entity_group_sq.c.id.label("id"),
-                    self.entity_group_sq.c.entity_class_id.label("class_id"),
-                    self.entity_group_sq.c.entity_id.label("group_id"),
-                    self.entity_group_sq.c.member_id.label("member_id"),
-                    self.entity_class_sq.c.name.label("class_name"),
-                    group_entity.c.name.label("group_name"),
-                    member_entity.c.name.label("member_name"),
-                    label("object_class_id", self._object_class_id()),
-                    label("relationship_class_id", self._relationship_class_id()),
-                )
-                .filter(self.entity_group_sq.c.entity_class_id == self.entity_class_sq.c.id)
-                .join(group_entity, self.entity_group_sq.c.entity_id == group_entity.c.id)
-                .join(member_entity, self.entity_group_sq.c.member_id == member_entity.c.id)
-                .subquery()
-            )
-        return self._ext_entity_group_sq
-
-    @property
-    def entity_parameter_definition_sq(self):
-        """
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._entity_parameter_definition_sq is None:
-            self._entity_parameter_definition_sq = (
-                self.query(
-                    self.parameter_definition_sq.c.id.label("id"),
-                    self.parameter_definition_sq.c.entity_class_id,
-                    self.parameter_definition_sq.c.object_class_id,
-                    self.parameter_definition_sq.c.relationship_class_id,
-                    self.entity_class_sq.c.name.label("entity_class_name"),
-                    label("object_class_name", self._object_class_name()),
-                    label("relationship_class_name", self._relationship_class_name()),
-                    label("object_class_id_list", self._object_class_id_list()),
-                    label("object_class_name_list", self._object_class_name_list()),
-                    self.parameter_definition_sq.c.name.label("parameter_name"),
-                    self.parameter_definition_sq.c.parameter_value_list_id.label("value_list_id"),
-                    self.parameter_value_list_sq.c.name.label("value_list_name"),
-                    self.parameter_definition_sq.c.default_value,
-                    self.parameter_definition_sq.c.default_type,
-                    self.parameter_definition_sq.c.list_value_id,
-                    self.parameter_definition_sq.c.description,
-                    self.parameter_definition_sq.c.commit_id,
-                )
-                .join(self.entity_class_sq, self.entity_class_sq.c.id == self.parameter_definition_sq.c.entity_class_id)
-                .outerjoin(
-                    self.parameter_value_list_sq,
-                    self.parameter_value_list_sq.c.id == self.parameter_definition_sq.c.parameter_value_list_id,
-                )
-                .outerjoin(
-                    self.wide_relationship_class_sq, self.wide_relationship_class_sq.c.id == self.entity_class_sq.c.id
-                )
-                .subquery()
-            )
-        return self._entity_parameter_definition_sq
-
-    @property
-    def object_parameter_definition_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT
-                pd.id,
-                oc.id AS object_class_id,
-                oc.name AS object_class_name,
-                pd.name AS parameter_name,
-                wpvl.id AS value_list_id,
-                wpvl.name AS value_list_name,
-                pd.default_value
-            FROM parameter_definition AS pd, object_class AS oc
-            ON wpdt.parameter_definition_id = pd.id
-            LEFT JOIN (
-                SELECT
-                    id,
-                    name,
-                    GROUP_CONCAT(value) AS value_list
-                FROM (
-                    SELECT id, name, value
-                    FROM parameter_value_list
-                    ORDER BY id, value_index
-                )
-                GROUP BY id, name
-            ) AS wpvl
-            ON wpvl.id = pd.parameter_value_list_id
-            WHERE pd.object_class_id = oc.id
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._object_parameter_definition_sq is None:
-            self._object_parameter_definition_sq = (
-                self.query(
-                    self.parameter_definition_sq.c.id.label("id"),
-                    self.parameter_definition_sq.c.entity_class_id,
-                    self.object_class_sq.c.name.label("entity_class_name"),
-                    self.object_class_sq.c.id.label("object_class_id"),
-                    self.object_class_sq.c.name.label("object_class_name"),
-                    self.parameter_definition_sq.c.name.label("parameter_name"),
-                    self.parameter_definition_sq.c.parameter_value_list_id.label("value_list_id"),
-                    self.parameter_value_list_sq.c.name.label("value_list_name"),
-                    self.parameter_definition_sq.c.default_value,
-                    self.parameter_definition_sq.c.default_type,
-                    self.parameter_definition_sq.c.description,
-                )
-                .filter(self.object_class_sq.c.id == self.parameter_definition_sq.c.object_class_id)
-                .outerjoin(
-                    self.parameter_value_list_sq,
-                    self.parameter_value_list_sq.c.id == self.parameter_definition_sq.c.parameter_value_list_id,
-                )
-                .subquery()
-            )
-        return self._object_parameter_definition_sq
-
-    @property
-    def relationship_parameter_definition_sq(self):
-        """A subquery of the form:
-
-        .. code-block:: sql
-
-            SELECT
-                pd.id,
-                wrc.id AS relationship_class_id,
-                wrc.name AS relationship_class_name,
-                wrc.object_class_id_list,
-                wrc.object_class_name_list,
-                pd.name AS parameter_name,
-                wpvl.id AS value_list_id,
-                wpvl.name AS value_list_name,
-                pd.default_value
-            FROM
-                parameter_definition AS pd,
-                (
-                    SELECT
-                        id,
-                        name,
-                        GROUP_CONCAT(object_class_id) AS object_class_id_list,
-                        GROUP_CONCAT(object_class_name) AS object_class_name_list
-                    FROM (
-                        SELECT
-                            rc.id,
-                            rc.name,
-                            oc.id AS object_class_id,
-                            oc.name AS object_class_name
-                        FROM relationship_class AS rc, object_class AS oc
-                        WHERE rc.object_class_id = oc.id
-                        ORDER BY rc.id, rc.dimension
-                    )
-                    GROUP BY id, name
-                ) AS wrc
-            ON wpdt.parameter_definition_id = pd.id
-            LEFT JOIN (
-                SELECT
-                    id,
-                    name,
-                    GROUP_CONCAT(value) AS value_list
-                FROM (
-                    SELECT id, name, value
-                    FROM parameter_value_list
-                    ORDER BY id, value_index
-                )
-                GROUP BY id, name
-            ) AS wpvl
-            ON wpvl.id = pd.parameter_value_list_id
-            WHERE pd.relationship_class_id = wrc.id
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._relationship_parameter_definition_sq is None:
-            self._relationship_parameter_definition_sq = (
-                self.query(
-                    self.parameter_definition_sq.c.id.label("id"),
-                    self.parameter_definition_sq.c.entity_class_id,
-                    self.wide_relationship_class_sq.c.name.label("entity_class_name"),
-                    self.wide_relationship_class_sq.c.id.label("relationship_class_id"),
-                    self.wide_relationship_class_sq.c.name.label("relationship_class_name"),
-                    self.wide_relationship_class_sq.c.object_class_id_list,
-                    self.wide_relationship_class_sq.c.object_class_name_list,
-                    self.parameter_definition_sq.c.name.label("parameter_name"),
-                    self.parameter_definition_sq.c.parameter_value_list_id.label("value_list_id"),
-                    self.parameter_value_list_sq.c.name.label("value_list_name"),
-                    self.parameter_definition_sq.c.default_value,
-                    self.parameter_definition_sq.c.default_type,
-                    self.parameter_definition_sq.c.description,
-                )
-                .filter(self.parameter_definition_sq.c.relationship_class_id == self.wide_relationship_class_sq.c.id)
-                .outerjoin(
-                    self.parameter_value_list_sq,
-                    self.parameter_value_list_sq.c.id == self.parameter_definition_sq.c.parameter_value_list_id,
-                )
-                .subquery()
-            )
-        return self._relationship_parameter_definition_sq
-
-    @property
-    def entity_parameter_value_sq(self):
-        """
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._entity_parameter_value_sq is None:
-            self._entity_parameter_value_sq = (
-                self.query(
-                    self.parameter_value_sq.c.id.label("id"),
-                    self.parameter_definition_sq.c.entity_class_id,
-                    self.parameter_definition_sq.c.object_class_id,
-                    self.parameter_definition_sq.c.relationship_class_id,
-                    self.entity_class_sq.c.name.label("entity_class_name"),
-                    label("object_class_name", self._object_class_name()),
-                    label("relationship_class_name", self._relationship_class_name()),
-                    label("object_class_id_list", self._object_class_id_list()),
-                    label("object_class_name_list", self._object_class_name_list()),
-                    self.parameter_value_sq.c.entity_id,
-                    self.entity_sq.c.name.label("entity_name"),
-                    self.parameter_value_sq.c.object_id,
-                    self.parameter_value_sq.c.relationship_id,
-                    label("object_name", self._object_name()),
-                    label("object_id_list", self._object_id_list()),
-                    label("object_name_list", self._object_name_list()),
-                    self.parameter_definition_sq.c.id.label("parameter_id"),
-                    self.parameter_definition_sq.c.name.label("parameter_name"),
-                    self.parameter_value_sq.c.alternative_id,
-                    self.alternative_sq.c.name.label("alternative_name"),
-                    self.parameter_value_sq.c.value,
-                    self.parameter_value_sq.c.type,
-                    self.parameter_value_sq.c.list_value_id,
-                    self.parameter_value_sq.c.commit_id,
-                )
-                .join(
-                    self.parameter_definition_sq,
-                    self.parameter_definition_sq.c.id == self.parameter_value_sq.c.parameter_definition_id,
-                )
-                .join(self.entity_sq, self.parameter_value_sq.c.entity_id == self.entity_sq.c.id)
-                .join(self.entity_class_sq, self.parameter_definition_sq.c.entity_class_id == self.entity_class_sq.c.id)
-                .join(self.alternative_sq, self.parameter_value_sq.c.alternative_id == self.alternative_sq.c.id)
-                .outerjoin(
-                    self.wide_relationship_class_sq, self.wide_relationship_class_sq.c.id == self.entity_class_sq.c.id
-                )
-                .outerjoin(self.wide_relationship_sq, self.wide_relationship_sq.c.id == self.entity_sq.c.id)
-                # object_id_list might be None when objects have been filtered out
-                .filter(
-                    or_(
-                        self.parameter_value_sq.c.relationship_id.is_(None),
-                        self.wide_relationship_sq.c.object_id_list.isnot(None),
-                    )
-                )
-                .subquery()
-            )
-        return self._entity_parameter_value_sq
-
-    @property
-    def object_parameter_value_sq(self):
-        """A subquery of the form:
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._object_parameter_value_sq is None:
-            self._object_parameter_value_sq = (
-                self.query(
-                    self.parameter_value_sq.c.id.label("id"),
-                    self.parameter_definition_sq.c.entity_class_id,
-                    self.object_class_sq.c.id.label("object_class_id"),
-                    self.object_class_sq.c.name.label("object_class_name"),
-                    self.parameter_value_sq.c.entity_id,
-                    self.object_sq.c.id.label("object_id"),
-                    self.object_sq.c.name.label("object_name"),
-                    self.parameter_definition_sq.c.id.label("parameter_id"),
-                    self.parameter_definition_sq.c.name.label("parameter_name"),
-                    self.parameter_value_sq.c.alternative_id,
-                    self.alternative_sq.c.name.label("alternative_name"),
-                    self.parameter_value_sq.c.value,
-                    self.parameter_value_sq.c.type,
-                )
-                .filter(self.parameter_definition_sq.c.id == self.parameter_value_sq.c.parameter_definition_id)
-                .filter(self.parameter_value_sq.c.object_id == self.object_sq.c.id)
-                .filter(self.parameter_definition_sq.c.object_class_id == self.object_class_sq.c.id)
-                .filter(self.parameter_value_sq.c.alternative_id == self.alternative_sq.c.id)
-                .subquery()
-            )
-        return self._object_parameter_value_sq
-
-    @property
-    def relationship_parameter_value_sq(self):
-        """A subquery of the form:
-
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._relationship_parameter_value_sq is None:
-            self._relationship_parameter_value_sq = (
-                self.query(
-                    self.parameter_value_sq.c.id.label("id"),
-                    self.parameter_definition_sq.c.entity_class_id,
-                    self.wide_relationship_class_sq.c.id.label("relationship_class_id"),
-                    self.wide_relationship_class_sq.c.name.label("relationship_class_name"),
-                    self.wide_relationship_class_sq.c.object_class_id_list,
-                    self.wide_relationship_class_sq.c.object_class_name_list,
-                    self.parameter_value_sq.c.entity_id,
-                    self.wide_relationship_sq.c.id.label("relationship_id"),
-                    self.wide_relationship_sq.c.object_id_list,
-                    self.wide_relationship_sq.c.object_name_list,
-                    self.parameter_definition_sq.c.id.label("parameter_id"),
-                    self.parameter_definition_sq.c.name.label("parameter_name"),
-                    self.parameter_value_sq.c.alternative_id,
-                    self.alternative_sq.c.name.label("alternative_name"),
-                    self.parameter_value_sq.c.value,
-                    self.parameter_value_sq.c.type,
-                )
-                .filter(self.parameter_definition_sq.c.id == self.parameter_value_sq.c.parameter_definition_id)
-                .filter(self.parameter_value_sq.c.relationship_id == self.wide_relationship_sq.c.id)
-                .filter(self.parameter_definition_sq.c.relationship_class_id == self.wide_relationship_class_sq.c.id)
-                .filter(self.parameter_value_sq.c.alternative_id == self.alternative_sq.c.id)
-                .subquery()
-            )
-        return self._relationship_parameter_value_sq
-
-    @property
-    def ext_feature_sq(self):
-        """
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._ext_feature_sq is None:
-            self._ext_feature_sq = (
-                self.query(
-                    self.feature_sq.c.id.label("id"),
-                    self.entity_class_sq.c.id.label("entity_class_id"),
-                    self.entity_class_sq.c.name.label("entity_class_name"),
-                    self.feature_sq.c.parameter_definition_id.label("parameter_definition_id"),
-                    self.parameter_definition_sq.c.name.label("parameter_definition_name"),
-                    self.parameter_value_list_sq.c.id.label("parameter_value_list_id"),
-                    self.parameter_value_list_sq.c.name.label("parameter_value_list_name"),
-                    self.feature_sq.c.description.label("description"),
-                    self.feature_sq.c.commit_id.label("commit_id"),
-                )
-                .filter(self.feature_sq.c.parameter_definition_id == self.parameter_definition_sq.c.id)
-                .filter(self.parameter_definition_sq.c.parameter_value_list_id == self.parameter_value_list_sq.c.id)
-                .filter(self.parameter_definition_sq.c.entity_class_id == self.entity_class_sq.c.id)
-                .subquery()
-            )
-        return self._ext_feature_sq
-
-    @property
-    def ext_tool_feature_sq(self):
-        """
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._ext_tool_feature_sq is None:
-            self._ext_tool_feature_sq = (
-                self.query(
-                    self.tool_feature_sq.c.id.label("id"),
-                    self.tool_feature_sq.c.tool_id.label("tool_id"),
-                    self.tool_sq.c.name.label("tool_name"),
-                    self.tool_feature_sq.c.feature_id.label("feature_id"),
-                    self.ext_feature_sq.c.entity_class_id.label("entity_class_id"),
-                    self.ext_feature_sq.c.entity_class_name.label("entity_class_name"),
-                    self.ext_feature_sq.c.parameter_definition_id.label("parameter_definition_id"),
-                    self.ext_feature_sq.c.parameter_definition_name.label("parameter_definition_name"),
-                    self.ext_feature_sq.c.parameter_value_list_id.label("parameter_value_list_id"),
-                    self.ext_feature_sq.c.parameter_value_list_name.label("parameter_value_list_name"),
-                    self.tool_feature_sq.c.required.label("required"),
-                    self.tool_feature_sq.c.commit_id.label("commit_id"),
-                )
-                .filter(self.tool_feature_sq.c.tool_id == self.tool_sq.c.id)
-                .filter(self.tool_feature_sq.c.feature_id == self.ext_feature_sq.c.id)
-                .subquery()
-            )
-        return self._ext_tool_feature_sq
-
-    @property
-    def ext_tool_feature_method_sq(self):
-        """
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._ext_tool_feature_method_sq is None:
-            self._ext_tool_feature_method_sq = (
-                self.query(
-                    self.tool_feature_method_sq.c.id,
-                    self.ext_tool_feature_sq.c.id.label("tool_feature_id"),
-                    self.ext_tool_feature_sq.c.tool_id,
-                    self.ext_tool_feature_sq.c.tool_name,
-                    self.ext_tool_feature_sq.c.feature_id,
-                    self.ext_tool_feature_sq.c.entity_class_id,
-                    self.ext_tool_feature_sq.c.entity_class_name,
-                    self.ext_tool_feature_sq.c.parameter_definition_id,
-                    self.ext_tool_feature_sq.c.parameter_definition_name,
-                    self.ext_tool_feature_sq.c.parameter_value_list_id,
-                    self.ext_tool_feature_sq.c.parameter_value_list_name,
-                    self.tool_feature_method_sq.c.method_index,
-                    self.list_value_sq.c.value.label("method"),
-                    self.tool_feature_method_sq.c.commit_id,
-                )
-                .filter(self.tool_feature_method_sq.c.tool_feature_id == self.ext_tool_feature_sq.c.id)
-                .filter(self.ext_tool_feature_sq.c.parameter_value_list_id == self.parameter_value_list_sq.c.id)
-                .filter(self.parameter_value_list_sq.c.id == self.list_value_sq.c.parameter_value_list_id)
-                .filter(self.tool_feature_method_sq.c.method_index == self.list_value_sq.c.index)
-                .subquery()
-            )
-        return self._ext_tool_feature_method_sq
-
-    @property
-    def ext_parameter_value_metadata_sq(self):
-        """
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._ext_parameter_value_metadata_sq is None:
-            self._ext_parameter_value_metadata_sq = (
-                self.query(
-                    self.parameter_value_metadata_sq.c.id,
-                    self.parameter_value_metadata_sq.c.parameter_value_id,
-                    self.metadata_sq.c.id.label("metadata_id"),
-                    self.entity_sq.c.name.label("entity_name"),
-                    self.parameter_definition_sq.c.name.label("parameter_name"),
-                    self.alternative_sq.c.name.label("alternative_name"),
-                    self.metadata_sq.c.name.label("metadata_name"),
-                    self.metadata_sq.c.value.label("metadata_value"),
-                    self.parameter_value_metadata_sq.c.commit_id,
-                )
-                .filter(self.parameter_value_metadata_sq.c.parameter_value_id == self.parameter_value_sq.c.id)
-                .filter(self.parameter_value_sq.c.parameter_definition_id == self.parameter_definition_sq.c.id)
-                .filter(self.parameter_value_sq.c.entity_id == self.entity_sq.c.id)
-                .filter(self.parameter_value_sq.c.alternative_id == self.alternative_sq.c.id)
-                .filter(self.parameter_value_metadata_sq.c.metadata_id == self.metadata_sq.c.id)
-                .subquery()
-            )
-        return self._ext_parameter_value_metadata_sq
-
-    @property
-    def ext_entity_metadata_sq(self):
-        """
-        Returns:
-            sqlalchemy.sql.expression.Alias
-        """
-        if self._ext_entity_metadata_sq is None:
-            self._ext_entity_metadata_sq = (
-                self.query(
-                    self.entity_metadata_sq.c.id,
-                    self.entity_metadata_sq.c.entity_id,
-                    self.metadata_sq.c.id.label("metadata_id"),
-                    self.entity_sq.c.name.label("entity_name"),
-                    self.metadata_sq.c.name.label("metadata_name"),
-                    self.metadata_sq.c.value.label("metadata_value"),
-                    self.entity_metadata_sq.c.commit_id,
-                )
-                .filter(self.entity_metadata_sq.c.entity_id == self.entity_sq.c.id)
-                .filter(self.entity_metadata_sq.c.metadata_id == self.metadata_sq.c.id)
-                .subquery()
-            )
-        return self._ext_entity_metadata_sq
-
-    def _make_entity_sq(self):
-        """
-        Creates a subquery for entities.
-
-        Returns:
-            Alias: an entity subquery
-        """
-        return self._subquery("entity")
-
-    def _make_entity_class_sq(self):
-        """
-        Creates a subquery for entity classes.
-
-        Returns:
-            Alias: an entity class subquery
-        """
-        return self._subquery("entity_class")
-
-    def _make_parameter_definition_sq(self):
-        """
-        Creates a subquery for parameter definitions.
-
-        Returns:
-            Alias: a parameter definition subquery
-        """
-        par_def_sq = self._subquery("parameter_definition")
-        list_value_id = case(
-            [(par_def_sq.c.default_type == "list_value_ref", cast(par_def_sq.c.default_value, Integer()))], else_=None
-        )
-        default_value = case(
-            [(par_def_sq.c.default_type == "list_value_ref", self.list_value_sq.c.value)],
-            else_=par_def_sq.c.default_value,
-        )
-        default_type = case(
-            [(par_def_sq.c.default_type == "list_value_ref", self.list_value_sq.c.type)],
-            else_=par_def_sq.c.default_type,
-        )
-        return (
-            self.query(
-                par_def_sq.c.id.label("id"),
-                par_def_sq.c.name.label("name"),
-                par_def_sq.c.description.label("description"),
-                par_def_sq.c.entity_class_id,
-                label("object_class_id", self._object_class_id()),
-                label("relationship_class_id", self._relationship_class_id()),
-                label("default_value", default_value),
-                label("default_type", default_type),
-                label("list_value_id", list_value_id),
-                par_def_sq.c.commit_id.label("commit_id"),
-                par_def_sq.c.parameter_value_list_id.label("parameter_value_list_id"),
-            )
-            .join(self.entity_class_sq, self.entity_class_sq.c.id == par_def_sq.c.entity_class_id)
-            .outerjoin(self.list_value_sq, self.list_value_sq.c.id == list_value_id)
-            .subquery()
-        )
-
-    def _make_parameter_value_sq(self):
-        """
-        Creates a subquery for parameter values.
-
-        Returns:
-            Alias: a parameter value subquery
-        """
-        par_val_sq = self._subquery("parameter_value")
-        list_value_id = case([(par_val_sq.c.type == "list_value_ref", cast(par_val_sq.c.value, Integer()))], else_=None)
-        value = case([(par_val_sq.c.type == "list_value_ref", self.list_value_sq.c.value)], else_=par_val_sq.c.value)
-        type_ = case([(par_val_sq.c.type == "list_value_ref", self.list_value_sq.c.type)], else_=par_val_sq.c.type)
-        return (
-            self.query(
-                par_val_sq.c.id.label("id"),
-                par_val_sq.c.parameter_definition_id,
-                par_val_sq.c.entity_class_id,
-                par_val_sq.c.entity_id,
-                label("object_class_id", self._object_class_id()),
-                label("relationship_class_id", self._relationship_class_id()),
-                label("object_id", self._object_id()),
-                label("relationship_id", self._relationship_id()),
-                label("value", value),
-                label("type", type_),
-                label("list_value_id", list_value_id),
-                par_val_sq.c.commit_id.label("commit_id"),
-                par_val_sq.c.alternative_id,
-            )
-            .join(self.entity_sq, self.entity_sq.c.id == par_val_sq.c.entity_id)
-            .join(self.entity_class_sq, self.entity_class_sq.c.id == par_val_sq.c.entity_class_id)
-            .outerjoin(self.list_value_sq, self.list_value_sq.c.id == list_value_id)
-            .subquery()
-        )
-
-    def _make_alternative_sq(self):
-        """
-        Creates a subquery for alternatives.
-
-        Returns:
-            Alias: an alternative subquery
-        """
-        return self._subquery("alternative")
-
-    def _make_scenario_sq(self):
-        """
-        Creates a subquery for scenarios.
-
-        Returns:
-            Alias: a scenario subquery
-        """
-        return self._subquery("scenario")
-
-    def _make_scenario_alternative_sq(self):
-        """
-        Creates a subquery for scenario alternatives.
-
-        Returns:
-            Alias: a scenario alternative subquery
-        """
-        return self._subquery("scenario_alternative")
-
-    def get_import_alternative(self, cache=None):
-        """Returns the id of the alternative to use as default for all import operations.
-
-        Returns:
-            int, str
-        """
-        if self._import_alternative_id is None:
-            self._create_import_alternative(cache=cache)
-        return self._import_alternative_id, self._import_alternative_name
-
-    def _create_import_alternative(self, cache=None):
-        """Creates the alternative to be used as default for all import operations."""
-        if "alternative" not in cache:
-            cache = self.make_cache({"alternative"})
-        self._import_alternative_name = "Base"
-        self._import_alternative_id = next(
-            (id_ for id_, alt in cache.get("alternative", {}).items() if alt.name == self._import_alternative_name),
-            None,
-        )
-        if not self._import_alternative_id:
-            ids = self._add_alternatives({"name": self._import_alternative_name})
-            self._import_alternative_id = next(iter(ids))
-
-    def override_entity_sq_maker(self, method):
-        """
-        Overrides the function that creates the ``entity_sq`` property.
-
-        Args:
-            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
-                returns entity subquery as an :class:`Alias` object
-        """
-        self._make_entity_sq = MethodType(method, self)
-        self._clear_subqueries("entity")
-
-    def restore_entity_sq_maker(self):
-        """Restores the original function that creates the ``entity_sq`` property."""
-        self._make_entity_sq = MethodType(DatabaseMappingBase._make_entity_sq, self)
-        self._clear_subqueries("entity")
-
-    def override_entity_class_sq_maker(self, method):
-        """
-        Overrides the function that creates the ``entity_class_sq`` property.
-
-        Args:
-            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
-                returns entity class subquery as an :class:`Alias` object
-        """
-        self._make_entity_class_sq = MethodType(method, self)
-        self._clear_subqueries("entity_class")
-
-    def restore_entity_class_sq_maker(self):
-        """Restores the original function that creates the ``entity_class_sq`` property."""
-        self._make_entity_class_sq = MethodType(DatabaseMappingBase._make_entity_class_sq, self)
-        self._clear_subqueries("entity_class")
-
-    def override_parameter_definition_sq_maker(self, method):
-        """
-        Overrides the function that creates the ``parameter_definition_sq`` property.
-
-        Args:
-            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
-                returns parameter definition subquery as an :class:`Alias` object
-        """
-        self._make_parameter_definition_sq = MethodType(method, self)
-        self._clear_subqueries("parameter_definition")
-
-    def restore_parameter_definition_sq_maker(self):
-        """Restores the original function that creates the ``parameter_definition_sq`` property."""
-        self._make_parameter_definition_sq = MethodType(DatabaseMappingBase._make_parameter_definition_sq, self)
-        self._clear_subqueries("parameter_definition")
-
-    def override_parameter_value_sq_maker(self, method):
-        """
-        Overrides the function that creates the ``parameter_value_sq`` property.
-
-        Args:
-            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
-                returns parameter value subquery as an :class:`Alias` object
-        """
-        self._make_parameter_value_sq = MethodType(method, self)
-        self._clear_subqueries("parameter_value")
-
-    def restore_parameter_value_sq_maker(self):
-        """Restores the original function that creates the ``parameter_value_sq`` property."""
-        self._make_parameter_value_sq = MethodType(DatabaseMappingBase._make_parameter_value_sq, self)
-        self._clear_subqueries("parameter_value")
-
-    def override_create_import_alternative(self, method):
-        """
-        Overrides the ``_create_import_alternative`` function.
-
-        Args:
-            method (Callable)
-        """
-        self._create_import_alternative = MethodType(method, self)
-        self._import_alternative_id = None
-
-    def override_alternative_sq_maker(self, method):
-        """
-        Overrides the function that creates the ``alternative_sq`` property.
-
-        Args:
-            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
-                returns alternative subquery as an :class:`Alias` object
-        """
-        self._make_alternative_sq = MethodType(method, self)
-        self._clear_subqueries("alternative")
-
-    def restore_alternative_sq_maker(self):
-        """Restores the original function that creates the ``alternative_sq`` property."""
-        self._make_alternative_sq = MethodType(DatabaseMappingBase._make_alternative_sq, self)
-        self._clear_subqueries("alternative")
-
-    def override_scenario_sq_maker(self, method):
-        """
-        Overrides the function that creates the ``scenario_sq`` property.
-
-        Args:
-            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
-                returns scenario subquery as an :class:`Alias` object
-        """
-        self._make_scenario_sq = MethodType(method, self)
-        self._clear_subqueries("scenario")
-
-    def restore_scenario_sq_maker(self):
-        """Restores the original function that creates the ``scenario_sq`` property."""
-        self._make_scenario_sq = MethodType(DatabaseMappingBase._make_scenario_sq, self)
-        self._clear_subqueries("scenario")
-
-    def override_scenario_alternative_sq_maker(self, method):
-        """
-        Overrides the function that creates the ``scenario_alternative_sq`` property.
-
-        Args:
-            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
-                returns scenario alternative subquery as an :class:`Alias` object
-        """
-        self._make_scenario_alternative_sq = MethodType(method, self)
-        self._clear_subqueries("scenario_alternative")
-
-    def restore_scenario_alternative_sq_maker(self):
-        """Restores the original function that creates the ``scenario_alternative_sq`` property."""
-        self._make_scenario_alternative_sq = MethodType(DatabaseMappingBase._make_scenario_alternative_sq, self)
-        self._clear_subqueries("scenario_alternative")
-
-    def _checked_execute(self, stmt, items):
-        if not items:
-            return
-        return self.connection.execute(stmt, items)
-
-    def _get_primary_key(self, tablename):
-        pk = self.composite_pks.get(tablename)
-        if pk is None:
-            table_id = self.table_ids.get(tablename, "id")
-            pk = (table_id,)
-        return pk
-
-    def _reset_mapping(self):
-        """Delete all records from all tables but don't drop the tables.
-        Useful for writing tests
-        """
-        for tablename in self._tablenames:
-            table = self._metadata.tables[tablename]
-            self.connection.execute(table.delete())
-        self.connection.execute("INSERT INTO alternative VALUES (1, 'Base', 'Base alternative', null)")
-
-    def make_cache(
-        self, tablenames, include_descendants=False, include_ancestors=False, force_tablenames=None, keep_existing=False
-    ):
-        if include_descendants:
-            tablenames |= {
-                descendant for tablename in tablenames for descendant in self.descendant_tablenames.get(tablename, ())
-            }
-        if include_ancestors:
-            tablenames |= {
-                ancestor for tablename in tablenames for ancestor in self.ancestor_tablenames.get(tablename, ())
-            }
-        if force_tablenames:
-            tablenames |= force_tablenames
-        for tablename in tablenames & self.cache_sqs.keys():
-            self._do_advance_cache_query(tablename, keep_existing)
-        return self.cache
-
-    def _advance_cache_query(self, tablename, callback=None):
-        advanced = False
-        if tablename not in self.cache:
-            advanced = True
-            self._do_advance_cache_query(tablename)
-        if callback is not None:
-            callback()
-        return advanced
-
-    def _do_advance_cache_query(self, tablename, keep_existing=False):
-        table_cache = self.cache.table_cache(tablename)
-        for x in self.query(getattr(self, self.cache_sqs[tablename])).yield_per(1000).enable_eagerloads(False):
-            table_cache.add_item(x._asdict(), keep_existing)
-
-    def _items_with_type_id(self, tablename, *items):
-        type_id = {
-            "object_class": self.object_class_type,
-            "relationship_class": self.relationship_class_type,
-            "object": self.object_entity_type,
-            "relationship": self.relationship_entity_type,
-        }.get(tablename)
-        if type_id is None:
-            yield from items
-            return
-        for item in items:
-            item["type_id"] = type_id
-            yield item
-
-    def _object_class_id(self):
-        return case([(self.entity_class_sq.c.type_id == self.object_class_type, self.entity_class_sq.c.id)], else_=None)
-
-    def _relationship_class_id(self):
-        return case(
-            [(self.entity_class_sq.c.type_id == self.relationship_class_type, self.entity_class_sq.c.id)], else_=None
-        )
-
-    def _object_id(self):
-        return case([(self.entity_sq.c.type_id == self.object_entity_type, self.entity_sq.c.id)], else_=None)
-
-    def _relationship_id(self):
-        return case([(self.entity_sq.c.type_id == self.relationship_entity_type, self.entity_sq.c.id)], else_=None)
-
-    def _object_class_name(self):
-        return case(
-            [(self.entity_class_sq.c.type_id == self.object_class_type, self.entity_class_sq.c.name)], else_=None
-        )
-
-    def _relationship_class_name(self):
-        return case(
-            [(self.entity_class_sq.c.type_id == self.relationship_class_type, self.entity_class_sq.c.name)], else_=None
-        )
-
-    def _object_class_id_list(self):
-        return case(
-            [
-                (
-                    self.entity_class_sq.c.type_id == self.relationship_class_type,
-                    self.wide_relationship_class_sq.c.object_class_id_list,
-                )
-            ],
-            else_=None,
-        )
-
-    def _object_class_name_list(self):
-        return case(
-            [
-                (
-                    self.entity_class_sq.c.type_id == self.relationship_class_type,
-                    self.wide_relationship_class_sq.c.object_class_name_list,
-                )
-            ],
-            else_=None,
-        )
-
-    def _object_name(self):
-        return case([(self.entity_sq.c.type_id == self.object_entity_type, self.entity_sq.c.name)], else_=None)
-
-    def _object_id_list(self):
-        return case(
-            [(self.entity_sq.c.type_id == self.relationship_entity_type, self.wide_relationship_sq.c.object_id_list)],
-            else_=None,
-        )
-
-    def _object_name_list(self):
-        return case(
-            [(self.entity_sq.c.type_id == self.relationship_entity_type, self.wide_relationship_sq.c.object_name_list)],
-            else_=None,
-        )
-
-    @staticmethod
-    def _metadata_usage_counts(cache):
-        """Counts references to metadata name, value pairs in entity_metadata and parameter_value_metadata tables.
-
-        Args:
-            cache (dict): database cache
-
-        Returns:
-            Counter: usage counts keyed by metadata id
-        """
-        usage_counts = Counter()
-        for entry in cache.get("entity_metadata", {}).values():
-            usage_counts[entry.metadata_id] += 1
-        for entry in cache.get("parameter_value_metadata", {}).values():
-            usage_counts[entry.metadata_id] += 1
-        return usage_counts
-
-    def __del__(self):
-        try:
-            self.connection.close()
-        except AttributeError:
-            pass
-
-    def make_temporary_table(self, table_name, *columns):
-        """Creates a temporary table.
-
-        Args:
-            table_name (str): table name
-            *columns: table's columns
-
-        Returns:
-            Table: created table
-        """
-        table = Table(table_name, self._metadata, *columns, prefixes=["TEMPORARY"])
-        table.drop(self.connection, checkfirst=True)
-        table.create(self.connection)
-        return table
-
-    def get_filter_configs(self):
-        return self._filter_configs
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""Provides :class:`.DatabaseMappingBase`.
+
+"""
+# TODO: Finish docstrings
+import uuid
+import hashlib
+import os
+import logging
+import time
+from collections import Counter
+from types import MethodType
+from contextlib import contextmanager
+from sqlalchemy import create_engine, case, MetaData, Table, Column, false, and_, func, inspect, cast, Integer, or_
+from sqlalchemy.sql.expression import label, Alias
+from sqlalchemy.engine.url import make_url, URL
+from sqlalchemy.orm import Session, aliased
+from sqlalchemy.exc import DatabaseError
+from sqlalchemy.event import listen
+from sqlalchemy.pool import NullPool
+from alembic.migration import MigrationContext
+from alembic.environment import EnvironmentContext
+from alembic.script import ScriptDirectory
+from alembic.config import Config
+from alembic.util.exc import CommandError
+from .exception import SpineDBAPIError, SpineDBVersionError
+from .helpers import (
+    _create_first_spine_database,
+    create_new_spine_database,
+    compare_schemas,
+    forward_sweep,
+    group_concat,
+    model_meta,
+    copy_database_bind,
+)
+from .filters.tools import pop_filter_configs
+from .spine_db_client import get_db_url_from_server
+from .db_cache import DBCache
+
+logging.getLogger("alembic").setLevel(logging.CRITICAL)
+
+
+class DatabaseMappingBase:
+    """Base class for all database mappings.
+
+    It provides the :meth:`query` method for custom db querying.
+    """
+
+    _session_kwargs = {}
+    ITEM_TYPES = (
+        "object_class",
+        "relationship_class",
+        "parameter_value_list",
+        "list_value",
+        "parameter_definition",
+        "object",
+        "relationship",
+        "entity_group",
+        "parameter_value",
+        "alternative",
+        "scenario",
+        "scenario_alternative",
+        "feature",
+        "tool",
+        "tool_feature",
+        "tool_feature_method",
+        "metadata",
+        "entity_metadata",
+        "parameter_value_metadata",
+    )
+
+    def __init__(
+        self,
+        db_url,
+        username=None,
+        upgrade=False,
+        codename=None,
+        create=False,
+        apply_filters=True,
+        memory=False,
+        sqlite_timeout=1800,
+        advance_cache_query=None,
+    ):
+        """
+        Args:
+            db_url (str or URL): A URL in RFC-1738 format pointing to the database to be mapped, or to a DB server.
+            username (str, optional): A user name. If ``None``, it gets replaced by the string ``"anon"``.
+            upgrade (bool): Whether or not the db at the given URL should be upgraded to the most recent version.
+            codename (str, optional): A name that uniquely identifies the class instance within a client application.
+            create (bool): Whether or not to create a Spine db at the given URL if it's not already.
+            apply_filters (bool): Whether or not filters in the URL's query part are applied to the database map.
+            memory (bool): Whether or not to use a sqlite memory db as replacement for this DB map.
+        """
+        if advance_cache_query is None:
+            advance_cache_query = self._advance_cache_query
+        # FIXME: We should also check the server memory property and use it here
+        db_url = get_db_url_from_server(db_url)
+        self.db_url = str(db_url)
+        if isinstance(db_url, str):
+            filter_configs, db_url = pop_filter_configs(db_url)
+        elif isinstance(db_url, URL):
+            filter_configs = db_url.query.pop("spinedbfilter", [])
+        else:
+            filter_configs = []
+        self._filter_configs = filter_configs if apply_filters else None
+        self.sa_url = make_url(db_url)
+        self.username = username if username else "anon"
+        self.codename = self._make_codename(codename)
+        self._memory = memory
+        self.committing = True
+        self._memory_dirty = False
+        self._original_engine = self.create_engine(
+            self.sa_url, upgrade=upgrade, create=create, sqlite_timeout=sqlite_timeout
+        )
+        # NOTE: The NullPool is needed to receive the close event (or any events), for some reason
+        self.engine = create_engine("sqlite://", poolclass=NullPool) if self._memory else self._original_engine
+        self.connection = self.engine.connect()
+        if self._memory:
+            copy_database_bind(self.connection, self._original_engine)
+        listen(self.engine, 'close', self._receive_engine_close)
+        self._metadata = MetaData(self.connection)
+        self._metadata.reflect()
+        self._tablenames = [t.name for t in self._metadata.sorted_tables]
+        self.session = Session(self.connection, **self._session_kwargs)
+        self.cache = DBCache(advance_cache_query)
+        # class and entity type id
+        self._object_class_type = None
+        self._relationship_class_type = None
+        self._object_entity_type = None
+        self._relationship_entity_type = None
+        # Subqueries that select everything from each table
+        self._commit_sq = None
+        self._alternative_sq = None
+        self._scenario_sq = None
+        self._scenario_alternative_sq = None
+        self._entity_class_sq = None
+        self._entity_sq = None
+        self._entity_class_type_sq = None
+        self._entity_type_sq = None
+        self._object_class_sq = None
+        self._object_sq = None
+        self._relationship_class_sq = None
+        self._relationship_sq = None
+        self._entity_group_sq = None
+        self._parameter_definition_sq = None
+        self._parameter_value_sq = None
+        self._parameter_value_list_sq = None
+        self._list_value_sq = None
+        self._feature_sq = None
+        self._tool_sq = None
+        self._tool_feature_sq = None
+        self._tool_feature_method_sq = None
+        self._metadata_sq = None
+        self._parameter_value_metadata_sq = None
+        self._entity_metadata_sq = None
+        self._clean_parameter_value_sq = None
+        # Special convenience subqueries that join two or more tables
+        self._ext_parameter_value_list_sq = None
+        self._wide_parameter_value_list_sq = None
+        self._ord_list_value_sq = None
+        self._ext_scenario_sq = None
+        self._wide_scenario_sq = None
+        self._linked_scenario_alternative_sq = None
+        self._ext_linked_scenario_alternative_sq = None
+        self._ext_object_sq = None
+        self._ext_relationship_class_sq = None
+        self._wide_relationship_class_sq = None
+        self._ext_relationship_class_object_parameter_definition_sq = None
+        self._wide_relationship_class_object_parameter_definition_sq = None
+        self._ext_relationship_sq = None
+        self._wide_relationship_sq = None
+        self._ext_entity_group_sq = None
+        self._entity_parameter_definition_sq = None
+        self._object_parameter_definition_sq = None
+        self._relationship_parameter_definition_sq = None
+        self._entity_parameter_value_sq = None
+        self._object_parameter_value_sq = None
+        self._relationship_parameter_value_sq = None
+        self._ext_feature_sq = None
+        self._ext_tool_feature_sq = None
+        self._ext_tool_feature_method_sq = None
+        self._ext_parameter_value_metadata_sq = None
+        self._ext_entity_metadata_sq = None
+        # Import alternative suff
+        self._import_alternative_id = None
+        self._import_alternative_name = None
+        self._table_to_sq_attr = {}
+        # Table primary ids map:
+        self.table_ids = {
+            "relationship_entity_class": "entity_class_id",
+            "object_class": "entity_class_id",
+            "relationship_class": "entity_class_id",
+            "object": "entity_id",
+            "relationship": "entity_id",
+            "relationship_entity": "entity_id",
+        }
+        self.composite_pks = {
+            "relationship_entity": ("entity_id", "dimension"),
+            "relationship_entity_class": ("entity_class_id", "dimension"),
+        }
+        # Subqueries used to populate cache
+        self.cache_sqs = {
+            "entity": "entity_sq",
+            "feature": "feature_sq",
+            "tool": "tool_sq",
+            "tool_feature": "tool_feature_sq",
+            "tool_feature_method": "tool_feature_method_sq",
+            "parameter_value_list": "parameter_value_list_sq",
+            "list_value": "list_value_sq",
+            "alternative": "alternative_sq",
+            "scenario": "scenario_sq",
+            "scenario_alternative": "scenario_alternative_sq",
+            "object_class": "object_class_sq",
+            "object": "object_sq",
+            "relationship_class": "wide_relationship_class_sq",
+            "relationship": "wide_relationship_sq",
+            "entity_group": "entity_group_sq",
+            "parameter_definition": "parameter_definition_sq",
+            "parameter_value": "clean_parameter_value_sq",
+            "metadata": "metadata_sq",
+            "entity_metadata": "ext_entity_metadata_sq",
+            "parameter_value_metadata": "ext_parameter_value_metadata_sq",
+            "commit": "commit_sq",
+        }
+        self.ancestor_tablenames = {
+            "feature": ("parameter_definition",),
+            "tool_feature": ("tool", "feature"),
+            "tool_feature_method": ("tool_feature", "parameter_value_list", "list_value"),
+            "scenario_alternative": ("scenario", "alternative"),
+            "relationship_class": ("object_class",),
+            "object": ("object_class",),
+            "entity_group": ("object_class", "relationship_class", "object", "relationship"),
+            "relationship": ("relationship_class", "object"),
+            "parameter_definition": ("object_class", "relationship_class", "parameter_value_list", "list_value"),
+            "parameter_value": (
+                "alternative",
+                "object_class",
+                "relationship_class",
+                "object",
+                "relationship",
+                "parameter_definition",
+                "parameter_value_list",
+                "list_value",
+            ),
+            "entity_metadata": ("metadata", "object", "object_class", "relationship", "relationship_class"),
+            "parameter_value_metadata": (
+                "metadata",
+                "parameter_value",
+                "parameter_definition",
+                "object",
+                "object_class",
+                "relationship",
+                "relationship_class",
+                "alternative",
+            ),
+            "list_value": ("parameter_value_list",),
+        }
+        self.descendant_tablenames = {
+            tablename: set(self._descendant_tablenames(tablename)) for tablename in self.cache_sqs
+        }
+        self.object_class_type = None
+        self.relationship_class_type = None
+        self.object_entity_type = None
+        self.relationship_entity_type = None
+
+    def _init_type_attributes(self):
+        self.object_class_type = (
+            self.query(self.entity_class_type_sq).filter(self.entity_class_type_sq.c.name == "object").first().id
+        )
+        self.relationship_class_type = (
+            self.query(self.entity_class_type_sq).filter(self.entity_class_type_sq.c.name == "relationship").first().id
+        )
+        self.object_entity_type = (
+            self.query(self.entity_type_sq).filter(self.entity_type_sq.c.name == "object").first().id
+        )
+        self.relationship_entity_type = (
+            self.query(self.entity_type_sq).filter(self.entity_type_sq.c.name == "relationship").first().id
+        )
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, _exc_type, _exc_val, _exc_tb):
+        self.connection.close()
+
+    @contextmanager
+    def override_committing(self, new_committing):
+        committing = self.committing
+        self.committing = new_committing
+        try:
+            yield None
+        finally:
+            self.committing = committing
+
+    def _descendant_tablenames(self, tablename):
+        child_tablenames = {
+            "alternative": ("parameter_value", "scenario_alternative"),
+            "scenario": ("scenario_alternative",),
+            "object_class": ("object", "relationship_class", "parameter_definition"),
+            "object": ("relationship", "parameter_value", "entity_group", "entity_metadata"),
+            "relationship_class": ("relationship", "parameter_definition"),
+            "relationship": ("parameter_value", "entity_group", "entity_metadata"),
+            "parameter_definition": ("parameter_value", "feature"),
+            "parameter_value_list": ("feature",),
+            "parameter_value": ("parameter_value_metadata", "entity_metadata"),
+            "feature": ("tool_feature",),
+            "tool": ("tool_feature",),
+            "tool_feature": ("tool_feature_method",),
+            "entity_metadata": ("metadata",),
+            "parameter_value_metadata": ("metadata",),
+        }
+        for parent, children in child_tablenames.items():
+            if tablename == parent:
+                for child in children:
+                    yield child
+                    yield from self._descendant_tablenames(child)
+
+    def sorted_tablenames(self):
+        tablenames = list(self.ITEM_TYPES)
+        sorted_tablenames = []
+        while tablenames:
+            tablename = tablenames.pop(0)
+            ancestors = self.ancestor_tablenames.get(tablename)
+            if ancestors is None or all(x in sorted_tablenames for x in ancestors):
+                sorted_tablenames.append(tablename)
+            else:
+                tablenames.append(tablename)
+        return sorted_tablenames
+
+    def commit_id(self):
+        return self._commit_id
+
+    def _make_commit_id(self):
+        return None
+
+    def _check_commit(self, comment):
+        """Raises if commit not possible.
+
+        Args:
+            comment (str): commit message
+        """
+        if not self.has_pending_changes():
+            raise SpineDBAPIError("Nothing to commit.")
+        if not comment:
+            raise SpineDBAPIError("Commit message cannot be empty.")
+
+    def _make_codename(self, codename):
+        if codename:
+            return str(codename)
+        if not self.sa_url.drivername.startswith("sqlite"):
+            return self.sa_url.database
+        if self.sa_url.database is not None:
+            return os.path.basename(self.sa_url.database)
+        hashing = hashlib.sha1()
+        hashing.update(bytes(str(time.time()), "utf-8"))
+        return hashing.hexdigest()
+
+    @staticmethod
+    def create_engine(sa_url, upgrade=False, create=False, sqlite_timeout=1800):
+        """Create engine.
+
+        Args
+            sa_url (URL)
+            upgrade (bool, optional): If True, upgrade the db to the latest version.
+            create (bool, optional): If True, create a new Spine db at the given url if none found.
+
+        Returns
+            Engine
+        """
+        if sa_url.drivername == "sqlite":
+            connect_args = {'timeout': sqlite_timeout}
+        else:
+            connect_args = {}
+        try:
+            engine = create_engine(sa_url, connect_args=connect_args)
+            with engine.connect():
+                pass
+        except Exception as e:
+            raise SpineDBAPIError(
+                f"Could not connect to '{sa_url}': {str(e)}. "
+                f"Please make sure that '{sa_url}' is a valid sqlalchemy URL."
+            ) from None
+        config = Config()
+        config.set_main_option("script_location", "spinedb_api:alembic")
+        script = ScriptDirectory.from_config(config)
+        head = script.get_current_head()
+        with engine.connect() as connection:
+            migration_context = MigrationContext.configure(connection)
+            try:
+                current = migration_context.get_current_revision()
+            except DatabaseError as error:
+                raise SpineDBAPIError(str(error)) from None
+            if current is None:
+                # No revision information. Check that the schema of the given url corresponds to a 'first' Spine db
+                # Otherwise we either raise or create a new Spine db at the url.
+                ref_engine = _create_first_spine_database("sqlite://")
+                if not compare_schemas(engine, ref_engine):
+                    if not create or inspect(engine).get_table_names():
+                        raise SpineDBAPIError(
+                            "Unable to determine db revision. "
+                            f"Please check that\n\n\t{sa_url}\n\nis the URL of a valid Spine db."
+                        )
+                    return create_new_spine_database(sa_url)
+            if current != head:
+                if not upgrade:
+                    try:
+                        script.get_revision(current)  # Check if current revision is part of alembic rev. history
+                    except CommandError:
+                        # Can't find 'current' revision
+                        raise SpineDBVersionError(
+                            url=sa_url, current=current, expected=head, upgrade_available=False
+                        ) from None
+                    raise SpineDBVersionError(url=sa_url, current=current, expected=head)
+
+                # Upgrade function
+                def upgrade_to_head(rev, context):
+                    return script._upgrade_revs("head", rev)
+
+                with EnvironmentContext(
+                    config,
+                    script,
+                    fn=upgrade_to_head,
+                    as_sql=False,
+                    starting_rev=None,
+                    destination_rev="head",
+                    tag=None,
+                ) as environment_context:
+                    environment_context.configure(connection=connection, target_metadata=model_meta)
+                    with environment_context.begin_transaction():
+                        environment_context.run_migrations()
+        return engine
+
+    def _receive_engine_close(self, dbapi_con, _connection_record):
+        if dbapi_con == self.connection.connection.connection and self._memory_dirty:
+            copy_database_bind(self._original_engine, self.connection)
+
+    def reconnect(self):
+        self.connection = self.engine.connect()
+
+    def in_(self, column, values):
+        """Returns an expression equivalent to column.in_(values), that circumvents the
+        'too many sql variables' problem in sqlite."""
+        if not values:
+            return false()
+        if not self.sa_url.drivername.startswith("sqlite"):
+            return column.in_(values)
+        in_value = Table(
+            "in_value_" + str(uuid.uuid4()),
+            MetaData(),
+            Column("value", column.type, primary_key=True),
+            prefixes=['TEMPORARY'],
+        )
+        in_value.create(self.connection, checkfirst=True)
+        python_type = column.type.python_type
+        self._checked_execute(in_value.insert(), [{"value": python_type(val)} for val in set(values)])
+        return column.in_(self.query(in_value.c.value))
+
+    def _get_table_to_sq_attr(self):
+        if not self._table_to_sq_attr:
+            self._table_to_sq_attr = self._make_table_to_sq_attr()
+        return self._table_to_sq_attr
+
+    def _make_table_to_sq_attr(self):
+        """Returns a dict mapping table names to subquery attribute names, involving that table."""
+        # This 'loads' our subquery attributes
+        for attr in dir(self):
+            getattr(self, attr)
+        table_to_sq_attr = {}
+        for attr, val in vars(self).items():
+            if not isinstance(val, Alias):
+                continue
+            tables = set()
+
+            def _func(x):
+                if isinstance(x, Table):
+                    tables.add(x.name)  # pylint: disable=cell-var-from-loop
+
+            forward_sweep(val, _func)
+            # Now `tables` contains all tables related to `val`
+            for table in tables:
+                table_to_sq_attr.setdefault(table, set()).add(attr)
+        return table_to_sq_attr
+
+    def _clear_subqueries(self, *tablenames):
+        """Set to `None` subquery attributes involving the affected tables.
+        This forces the subqueries to be refreshed when the corresponding property is accessed.
+        """
+        attr_names = set(attr for tablename in tablenames for attr in self._get_table_to_sq_attr().get(tablename, []))
+        for attr_name in attr_names:
+            setattr(self, attr_name, None)
+        for tablename in list(self.cache):
+            if self.cache.pop(tablename, None):
+                self._do_advance_cache_query(tablename)
+
+    def query(self, *args, **kwargs):
+        """Return a sqlalchemy :class:`~sqlalchemy.orm.query.Query` object applied
+        to this :class:`.DatabaseMappingBase`.
+
+        To perform custom ``SELECT`` statements, call this method with one or more of the class documented
+        :class:`~sqlalchemy.sql.expression.Alias` properties. For example, to select the object class with
+        ``id`` equal to 1::
+
+            from spinedb_api import DatabaseMapping
+            url = 'sqlite:///spine.db'
+            ...
+            db_map = DatabaseMapping(url)
+            db_map.query(db_map.object_class_sq).filter_by(id=1).one_or_none()
+
+        To perform more complex queries, just use this method in combination with the SQLAlchemy API.
+        For example, to select all object class names and the names of their objects concatenated in a string::
+
+            from sqlalchemy import func
+
+            db_map.query(
+                db_map.object_class_sq.c.name, func.group_concat(db_map.object_sq.c.name)
+            ).filter(
+                db_map.object_sq.c.class_id == db_map.object_class_sq.c.id
+            ).group_by(db_map.object_class_sq.c.name).all()
+        """
+        return self.session.query(*args, **kwargs)
+
+    def _subquery(self, tablename):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM tablename
+
+        Args:
+            tablename (str): the table to be queried.
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        table = self._metadata.tables[tablename]
+        return self.query(table).subquery()
+
+    @property
+    def alternative_sq(self):
+        if self._alternative_sq is None:
+            self._alternative_sq = self._make_alternative_sq()
+        return self._alternative_sq
+
+    @property
+    def scenario_sq(self):
+        if self._scenario_sq is None:
+            self._scenario_sq = self._make_scenario_sq()
+        return self._scenario_sq
+
+    @property
+    def scenario_alternative_sq(self):
+        if self._scenario_alternative_sq is None:
+            self._scenario_alternative_sq = self._make_scenario_alternative_sq()
+        return self._scenario_alternative_sq
+
+    @property
+    def entity_class_type_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM class_type
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._entity_class_type_sq is None:
+            self._entity_class_type_sq = self._subquery("entity_class_type")
+        return self._entity_class_type_sq
+
+    @property
+    def entity_type_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM class_type
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._entity_type_sq is None:
+            self._entity_type_sq = self._subquery("entity_type")
+        return self._entity_type_sq
+
+    @property
+    def entity_class_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM class
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._entity_class_sq is None:
+            self._entity_class_sq = self._make_entity_class_sq()
+        return self._entity_class_sq
+
+    @property
+    def entity_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM entity
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._entity_sq is None:
+            self._entity_sq = self._make_entity_sq()
+        return self._entity_sq
+
+    @property
+    def object_class_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM object_class
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._object_class_sq is None:
+            object_class_sq = self._subquery("object_class")
+            self._object_class_sq = (
+                self.query(
+                    self.entity_class_sq.c.id.label("id"),
+                    self.entity_class_sq.c.name.label("name"),
+                    self.entity_class_sq.c.description.label("description"),
+                    self.entity_class_sq.c.display_order.label("display_order"),
+                    self.entity_class_sq.c.display_icon.label("display_icon"),
+                    self.entity_class_sq.c.hidden.label("hidden"),
+                    self.entity_class_sq.c.commit_id.label("commit_id"),
+                )
+                .filter(self.entity_class_sq.c.id == object_class_sq.c.entity_class_id)
+                .subquery()
+            )
+        return self._object_class_sq
+
+    @property
+    def object_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM object
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._object_sq is None:
+            object_sq = self._subquery("object")
+            self._object_sq = (
+                self.query(
+                    self.entity_sq.c.id.label("id"),
+                    self.entity_sq.c.class_id.label("class_id"),
+                    self.entity_sq.c.name.label("name"),
+                    self.entity_sq.c.description.label("description"),
+                    self.entity_sq.c.commit_id.label("commit_id"),
+                )
+                .filter(self.entity_sq.c.id == object_sq.c.entity_id)
+                .subquery()
+            )
+        return self._object_sq
+
+    @property
+    def relationship_class_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM relationship_class
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._relationship_class_sq is None:
+            rel_ent_cls_sq = self._subquery("relationship_entity_class")
+            self._relationship_class_sq = (
+                self.query(
+                    rel_ent_cls_sq.c.entity_class_id.label("id"),
+                    rel_ent_cls_sq.c.dimension.label("dimension"),
+                    rel_ent_cls_sq.c.member_class_id.label("object_class_id"),
+                    self.entity_class_sq.c.name.label("name"),
+                    self.entity_class_sq.c.description.label("description"),
+                    self.entity_class_sq.c.display_icon.label("display_icon"),
+                    self.entity_class_sq.c.hidden.label("hidden"),
+                    self.entity_class_sq.c.commit_id.label("commit_id"),
+                )
+                .filter(self.entity_class_sq.c.id == rel_ent_cls_sq.c.entity_class_id)
+                .subquery()
+            )
+        return self._relationship_class_sq
+
+    @property
+    def relationship_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM relationship
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._relationship_sq is None:
+            rel_ent_sq = self._subquery("relationship_entity")
+            self._relationship_sq = (
+                self.query(
+                    rel_ent_sq.c.entity_id.label("id"),
+                    rel_ent_sq.c.dimension.label("dimension"),
+                    rel_ent_sq.c.member_id.label("object_id"),
+                    rel_ent_sq.c.entity_class_id.label("class_id"),
+                    self.entity_sq.c.name.label("name"),
+                    self.entity_sq.c.commit_id.label("commit_id"),
+                )
+                .filter(self.entity_sq.c.id == rel_ent_sq.c.entity_id)
+                .subquery()
+            )
+        return self._relationship_sq
+
+    @property
+    def entity_group_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM entity_group
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._entity_group_sq is None:
+            group_entity = aliased(self.entity_sq)
+            member_entity = aliased(self.entity_sq)
+            entity_group_sq = self._subquery("entity_group")
+            self._entity_group_sq = (
+                self.query(
+                    entity_group_sq.c.id,
+                    entity_group_sq.c.entity_class_id,
+                    group_entity.c.id.label("entity_id"),
+                    member_entity.c.id.label("member_id"),
+                )
+                .join(group_entity, group_entity.c.id == entity_group_sq.c.entity_id)
+                .join(member_entity, member_entity.c.id == entity_group_sq.c.member_id)
+                .subquery()
+            )
+        return self._entity_group_sq
+
+    @property
+    def parameter_definition_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM parameter_definition
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+
+        if self._parameter_definition_sq is None:
+            self._parameter_definition_sq = self._make_parameter_definition_sq()
+        return self._parameter_definition_sq
+
+    @property
+    def parameter_value_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM parameter_value
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._parameter_value_sq is None:
+            self._parameter_value_sq = self._make_parameter_value_sq()
+        return self._parameter_value_sq
+
+    @property
+    def clean_parameter_value_sq(self):
+        """A subquery of the parameter_value table that excludes rows with filtered entities.
+        This yields the correct results whenever there are both a scenario filter that filters some parameter values,
+        and a tool filter that then filters some entities based on the value of some their parameters
+        after the scenario filtering. Mildly insane.
+        """
+        if self._clean_parameter_value_sq is None:
+            self._clean_parameter_value_sq = (
+                self.query(self.parameter_value_sq)
+                .join(self.entity_sq, self.entity_sq.c.id == self.parameter_value_sq.c.entity_id)
+                .subquery()
+            )
+        return self._clean_parameter_value_sq
+
+    @property
+    def parameter_value_list_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT * FROM parameter_value_list
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._parameter_value_list_sq is None:
+            self._parameter_value_list_sq = self._subquery("parameter_value_list")
+        return self._parameter_value_list_sq
+
+    @property
+    def list_value_sq(self):
+        if self._list_value_sq is None:
+            self._list_value_sq = self._subquery("list_value")
+        return self._list_value_sq
+
+    @property
+    def feature_sq(self):
+        if self._feature_sq is None:
+            self._feature_sq = self._subquery("feature")
+        return self._feature_sq
+
+    @property
+    def tool_sq(self):
+        if self._tool_sq is None:
+            self._tool_sq = self._subquery("tool")
+        return self._tool_sq
+
+    @property
+    def tool_feature_sq(self):
+        if self._tool_feature_sq is None:
+            self._tool_feature_sq = self._subquery("tool_feature")
+        return self._tool_feature_sq
+
+    @property
+    def tool_feature_method_sq(self):
+        if self._tool_feature_method_sq is None:
+            self._tool_feature_method_sq = self._subquery("tool_feature_method")
+        return self._tool_feature_method_sq
+
+    @property
+    def metadata_sq(self):
+        if self._metadata_sq is None:
+            self._metadata_sq = self._subquery("metadata")
+        return self._metadata_sq
+
+    @property
+    def parameter_value_metadata_sq(self):
+        if self._parameter_value_metadata_sq is None:
+            self._parameter_value_metadata_sq = self._subquery("parameter_value_metadata")
+        return self._parameter_value_metadata_sq
+
+    @property
+    def entity_metadata_sq(self):
+        if self._entity_metadata_sq is None:
+            self._entity_metadata_sq = self._subquery("entity_metadata")
+        return self._entity_metadata_sq
+
+    @property
+    def commit_sq(self):
+        if self._commit_sq is None:
+            commit_sq = self._subquery("commit")
+            self._commit_sq = self.query(commit_sq).filter(commit_sq.c.comment != "").subquery()
+        return self._commit_sq
+
+    @property
+    def ext_parameter_value_list_sq(self):
+        if self._ext_parameter_value_list_sq is None:
+            self._ext_parameter_value_list_sq = (
+                self.query(
+                    self.parameter_value_list_sq.c.id,
+                    self.parameter_value_list_sq.c.name,
+                    self.parameter_value_list_sq.c.commit_id,
+                    self.list_value_sq.c.id.label("value_id"),
+                    self.list_value_sq.c.index.label("value_index"),
+                ).outerjoin(
+                    self.list_value_sq,
+                    self.list_value_sq.c.parameter_value_list_id == self.parameter_value_list_sq.c.id,
+                )
+            ).subquery()
+        return self._ext_parameter_value_list_sq
+
+    @property
+    def wide_parameter_value_list_sq(self):
+        if self._wide_parameter_value_list_sq is None:
+            self._wide_parameter_value_list_sq = (
+                self.query(
+                    self.ext_parameter_value_list_sq.c.id,
+                    self.ext_parameter_value_list_sq.c.name,
+                    self.ext_parameter_value_list_sq.c.commit_id,
+                    group_concat(
+                        self.ext_parameter_value_list_sq.c.value_id, self.ext_parameter_value_list_sq.c.value_index
+                    ).label("value_id_list"),
+                    group_concat(
+                        self.ext_parameter_value_list_sq.c.value_index, self.ext_parameter_value_list_sq.c.value_index
+                    ).label("value_index_list"),
+                ).group_by(
+                    self.ext_parameter_value_list_sq.c.id,
+                    self.ext_parameter_value_list_sq.c.name,
+                    self.ext_parameter_value_list_sq.c.commit_id,
+                )
+            ).subquery()
+        return self._wide_parameter_value_list_sq
+
+    @property
+    def ord_list_value_sq(self):
+        if self._ord_list_value_sq is None:
+            self._ord_list_value_sq = (
+                self.query(
+                    self.list_value_sq.c.id,
+                    self.list_value_sq.c.parameter_value_list_id,
+                    self.list_value_sq.c.index,
+                    self.list_value_sq.c.value,
+                    self.list_value_sq.c.type,
+                    self.list_value_sq.c.commit_id,
+                )
+                .order_by(self.list_value_sq.c.parameter_value_list_id, self.list_value_sq.c.index)
+                .subquery()
+            )
+        return self._ord_list_value_sq
+
+    @property
+    def ext_scenario_sq(self):
+        if self._ext_scenario_sq is None:
+            self._ext_scenario_sq = (
+                self.query(
+                    self.scenario_sq.c.id.label("id"),
+                    self.scenario_sq.c.name.label("name"),
+                    self.scenario_sq.c.description.label("description"),
+                    self.scenario_sq.c.active.label("active"),
+                    self.scenario_alternative_sq.c.alternative_id.label("alternative_id"),
+                    self.scenario_alternative_sq.c.rank.label("rank"),
+                    self.alternative_sq.c.name.label("alternative_name"),
+                    self.scenario_sq.c.commit_id.label("commit_id"),
+                )
+                .outerjoin(
+                    self.scenario_alternative_sq, self.scenario_alternative_sq.c.scenario_id == self.scenario_sq.c.id
+                )
+                .outerjoin(
+                    self.alternative_sq, self.alternative_sq.c.id == self.scenario_alternative_sq.c.alternative_id
+                )
+                .order_by(self.scenario_sq.c.id, self.scenario_alternative_sq.c.rank)
+                .subquery()
+            )
+        return self._ext_scenario_sq
+
+    @property
+    def wide_scenario_sq(self):
+        if self._wide_scenario_sq is None:
+            self._wide_scenario_sq = (
+                self.query(
+                    self.ext_scenario_sq.c.id.label("id"),
+                    self.ext_scenario_sq.c.name.label("name"),
+                    self.ext_scenario_sq.c.description.label("description"),
+                    self.ext_scenario_sq.c.active.label("active"),
+                    self.ext_scenario_sq.c.commit_id.label("commit_id"),
+                    group_concat(self.ext_scenario_sq.c.alternative_id, self.ext_scenario_sq.c.rank).label(
+                        "alternative_id_list"
+                    ),
+                    group_concat(self.ext_scenario_sq.c.alternative_name, self.ext_scenario_sq.c.rank).label(
+                        "alternative_name_list"
+                    ),
+                )
+                .group_by(
+                    self.ext_scenario_sq.c.id,
+                    self.ext_scenario_sq.c.name,
+                    self.ext_scenario_sq.c.description,
+                    self.ext_scenario_sq.c.active,
+                    self.ext_scenario_sq.c.commit_id,
+                )
+                .subquery()
+            )
+        return self._wide_scenario_sq
+
+    @property
+    def linked_scenario_alternative_sq(self):
+        if self._linked_scenario_alternative_sq is None:
+            scenario_next_alternative = aliased(self.scenario_alternative_sq)
+            self._linked_scenario_alternative_sq = (
+                self.query(
+                    self.scenario_alternative_sq.c.id.label("id"),
+                    self.scenario_alternative_sq.c.scenario_id.label("scenario_id"),
+                    self.scenario_alternative_sq.c.alternative_id.label("alternative_id"),
+                    self.scenario_alternative_sq.c.rank.label("rank"),
+                    scenario_next_alternative.c.alternative_id.label("before_alternative_id"),
+                    scenario_next_alternative.c.rank.label("before_rank"),
+                    self.scenario_alternative_sq.c.commit_id.label("commit_id"),
+                )
+                .outerjoin(
+                    scenario_next_alternative,
+                    and_(
+                        scenario_next_alternative.c.scenario_id == self.scenario_alternative_sq.c.scenario_id,
+                        scenario_next_alternative.c.rank == self.scenario_alternative_sq.c.rank + 1,
+                    ),
+                )
+                .order_by(self.scenario_alternative_sq.c.scenario_id, self.scenario_alternative_sq.c.rank)
+                .subquery()
+            )
+        return self._linked_scenario_alternative_sq
+
+    @property
+    def ext_linked_scenario_alternative_sq(self):
+        if self._ext_linked_scenario_alternative_sq is None:
+            next_alternative = aliased(self.alternative_sq)
+            self._ext_linked_scenario_alternative_sq = (
+                self.query(
+                    self.linked_scenario_alternative_sq.c.id.label("id"),
+                    self.linked_scenario_alternative_sq.c.scenario_id.label("scenario_id"),
+                    self.scenario_sq.c.name.label("scenario_name"),
+                    self.linked_scenario_alternative_sq.c.alternative_id.label("alternative_id"),
+                    self.alternative_sq.c.name.label("alternative_name"),
+                    self.linked_scenario_alternative_sq.c.rank.label("rank"),
+                    self.linked_scenario_alternative_sq.c.before_alternative_id.label("before_alternative_id"),
+                    self.linked_scenario_alternative_sq.c.before_rank.label("before_rank"),
+                    next_alternative.c.name.label("before_alternative_name"),
+                    self.linked_scenario_alternative_sq.c.commit_id.label("commit_id"),
+                )
+                .filter(self.linked_scenario_alternative_sq.c.scenario_id == self.scenario_sq.c.id)
+                .filter(self.alternative_sq.c.id == self.linked_scenario_alternative_sq.c.alternative_id)
+                .outerjoin(
+                    next_alternative,
+                    next_alternative.c.id == self.linked_scenario_alternative_sq.c.before_alternative_id,
+                )
+                .subquery()
+            )
+        return self._ext_linked_scenario_alternative_sq
+
+    @property
+    def ext_object_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT
+                o.id,
+                o.class_id,
+                oc.name AS class_name,
+                o.name,
+                o.description,
+            FROM object AS o, object_class AS oc
+            WHERE o.class_id = oc.id
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._ext_object_sq is None:
+            self._ext_object_sq = (
+                self.query(
+                    self.object_sq.c.id.label("id"),
+                    self.object_sq.c.class_id.label("class_id"),
+                    self.object_class_sq.c.name.label("class_name"),
+                    self.object_sq.c.name.label("name"),
+                    self.object_sq.c.description.label("description"),
+                    self.entity_group_sq.c.entity_id.label("group_id"),
+                    self.object_sq.c.commit_id.label("commit_id"),
+                )
+                .filter(self.object_sq.c.class_id == self.object_class_sq.c.id)
+                .outerjoin(self.entity_group_sq, self.entity_group_sq.c.entity_id == self.object_sq.c.id)
+                .distinct(self.entity_group_sq.c.entity_id)
+                .subquery()
+            )
+        return self._ext_object_sq
+
+    @property
+    def ext_relationship_class_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT
+                rc.id,
+                rc.name,
+                oc.id AS object_class_id,
+                oc.name AS object_class_name
+            FROM relationship_class AS rc, object_class AS oc
+            WHERE rc.object_class_id = oc.id
+            ORDER BY rc.id, rc.dimension
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._ext_relationship_class_sq is None:
+            self._ext_relationship_class_sq = (
+                self.query(
+                    self.relationship_class_sq.c.id.label("id"),
+                    self.relationship_class_sq.c.name.label("name"),
+                    self.relationship_class_sq.c.description.label("description"),
+                    self.relationship_class_sq.c.dimension.label("dimension"),
+                    self.relationship_class_sq.c.display_icon.label("display_icon"),
+                    self.object_class_sq.c.id.label("object_class_id"),
+                    self.object_class_sq.c.name.label("object_class_name"),
+                    self.relationship_class_sq.c.commit_id.label("commit_id"),
+                )
+                .filter(self.relationship_class_sq.c.object_class_id == self.object_class_sq.c.id)
+                .order_by(self.relationship_class_sq.c.id, self.relationship_class_sq.c.dimension)
+                .subquery()
+            )
+        return self._ext_relationship_class_sq
+
+    @property
+    def wide_relationship_class_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT
+                id,
+                name,
+                GROUP_CONCAT(object_class_id) AS object_class_id_list,
+                GROUP_CONCAT(object_class_name) AS object_class_name_list
+            FROM (
+                SELECT
+                    rc.id,
+                    rc.name,
+                    oc.id AS object_class_id,
+                    oc.name AS object_class_name
+                FROM relationship_class AS rc, object_class AS oc
+                WHERE rc.object_class_id = oc.id
+                ORDER BY rc.id, rc.dimension
+            )
+            GROUP BY id, name
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._wide_relationship_class_sq is None:
+            self._wide_relationship_class_sq = (
+                self.query(
+                    self.ext_relationship_class_sq.c.id,
+                    self.ext_relationship_class_sq.c.name,
+                    self.ext_relationship_class_sq.c.description,
+                    self.ext_relationship_class_sq.c.display_icon,
+                    self.ext_relationship_class_sq.c.commit_id,
+                    group_concat(
+                        self.ext_relationship_class_sq.c.object_class_id, self.ext_relationship_class_sq.c.dimension
+                    ).label("object_class_id_list"),
+                    group_concat(
+                        self.ext_relationship_class_sq.c.object_class_name, self.ext_relationship_class_sq.c.dimension
+                    ).label("object_class_name_list"),
+                )
+                .group_by(
+                    self.ext_relationship_class_sq.c.id,
+                    self.ext_relationship_class_sq.c.name,
+                    self.ext_relationship_class_sq.c.description,
+                    self.ext_relationship_class_sq.c.display_icon,
+                    self.ext_relationship_class_sq.c.commit_id,
+                )
+                .subquery()
+            )
+        return self._wide_relationship_class_sq
+
+    @property
+    def ext_relationship_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT
+                r.id,
+                r.class_id,
+                r.name,
+                o.id AS object_id,
+                o.name AS object_name,
+                o.class_id AS object_class_id,
+            FROM relationship as r, object AS o
+            WHERE r.object_id = o.id
+            ORDER BY r.id, r.dimension
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._ext_relationship_sq is None:
+            self._ext_relationship_sq = (
+                self.query(
+                    self.relationship_sq.c.id.label("id"),
+                    self.relationship_sq.c.name.label("name"),
+                    self.relationship_sq.c.class_id.label("class_id"),
+                    self.relationship_sq.c.dimension.label("dimension"),
+                    self.wide_relationship_class_sq.c.name.label("class_name"),
+                    self.ext_object_sq.c.id.label("object_id"),
+                    self.ext_object_sq.c.name.label("object_name"),
+                    self.ext_object_sq.c.class_id.label("object_class_id"),
+                    self.ext_object_sq.c.class_name.label("object_class_name"),
+                    self.relationship_sq.c.commit_id.label("commit_id"),
+                )
+                .filter(self.relationship_sq.c.class_id == self.wide_relationship_class_sq.c.id)
+                .outerjoin(self.ext_object_sq, self.relationship_sq.c.object_id == self.ext_object_sq.c.id)
+                .order_by(self.relationship_sq.c.id, self.relationship_sq.c.dimension)
+                .subquery()
+            )
+        return self._ext_relationship_sq
+
+    @property
+    def wide_relationship_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT
+                id,
+                class_id,
+                class_name,
+                name,
+                GROUP_CONCAT(object_id) AS object_id_list,
+                GROUP_CONCAT(object_name) AS object_name_list
+            FROM (
+                SELECT
+                    r.id,
+                    r.class_id,
+                    r.name,
+                    o.id AS object_id,
+                    o.name AS object_name
+                FROM relationship as r, object AS o
+                WHERE r.object_id = o.id
+                ORDER BY r.id, r.dimension
+            )
+            GROUP BY id, class_id, name
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._wide_relationship_sq is None:
+            self._wide_relationship_sq = (
+                self.query(
+                    self.ext_relationship_sq.c.id,
+                    self.ext_relationship_sq.c.name,
+                    self.ext_relationship_sq.c.class_id,
+                    self.ext_relationship_sq.c.class_name,
+                    self.ext_relationship_sq.c.commit_id,
+                    group_concat(self.ext_relationship_sq.c.object_id, self.ext_relationship_sq.c.dimension).label(
+                        "object_id_list"
+                    ),
+                    group_concat(self.ext_relationship_sq.c.object_name, self.ext_relationship_sq.c.dimension).label(
+                        "object_name_list"
+                    ),
+                    group_concat(
+                        self.ext_relationship_sq.c.object_class_id, self.ext_relationship_sq.c.dimension
+                    ).label("object_class_id_list"),
+                    group_concat(
+                        self.ext_relationship_sq.c.object_class_name, self.ext_relationship_sq.c.dimension
+                    ).label("object_class_name_list"),
+                )
+                .group_by(
+                    self.ext_relationship_sq.c.id,
+                    self.ext_relationship_sq.c.name,
+                    self.ext_relationship_sq.c.class_id,
+                    self.ext_relationship_sq.c.class_name,
+                    self.ext_relationship_sq.c.commit_id,
+                )
+                # dimension count might be higher than object count when objects have been filtered out
+                .having(
+                    func.count(self.ext_relationship_sq.c.dimension) == func.count(self.ext_relationship_sq.c.object_id)
+                )
+                .subquery()
+            )
+        return self._wide_relationship_sq
+
+    @property
+    def ext_entity_group_sq(self):
+        """A subquery of the form:
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._ext_entity_group_sq is None:
+            group_entity = aliased(self.entity_sq)
+            member_entity = aliased(self.entity_sq)
+            self._ext_entity_group_sq = (
+                self.query(
+                    self.entity_group_sq.c.id.label("id"),
+                    self.entity_group_sq.c.entity_class_id.label("class_id"),
+                    self.entity_group_sq.c.entity_id.label("group_id"),
+                    self.entity_group_sq.c.member_id.label("member_id"),
+                    self.entity_class_sq.c.name.label("class_name"),
+                    group_entity.c.name.label("group_name"),
+                    member_entity.c.name.label("member_name"),
+                    label("object_class_id", self._object_class_id()),
+                    label("relationship_class_id", self._relationship_class_id()),
+                )
+                .filter(self.entity_group_sq.c.entity_class_id == self.entity_class_sq.c.id)
+                .join(group_entity, self.entity_group_sq.c.entity_id == group_entity.c.id)
+                .join(member_entity, self.entity_group_sq.c.member_id == member_entity.c.id)
+                .subquery()
+            )
+        return self._ext_entity_group_sq
+
+    @property
+    def entity_parameter_definition_sq(self):
+        """
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._entity_parameter_definition_sq is None:
+            self._entity_parameter_definition_sq = (
+                self.query(
+                    self.parameter_definition_sq.c.id.label("id"),
+                    self.parameter_definition_sq.c.entity_class_id,
+                    self.parameter_definition_sq.c.object_class_id,
+                    self.parameter_definition_sq.c.relationship_class_id,
+                    self.entity_class_sq.c.name.label("entity_class_name"),
+                    label("object_class_name", self._object_class_name()),
+                    label("relationship_class_name", self._relationship_class_name()),
+                    label("object_class_id_list", self._object_class_id_list()),
+                    label("object_class_name_list", self._object_class_name_list()),
+                    self.parameter_definition_sq.c.name.label("parameter_name"),
+                    self.parameter_definition_sq.c.parameter_value_list_id.label("value_list_id"),
+                    self.parameter_value_list_sq.c.name.label("value_list_name"),
+                    self.parameter_definition_sq.c.default_value,
+                    self.parameter_definition_sq.c.default_type,
+                    self.parameter_definition_sq.c.list_value_id,
+                    self.parameter_definition_sq.c.description,
+                    self.parameter_definition_sq.c.commit_id,
+                )
+                .join(self.entity_class_sq, self.entity_class_sq.c.id == self.parameter_definition_sq.c.entity_class_id)
+                .outerjoin(
+                    self.parameter_value_list_sq,
+                    self.parameter_value_list_sq.c.id == self.parameter_definition_sq.c.parameter_value_list_id,
+                )
+                .outerjoin(
+                    self.wide_relationship_class_sq, self.wide_relationship_class_sq.c.id == self.entity_class_sq.c.id
+                )
+                .subquery()
+            )
+        return self._entity_parameter_definition_sq
+
+    @property
+    def object_parameter_definition_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT
+                pd.id,
+                oc.id AS object_class_id,
+                oc.name AS object_class_name,
+                pd.name AS parameter_name,
+                wpvl.id AS value_list_id,
+                wpvl.name AS value_list_name,
+                pd.default_value
+            FROM parameter_definition AS pd, object_class AS oc
+            ON wpdt.parameter_definition_id = pd.id
+            LEFT JOIN (
+                SELECT
+                    id,
+                    name,
+                    GROUP_CONCAT(value) AS value_list
+                FROM (
+                    SELECT id, name, value
+                    FROM parameter_value_list
+                    ORDER BY id, value_index
+                )
+                GROUP BY id, name
+            ) AS wpvl
+            ON wpvl.id = pd.parameter_value_list_id
+            WHERE pd.object_class_id = oc.id
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._object_parameter_definition_sq is None:
+            self._object_parameter_definition_sq = (
+                self.query(
+                    self.parameter_definition_sq.c.id.label("id"),
+                    self.parameter_definition_sq.c.entity_class_id,
+                    self.object_class_sq.c.name.label("entity_class_name"),
+                    self.object_class_sq.c.id.label("object_class_id"),
+                    self.object_class_sq.c.name.label("object_class_name"),
+                    self.parameter_definition_sq.c.name.label("parameter_name"),
+                    self.parameter_definition_sq.c.parameter_value_list_id.label("value_list_id"),
+                    self.parameter_value_list_sq.c.name.label("value_list_name"),
+                    self.parameter_definition_sq.c.default_value,
+                    self.parameter_definition_sq.c.default_type,
+                    self.parameter_definition_sq.c.description,
+                )
+                .filter(self.object_class_sq.c.id == self.parameter_definition_sq.c.object_class_id)
+                .outerjoin(
+                    self.parameter_value_list_sq,
+                    self.parameter_value_list_sq.c.id == self.parameter_definition_sq.c.parameter_value_list_id,
+                )
+                .subquery()
+            )
+        return self._object_parameter_definition_sq
+
+    @property
+    def relationship_parameter_definition_sq(self):
+        """A subquery of the form:
+
+        .. code-block:: sql
+
+            SELECT
+                pd.id,
+                wrc.id AS relationship_class_id,
+                wrc.name AS relationship_class_name,
+                wrc.object_class_id_list,
+                wrc.object_class_name_list,
+                pd.name AS parameter_name,
+                wpvl.id AS value_list_id,
+                wpvl.name AS value_list_name,
+                pd.default_value
+            FROM
+                parameter_definition AS pd,
+                (
+                    SELECT
+                        id,
+                        name,
+                        GROUP_CONCAT(object_class_id) AS object_class_id_list,
+                        GROUP_CONCAT(object_class_name) AS object_class_name_list
+                    FROM (
+                        SELECT
+                            rc.id,
+                            rc.name,
+                            oc.id AS object_class_id,
+                            oc.name AS object_class_name
+                        FROM relationship_class AS rc, object_class AS oc
+                        WHERE rc.object_class_id = oc.id
+                        ORDER BY rc.id, rc.dimension
+                    )
+                    GROUP BY id, name
+                ) AS wrc
+            ON wpdt.parameter_definition_id = pd.id
+            LEFT JOIN (
+                SELECT
+                    id,
+                    name,
+                    GROUP_CONCAT(value) AS value_list
+                FROM (
+                    SELECT id, name, value
+                    FROM parameter_value_list
+                    ORDER BY id, value_index
+                )
+                GROUP BY id, name
+            ) AS wpvl
+            ON wpvl.id = pd.parameter_value_list_id
+            WHERE pd.relationship_class_id = wrc.id
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._relationship_parameter_definition_sq is None:
+            self._relationship_parameter_definition_sq = (
+                self.query(
+                    self.parameter_definition_sq.c.id.label("id"),
+                    self.parameter_definition_sq.c.entity_class_id,
+                    self.wide_relationship_class_sq.c.name.label("entity_class_name"),
+                    self.wide_relationship_class_sq.c.id.label("relationship_class_id"),
+                    self.wide_relationship_class_sq.c.name.label("relationship_class_name"),
+                    self.wide_relationship_class_sq.c.object_class_id_list,
+                    self.wide_relationship_class_sq.c.object_class_name_list,
+                    self.parameter_definition_sq.c.name.label("parameter_name"),
+                    self.parameter_definition_sq.c.parameter_value_list_id.label("value_list_id"),
+                    self.parameter_value_list_sq.c.name.label("value_list_name"),
+                    self.parameter_definition_sq.c.default_value,
+                    self.parameter_definition_sq.c.default_type,
+                    self.parameter_definition_sq.c.description,
+                )
+                .filter(self.parameter_definition_sq.c.relationship_class_id == self.wide_relationship_class_sq.c.id)
+                .outerjoin(
+                    self.parameter_value_list_sq,
+                    self.parameter_value_list_sq.c.id == self.parameter_definition_sq.c.parameter_value_list_id,
+                )
+                .subquery()
+            )
+        return self._relationship_parameter_definition_sq
+
+    @property
+    def entity_parameter_value_sq(self):
+        """
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._entity_parameter_value_sq is None:
+            self._entity_parameter_value_sq = (
+                self.query(
+                    self.parameter_value_sq.c.id.label("id"),
+                    self.parameter_definition_sq.c.entity_class_id,
+                    self.parameter_definition_sq.c.object_class_id,
+                    self.parameter_definition_sq.c.relationship_class_id,
+                    self.entity_class_sq.c.name.label("entity_class_name"),
+                    label("object_class_name", self._object_class_name()),
+                    label("relationship_class_name", self._relationship_class_name()),
+                    label("object_class_id_list", self._object_class_id_list()),
+                    label("object_class_name_list", self._object_class_name_list()),
+                    self.parameter_value_sq.c.entity_id,
+                    self.entity_sq.c.name.label("entity_name"),
+                    self.parameter_value_sq.c.object_id,
+                    self.parameter_value_sq.c.relationship_id,
+                    label("object_name", self._object_name()),
+                    label("object_id_list", self._object_id_list()),
+                    label("object_name_list", self._object_name_list()),
+                    self.parameter_definition_sq.c.id.label("parameter_id"),
+                    self.parameter_definition_sq.c.name.label("parameter_name"),
+                    self.parameter_value_sq.c.alternative_id,
+                    self.alternative_sq.c.name.label("alternative_name"),
+                    self.parameter_value_sq.c.value,
+                    self.parameter_value_sq.c.type,
+                    self.parameter_value_sq.c.list_value_id,
+                    self.parameter_value_sq.c.commit_id,
+                )
+                .join(
+                    self.parameter_definition_sq,
+                    self.parameter_definition_sq.c.id == self.parameter_value_sq.c.parameter_definition_id,
+                )
+                .join(self.entity_sq, self.parameter_value_sq.c.entity_id == self.entity_sq.c.id)
+                .join(self.entity_class_sq, self.parameter_definition_sq.c.entity_class_id == self.entity_class_sq.c.id)
+                .join(self.alternative_sq, self.parameter_value_sq.c.alternative_id == self.alternative_sq.c.id)
+                .outerjoin(
+                    self.wide_relationship_class_sq, self.wide_relationship_class_sq.c.id == self.entity_class_sq.c.id
+                )
+                .outerjoin(self.wide_relationship_sq, self.wide_relationship_sq.c.id == self.entity_sq.c.id)
+                # object_id_list might be None when objects have been filtered out
+                .filter(
+                    or_(
+                        self.parameter_value_sq.c.relationship_id.is_(None),
+                        self.wide_relationship_sq.c.object_id_list.isnot(None),
+                    )
+                )
+                .subquery()
+            )
+        return self._entity_parameter_value_sq
+
+    @property
+    def object_parameter_value_sq(self):
+        """A subquery of the form:
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._object_parameter_value_sq is None:
+            self._object_parameter_value_sq = (
+                self.query(
+                    self.parameter_value_sq.c.id.label("id"),
+                    self.parameter_definition_sq.c.entity_class_id,
+                    self.object_class_sq.c.id.label("object_class_id"),
+                    self.object_class_sq.c.name.label("object_class_name"),
+                    self.parameter_value_sq.c.entity_id,
+                    self.object_sq.c.id.label("object_id"),
+                    self.object_sq.c.name.label("object_name"),
+                    self.parameter_definition_sq.c.id.label("parameter_id"),
+                    self.parameter_definition_sq.c.name.label("parameter_name"),
+                    self.parameter_value_sq.c.alternative_id,
+                    self.alternative_sq.c.name.label("alternative_name"),
+                    self.parameter_value_sq.c.value,
+                    self.parameter_value_sq.c.type,
+                )
+                .filter(self.parameter_definition_sq.c.id == self.parameter_value_sq.c.parameter_definition_id)
+                .filter(self.parameter_value_sq.c.object_id == self.object_sq.c.id)
+                .filter(self.parameter_definition_sq.c.object_class_id == self.object_class_sq.c.id)
+                .filter(self.parameter_value_sq.c.alternative_id == self.alternative_sq.c.id)
+                .subquery()
+            )
+        return self._object_parameter_value_sq
+
+    @property
+    def relationship_parameter_value_sq(self):
+        """A subquery of the form:
+
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._relationship_parameter_value_sq is None:
+            self._relationship_parameter_value_sq = (
+                self.query(
+                    self.parameter_value_sq.c.id.label("id"),
+                    self.parameter_definition_sq.c.entity_class_id,
+                    self.wide_relationship_class_sq.c.id.label("relationship_class_id"),
+                    self.wide_relationship_class_sq.c.name.label("relationship_class_name"),
+                    self.wide_relationship_class_sq.c.object_class_id_list,
+                    self.wide_relationship_class_sq.c.object_class_name_list,
+                    self.parameter_value_sq.c.entity_id,
+                    self.wide_relationship_sq.c.id.label("relationship_id"),
+                    self.wide_relationship_sq.c.object_id_list,
+                    self.wide_relationship_sq.c.object_name_list,
+                    self.parameter_definition_sq.c.id.label("parameter_id"),
+                    self.parameter_definition_sq.c.name.label("parameter_name"),
+                    self.parameter_value_sq.c.alternative_id,
+                    self.alternative_sq.c.name.label("alternative_name"),
+                    self.parameter_value_sq.c.value,
+                    self.parameter_value_sq.c.type,
+                )
+                .filter(self.parameter_definition_sq.c.id == self.parameter_value_sq.c.parameter_definition_id)
+                .filter(self.parameter_value_sq.c.relationship_id == self.wide_relationship_sq.c.id)
+                .filter(self.parameter_definition_sq.c.relationship_class_id == self.wide_relationship_class_sq.c.id)
+                .filter(self.parameter_value_sq.c.alternative_id == self.alternative_sq.c.id)
+                .subquery()
+            )
+        return self._relationship_parameter_value_sq
+
+    @property
+    def ext_feature_sq(self):
+        """
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._ext_feature_sq is None:
+            self._ext_feature_sq = (
+                self.query(
+                    self.feature_sq.c.id.label("id"),
+                    self.entity_class_sq.c.id.label("entity_class_id"),
+                    self.entity_class_sq.c.name.label("entity_class_name"),
+                    self.feature_sq.c.parameter_definition_id.label("parameter_definition_id"),
+                    self.parameter_definition_sq.c.name.label("parameter_definition_name"),
+                    self.parameter_value_list_sq.c.id.label("parameter_value_list_id"),
+                    self.parameter_value_list_sq.c.name.label("parameter_value_list_name"),
+                    self.feature_sq.c.description.label("description"),
+                    self.feature_sq.c.commit_id.label("commit_id"),
+                )
+                .filter(self.feature_sq.c.parameter_definition_id == self.parameter_definition_sq.c.id)
+                .filter(self.parameter_definition_sq.c.parameter_value_list_id == self.parameter_value_list_sq.c.id)
+                .filter(self.parameter_definition_sq.c.entity_class_id == self.entity_class_sq.c.id)
+                .subquery()
+            )
+        return self._ext_feature_sq
+
+    @property
+    def ext_tool_feature_sq(self):
+        """
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._ext_tool_feature_sq is None:
+            self._ext_tool_feature_sq = (
+                self.query(
+                    self.tool_feature_sq.c.id.label("id"),
+                    self.tool_feature_sq.c.tool_id.label("tool_id"),
+                    self.tool_sq.c.name.label("tool_name"),
+                    self.tool_feature_sq.c.feature_id.label("feature_id"),
+                    self.ext_feature_sq.c.entity_class_id.label("entity_class_id"),
+                    self.ext_feature_sq.c.entity_class_name.label("entity_class_name"),
+                    self.ext_feature_sq.c.parameter_definition_id.label("parameter_definition_id"),
+                    self.ext_feature_sq.c.parameter_definition_name.label("parameter_definition_name"),
+                    self.ext_feature_sq.c.parameter_value_list_id.label("parameter_value_list_id"),
+                    self.ext_feature_sq.c.parameter_value_list_name.label("parameter_value_list_name"),
+                    self.tool_feature_sq.c.required.label("required"),
+                    self.tool_feature_sq.c.commit_id.label("commit_id"),
+                )
+                .filter(self.tool_feature_sq.c.tool_id == self.tool_sq.c.id)
+                .filter(self.tool_feature_sq.c.feature_id == self.ext_feature_sq.c.id)
+                .subquery()
+            )
+        return self._ext_tool_feature_sq
+
+    @property
+    def ext_tool_feature_method_sq(self):
+        """
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._ext_tool_feature_method_sq is None:
+            self._ext_tool_feature_method_sq = (
+                self.query(
+                    self.tool_feature_method_sq.c.id,
+                    self.ext_tool_feature_sq.c.id.label("tool_feature_id"),
+                    self.ext_tool_feature_sq.c.tool_id,
+                    self.ext_tool_feature_sq.c.tool_name,
+                    self.ext_tool_feature_sq.c.feature_id,
+                    self.ext_tool_feature_sq.c.entity_class_id,
+                    self.ext_tool_feature_sq.c.entity_class_name,
+                    self.ext_tool_feature_sq.c.parameter_definition_id,
+                    self.ext_tool_feature_sq.c.parameter_definition_name,
+                    self.ext_tool_feature_sq.c.parameter_value_list_id,
+                    self.ext_tool_feature_sq.c.parameter_value_list_name,
+                    self.tool_feature_method_sq.c.method_index,
+                    self.list_value_sq.c.value.label("method"),
+                    self.tool_feature_method_sq.c.commit_id,
+                )
+                .filter(self.tool_feature_method_sq.c.tool_feature_id == self.ext_tool_feature_sq.c.id)
+                .filter(self.ext_tool_feature_sq.c.parameter_value_list_id == self.parameter_value_list_sq.c.id)
+                .filter(self.parameter_value_list_sq.c.id == self.list_value_sq.c.parameter_value_list_id)
+                .filter(self.tool_feature_method_sq.c.method_index == self.list_value_sq.c.index)
+                .subquery()
+            )
+        return self._ext_tool_feature_method_sq
+
+    @property
+    def ext_parameter_value_metadata_sq(self):
+        """
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._ext_parameter_value_metadata_sq is None:
+            self._ext_parameter_value_metadata_sq = (
+                self.query(
+                    self.parameter_value_metadata_sq.c.id,
+                    self.parameter_value_metadata_sq.c.parameter_value_id,
+                    self.metadata_sq.c.id.label("metadata_id"),
+                    self.entity_sq.c.name.label("entity_name"),
+                    self.parameter_definition_sq.c.name.label("parameter_name"),
+                    self.alternative_sq.c.name.label("alternative_name"),
+                    self.metadata_sq.c.name.label("metadata_name"),
+                    self.metadata_sq.c.value.label("metadata_value"),
+                    self.parameter_value_metadata_sq.c.commit_id,
+                )
+                .filter(self.parameter_value_metadata_sq.c.parameter_value_id == self.parameter_value_sq.c.id)
+                .filter(self.parameter_value_sq.c.parameter_definition_id == self.parameter_definition_sq.c.id)
+                .filter(self.parameter_value_sq.c.entity_id == self.entity_sq.c.id)
+                .filter(self.parameter_value_sq.c.alternative_id == self.alternative_sq.c.id)
+                .filter(self.parameter_value_metadata_sq.c.metadata_id == self.metadata_sq.c.id)
+                .subquery()
+            )
+        return self._ext_parameter_value_metadata_sq
+
+    @property
+    def ext_entity_metadata_sq(self):
+        """
+        Returns:
+            sqlalchemy.sql.expression.Alias
+        """
+        if self._ext_entity_metadata_sq is None:
+            self._ext_entity_metadata_sq = (
+                self.query(
+                    self.entity_metadata_sq.c.id,
+                    self.entity_metadata_sq.c.entity_id,
+                    self.metadata_sq.c.id.label("metadata_id"),
+                    self.entity_sq.c.name.label("entity_name"),
+                    self.metadata_sq.c.name.label("metadata_name"),
+                    self.metadata_sq.c.value.label("metadata_value"),
+                    self.entity_metadata_sq.c.commit_id,
+                )
+                .filter(self.entity_metadata_sq.c.entity_id == self.entity_sq.c.id)
+                .filter(self.entity_metadata_sq.c.metadata_id == self.metadata_sq.c.id)
+                .subquery()
+            )
+        return self._ext_entity_metadata_sq
+
+    def _make_entity_sq(self):
+        """
+        Creates a subquery for entities.
+
+        Returns:
+            Alias: an entity subquery
+        """
+        return self._subquery("entity")
+
+    def _make_entity_class_sq(self):
+        """
+        Creates a subquery for entity classes.
+
+        Returns:
+            Alias: an entity class subquery
+        """
+        return self._subquery("entity_class")
+
+    def _make_parameter_definition_sq(self):
+        """
+        Creates a subquery for parameter definitions.
+
+        Returns:
+            Alias: a parameter definition subquery
+        """
+        par_def_sq = self._subquery("parameter_definition")
+        list_value_id = case(
+            [(par_def_sq.c.default_type == "list_value_ref", cast(par_def_sq.c.default_value, Integer()))], else_=None
+        )
+        default_value = case(
+            [(par_def_sq.c.default_type == "list_value_ref", self.list_value_sq.c.value)],
+            else_=par_def_sq.c.default_value,
+        )
+        default_type = case(
+            [(par_def_sq.c.default_type == "list_value_ref", self.list_value_sq.c.type)],
+            else_=par_def_sq.c.default_type,
+        )
+        return (
+            self.query(
+                par_def_sq.c.id.label("id"),
+                par_def_sq.c.name.label("name"),
+                par_def_sq.c.description.label("description"),
+                par_def_sq.c.entity_class_id,
+                label("object_class_id", self._object_class_id()),
+                label("relationship_class_id", self._relationship_class_id()),
+                label("default_value", default_value),
+                label("default_type", default_type),
+                label("list_value_id", list_value_id),
+                par_def_sq.c.commit_id.label("commit_id"),
+                par_def_sq.c.parameter_value_list_id.label("parameter_value_list_id"),
+            )
+            .join(self.entity_class_sq, self.entity_class_sq.c.id == par_def_sq.c.entity_class_id)
+            .outerjoin(self.list_value_sq, self.list_value_sq.c.id == list_value_id)
+            .subquery()
+        )
+
+    def _make_parameter_value_sq(self):
+        """
+        Creates a subquery for parameter values.
+
+        Returns:
+            Alias: a parameter value subquery
+        """
+        par_val_sq = self._subquery("parameter_value")
+        list_value_id = case([(par_val_sq.c.type == "list_value_ref", cast(par_val_sq.c.value, Integer()))], else_=None)
+        value = case([(par_val_sq.c.type == "list_value_ref", self.list_value_sq.c.value)], else_=par_val_sq.c.value)
+        type_ = case([(par_val_sq.c.type == "list_value_ref", self.list_value_sq.c.type)], else_=par_val_sq.c.type)
+        return (
+            self.query(
+                par_val_sq.c.id.label("id"),
+                par_val_sq.c.parameter_definition_id,
+                par_val_sq.c.entity_class_id,
+                par_val_sq.c.entity_id,
+                label("object_class_id", self._object_class_id()),
+                label("relationship_class_id", self._relationship_class_id()),
+                label("object_id", self._object_id()),
+                label("relationship_id", self._relationship_id()),
+                label("value", value),
+                label("type", type_),
+                label("list_value_id", list_value_id),
+                par_val_sq.c.commit_id.label("commit_id"),
+                par_val_sq.c.alternative_id,
+            )
+            .join(self.entity_sq, self.entity_sq.c.id == par_val_sq.c.entity_id)
+            .join(self.entity_class_sq, self.entity_class_sq.c.id == par_val_sq.c.entity_class_id)
+            .outerjoin(self.list_value_sq, self.list_value_sq.c.id == list_value_id)
+            .subquery()
+        )
+
+    def _make_alternative_sq(self):
+        """
+        Creates a subquery for alternatives.
+
+        Returns:
+            Alias: an alternative subquery
+        """
+        return self._subquery("alternative")
+
+    def _make_scenario_sq(self):
+        """
+        Creates a subquery for scenarios.
+
+        Returns:
+            Alias: a scenario subquery
+        """
+        return self._subquery("scenario")
+
+    def _make_scenario_alternative_sq(self):
+        """
+        Creates a subquery for scenario alternatives.
+
+        Returns:
+            Alias: a scenario alternative subquery
+        """
+        return self._subquery("scenario_alternative")
+
+    def get_import_alternative(self, cache=None):
+        """Returns the id of the alternative to use as default for all import operations.
+
+        Returns:
+            int, str
+        """
+        if self._import_alternative_id is None:
+            self._create_import_alternative(cache=cache)
+        return self._import_alternative_id, self._import_alternative_name
+
+    def _create_import_alternative(self, cache=None):
+        """Creates the alternative to be used as default for all import operations."""
+        if "alternative" not in cache:
+            cache = self.make_cache({"alternative"})
+        self._import_alternative_name = "Base"
+        self._import_alternative_id = next(
+            (id_ for id_, alt in cache.get("alternative", {}).items() if alt.name == self._import_alternative_name),
+            None,
+        )
+        if not self._import_alternative_id:
+            ids = self._add_alternatives({"name": self._import_alternative_name})
+            self._import_alternative_id = next(iter(ids))
+
+    def override_entity_sq_maker(self, method):
+        """
+        Overrides the function that creates the ``entity_sq`` property.
+
+        Args:
+            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
+                returns entity subquery as an :class:`Alias` object
+        """
+        self._make_entity_sq = MethodType(method, self)
+        self._clear_subqueries("entity")
+
+    def restore_entity_sq_maker(self):
+        """Restores the original function that creates the ``entity_sq`` property."""
+        self._make_entity_sq = MethodType(DatabaseMappingBase._make_entity_sq, self)
+        self._clear_subqueries("entity")
+
+    def override_entity_class_sq_maker(self, method):
+        """
+        Overrides the function that creates the ``entity_class_sq`` property.
+
+        Args:
+            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
+                returns entity class subquery as an :class:`Alias` object
+        """
+        self._make_entity_class_sq = MethodType(method, self)
+        self._clear_subqueries("entity_class")
+
+    def restore_entity_class_sq_maker(self):
+        """Restores the original function that creates the ``entity_class_sq`` property."""
+        self._make_entity_class_sq = MethodType(DatabaseMappingBase._make_entity_class_sq, self)
+        self._clear_subqueries("entity_class")
+
+    def override_parameter_definition_sq_maker(self, method):
+        """
+        Overrides the function that creates the ``parameter_definition_sq`` property.
+
+        Args:
+            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
+                returns parameter definition subquery as an :class:`Alias` object
+        """
+        self._make_parameter_definition_sq = MethodType(method, self)
+        self._clear_subqueries("parameter_definition")
+
+    def restore_parameter_definition_sq_maker(self):
+        """Restores the original function that creates the ``parameter_definition_sq`` property."""
+        self._make_parameter_definition_sq = MethodType(DatabaseMappingBase._make_parameter_definition_sq, self)
+        self._clear_subqueries("parameter_definition")
+
+    def override_parameter_value_sq_maker(self, method):
+        """
+        Overrides the function that creates the ``parameter_value_sq`` property.
+
+        Args:
+            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
+                returns parameter value subquery as an :class:`Alias` object
+        """
+        self._make_parameter_value_sq = MethodType(method, self)
+        self._clear_subqueries("parameter_value")
+
+    def restore_parameter_value_sq_maker(self):
+        """Restores the original function that creates the ``parameter_value_sq`` property."""
+        self._make_parameter_value_sq = MethodType(DatabaseMappingBase._make_parameter_value_sq, self)
+        self._clear_subqueries("parameter_value")
+
+    def override_create_import_alternative(self, method):
+        """
+        Overrides the ``_create_import_alternative`` function.
+
+        Args:
+            method (Callable)
+        """
+        self._create_import_alternative = MethodType(method, self)
+        self._import_alternative_id = None
+
+    def override_alternative_sq_maker(self, method):
+        """
+        Overrides the function that creates the ``alternative_sq`` property.
+
+        Args:
+            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
+                returns alternative subquery as an :class:`Alias` object
+        """
+        self._make_alternative_sq = MethodType(method, self)
+        self._clear_subqueries("alternative")
+
+    def restore_alternative_sq_maker(self):
+        """Restores the original function that creates the ``alternative_sq`` property."""
+        self._make_alternative_sq = MethodType(DatabaseMappingBase._make_alternative_sq, self)
+        self._clear_subqueries("alternative")
+
+    def override_scenario_sq_maker(self, method):
+        """
+        Overrides the function that creates the ``scenario_sq`` property.
+
+        Args:
+            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
+                returns scenario subquery as an :class:`Alias` object
+        """
+        self._make_scenario_sq = MethodType(method, self)
+        self._clear_subqueries("scenario")
+
+    def restore_scenario_sq_maker(self):
+        """Restores the original function that creates the ``scenario_sq`` property."""
+        self._make_scenario_sq = MethodType(DatabaseMappingBase._make_scenario_sq, self)
+        self._clear_subqueries("scenario")
+
+    def override_scenario_alternative_sq_maker(self, method):
+        """
+        Overrides the function that creates the ``scenario_alternative_sq`` property.
+
+        Args:
+            method (Callable): a function that accepts a :class:`DatabaseMappingBase` as its argument and
+                returns scenario alternative subquery as an :class:`Alias` object
+        """
+        self._make_scenario_alternative_sq = MethodType(method, self)
+        self._clear_subqueries("scenario_alternative")
+
+    def restore_scenario_alternative_sq_maker(self):
+        """Restores the original function that creates the ``scenario_alternative_sq`` property."""
+        self._make_scenario_alternative_sq = MethodType(DatabaseMappingBase._make_scenario_alternative_sq, self)
+        self._clear_subqueries("scenario_alternative")
+
+    def _checked_execute(self, stmt, items):
+        if not items:
+            return
+        return self.connection.execute(stmt, items)
+
+    def _get_primary_key(self, tablename):
+        pk = self.composite_pks.get(tablename)
+        if pk is None:
+            table_id = self.table_ids.get(tablename, "id")
+            pk = (table_id,)
+        return pk
+
+    def _reset_mapping(self):
+        """Delete all records from all tables but don't drop the tables.
+        Useful for writing tests
+        """
+        for tablename in self._tablenames:
+            table = self._metadata.tables[tablename]
+            self.connection.execute(table.delete())
+        self.connection.execute("INSERT INTO alternative VALUES (1, 'Base', 'Base alternative', null)")
+
+    def make_cache(
+        self, tablenames, include_descendants=False, include_ancestors=False, force_tablenames=None, keep_existing=False
+    ):
+        if include_descendants:
+            tablenames |= {
+                descendant for tablename in tablenames for descendant in self.descendant_tablenames.get(tablename, ())
+            }
+        if include_ancestors:
+            tablenames |= {
+                ancestor for tablename in tablenames for ancestor in self.ancestor_tablenames.get(tablename, ())
+            }
+        if force_tablenames:
+            tablenames |= force_tablenames
+        for tablename in tablenames & self.cache_sqs.keys():
+            self._do_advance_cache_query(tablename, keep_existing)
+        return self.cache
+
+    def _advance_cache_query(self, tablename, callback=None):
+        advanced = False
+        if tablename not in self.cache:
+            advanced = True
+            self._do_advance_cache_query(tablename)
+        if callback is not None:
+            callback()
+        return advanced
+
+    def _do_advance_cache_query(self, tablename, keep_existing=False):
+        table_cache = self.cache.table_cache(tablename)
+        for x in self.query(getattr(self, self.cache_sqs[tablename])).yield_per(1000).enable_eagerloads(False):
+            table_cache.add_item(x._asdict(), keep_existing)
+
+    def _items_with_type_id(self, tablename, *items):
+        type_id = {
+            "object_class": self.object_class_type,
+            "relationship_class": self.relationship_class_type,
+            "object": self.object_entity_type,
+            "relationship": self.relationship_entity_type,
+        }.get(tablename)
+        if type_id is None:
+            yield from items
+            return
+        for item in items:
+            item["type_id"] = type_id
+            yield item
+
+    def _object_class_id(self):
+        return case([(self.entity_class_sq.c.type_id == self.object_class_type, self.entity_class_sq.c.id)], else_=None)
+
+    def _relationship_class_id(self):
+        return case(
+            [(self.entity_class_sq.c.type_id == self.relationship_class_type, self.entity_class_sq.c.id)], else_=None
+        )
+
+    def _object_id(self):
+        return case([(self.entity_sq.c.type_id == self.object_entity_type, self.entity_sq.c.id)], else_=None)
+
+    def _relationship_id(self):
+        return case([(self.entity_sq.c.type_id == self.relationship_entity_type, self.entity_sq.c.id)], else_=None)
+
+    def _object_class_name(self):
+        return case(
+            [(self.entity_class_sq.c.type_id == self.object_class_type, self.entity_class_sq.c.name)], else_=None
+        )
+
+    def _relationship_class_name(self):
+        return case(
+            [(self.entity_class_sq.c.type_id == self.relationship_class_type, self.entity_class_sq.c.name)], else_=None
+        )
+
+    def _object_class_id_list(self):
+        return case(
+            [
+                (
+                    self.entity_class_sq.c.type_id == self.relationship_class_type,
+                    self.wide_relationship_class_sq.c.object_class_id_list,
+                )
+            ],
+            else_=None,
+        )
+
+    def _object_class_name_list(self):
+        return case(
+            [
+                (
+                    self.entity_class_sq.c.type_id == self.relationship_class_type,
+                    self.wide_relationship_class_sq.c.object_class_name_list,
+                )
+            ],
+            else_=None,
+        )
+
+    def _object_name(self):
+        return case([(self.entity_sq.c.type_id == self.object_entity_type, self.entity_sq.c.name)], else_=None)
+
+    def _object_id_list(self):
+        return case(
+            [(self.entity_sq.c.type_id == self.relationship_entity_type, self.wide_relationship_sq.c.object_id_list)],
+            else_=None,
+        )
+
+    def _object_name_list(self):
+        return case(
+            [(self.entity_sq.c.type_id == self.relationship_entity_type, self.wide_relationship_sq.c.object_name_list)],
+            else_=None,
+        )
+
+    @staticmethod
+    def _metadata_usage_counts(cache):
+        """Counts references to metadata name, value pairs in entity_metadata and parameter_value_metadata tables.
+
+        Args:
+            cache (dict): database cache
+
+        Returns:
+            Counter: usage counts keyed by metadata id
+        """
+        usage_counts = Counter()
+        for entry in cache.get("entity_metadata", {}).values():
+            usage_counts[entry.metadata_id] += 1
+        for entry in cache.get("parameter_value_metadata", {}).values():
+            usage_counts[entry.metadata_id] += 1
+        return usage_counts
+
+    def __del__(self):
+        try:
+            self.connection.close()
+        except AttributeError:
+            pass
+
+    def make_temporary_table(self, table_name, *columns):
+        """Creates a temporary table.
+
+        Args:
+            table_name (str): table name
+            *columns: table's columns
+
+        Returns:
+            Table: created table
+        """
+        table = Table(table_name, self._metadata, *columns, prefixes=["TEMPORARY"])
+        table.drop(self.connection, checkfirst=True)
+        table.create(self.connection)
+        return table
+
+    def get_filter_configs(self):
+        return self._filter_configs
```

### Comparing `spinedb_api-0.30.3/spinedb_api/db_mapping_check_mixin.py` & `spinedb_api-0.30.4/spinedb_api/db_mapping_check_mixin.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,896 +1,896 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""Provides :class:`.DatabaseMappingCheckMixin`.
-
-"""
-# TODO: Review docstrings, they are almost good
-
-from contextlib import contextmanager
-from itertools import chain
-from .exception import SpineIntegrityError
-from .check_functions import (
-    check_alternative,
-    check_scenario,
-    check_scenario_alternative,
-    check_object_class,
-    check_object,
-    check_wide_relationship_class,
-    check_wide_relationship,
-    check_entity_group,
-    check_parameter_definition,
-    check_parameter_value,
-    check_parameter_value_list,
-    check_list_value,
-    check_feature,
-    check_tool,
-    check_tool_feature,
-    check_tool_feature_method,
-    check_entity_metadata,
-    check_metadata,
-    check_parameter_value_metadata,
-)
-from .parameter_value import from_database
-
-
-# NOTE: To check for an update we remove the current instance from our lookup dictionary,
-# check for an insert of the updated instance,
-# and finally reinsert the instance to the dictionary
-class DatabaseMappingCheckMixin:
-    """Provides methods to check whether insert and update operations violate Spine db integrity constraints."""
-
-    def check_items(self, tablename, *items, for_update=False, strict=False, cache=None):
-        return {
-            "alternative": self.check_alternatives,
-            "scenario": self.check_scenarios,
-            "scenario_alternative": self.check_scenario_alternatives,
-            "object": self.check_objects,
-            "object_class": self.check_object_classes,
-            "relationship_class": self.check_wide_relationship_classes,
-            "relationship": self.check_wide_relationships,
-            "entity_group": self.check_entity_groups,
-            "parameter_definition": self.check_parameter_definitions,
-            "parameter_value": self.check_parameter_values,
-            "parameter_value_list": self.check_parameter_value_lists,
-            "list_value": self.check_list_values,
-            "feature": self.check_features,
-            "tool": self.check_tools,
-            "tool_feature": self.check_tool_features,
-            "tool_feature_method": self.check_tool_feature_methods,
-            "metadata": self.check_metadata,
-            "entity_metadata": self.check_entity_metadata,
-            "parameter_value_metadata": self.check_parameter_value_metadata,
-        }[tablename](*items, for_update=for_update, strict=strict, cache=cache)
-
-    def check_features(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether features passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"feature"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        feature_ids = {x.parameter_definition_id: x.id for x in cache.get("feature", {}).values()}
-        parameter_definitions = {
-            x.id: {
-                "name": x.parameter_name,
-                "entity_class_id": x.entity_class_id,
-                "parameter_value_list_id": x.value_list_id,
-            }
-            for x in cache.get("parameter_definition", {}).values()
-        }
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "feature", item, {("parameter_definition_id",): feature_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_feature(item, feature_ids, parameter_definitions)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_tools(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether tools passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"tool"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        tool_ids = {x.name: x.id for x in cache.get("tool", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "tool", item, {("name",): tool_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_tool(item, tool_ids)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_tool_features(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether tool features passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"tool_feature"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        tool_feature_ids = {(x.tool_id, x.feature_id): x.id for x in cache.get("tool_feature", {}).values()}
-        tools = {x.id: x._asdict() for x in cache.get("tool", {}).values()}
-        features = {
-            x.id: {
-                "name": x.entity_class_name + "/" + x.parameter_definition_name,
-                "parameter_value_list_id": x.parameter_value_list_id,
-            }
-            for x in cache.get("feature", {}).values()
-        }
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "tool_feature",
-                    item,
-                    {("tool_id", "feature_id"): tool_feature_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_tool_feature(item, tool_feature_ids, tools, features)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_tool_feature_methods(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether tool feature methods passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"tool_feature_method"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        tool_feature_method_ids = {
-            (x.tool_feature_id, x.method_index): x.id for x in cache.get("tool_feature_method", {}).values()
-        }
-        tool_features = {x.id: x._asdict() for x in cache.get("tool_feature", {}).values()}
-        parameter_value_lists = {
-            x.id: {"name": x.name, "value_index_list": x.value_index_list}
-            for x in cache.get("parameter_value_list", {}).values()
-        }
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "tool_feature_method",
-                    item,
-                    {("tool_feature_id", "method_index"): tool_feature_method_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_tool_feature_method(item, tool_feature_method_ids, tool_features, parameter_value_lists)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_alternatives(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether alternatives passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"alternative"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        alternative_ids = {x.name: x.id for x in cache.get("alternative", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "alternative", item, {("name",): alternative_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_alternative(item, alternative_ids)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_scenarios(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether scenarios passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"scenario"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        scenario_ids = {x.name: x.id for x in cache.get("scenario", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "scenario", item, {("name",): scenario_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_scenario(item, scenario_ids)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_scenario_alternatives(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether scenario alternatives passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"scenario_alternative"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        ids_by_alt_id = {}
-        ids_by_rank = {}
-        for item in cache.get("scenario_alternative", {}).values():
-            ids_by_alt_id[item.scenario_id, item.alternative_id] = item.id
-            ids_by_rank[item.scenario_id, item.rank] = item.id
-        scenario_names = {s.id: s.name for s in cache.get("scenario", {}).values()}
-        alternative_names = {s.id: s.name for s in cache.get("alternative", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "scenario_alternative",
-                    item,
-                    {("scenario_id", "alternative_id"): ids_by_alt_id, ("scenario_id", "rank"): ids_by_rank},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_scenario_alternative(item, ids_by_alt_id, ids_by_rank, scenario_names, alternative_names)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_object_classes(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether object classes passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"object_class"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        object_class_ids = {x.name: x.id for x in cache.get("object_class", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "object_class", item, {("name",): object_class_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_object_class(item, object_class_ids, self.object_class_type)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_objects(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether objects passed as argument respect integrity constraints.
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"object"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        object_ids = {(x.class_id, x.name): x.id for x in cache.get("object", {}).values()}
-        object_class_ids = [x.id for x in cache.get("object_class", {}).values()]
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "object", item, {("class_id", "name"): object_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_object(item, object_ids, object_class_ids, self.object_entity_type)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_wide_relationship_classes(self, *wide_items, for_update=False, strict=False, cache=None):
-        """Check whether relationship classes passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"relationship_class"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_wide_items = list()
-        relationship_class_ids = {x.name: x.id for x in cache.get("relationship_class", {}).values()}
-        object_class_ids = [x.id for x in cache.get("object_class", {}).values()]
-        for wide_item in wide_items:
-            try:
-                with self._manage_stocks(
-                    "relationship_class",
-                    wide_item,
-                    {("name",): relationship_class_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as wide_item:
-                    check_wide_relationship_class(
-                        wide_item, relationship_class_ids, object_class_ids, self.relationship_class_type
-                    )
-                    checked_wide_items.append(wide_item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_wide_items, intgr_error_log
-
-    def check_wide_relationships(self, *wide_items, for_update=False, strict=False, cache=None):
-        """Check whether relationships passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"relationship"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_wide_items = list()
-        relationship_ids_by_name = {(x.class_id, x.name): x.id for x in cache.get("relationship", {}).values()}
-        relationship_ids_by_obj_lst = {
-            (x.class_id, x.object_id_list): x.id for x in cache.get("relationship", {}).values()
-        }
-        relationship_classes = {
-            x.id: {"object_class_id_list": x.object_class_id_list, "name": x.name}
-            for x in cache.get("relationship_class", {}).values()
-        }
-        objects = {x.id: {"class_id": x.class_id, "name": x.name} for x in cache.get("object", {}).values()}
-        for wide_item in wide_items:
-            try:
-                with self._manage_stocks(
-                    "relationship",
-                    wide_item,
-                    {
-                        ("class_id", "name"): relationship_ids_by_name,
-                        ("class_id", "object_id_list"): relationship_ids_by_obj_lst,
-                    },
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as wide_item:
-                    check_wide_relationship(
-                        wide_item,
-                        relationship_ids_by_name,
-                        relationship_ids_by_obj_lst,
-                        relationship_classes,
-                        objects,
-                        self.relationship_entity_type,
-                    )
-                    checked_wide_items.append(wide_item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_wide_items, intgr_error_log
-
-    def check_entity_groups(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether entity groups passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"entity_group"}, include_ancestors=True)
-        intgr_error_log = list()
-        checked_items = list()
-        current_ids = {(x.group_id, x.member_id): x.id for x in cache.get("entity_group", {}).values()}
-        entities = {}
-        for entity in chain(cache.get("object", {}).values(), cache.get("relationship", {}).values()):
-            entities.setdefault(entity.class_id, dict())[entity.id] = entity._asdict()
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "entity_group", item, {("entity_id", "member_id"): current_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_entity_group(item, current_ids, entities)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_parameter_definitions(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether parameter definitions passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns:
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"parameter_definition", "parameter_value"}, include_ancestors=True)
-        parameter_definition_ids_with_values = {
-            value.parameter_id for value in cache.get("parameter_value", {}).values()
-        }
-        intgr_error_log = []
-        checked_items = list()
-        parameter_definition_ids = {
-            (x.entity_class_id, x.parameter_name): x.id for x in cache.get("parameter_definition", {}).values()
-        }
-        object_class_ids = {x.id for x in cache.get("object_class", {}).values()}
-        relationship_class_ids = {x.id for x in cache.get("relationship_class", {}).values()}
-        entity_class_ids = object_class_ids | relationship_class_ids
-        parameter_value_lists = {x.id: x.value_id_list for x in cache.get("parameter_value_list", {}).values()}
-        list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
-        for item in items:
-            object_class_id = item.get("object_class_id")
-            relationship_class_id = item.get("relationship_class_id")
-            if object_class_id and relationship_class_id:
-                e = SpineIntegrityError("Can't associate a parameter to both an object and a relationship class.")
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-                continue
-            if object_class_id:
-                class_ids = object_class_ids
-            elif relationship_class_id:
-                class_ids = relationship_class_ids
-            else:
-                class_ids = entity_class_ids
-            entity_class_id = object_class_id or relationship_class_id
-            if entity_class_id is not None:
-                item["entity_class_id"] = entity_class_id
-            try:
-                if (
-                    for_update
-                    and item["id"] in parameter_definition_ids_with_values
-                    and item["parameter_value_list_id"] != cache["parameter_definition"][item["id"]].value_list_id
-                ):
-                    raise SpineIntegrityError(
-                        f"Can't change value list on parameter {item['name']} because it has parameter values."
-                    )
-                with self._manage_stocks(
-                    "parameter_definition",
-                    item,
-                    {("entity_class_id", "name"): parameter_definition_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as full_item:
-                    check_parameter_definition(
-                        full_item, parameter_definition_ids, class_ids, parameter_value_lists, list_values
-                    )
-                    checked_items.append(full_item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_parameter_values(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether parameter values passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"parameter_value"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        parameter_value_ids = {
-            (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
-        }
-        parameter_definitions = {
-            x.id: {
-                "name": x.parameter_name,
-                "entity_class_id": x.entity_class_id,
-                "parameter_value_list_id": x.value_list_id,
-            }
-            for x in cache.get("parameter_definition", {}).values()
-        }
-        entities = {
-            x.id: {"class_id": x.class_id, "name": x.name}
-            for x in chain(cache.get("object", {}).values(), cache.get("relationship", {}).values())
-        }
-        parameter_value_lists = {x.id: x.value_id_list for x in cache.get("parameter_value_list", {}).values()}
-        list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
-        alternatives = set(a.id for a in cache.get("alternative", {}).values())
-        for item in items:
-            entity_id = item.get("object_id") or item.get("relationship_id")
-            if entity_id is not None:
-                item["entity_id"] = entity_id
-            try:
-                with self._manage_stocks(
-                    "parameter_value",
-                    item,
-                    {("entity_id", "parameter_definition_id", "alternative_id"): parameter_value_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_parameter_value(
-                        item,
-                        parameter_value_ids,
-                        parameter_definitions,
-                        entities,
-                        parameter_value_lists,
-                        list_values,
-                        alternatives,
-                    )
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_parameter_value_lists(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether parameter value-lists passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"parameter_value_list"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        parameter_value_list_ids = {x.name: x.id for x in cache.get("parameter_value_list", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "parameter_value_list",
-                    item,
-                    {("name",): parameter_value_list_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_parameter_value_list(item, parameter_value_list_ids)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_list_values(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether list values passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"list_value"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        list_value_ids_by_index = {
-            (x.parameter_value_list_id, x.index): x.id for x in cache.get("list_value", {}).values()
-        }
-        list_value_ids_by_value = {
-            (x.parameter_value_list_id, x.type, x.value): x.id for x in cache.get("list_value", {}).values()
-        }
-        list_names_by_id = {x.id: x.name for x in cache.get("parameter_value_list", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "list_value",
-                    item,
-                    {
-                        ("parameter_value_list_id", "index"): list_value_ids_by_index,
-                        ("parameter_value_list_id", "type", "value"): list_value_ids_by_value,
-                    },
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_list_value(item, list_names_by_id, list_value_ids_by_index, list_value_ids_by_value)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_metadata(self, *items, for_update=False, strict=False, cache=None):
-        """Checks whether metadata respects integrity constraints.
-
-        Args:
-            *items: One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-            cache (dict, optional): Database cache
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"metadata"})
-        intgr_error_log = []
-        checked_items = list()
-        metadata = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "metadata", item, {("name", "value"): metadata}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_metadata(item, metadata)
-                    if (item["name"], item["value"]) not in metadata:
-                        checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_entity_metadata(self, *items, for_update=False, strict=False, cache=None):
-        """Checks whether entity metadata respects integrity constraints.
-
-        Args:
-            *items: One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-            cache (dict, optional): Database cache
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"entity_metadata"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        entities = {x.id for x in cache.get("object", {}).values()}
-        entities |= {x.id for x in cache.get("relationship", {}).values()}
-        metadata = {x.id for x in cache.get("metadata", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks("entity_metadata", item, {}, for_update, cache, intgr_error_log) as item:
-                    check_entity_metadata(item, entities, metadata)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_parameter_value_metadata(self, *items, for_update=False, strict=False, cache=None):
-        """Checks whether parameter value metadata respects integrity constraints.
-
-        Args:
-            *items: One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-            cache (dict, optional): Database cache
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"parameter_value_metadata"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        values = {x.id for x in cache.get("parameter_value", {}).values()}
-        metadata = {x.id for x in cache.get("metadata", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "parameter_value_metadata", item, {}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_parameter_value_metadata(item, values, metadata)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    @contextmanager
-    def _manage_stocks(self, item_type, item, existing_ids_by_pk, for_update, cache, intgr_error_log):
-        if for_update:
-            try:
-                id_ = item["id"]
-            except KeyError:
-                raise SpineIntegrityError(f"Missing {item_type} identifier.") from None
-            try:
-                full_item = cache.get(item_type, {})[id_]
-            except KeyError:
-                raise SpineIntegrityError(f"{item_type} not found.") from None
-        else:
-            id_ = None
-            full_item = cache.make_item(item_type, item)
-        try:
-            existing_ids_by_key = {
-                _get_key(full_item, pk): existing_ids for pk, existing_ids in existing_ids_by_pk.items()
-            }
-        except KeyError as e:
-            raise SpineIntegrityError(f"Missing key field {e} for {item_type}.") from None
-        if for_update:
-            try:
-                # Remove from existing
-                for key, existing_ids in existing_ids_by_key.items():
-                    del existing_ids[key]
-            except KeyError:
-                raise SpineIntegrityError(f"{item_type} not found.") from None
-            intgr_error_log += _fix_immutable_fields(item_type, full_item, item)
-            full_item.update(item)
-        try:
-            yield full_item
-            # Check is performed at this point
-        except SpineIntegrityError:  # pylint: disable=try-except-raise
-            # Check didn't pass, so reraise
-            raise
-        else:
-            # Check passed, so add to existing
-            for key, existing_ids in existing_ids_by_key.items():
-                existing_ids[key] = id_
-            if for_update:
-                cache.get(item_type, {})[id_] = full_item
-
-
-def _get_key_values(item, pk):
-    for field in pk:
-        value = item[field]
-        if isinstance(value, list):
-            value = tuple(value)
-        yield value
-
-
-def _get_key(item, pk):
-    key = tuple(_get_key_values(item, pk))
-    if len(key) > 1:
-        return key
-    return key[0]
-
-
-def _fix_immutable_fields(item_type, current_item, item):
-    immutable_fields = {
-        "object": ("class_id",),
-        "relationship_class": ("object_class_id_list",),
-        "relationship": ("class_id",),
-        "parameter_definition": ("entity_class_id", "object_class_id", "relationship_class_id"),
-        "parameter_value": ("entity_class_id", "object_class_id", "relationship_class_id"),
-    }.get(item_type, ())
-    fixed = []
-    for field in immutable_fields:
-        if current_item.get(field) is None:
-            continue
-        if field in item and item[field] != current_item[field]:
-            fixed.append(field)
-        item[field] = current_item[field]
-    if fixed:
-        fixed = ', '.join([f"'{field}'" for field in fixed])
-        return [SpineIntegrityError(f"Can't update fixed fields {fixed}")]
-    return []
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""Provides :class:`.DatabaseMappingCheckMixin`.
+
+"""
+# TODO: Review docstrings, they are almost good
+
+from contextlib import contextmanager
+from itertools import chain
+from .exception import SpineIntegrityError
+from .check_functions import (
+    check_alternative,
+    check_scenario,
+    check_scenario_alternative,
+    check_object_class,
+    check_object,
+    check_wide_relationship_class,
+    check_wide_relationship,
+    check_entity_group,
+    check_parameter_definition,
+    check_parameter_value,
+    check_parameter_value_list,
+    check_list_value,
+    check_feature,
+    check_tool,
+    check_tool_feature,
+    check_tool_feature_method,
+    check_entity_metadata,
+    check_metadata,
+    check_parameter_value_metadata,
+)
+from .parameter_value import from_database
+
+
+# NOTE: To check for an update we remove the current instance from our lookup dictionary,
+# check for an insert of the updated instance,
+# and finally reinsert the instance to the dictionary
+class DatabaseMappingCheckMixin:
+    """Provides methods to check whether insert and update operations violate Spine db integrity constraints."""
+
+    def check_items(self, tablename, *items, for_update=False, strict=False, cache=None):
+        return {
+            "alternative": self.check_alternatives,
+            "scenario": self.check_scenarios,
+            "scenario_alternative": self.check_scenario_alternatives,
+            "object": self.check_objects,
+            "object_class": self.check_object_classes,
+            "relationship_class": self.check_wide_relationship_classes,
+            "relationship": self.check_wide_relationships,
+            "entity_group": self.check_entity_groups,
+            "parameter_definition": self.check_parameter_definitions,
+            "parameter_value": self.check_parameter_values,
+            "parameter_value_list": self.check_parameter_value_lists,
+            "list_value": self.check_list_values,
+            "feature": self.check_features,
+            "tool": self.check_tools,
+            "tool_feature": self.check_tool_features,
+            "tool_feature_method": self.check_tool_feature_methods,
+            "metadata": self.check_metadata,
+            "entity_metadata": self.check_entity_metadata,
+            "parameter_value_metadata": self.check_parameter_value_metadata,
+        }[tablename](*items, for_update=for_update, strict=strict, cache=cache)
+
+    def check_features(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether features passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"feature"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        feature_ids = {x.parameter_definition_id: x.id for x in cache.get("feature", {}).values()}
+        parameter_definitions = {
+            x.id: {
+                "name": x.parameter_name,
+                "entity_class_id": x.entity_class_id,
+                "parameter_value_list_id": x.value_list_id,
+            }
+            for x in cache.get("parameter_definition", {}).values()
+        }
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "feature", item, {("parameter_definition_id",): feature_ids}, for_update, cache, intgr_error_log
+                ) as item:
+                    check_feature(item, feature_ids, parameter_definitions)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_tools(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether tools passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"tool"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        tool_ids = {x.name: x.id for x in cache.get("tool", {}).values()}
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "tool", item, {("name",): tool_ids}, for_update, cache, intgr_error_log
+                ) as item:
+                    check_tool(item, tool_ids)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_tool_features(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether tool features passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"tool_feature"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        tool_feature_ids = {(x.tool_id, x.feature_id): x.id for x in cache.get("tool_feature", {}).values()}
+        tools = {x.id: x._asdict() for x in cache.get("tool", {}).values()}
+        features = {
+            x.id: {
+                "name": x.entity_class_name + "/" + x.parameter_definition_name,
+                "parameter_value_list_id": x.parameter_value_list_id,
+            }
+            for x in cache.get("feature", {}).values()
+        }
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "tool_feature",
+                    item,
+                    {("tool_id", "feature_id"): tool_feature_ids},
+                    for_update,
+                    cache,
+                    intgr_error_log,
+                ) as item:
+                    check_tool_feature(item, tool_feature_ids, tools, features)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_tool_feature_methods(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether tool feature methods passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"tool_feature_method"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        tool_feature_method_ids = {
+            (x.tool_feature_id, x.method_index): x.id for x in cache.get("tool_feature_method", {}).values()
+        }
+        tool_features = {x.id: x._asdict() for x in cache.get("tool_feature", {}).values()}
+        parameter_value_lists = {
+            x.id: {"name": x.name, "value_index_list": x.value_index_list}
+            for x in cache.get("parameter_value_list", {}).values()
+        }
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "tool_feature_method",
+                    item,
+                    {("tool_feature_id", "method_index"): tool_feature_method_ids},
+                    for_update,
+                    cache,
+                    intgr_error_log,
+                ) as item:
+                    check_tool_feature_method(item, tool_feature_method_ids, tool_features, parameter_value_lists)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_alternatives(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether alternatives passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"alternative"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        alternative_ids = {x.name: x.id for x in cache.get("alternative", {}).values()}
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "alternative", item, {("name",): alternative_ids}, for_update, cache, intgr_error_log
+                ) as item:
+                    check_alternative(item, alternative_ids)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_scenarios(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether scenarios passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"scenario"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        scenario_ids = {x.name: x.id for x in cache.get("scenario", {}).values()}
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "scenario", item, {("name",): scenario_ids}, for_update, cache, intgr_error_log
+                ) as item:
+                    check_scenario(item, scenario_ids)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_scenario_alternatives(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether scenario alternatives passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"scenario_alternative"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        ids_by_alt_id = {}
+        ids_by_rank = {}
+        for item in cache.get("scenario_alternative", {}).values():
+            ids_by_alt_id[item.scenario_id, item.alternative_id] = item.id
+            ids_by_rank[item.scenario_id, item.rank] = item.id
+        scenario_names = {s.id: s.name for s in cache.get("scenario", {}).values()}
+        alternative_names = {s.id: s.name for s in cache.get("alternative", {}).values()}
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "scenario_alternative",
+                    item,
+                    {("scenario_id", "alternative_id"): ids_by_alt_id, ("scenario_id", "rank"): ids_by_rank},
+                    for_update,
+                    cache,
+                    intgr_error_log,
+                ) as item:
+                    check_scenario_alternative(item, ids_by_alt_id, ids_by_rank, scenario_names, alternative_names)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_object_classes(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether object classes passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"object_class"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        object_class_ids = {x.name: x.id for x in cache.get("object_class", {}).values()}
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "object_class", item, {("name",): object_class_ids}, for_update, cache, intgr_error_log
+                ) as item:
+                    check_object_class(item, object_class_ids, self.object_class_type)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_objects(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether objects passed as argument respect integrity constraints.
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"object"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        object_ids = {(x.class_id, x.name): x.id for x in cache.get("object", {}).values()}
+        object_class_ids = [x.id for x in cache.get("object_class", {}).values()]
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "object", item, {("class_id", "name"): object_ids}, for_update, cache, intgr_error_log
+                ) as item:
+                    check_object(item, object_ids, object_class_ids, self.object_entity_type)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_wide_relationship_classes(self, *wide_items, for_update=False, strict=False, cache=None):
+        """Check whether relationship classes passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"relationship_class"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_wide_items = list()
+        relationship_class_ids = {x.name: x.id for x in cache.get("relationship_class", {}).values()}
+        object_class_ids = [x.id for x in cache.get("object_class", {}).values()]
+        for wide_item in wide_items:
+            try:
+                with self._manage_stocks(
+                    "relationship_class",
+                    wide_item,
+                    {("name",): relationship_class_ids},
+                    for_update,
+                    cache,
+                    intgr_error_log,
+                ) as wide_item:
+                    check_wide_relationship_class(
+                        wide_item, relationship_class_ids, object_class_ids, self.relationship_class_type
+                    )
+                    checked_wide_items.append(wide_item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_wide_items, intgr_error_log
+
+    def check_wide_relationships(self, *wide_items, for_update=False, strict=False, cache=None):
+        """Check whether relationships passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"relationship"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_wide_items = list()
+        relationship_ids_by_name = {(x.class_id, x.name): x.id for x in cache.get("relationship", {}).values()}
+        relationship_ids_by_obj_lst = {
+            (x.class_id, x.object_id_list): x.id for x in cache.get("relationship", {}).values()
+        }
+        relationship_classes = {
+            x.id: {"object_class_id_list": x.object_class_id_list, "name": x.name}
+            for x in cache.get("relationship_class", {}).values()
+        }
+        objects = {x.id: {"class_id": x.class_id, "name": x.name} for x in cache.get("object", {}).values()}
+        for wide_item in wide_items:
+            try:
+                with self._manage_stocks(
+                    "relationship",
+                    wide_item,
+                    {
+                        ("class_id", "name"): relationship_ids_by_name,
+                        ("class_id", "object_id_list"): relationship_ids_by_obj_lst,
+                    },
+                    for_update,
+                    cache,
+                    intgr_error_log,
+                ) as wide_item:
+                    check_wide_relationship(
+                        wide_item,
+                        relationship_ids_by_name,
+                        relationship_ids_by_obj_lst,
+                        relationship_classes,
+                        objects,
+                        self.relationship_entity_type,
+                    )
+                    checked_wide_items.append(wide_item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_wide_items, intgr_error_log
+
+    def check_entity_groups(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether entity groups passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"entity_group"}, include_ancestors=True)
+        intgr_error_log = list()
+        checked_items = list()
+        current_ids = {(x.group_id, x.member_id): x.id for x in cache.get("entity_group", {}).values()}
+        entities = {}
+        for entity in chain(cache.get("object", {}).values(), cache.get("relationship", {}).values()):
+            entities.setdefault(entity.class_id, dict())[entity.id] = entity._asdict()
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "entity_group", item, {("entity_id", "member_id"): current_ids}, for_update, cache, intgr_error_log
+                ) as item:
+                    check_entity_group(item, current_ids, entities)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_parameter_definitions(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether parameter definitions passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns:
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"parameter_definition", "parameter_value"}, include_ancestors=True)
+        parameter_definition_ids_with_values = {
+            value.parameter_id for value in cache.get("parameter_value", {}).values()
+        }
+        intgr_error_log = []
+        checked_items = list()
+        parameter_definition_ids = {
+            (x.entity_class_id, x.parameter_name): x.id for x in cache.get("parameter_definition", {}).values()
+        }
+        object_class_ids = {x.id for x in cache.get("object_class", {}).values()}
+        relationship_class_ids = {x.id for x in cache.get("relationship_class", {}).values()}
+        entity_class_ids = object_class_ids | relationship_class_ids
+        parameter_value_lists = {x.id: x.value_id_list for x in cache.get("parameter_value_list", {}).values()}
+        list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
+        for item in items:
+            object_class_id = item.get("object_class_id")
+            relationship_class_id = item.get("relationship_class_id")
+            if object_class_id and relationship_class_id:
+                e = SpineIntegrityError("Can't associate a parameter to both an object and a relationship class.")
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+                continue
+            if object_class_id:
+                class_ids = object_class_ids
+            elif relationship_class_id:
+                class_ids = relationship_class_ids
+            else:
+                class_ids = entity_class_ids
+            entity_class_id = object_class_id or relationship_class_id
+            if entity_class_id is not None:
+                item["entity_class_id"] = entity_class_id
+            try:
+                if (
+                    for_update
+                    and item["id"] in parameter_definition_ids_with_values
+                    and item["parameter_value_list_id"] != cache["parameter_definition"][item["id"]].value_list_id
+                ):
+                    raise SpineIntegrityError(
+                        f"Can't change value list on parameter {item['name']} because it has parameter values."
+                    )
+                with self._manage_stocks(
+                    "parameter_definition",
+                    item,
+                    {("entity_class_id", "name"): parameter_definition_ids},
+                    for_update,
+                    cache,
+                    intgr_error_log,
+                ) as full_item:
+                    check_parameter_definition(
+                        full_item, parameter_definition_ids, class_ids, parameter_value_lists, list_values
+                    )
+                    checked_items.append(full_item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_parameter_values(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether parameter values passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"parameter_value"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        parameter_value_ids = {
+            (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
+        }
+        parameter_definitions = {
+            x.id: {
+                "name": x.parameter_name,
+                "entity_class_id": x.entity_class_id,
+                "parameter_value_list_id": x.value_list_id,
+            }
+            for x in cache.get("parameter_definition", {}).values()
+        }
+        entities = {
+            x.id: {"class_id": x.class_id, "name": x.name}
+            for x in chain(cache.get("object", {}).values(), cache.get("relationship", {}).values())
+        }
+        parameter_value_lists = {x.id: x.value_id_list for x in cache.get("parameter_value_list", {}).values()}
+        list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
+        alternatives = set(a.id for a in cache.get("alternative", {}).values())
+        for item in items:
+            entity_id = item.get("object_id") or item.get("relationship_id")
+            if entity_id is not None:
+                item["entity_id"] = entity_id
+            try:
+                with self._manage_stocks(
+                    "parameter_value",
+                    item,
+                    {("entity_id", "parameter_definition_id", "alternative_id"): parameter_value_ids},
+                    for_update,
+                    cache,
+                    intgr_error_log,
+                ) as item:
+                    check_parameter_value(
+                        item,
+                        parameter_value_ids,
+                        parameter_definitions,
+                        entities,
+                        parameter_value_lists,
+                        list_values,
+                        alternatives,
+                    )
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_parameter_value_lists(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether parameter value-lists passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"parameter_value_list"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        parameter_value_list_ids = {x.name: x.id for x in cache.get("parameter_value_list", {}).values()}
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "parameter_value_list",
+                    item,
+                    {("name",): parameter_value_list_ids},
+                    for_update,
+                    cache,
+                    intgr_error_log,
+                ) as item:
+                    check_parameter_value_list(item, parameter_value_list_ids)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_list_values(self, *items, for_update=False, strict=False, cache=None):
+        """Check whether list values passed as argument respect integrity constraints.
+
+        Args:
+            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"list_value"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        list_value_ids_by_index = {
+            (x.parameter_value_list_id, x.index): x.id for x in cache.get("list_value", {}).values()
+        }
+        list_value_ids_by_value = {
+            (x.parameter_value_list_id, x.type, x.value): x.id for x in cache.get("list_value", {}).values()
+        }
+        list_names_by_id = {x.id: x.name for x in cache.get("parameter_value_list", {}).values()}
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "list_value",
+                    item,
+                    {
+                        ("parameter_value_list_id", "index"): list_value_ids_by_index,
+                        ("parameter_value_list_id", "type", "value"): list_value_ids_by_value,
+                    },
+                    for_update,
+                    cache,
+                    intgr_error_log,
+                ) as item:
+                    check_list_value(item, list_names_by_id, list_value_ids_by_index, list_value_ids_by_value)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_metadata(self, *items, for_update=False, strict=False, cache=None):
+        """Checks whether metadata respects integrity constraints.
+
+        Args:
+            *items: One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+            cache (dict, optional): Database cache
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"metadata"})
+        intgr_error_log = []
+        checked_items = list()
+        metadata = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "metadata", item, {("name", "value"): metadata}, for_update, cache, intgr_error_log
+                ) as item:
+                    check_metadata(item, metadata)
+                    if (item["name"], item["value"]) not in metadata:
+                        checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_entity_metadata(self, *items, for_update=False, strict=False, cache=None):
+        """Checks whether entity metadata respects integrity constraints.
+
+        Args:
+            *items: One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+            cache (dict, optional): Database cache
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"entity_metadata"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        entities = {x.id for x in cache.get("object", {}).values()}
+        entities |= {x.id for x in cache.get("relationship", {}).values()}
+        metadata = {x.id for x in cache.get("metadata", {}).values()}
+        for item in items:
+            try:
+                with self._manage_stocks("entity_metadata", item, {}, for_update, cache, intgr_error_log) as item:
+                    check_entity_metadata(item, entities, metadata)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    def check_parameter_value_metadata(self, *items, for_update=False, strict=False, cache=None):
+        """Checks whether parameter value metadata respects integrity constraints.
+
+        Args:
+            *items: One or more Python :class:`dict` objects representing the items to be checked.
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if one of the items violates an integrity constraint.
+            cache (dict, optional): Database cache
+
+        Returns
+            list: items that passed the check.
+            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
+        """
+        if cache is None:
+            cache = self.make_cache({"parameter_value_metadata"}, include_ancestors=True)
+        intgr_error_log = []
+        checked_items = list()
+        values = {x.id for x in cache.get("parameter_value", {}).values()}
+        metadata = {x.id for x in cache.get("metadata", {}).values()}
+        for item in items:
+            try:
+                with self._manage_stocks(
+                    "parameter_value_metadata", item, {}, for_update, cache, intgr_error_log
+                ) as item:
+                    check_parameter_value_metadata(item, values, metadata)
+                    checked_items.append(item)
+            except SpineIntegrityError as e:
+                if strict:
+                    raise e
+                intgr_error_log.append(e)
+        return checked_items, intgr_error_log
+
+    @contextmanager
+    def _manage_stocks(self, item_type, item, existing_ids_by_pk, for_update, cache, intgr_error_log):
+        if for_update:
+            try:
+                id_ = item["id"]
+            except KeyError:
+                raise SpineIntegrityError(f"Missing {item_type} identifier.") from None
+            try:
+                full_item = cache.get(item_type, {})[id_]
+            except KeyError:
+                raise SpineIntegrityError(f"{item_type} not found.") from None
+        else:
+            id_ = None
+            full_item = cache.make_item(item_type, item)
+        try:
+            existing_ids_by_key = {
+                _get_key(full_item, pk): existing_ids for pk, existing_ids in existing_ids_by_pk.items()
+            }
+        except KeyError as e:
+            raise SpineIntegrityError(f"Missing key field {e} for {item_type}.") from None
+        if for_update:
+            try:
+                # Remove from existing
+                for key, existing_ids in existing_ids_by_key.items():
+                    del existing_ids[key]
+            except KeyError:
+                raise SpineIntegrityError(f"{item_type} not found.") from None
+            intgr_error_log += _fix_immutable_fields(item_type, full_item, item)
+            full_item.update(item)
+        try:
+            yield full_item
+            # Check is performed at this point
+        except SpineIntegrityError:  # pylint: disable=try-except-raise
+            # Check didn't pass, so reraise
+            raise
+        else:
+            # Check passed, so add to existing
+            for key, existing_ids in existing_ids_by_key.items():
+                existing_ids[key] = id_
+            if for_update:
+                cache.get(item_type, {})[id_] = full_item
+
+
+def _get_key_values(item, pk):
+    for field in pk:
+        value = item[field]
+        if isinstance(value, list):
+            value = tuple(value)
+        yield value
+
+
+def _get_key(item, pk):
+    key = tuple(_get_key_values(item, pk))
+    if len(key) > 1:
+        return key
+    return key[0]
+
+
+def _fix_immutable_fields(item_type, current_item, item):
+    immutable_fields = {
+        "object": ("class_id",),
+        "relationship_class": ("object_class_id_list",),
+        "relationship": ("class_id",),
+        "parameter_definition": ("entity_class_id", "object_class_id", "relationship_class_id"),
+        "parameter_value": ("entity_class_id", "object_class_id", "relationship_class_id"),
+    }.get(item_type, ())
+    fixed = []
+    for field in immutable_fields:
+        if current_item.get(field) is None:
+            continue
+        if field in item and item[field] != current_item[field]:
+            fixed.append(field)
+        item[field] = current_item[field]
+    if fixed:
+        fixed = ', '.join([f"'{field}'" for field in fixed])
+        return [SpineIntegrityError(f"Can't update fixed fields {fixed}")]
+    return []
```

### Comparing `spinedb_api-0.30.3/spinedb_api/db_mapping_remove_mixin.py` & `spinedb_api-0.30.4/spinedb_api/db_mapping_remove_mixin.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,301 +1,301 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""Provides :class:`.DiffDatabaseMappingRemoveMixin`.
-
-"""
-
-from sqlalchemy.exc import DBAPIError
-from .exception import SpineDBAPIError
-
-# TODO: improve docstrings
-
-
-class DatabaseMappingRemoveMixin:
-    """Provides the :meth:`remove_items` method to stage ``REMOVE`` operations over a Spine db."""
-
-    # pylint: disable=redefined-builtin
-    def cascade_remove_items(self, cache=None, **kwargs):
-        """Removes items by id in cascade.
-
-        Args:
-            **kwargs: keyword is table name, argument is list of ids to remove
-
-        Returns:
-            list of CacheItem: removed items
-        """
-        cascading_ids = self.cascading_ids(cache=cache, **kwargs)
-        return self.remove_items(**cascading_ids)
-
-    def remove_items(self, **kwargs):
-        """Removes items by id, *not in cascade*.
-
-        Args:
-            **kwargs: keyword is table name, argument is list of ids to remove
-
-        Returns:
-            list of CacheItems: removed items
-        """
-        if not self.committing:
-            return
-        self._make_commit_id()
-        removed_items = []
-        for tablename, ids in kwargs.items():
-            if not ids:
-                continue
-            table_id = self.table_ids.get(tablename, "id")
-            table = self._metadata.tables[tablename]
-            delete = table.delete().where(self.in_(getattr(table.c, table_id), ids))
-            try:
-                self.connection.execute(delete)
-                table_cache = self.cache.get(tablename)
-                if table_cache:
-                    for id_ in ids:
-                        removed_items += table_cache.remove_item(id_)
-            except DBAPIError as e:
-                msg = f"DBAPIError while removing {tablename} items: {e.orig.args}"
-                raise SpineDBAPIError(msg) from e
-        return removed_items
-
-    # pylint: disable=redefined-builtin
-    def cascading_ids(self, cache=None, **kwargs):
-        """Returns cascading ids.
-
-        Keyword args:
-            cache (dict, optional)
-            **kwargs: set of ids keyed by table name to be removed
-
-        Returns:
-            cascading_ids (dict): cascading ids keyed by table name
-        """
-        if cache is None:
-            try:
-                cache = self.make_cache(
-                    set(kwargs),
-                    include_descendants=True,
-                    force_tablenames={"entity_metadata", "parameter_value_metadata"}
-                    if any(x in kwargs for x in ("entity_metadata", "parameter_value_metadata", "metadata"))
-                    else None,
-                    keep_existing=True,
-                )
-            except DBAPIError as e:
-                raise SpineDBAPIError(f"Fail to get cascading ids: {e.orig.args}")
-        ids = {}
-        self._merge(ids, self._object_class_cascading_ids(kwargs.get("object_class", set()), cache))
-        self._merge(ids, self._object_cascading_ids(kwargs.get("object", set()), cache))
-        self._merge(ids, self._relationship_class_cascading_ids(kwargs.get("relationship_class", set()), cache))
-        self._merge(ids, self._relationship_cascading_ids(kwargs.get("relationship", set()), cache))
-        self._merge(ids, self._entity_group_cascading_ids(kwargs.get("entity_group", set()), cache))
-        self._merge(ids, self._parameter_definition_cascading_ids(kwargs.get("parameter_definition", set()), cache))
-        self._merge(ids, self._parameter_value_cascading_ids(kwargs.get("parameter_value", set()), cache))
-        self._merge(ids, self._parameter_value_list_cascading_ids(kwargs.get("parameter_value_list", set()), cache))
-        self._merge(ids, self._list_value_cascading_ids(kwargs.get("list_value", set()), cache))
-        self._merge(ids, self._alternative_cascading_ids(kwargs.get("alternative", set()), cache))
-        self._merge(ids, self._scenario_cascading_ids(kwargs.get("scenario", set()), cache))
-        self._merge(ids, self._scenario_alternatives_cascading_ids(kwargs.get("scenario_alternative", set()), cache))
-        self._merge(ids, self._feature_cascading_ids(kwargs.get("feature", set()), cache))
-        self._merge(ids, self._tool_cascading_ids(kwargs.get("tool", set()), cache))
-        self._merge(ids, self._tool_feature_cascading_ids(kwargs.get("tool_feature", set()), cache))
-        self._merge(ids, self._tool_feature_method_cascading_ids(kwargs.get("tool_feature_method", set()), cache))
-        self._merge(ids, self._metadata_cascading_ids(kwargs.get("metadata", set()), cache))
-        self._merge(ids, self._entity_metadata_cascading_ids(kwargs.get("entity_metadata", set()), cache))
-        self._merge(
-            ids, self._parameter_value_metadata_cascading_ids(kwargs.get("parameter_value_metadata", set()), cache)
-        )
-        sorted_ids = {}
-        tablenames = list(ids)
-        while tablenames:
-            tablename = tablenames.pop(0)
-            ancestors = self.ancestor_tablenames.get(tablename)
-            if ancestors is None or all(x in sorted_ids for x in ancestors):
-                sorted_ids[tablename] = ids.pop(tablename)
-            else:
-                tablenames.append(tablename)
-        return sorted_ids
-
-    @staticmethod
-    def _merge(left, right):
-        for tablename, ids in right.items():
-            left.setdefault(tablename, set()).update(ids)
-
-    def _alternative_cascading_ids(self, ids, cache):
-        """Returns alternative cascading ids."""
-        cascading_ids = {"alternative": set(ids)}
-        parameter_values = [x for x in dict.values(cache.get("parameter_value", {})) if x.alternative_id in ids]
-        scenario_alternatives = [
-            x for x in dict.values(cache.get("scenario_alternative", {})) if x.alternative_id in ids
-        ]
-        self._merge(cascading_ids, self._parameter_value_cascading_ids({x.id for x in parameter_values}, cache))
-        self._merge(
-            cascading_ids, self._scenario_alternatives_cascading_ids({x.id for x in scenario_alternatives}, cache)
-        )
-        return cascading_ids
-
-    def _scenario_cascading_ids(self, ids, cache):
-        cascading_ids = {"scenario": set(ids)}
-        scenario_alternatives = [x for x in dict.values(cache.get("scenario_alternative", {})) if x.scenario_id in ids]
-        self._merge(
-            cascading_ids, self._scenario_alternatives_cascading_ids({x.id for x in scenario_alternatives}, cache)
-        )
-        return cascading_ids
-
-    def _object_class_cascading_ids(self, ids, cache):
-        """Returns object class cascading ids."""
-        cascading_ids = {"entity_class": set(ids), "object_class": set(ids)}
-        objects = [x for x in dict.values(cache.get("object", {})) if x.class_id in ids]
-        relationship_classes = (
-            x for x in dict.values(cache.get("relationship_class", {})) if set(x.object_class_id_list).intersection(ids)
-        )
-        paramerer_definitions = [
-            x for x in dict.values(cache.get("parameter_definition", {})) if x.entity_class_id in ids
-        ]
-        self._merge(cascading_ids, self._object_cascading_ids({x.id for x in objects}, cache))
-        self._merge(cascading_ids, self._relationship_class_cascading_ids({x.id for x in relationship_classes}, cache))
-        self._merge(
-            cascading_ids, self._parameter_definition_cascading_ids({x.id for x in paramerer_definitions}, cache)
-        )
-        return cascading_ids
-
-    def _object_cascading_ids(self, ids, cache):
-        """Returns object cascading ids."""
-        cascading_ids = {"entity": set(ids), "object": set(ids)}
-        relationships = (
-            x for x in dict.values(cache.get("relationship", {})) if set(x.object_id_list).intersection(ids)
-        )
-        parameter_values = [x for x in dict.values(cache.get("parameter_value", {})) if x.entity_id in ids]
-        groups = [x for x in dict.values(cache.get("entity_group", {})) if {x.group_id, x.member_id}.intersection(ids)]
-        entity_metadata_ids = {x.id for x in dict.values(cache.get("entity_metadata", {})) if x.entity_id in ids}
-        self._merge(cascading_ids, self._relationship_cascading_ids({x.id for x in relationships}, cache))
-        self._merge(cascading_ids, self._parameter_value_cascading_ids({x.id for x in parameter_values}, cache))
-        self._merge(cascading_ids, self._entity_group_cascading_ids({x.id for x in groups}, cache))
-        self._merge(cascading_ids, self._entity_metadata_cascading_ids(entity_metadata_ids, cache))
-        return cascading_ids
-
-    def _relationship_class_cascading_ids(self, ids, cache):
-        """Returns relationship class cascading ids."""
-        cascading_ids = {
-            "relationship_class": set(ids),
-            "relationship_entity_class": set(ids),
-            "entity_class": set(ids),
-        }
-        relationships = [x for x in dict.values(cache.get("relationship", {})) if x.class_id in ids]
-        paramerer_definitions = [
-            x for x in dict.values(cache.get("parameter_definition", {})) if x.entity_class_id in ids
-        ]
-        self._merge(cascading_ids, self._relationship_cascading_ids({x.id for x in relationships}, cache))
-        self._merge(
-            cascading_ids, self._parameter_definition_cascading_ids({x.id for x in paramerer_definitions}, cache)
-        )
-        return cascading_ids
-
-    def _relationship_cascading_ids(self, ids, cache):
-        """Returns relationship cascading ids."""
-        cascading_ids = {"relationship": set(ids), "entity": set(ids), "relationship_entity": set(ids)}
-        parameter_values = [x for x in dict.values(cache.get("parameter_value", {})) if x.entity_id in ids]
-        groups = [x for x in dict.values(cache.get("entity_group", {})) if {x.group_id, x.member_id}.intersection(ids)]
-        entity_metadata_ids = {x.id for x in dict.values(cache.get("entity_metadata", {})) if x.entity_id in ids}
-        self._merge(cascading_ids, self._parameter_value_cascading_ids({x.id for x in parameter_values}, cache))
-        self._merge(cascading_ids, self._entity_group_cascading_ids({x.id for x in groups}, cache))
-        self._merge(cascading_ids, self._entity_metadata_cascading_ids(entity_metadata_ids, cache))
-        return cascading_ids
-
-    def _entity_group_cascading_ids(self, ids, cache):  # pylint: disable=no-self-use
-        """Returns entity group cascading ids."""
-        return {"entity_group": set(ids)}
-
-    def _parameter_definition_cascading_ids(self, ids, cache):
-        """Returns parameter definition cascading ids."""
-        cascading_ids = {"parameter_definition": set(ids)}
-        parameter_values = [x for x in dict.values(cache.get("parameter_value", {})) if x.parameter_id in ids]
-        features = [x for x in dict.values(cache.get("feature", {})) if x.parameter_definition_id in ids]
-        self._merge(cascading_ids, self._parameter_value_cascading_ids({x.id for x in parameter_values}, cache))
-        self._merge(cascading_ids, self._feature_cascading_ids({x.id for x in features}, cache))
-        return cascading_ids
-
-    def _parameter_value_cascading_ids(self, ids, cache):  # pylint: disable=no-self-use
-        """Returns parameter value cascading ids."""
-        cascading_ids = {"parameter_value": set(ids)}
-        value_metadata_ids = {
-            x.id for x in dict.values(cache.get("parameter_value_metadata", {})) if x.parameter_value_id in ids
-        }
-        self._merge(cascading_ids, self._parameter_value_metadata_cascading_ids(value_metadata_ids, cache))
-        return cascading_ids
-
-    def _parameter_value_list_cascading_ids(self, ids, cache):  # pylint: disable=no-self-use
-        """Returns parameter value list cascading ids and adds them to the given dictionaries."""
-        cascading_ids = {"parameter_value_list": set(ids)}
-        features = [x for x in dict.values(cache.get("feature", {})) if x.parameter_value_list_id in ids]
-        self._merge(cascading_ids, self._feature_cascading_ids({x.id for x in features}, cache))
-        return cascading_ids
-
-    def _list_value_cascading_ids(self, ids, cache):  # pylint: disable=no-self-use
-        """Returns parameter value list value cascading ids."""
-        return {"list_value": set(ids)}
-
-    def _scenario_alternatives_cascading_ids(self, ids, cache):
-        return {"scenario_alternative": set(ids)}
-
-    def _feature_cascading_ids(self, ids, cache):
-        cascading_ids = {"feature": set(ids)}
-        tool_features = [x for x in dict.values(cache.get("tool_feature", {})) if x.feature_id in ids]
-        self._merge(cascading_ids, self._tool_feature_cascading_ids({x.id for x in tool_features}, cache))
-        return cascading_ids
-
-    def _tool_cascading_ids(self, ids, cache):
-        cascading_ids = {"tool": set(ids)}
-        tool_features = [x for x in dict.values(cache.get("tool_feature", {})) if x.tool_id in ids]
-        self._merge(cascading_ids, self._tool_feature_cascading_ids({x.id for x in tool_features}, cache))
-        return cascading_ids
-
-    def _tool_feature_cascading_ids(self, ids, cache):
-        cascading_ids = {"tool_feature": set(ids)}
-        tool_feature_methods = [
-            x for x in dict.values(cache.get("tool_feature_method", {})) if x.tool_feature_id in ids
-        ]
-        self._merge(cascading_ids, self._tool_feature_method_cascading_ids({x.id for x in tool_feature_methods}, cache))
-        return cascading_ids
-
-    def _tool_feature_method_cascading_ids(self, ids, cache):
-        return {"tool_feature_method": set(ids)}
-
-    def _metadata_cascading_ids(self, ids, cache):
-        cascading_ids = {"metadata": set(ids)}
-        entity_metadata = {
-            "entity_metadata": {x.id for x in dict.values(cache.get("entity_metadata", {})) if x.metadata_id in ids}
-        }
-        self._merge(cascading_ids, entity_metadata)
-        value_metadata = {
-            "parameter_value_metadata": {
-                x.id for x in dict.values(cache.get("parameter_value_metadata", {})) if x.metadata_id in ids
-            }
-        }
-        self._merge(cascading_ids, value_metadata)
-        return cascading_ids
-
-    def _non_referenced_metadata_ids(self, ids, metadata_table_name, cache):
-        metadata_id_counts = self._metadata_usage_counts(cache)
-        cascading_ids = {}
-        metadata = cache.get(metadata_table_name, {})
-        for id_ in ids:
-            metadata_id_counts[metadata[id_].metadata_id] -= 1
-        zero_count_metadata_ids = {id_ for id_, count in metadata_id_counts.items() if count == 0}
-        self._merge(cascading_ids, {"metadata": zero_count_metadata_ids})
-        return cascading_ids
-
-    def _entity_metadata_cascading_ids(self, ids, cache):
-        cascading_ids = {"entity_metadata": set(ids)}
-        cascading_ids.update(self._non_referenced_metadata_ids(ids, "entity_metadata", cache))
-        return cascading_ids
-
-    def _parameter_value_metadata_cascading_ids(self, ids, cache):
-        cascading_ids = {"parameter_value_metadata": set(ids)}
-        cascading_ids.update(self._non_referenced_metadata_ids(ids, "parameter_value_metadata", cache))
-        return cascading_ids
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""Provides :class:`.DiffDatabaseMappingRemoveMixin`.
+
+"""
+
+from sqlalchemy.exc import DBAPIError
+from .exception import SpineDBAPIError
+
+# TODO: improve docstrings
+
+
+class DatabaseMappingRemoveMixin:
+    """Provides the :meth:`remove_items` method to stage ``REMOVE`` operations over a Spine db."""
+
+    # pylint: disable=redefined-builtin
+    def cascade_remove_items(self, cache=None, **kwargs):
+        """Removes items by id in cascade.
+
+        Args:
+            **kwargs: keyword is table name, argument is list of ids to remove
+
+        Returns:
+            list of CacheItem: removed items
+        """
+        cascading_ids = self.cascading_ids(cache=cache, **kwargs)
+        return self.remove_items(**cascading_ids)
+
+    def remove_items(self, **kwargs):
+        """Removes items by id, *not in cascade*.
+
+        Args:
+            **kwargs: keyword is table name, argument is list of ids to remove
+
+        Returns:
+            list of CacheItems: removed items
+        """
+        if not self.committing:
+            return
+        self._make_commit_id()
+        removed_items = []
+        for tablename, ids in kwargs.items():
+            if not ids:
+                continue
+            table_id = self.table_ids.get(tablename, "id")
+            table = self._metadata.tables[tablename]
+            delete = table.delete().where(self.in_(getattr(table.c, table_id), ids))
+            try:
+                self.connection.execute(delete)
+                table_cache = self.cache.get(tablename)
+                if table_cache:
+                    for id_ in ids:
+                        removed_items += table_cache.remove_item(id_)
+            except DBAPIError as e:
+                msg = f"DBAPIError while removing {tablename} items: {e.orig.args}"
+                raise SpineDBAPIError(msg) from e
+        return removed_items
+
+    # pylint: disable=redefined-builtin
+    def cascading_ids(self, cache=None, **kwargs):
+        """Returns cascading ids.
+
+        Keyword args:
+            cache (dict, optional)
+            **kwargs: set of ids keyed by table name to be removed
+
+        Returns:
+            cascading_ids (dict): cascading ids keyed by table name
+        """
+        if cache is None:
+            try:
+                cache = self.make_cache(
+                    set(kwargs),
+                    include_descendants=True,
+                    force_tablenames={"entity_metadata", "parameter_value_metadata"}
+                    if any(x in kwargs for x in ("entity_metadata", "parameter_value_metadata", "metadata"))
+                    else None,
+                    keep_existing=True,
+                )
+            except DBAPIError as e:
+                raise SpineDBAPIError(f"Fail to get cascading ids: {e.orig.args}")
+        ids = {}
+        self._merge(ids, self._object_class_cascading_ids(kwargs.get("object_class", set()), cache))
+        self._merge(ids, self._object_cascading_ids(kwargs.get("object", set()), cache))
+        self._merge(ids, self._relationship_class_cascading_ids(kwargs.get("relationship_class", set()), cache))
+        self._merge(ids, self._relationship_cascading_ids(kwargs.get("relationship", set()), cache))
+        self._merge(ids, self._entity_group_cascading_ids(kwargs.get("entity_group", set()), cache))
+        self._merge(ids, self._parameter_definition_cascading_ids(kwargs.get("parameter_definition", set()), cache))
+        self._merge(ids, self._parameter_value_cascading_ids(kwargs.get("parameter_value", set()), cache))
+        self._merge(ids, self._parameter_value_list_cascading_ids(kwargs.get("parameter_value_list", set()), cache))
+        self._merge(ids, self._list_value_cascading_ids(kwargs.get("list_value", set()), cache))
+        self._merge(ids, self._alternative_cascading_ids(kwargs.get("alternative", set()), cache))
+        self._merge(ids, self._scenario_cascading_ids(kwargs.get("scenario", set()), cache))
+        self._merge(ids, self._scenario_alternatives_cascading_ids(kwargs.get("scenario_alternative", set()), cache))
+        self._merge(ids, self._feature_cascading_ids(kwargs.get("feature", set()), cache))
+        self._merge(ids, self._tool_cascading_ids(kwargs.get("tool", set()), cache))
+        self._merge(ids, self._tool_feature_cascading_ids(kwargs.get("tool_feature", set()), cache))
+        self._merge(ids, self._tool_feature_method_cascading_ids(kwargs.get("tool_feature_method", set()), cache))
+        self._merge(ids, self._metadata_cascading_ids(kwargs.get("metadata", set()), cache))
+        self._merge(ids, self._entity_metadata_cascading_ids(kwargs.get("entity_metadata", set()), cache))
+        self._merge(
+            ids, self._parameter_value_metadata_cascading_ids(kwargs.get("parameter_value_metadata", set()), cache)
+        )
+        sorted_ids = {}
+        tablenames = list(ids)
+        while tablenames:
+            tablename = tablenames.pop(0)
+            ancestors = self.ancestor_tablenames.get(tablename)
+            if ancestors is None or all(x in sorted_ids for x in ancestors):
+                sorted_ids[tablename] = ids.pop(tablename)
+            else:
+                tablenames.append(tablename)
+        return sorted_ids
+
+    @staticmethod
+    def _merge(left, right):
+        for tablename, ids in right.items():
+            left.setdefault(tablename, set()).update(ids)
+
+    def _alternative_cascading_ids(self, ids, cache):
+        """Returns alternative cascading ids."""
+        cascading_ids = {"alternative": set(ids)}
+        parameter_values = [x for x in dict.values(cache.get("parameter_value", {})) if x.alternative_id in ids]
+        scenario_alternatives = [
+            x for x in dict.values(cache.get("scenario_alternative", {})) if x.alternative_id in ids
+        ]
+        self._merge(cascading_ids, self._parameter_value_cascading_ids({x.id for x in parameter_values}, cache))
+        self._merge(
+            cascading_ids, self._scenario_alternatives_cascading_ids({x.id for x in scenario_alternatives}, cache)
+        )
+        return cascading_ids
+
+    def _scenario_cascading_ids(self, ids, cache):
+        cascading_ids = {"scenario": set(ids)}
+        scenario_alternatives = [x for x in dict.values(cache.get("scenario_alternative", {})) if x.scenario_id in ids]
+        self._merge(
+            cascading_ids, self._scenario_alternatives_cascading_ids({x.id for x in scenario_alternatives}, cache)
+        )
+        return cascading_ids
+
+    def _object_class_cascading_ids(self, ids, cache):
+        """Returns object class cascading ids."""
+        cascading_ids = {"entity_class": set(ids), "object_class": set(ids)}
+        objects = [x for x in dict.values(cache.get("object", {})) if x.class_id in ids]
+        relationship_classes = (
+            x for x in dict.values(cache.get("relationship_class", {})) if set(x.object_class_id_list).intersection(ids)
+        )
+        paramerer_definitions = [
+            x for x in dict.values(cache.get("parameter_definition", {})) if x.entity_class_id in ids
+        ]
+        self._merge(cascading_ids, self._object_cascading_ids({x.id for x in objects}, cache))
+        self._merge(cascading_ids, self._relationship_class_cascading_ids({x.id for x in relationship_classes}, cache))
+        self._merge(
+            cascading_ids, self._parameter_definition_cascading_ids({x.id for x in paramerer_definitions}, cache)
+        )
+        return cascading_ids
+
+    def _object_cascading_ids(self, ids, cache):
+        """Returns object cascading ids."""
+        cascading_ids = {"entity": set(ids), "object": set(ids)}
+        relationships = (
+            x for x in dict.values(cache.get("relationship", {})) if set(x.object_id_list).intersection(ids)
+        )
+        parameter_values = [x for x in dict.values(cache.get("parameter_value", {})) if x.entity_id in ids]
+        groups = [x for x in dict.values(cache.get("entity_group", {})) if {x.group_id, x.member_id}.intersection(ids)]
+        entity_metadata_ids = {x.id for x in dict.values(cache.get("entity_metadata", {})) if x.entity_id in ids}
+        self._merge(cascading_ids, self._relationship_cascading_ids({x.id for x in relationships}, cache))
+        self._merge(cascading_ids, self._parameter_value_cascading_ids({x.id for x in parameter_values}, cache))
+        self._merge(cascading_ids, self._entity_group_cascading_ids({x.id for x in groups}, cache))
+        self._merge(cascading_ids, self._entity_metadata_cascading_ids(entity_metadata_ids, cache))
+        return cascading_ids
+
+    def _relationship_class_cascading_ids(self, ids, cache):
+        """Returns relationship class cascading ids."""
+        cascading_ids = {
+            "relationship_class": set(ids),
+            "relationship_entity_class": set(ids),
+            "entity_class": set(ids),
+        }
+        relationships = [x for x in dict.values(cache.get("relationship", {})) if x.class_id in ids]
+        paramerer_definitions = [
+            x for x in dict.values(cache.get("parameter_definition", {})) if x.entity_class_id in ids
+        ]
+        self._merge(cascading_ids, self._relationship_cascading_ids({x.id for x in relationships}, cache))
+        self._merge(
+            cascading_ids, self._parameter_definition_cascading_ids({x.id for x in paramerer_definitions}, cache)
+        )
+        return cascading_ids
+
+    def _relationship_cascading_ids(self, ids, cache):
+        """Returns relationship cascading ids."""
+        cascading_ids = {"relationship": set(ids), "entity": set(ids), "relationship_entity": set(ids)}
+        parameter_values = [x for x in dict.values(cache.get("parameter_value", {})) if x.entity_id in ids]
+        groups = [x for x in dict.values(cache.get("entity_group", {})) if {x.group_id, x.member_id}.intersection(ids)]
+        entity_metadata_ids = {x.id for x in dict.values(cache.get("entity_metadata", {})) if x.entity_id in ids}
+        self._merge(cascading_ids, self._parameter_value_cascading_ids({x.id for x in parameter_values}, cache))
+        self._merge(cascading_ids, self._entity_group_cascading_ids({x.id for x in groups}, cache))
+        self._merge(cascading_ids, self._entity_metadata_cascading_ids(entity_metadata_ids, cache))
+        return cascading_ids
+
+    def _entity_group_cascading_ids(self, ids, cache):  # pylint: disable=no-self-use
+        """Returns entity group cascading ids."""
+        return {"entity_group": set(ids)}
+
+    def _parameter_definition_cascading_ids(self, ids, cache):
+        """Returns parameter definition cascading ids."""
+        cascading_ids = {"parameter_definition": set(ids)}
+        parameter_values = [x for x in dict.values(cache.get("parameter_value", {})) if x.parameter_id in ids]
+        features = [x for x in dict.values(cache.get("feature", {})) if x.parameter_definition_id in ids]
+        self._merge(cascading_ids, self._parameter_value_cascading_ids({x.id for x in parameter_values}, cache))
+        self._merge(cascading_ids, self._feature_cascading_ids({x.id for x in features}, cache))
+        return cascading_ids
+
+    def _parameter_value_cascading_ids(self, ids, cache):  # pylint: disable=no-self-use
+        """Returns parameter value cascading ids."""
+        cascading_ids = {"parameter_value": set(ids)}
+        value_metadata_ids = {
+            x.id for x in dict.values(cache.get("parameter_value_metadata", {})) if x.parameter_value_id in ids
+        }
+        self._merge(cascading_ids, self._parameter_value_metadata_cascading_ids(value_metadata_ids, cache))
+        return cascading_ids
+
+    def _parameter_value_list_cascading_ids(self, ids, cache):  # pylint: disable=no-self-use
+        """Returns parameter value list cascading ids and adds them to the given dictionaries."""
+        cascading_ids = {"parameter_value_list": set(ids)}
+        features = [x for x in dict.values(cache.get("feature", {})) if x.parameter_value_list_id in ids]
+        self._merge(cascading_ids, self._feature_cascading_ids({x.id for x in features}, cache))
+        return cascading_ids
+
+    def _list_value_cascading_ids(self, ids, cache):  # pylint: disable=no-self-use
+        """Returns parameter value list value cascading ids."""
+        return {"list_value": set(ids)}
+
+    def _scenario_alternatives_cascading_ids(self, ids, cache):
+        return {"scenario_alternative": set(ids)}
+
+    def _feature_cascading_ids(self, ids, cache):
+        cascading_ids = {"feature": set(ids)}
+        tool_features = [x for x in dict.values(cache.get("tool_feature", {})) if x.feature_id in ids]
+        self._merge(cascading_ids, self._tool_feature_cascading_ids({x.id for x in tool_features}, cache))
+        return cascading_ids
+
+    def _tool_cascading_ids(self, ids, cache):
+        cascading_ids = {"tool": set(ids)}
+        tool_features = [x for x in dict.values(cache.get("tool_feature", {})) if x.tool_id in ids]
+        self._merge(cascading_ids, self._tool_feature_cascading_ids({x.id for x in tool_features}, cache))
+        return cascading_ids
+
+    def _tool_feature_cascading_ids(self, ids, cache):
+        cascading_ids = {"tool_feature": set(ids)}
+        tool_feature_methods = [
+            x for x in dict.values(cache.get("tool_feature_method", {})) if x.tool_feature_id in ids
+        ]
+        self._merge(cascading_ids, self._tool_feature_method_cascading_ids({x.id for x in tool_feature_methods}, cache))
+        return cascading_ids
+
+    def _tool_feature_method_cascading_ids(self, ids, cache):
+        return {"tool_feature_method": set(ids)}
+
+    def _metadata_cascading_ids(self, ids, cache):
+        cascading_ids = {"metadata": set(ids)}
+        entity_metadata = {
+            "entity_metadata": {x.id for x in dict.values(cache.get("entity_metadata", {})) if x.metadata_id in ids}
+        }
+        self._merge(cascading_ids, entity_metadata)
+        value_metadata = {
+            "parameter_value_metadata": {
+                x.id for x in dict.values(cache.get("parameter_value_metadata", {})) if x.metadata_id in ids
+            }
+        }
+        self._merge(cascading_ids, value_metadata)
+        return cascading_ids
+
+    def _non_referenced_metadata_ids(self, ids, metadata_table_name, cache):
+        metadata_id_counts = self._metadata_usage_counts(cache)
+        cascading_ids = {}
+        metadata = cache.get(metadata_table_name, {})
+        for id_ in ids:
+            metadata_id_counts[metadata[id_].metadata_id] -= 1
+        zero_count_metadata_ids = {id_ for id_, count in metadata_id_counts.items() if count == 0}
+        self._merge(cascading_ids, {"metadata": zero_count_metadata_ids})
+        return cascading_ids
+
+    def _entity_metadata_cascading_ids(self, ids, cache):
+        cascading_ids = {"entity_metadata": set(ids)}
+        cascading_ids.update(self._non_referenced_metadata_ids(ids, "entity_metadata", cache))
+        return cascading_ids
+
+    def _parameter_value_metadata_cascading_ids(self, ids, cache):
+        cascading_ids = {"parameter_value_metadata": set(ids)}
+        cascading_ids.update(self._non_referenced_metadata_ids(ids, "parameter_value_metadata", cache))
+        return cascading_ids
```

### Comparing `spinedb_api-0.30.3/spinedb_api/db_mapping_update_mixin.py` & `spinedb_api-0.30.4/spinedb_api/db_mapping_update_mixin.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,352 +1,352 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""Provides :class:`DatabaseMappingUpdateMixin`.
-
-"""
-from collections import Counter
-from sqlalchemy.exc import DBAPIError
-from sqlalchemy.sql.expression import bindparam
-from .exception import SpineDBAPIError, SpineIntegrityError
-
-
-class DatabaseMappingUpdateMixin:
-    """Provides methods to perform ``UPDATE`` operations over a Spine db."""
-
-    def _add_commit_id(self, *items):
-        for item in items:
-            item["commit_id"] = self._make_commit_id()
-
-    def _update_items(self, tablename, *items):
-        if not items:
-            return set()
-        # Special cases
-        if tablename == "relationship":
-            return self._update_wide_relationships(*items)
-        real_tablename = {
-            "object_class": "entity_class",
-            "relationship_class": "entity_class",
-            "object": "entity",
-            "relationship": "entity",
-        }.get(tablename, tablename)
-        items = self._items_with_type_id(tablename, *items)
-        return self._do_update_items(real_tablename, *items)
-
-    def _do_update_items(self, tablename, *items):
-        if self.committing:
-            self._add_commit_id(*items)
-            table = self._metadata.tables[tablename]
-            upd = table.update()
-            for k in self._get_primary_key(tablename):
-                upd = upd.where(getattr(table.c, k) == bindparam(k))
-            upd = upd.values({key: bindparam(key) for key in table.columns.keys() & items[0].keys()})
-            try:
-                self._checked_execute(upd, [{**item} for item in items])
-            except DBAPIError as e:
-                msg = f"DBAPIError while updating '{tablename}' items: {e.orig.args}"
-                raise SpineDBAPIError(msg)
-        return {x["id"] for x in items}
-
-    def update_items(self, tablename, *items, check=True, strict=False, return_items=False, cache=None):
-        """Updates items.
-
-        Args:
-            tablename (str): Target database table name
-            *items: One or more Python :class:`dict` objects representing the items to be inserted.
-            check (bool): Whether or not to check integrity
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if the insertion of one of the items violates an integrity constraint.
-            return_items (bool): Return full items rather than just ids
-            cache (dict): A dict mapping table names to a list of dictionary items, to use as db replacement
-                for queries
-
-        Returns:
-            set: ids or items successfully updated
-            list(SpineIntegrityError): found violations
-        """
-        if check:
-            checked_items, intgr_error_log = self.check_items(
-                tablename, *items, for_update=True, strict=strict, cache=cache
-            )
-        else:
-            checked_items, intgr_error_log = list(items), []
-        try:
-            updated_ids = self._update_items(tablename, *checked_items)
-        except SpineDBAPIError as e:
-            intgr_error_log.append(f"Fail to update items: {e}")
-        if return_items:
-            return checked_items, intgr_error_log
-        return updated_ids, intgr_error_log
-
-    def update_alternatives(self, *items, **kwargs):
-        return self.update_items("alternative", *items, **kwargs)
-
-    def _update_alternatives(self, *items):
-        return self._update_items("alternative", *items)
-
-    def update_scenarios(self, *items, **kwargs):
-        return self.update_items("scenario", *items, **kwargs)
-
-    def _update_scenarios(self, *items):
-        return self._update_items("scenario", *items)
-
-    def update_scenario_alternatives(self, *items, **kwargs):
-        return self.update_items("scenario_alternative", *items, **kwargs)
-
-    def _update_scenario_alternatives(self, *items):
-        return self._update_items("scenario_alternative", *items)
-
-    def update_object_classes(self, *items, **kwargs):
-        return self.update_items("object_class", *items, **kwargs)
-
-    def _update_object_classes(self, *items):
-        return self._update_items("object_class", *items)
-
-    def update_objects(self, *items, **kwargs):
-        return self.update_items("object", *items, **kwargs)
-
-    def _update_objects(self, *items):
-        return self._update_items("object", *items)
-
-    def update_wide_relationship_classes(self, *items, **kwargs):
-        return self.update_items("relationship_class", *items, **kwargs)
-
-    def _update_wide_relationship_classes(self, *items):
-        return self._update_items("relationship_class", *items)
-
-    def update_wide_relationships(self, *items, **kwargs):
-        return self.update_items("relationship", *items, **kwargs)
-
-    def _update_wide_relationships(self, *items):
-        items = self._items_with_type_id("relationship", *items)
-        entity_items = []
-        relationship_entity_items = []
-        for item in items:
-            entity_id = item["id"]
-            class_id = item["class_id"]
-            ent_item = {
-                "id": entity_id,
-                "class_id": class_id,
-                "name": item["name"],
-                "description": item.get("description"),
-            }
-            object_class_id_list = item["object_class_id_list"]
-            object_id_list = item["object_id_list"]
-            entity_items.append(ent_item)
-            for dimension, (member_class_id, member_id) in enumerate(zip(object_class_id_list, object_id_list)):
-                rel_ent_item = {
-                    "id": None,  # Need to have an "id" field to make _update_items() happy.
-                    "entity_class_id": class_id,
-                    "entity_id": entity_id,
-                    "dimension": dimension,
-                    "member_class_id": member_class_id,
-                    "member_id": member_id,
-                }
-                relationship_entity_items.append(rel_ent_item)
-        entity_ids = self._update_items("entity", *entity_items)
-        self._update_items("relationship_entity", *relationship_entity_items)
-        return entity_ids
-
-    def update_parameter_definitions(self, *items, **kwargs):
-        return self.update_items("parameter_definition", *items, **kwargs)
-
-    def _update_parameter_definitions(self, *items):
-        return self._update_items("parameter_definition", *items)
-
-    def update_parameter_values(self, *items, **kwargs):
-        return self.update_items("parameter_value", *items, **kwargs)
-
-    def _update_parameter_values(self, *items):
-        return self._update_items("parameter_value", *items)
-
-    def update_features(self, *items, **kwargs):
-        return self.update_items("feature", *items, **kwargs)
-
-    def _update_features(self, *items):
-        return self._update_items("feature", *items)
-
-    def update_tools(self, *items, **kwargs):
-        return self.update_items("tool", *items, **kwargs)
-
-    def _update_tools(self, *items):
-        return self._update_items("tool", *items)
-
-    def update_tool_features(self, *items, **kwargs):
-        return self.update_items("tool_feature", *items, **kwargs)
-
-    def _update_tool_features(self, *items):
-        return self._update_items("tool_feature", *items)
-
-    def update_tool_feature_methods(self, *items, **kwargs):
-        return self.update_items("tool_feature_method", *items, **kwargs)
-
-    def _update_tool_feature_methods(self, *items):
-        return self._update_items("tool_feature_method", *items)
-
-    def update_parameter_value_lists(self, *items, **kwargs):
-        return self.update_items("parameter_value_list", *items, **kwargs)
-
-    def _update_parameter_value_lists(self, *items):
-        return self._update_items("parameter_value_list", *items)
-
-    def update_list_values(self, *items, **kwargs):
-        return self.update_items("list_value", *items, **kwargs)
-
-    def _update_list_values(self, *items):
-        return self._update_items("list_value", *items)
-
-    def update_metadata(self, *items, **kwargs):
-        return self.update_items("metadata", *items, **kwargs)
-
-    def _update_metadata(self, *items):
-        return self._update_items("metadata", *items)
-
-    def update_ext_entity_metadata(self, *items, check=True, strict=False, return_items=False, cache=None):
-        updated_items, errors = self._update_ext_item_metadata(
-            "entity_metadata", *items, check=check, strict=strict, cache=cache
-        )
-        if return_items:
-            return updated_items, errors
-        return {i["id"] for i in updated_items}, errors
-
-    def update_ext_parameter_value_metadata(self, *items, check=True, strict=False, return_items=False, cache=None):
-        updated_items, errors = self._update_ext_item_metadata(
-            "parameter_value_metadata", *items, check=check, strict=strict, cache=cache
-        )
-        if return_items:
-            return updated_items, errors
-        return {i["id"] for i in updated_items}, errors
-
-    def _update_ext_item_metadata(self, metadata_table, *items, check=True, strict=False, cache=None):
-        if cache is None:
-            cache = self.make_cache({"entity_metadata", "parameter_value_metadata", "metadata"})
-        metadata_ids = {}
-        for entry in cache.get("metadata", {}).values():
-            metadata_ids.setdefault(entry.name, {})[entry.value] = entry.id
-        item_metadata_cache = cache[metadata_table]
-        metadata_usage_counts = self._metadata_usage_counts(cache)
-        updatable_items = []
-        homeless_items = []
-        for item in items:
-            metadata_name = item["metadata_name"]
-            metadata_value = item["metadata_value"]
-            metadata_id = metadata_ids.get(metadata_name, {}).get(metadata_value)
-            if metadata_id is None:
-                homeless_items.append(item)
-                continue
-            item["metadata_id"] = metadata_id
-            previous_metadata_id = item_metadata_cache[item["id"]]["metadata_id"]
-            metadata_usage_counts[previous_metadata_id] -= 1
-            metadata_usage_counts[metadata_id] += 1
-            updatable_items.append(item)
-        homeless_item_metadata_usage_counts = Counter()
-        for item in homeless_items:
-            homeless_item_metadata_usage_counts[item_metadata_cache[item["id"]].metadata_id] += 1
-        updatable_metadata_items = []
-        future_metadata_ids = {}
-        for metadata_id, count in homeless_item_metadata_usage_counts.items():
-            if count == metadata_usage_counts[metadata_id]:
-                for cached_item in item_metadata_cache.values():
-                    if cached_item["metadata_id"] == metadata_id:
-                        found = False
-                        for item in homeless_items:
-                            if item["id"] == cached_item["id"]:
-                                metadata_name = item["metadata_name"]
-                                metadata_value = item["metadata_value"]
-                                updatable_metadata_items.append(
-                                    {"id": metadata_id, "name": metadata_name, "value": metadata_value}
-                                )
-                                future_metadata_ids.setdefault(metadata_name, {})[metadata_value] = metadata_id
-                                metadata_usage_counts[metadata_id] = 0
-                                found = True
-                                break
-                        if found:
-                            break
-        items_needing_new_metadata = []
-        for item in homeless_items:
-            metadata_name = item["metadata_name"]
-            metadata_value = item["metadata_value"]
-            metadata_id = future_metadata_ids.get(metadata_name, {}).get(metadata_value)
-            if metadata_id is None:
-                items_needing_new_metadata.append(item)
-                continue
-            if item_metadata_cache[item["id"]]["metadata_id"] == metadata_id:
-                continue
-            item["metadata_id"] = metadata_id
-            updatable_items.append(item)
-        all_items = []
-        errors = []
-        if updatable_metadata_items:
-            updated_metadata, errors = self.update_metadata(
-                *updatable_metadata_items, check=False, strict=strict, return_items=True, cache=cache
-            )
-            all_items += updated_metadata
-            if errors:
-                return all_items, errors
-        addable_metadata = [
-            {"name": i["metadata_name"], "value": i["metadata_value"]} for i in items_needing_new_metadata
-        ]
-        added_metadata = []
-        if addable_metadata:
-            added_metadata, metadata_add_errors = self.add_metadata(
-                *addable_metadata, check=False, strict=strict, return_items=True, cache=cache
-            )
-            all_items += added_metadata
-            errors += metadata_add_errors
-            if errors:
-                return all_items, errors
-        added_metadata_ids = {}
-        for item in added_metadata:
-            added_metadata_ids.setdefault(item["name"], {})[item["value"]] = item["id"]
-        for item in items_needing_new_metadata:
-            item["metadata_id"] = added_metadata_ids[item["metadata_name"]][item["metadata_value"]]
-            updatable_items.append(item)
-        if updatable_items:
-            # Force-clear cache before updating item metadata to ensure that added/updated metadata is found.
-            updated_item_metadata, item_metadata_errors = self.update_items(
-                metadata_table, *updatable_items, check=check, strict=strict, return_items=True
-            )
-            all_items += updated_item_metadata
-            errors += item_metadata_errors
-        return all_items, errors
-
-    def get_data_to_set_scenario_alternatives(self, *items, cache=None):
-        """Returns data to add and remove, in order to set wide scenario alternatives.
-
-        Args:
-            *items: One or more wide scenario :class:`dict` objects to set.
-                Each item must include the following keys:
-
-                - "id": integer scenario id
-                - "alternative_id_list": list of alternative ids for that scenario
-
-        Returns
-            list: narrow scenario_alternative :class:`dict` objects to add.
-            set: integer scenario_alternative ids to remove
-        """
-        if cache is None:
-            cache = self.make_cache("scenario_alternative", "scenario")
-        current_alternative_id_lists = {x.id: x.alternative_id_list for x in cache.get("scenario", {}).values()}
-        scenario_alternative_ids = {
-            (x.scenario_id, x.alternative_id): x.id for x in cache.get("scenario_alternative", {}).values()
-        }
-        items_to_add = list()
-        ids_to_remove = set()
-        for item in items:
-            scenario_id = item["id"]
-            alternative_id_list = item["alternative_id_list"]
-            current_alternative_id_list = current_alternative_id_lists[scenario_id]
-            for k, alternative_id in enumerate(alternative_id_list):
-                item_to_add = {"scenario_id": scenario_id, "alternative_id": alternative_id, "rank": k + 1}
-                items_to_add.append(item_to_add)
-            for alternative_id in current_alternative_id_list:
-                ids_to_remove.add(scenario_alternative_ids[scenario_id, alternative_id])
-        return items_to_add, ids_to_remove
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""Provides :class:`DatabaseMappingUpdateMixin`.
+
+"""
+from collections import Counter
+from sqlalchemy.exc import DBAPIError
+from sqlalchemy.sql.expression import bindparam
+from .exception import SpineDBAPIError, SpineIntegrityError
+
+
+class DatabaseMappingUpdateMixin:
+    """Provides methods to perform ``UPDATE`` operations over a Spine db."""
+
+    def _add_commit_id(self, *items):
+        for item in items:
+            item["commit_id"] = self._make_commit_id()
+
+    def _update_items(self, tablename, *items):
+        if not items:
+            return set()
+        # Special cases
+        if tablename == "relationship":
+            return self._update_wide_relationships(*items)
+        real_tablename = {
+            "object_class": "entity_class",
+            "relationship_class": "entity_class",
+            "object": "entity",
+            "relationship": "entity",
+        }.get(tablename, tablename)
+        items = self._items_with_type_id(tablename, *items)
+        return self._do_update_items(real_tablename, *items)
+
+    def _do_update_items(self, tablename, *items):
+        if self.committing:
+            self._add_commit_id(*items)
+            table = self._metadata.tables[tablename]
+            upd = table.update()
+            for k in self._get_primary_key(tablename):
+                upd = upd.where(getattr(table.c, k) == bindparam(k))
+            upd = upd.values({key: bindparam(key) for key in table.columns.keys() & items[0].keys()})
+            try:
+                self._checked_execute(upd, [{**item} for item in items])
+            except DBAPIError as e:
+                msg = f"DBAPIError while updating '{tablename}' items: {e.orig.args}"
+                raise SpineDBAPIError(msg)
+        return {x["id"] for x in items}
+
+    def update_items(self, tablename, *items, check=True, strict=False, return_items=False, cache=None):
+        """Updates items.
+
+        Args:
+            tablename (str): Target database table name
+            *items: One or more Python :class:`dict` objects representing the items to be inserted.
+            check (bool): Whether or not to check integrity
+            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
+                if the insertion of one of the items violates an integrity constraint.
+            return_items (bool): Return full items rather than just ids
+            cache (dict): A dict mapping table names to a list of dictionary items, to use as db replacement
+                for queries
+
+        Returns:
+            set: ids or items successfully updated
+            list(SpineIntegrityError): found violations
+        """
+        if check:
+            checked_items, intgr_error_log = self.check_items(
+                tablename, *items, for_update=True, strict=strict, cache=cache
+            )
+        else:
+            checked_items, intgr_error_log = list(items), []
+        try:
+            updated_ids = self._update_items(tablename, *checked_items)
+        except SpineDBAPIError as e:
+            intgr_error_log.append(f"Fail to update items: {e}")
+        if return_items:
+            return checked_items, intgr_error_log
+        return updated_ids, intgr_error_log
+
+    def update_alternatives(self, *items, **kwargs):
+        return self.update_items("alternative", *items, **kwargs)
+
+    def _update_alternatives(self, *items):
+        return self._update_items("alternative", *items)
+
+    def update_scenarios(self, *items, **kwargs):
+        return self.update_items("scenario", *items, **kwargs)
+
+    def _update_scenarios(self, *items):
+        return self._update_items("scenario", *items)
+
+    def update_scenario_alternatives(self, *items, **kwargs):
+        return self.update_items("scenario_alternative", *items, **kwargs)
+
+    def _update_scenario_alternatives(self, *items):
+        return self._update_items("scenario_alternative", *items)
+
+    def update_object_classes(self, *items, **kwargs):
+        return self.update_items("object_class", *items, **kwargs)
+
+    def _update_object_classes(self, *items):
+        return self._update_items("object_class", *items)
+
+    def update_objects(self, *items, **kwargs):
+        return self.update_items("object", *items, **kwargs)
+
+    def _update_objects(self, *items):
+        return self._update_items("object", *items)
+
+    def update_wide_relationship_classes(self, *items, **kwargs):
+        return self.update_items("relationship_class", *items, **kwargs)
+
+    def _update_wide_relationship_classes(self, *items):
+        return self._update_items("relationship_class", *items)
+
+    def update_wide_relationships(self, *items, **kwargs):
+        return self.update_items("relationship", *items, **kwargs)
+
+    def _update_wide_relationships(self, *items):
+        items = self._items_with_type_id("relationship", *items)
+        entity_items = []
+        relationship_entity_items = []
+        for item in items:
+            entity_id = item["id"]
+            class_id = item["class_id"]
+            ent_item = {
+                "id": entity_id,
+                "class_id": class_id,
+                "name": item["name"],
+                "description": item.get("description"),
+            }
+            object_class_id_list = item["object_class_id_list"]
+            object_id_list = item["object_id_list"]
+            entity_items.append(ent_item)
+            for dimension, (member_class_id, member_id) in enumerate(zip(object_class_id_list, object_id_list)):
+                rel_ent_item = {
+                    "id": None,  # Need to have an "id" field to make _update_items() happy.
+                    "entity_class_id": class_id,
+                    "entity_id": entity_id,
+                    "dimension": dimension,
+                    "member_class_id": member_class_id,
+                    "member_id": member_id,
+                }
+                relationship_entity_items.append(rel_ent_item)
+        entity_ids = self._update_items("entity", *entity_items)
+        self._update_items("relationship_entity", *relationship_entity_items)
+        return entity_ids
+
+    def update_parameter_definitions(self, *items, **kwargs):
+        return self.update_items("parameter_definition", *items, **kwargs)
+
+    def _update_parameter_definitions(self, *items):
+        return self._update_items("parameter_definition", *items)
+
+    def update_parameter_values(self, *items, **kwargs):
+        return self.update_items("parameter_value", *items, **kwargs)
+
+    def _update_parameter_values(self, *items):
+        return self._update_items("parameter_value", *items)
+
+    def update_features(self, *items, **kwargs):
+        return self.update_items("feature", *items, **kwargs)
+
+    def _update_features(self, *items):
+        return self._update_items("feature", *items)
+
+    def update_tools(self, *items, **kwargs):
+        return self.update_items("tool", *items, **kwargs)
+
+    def _update_tools(self, *items):
+        return self._update_items("tool", *items)
+
+    def update_tool_features(self, *items, **kwargs):
+        return self.update_items("tool_feature", *items, **kwargs)
+
+    def _update_tool_features(self, *items):
+        return self._update_items("tool_feature", *items)
+
+    def update_tool_feature_methods(self, *items, **kwargs):
+        return self.update_items("tool_feature_method", *items, **kwargs)
+
+    def _update_tool_feature_methods(self, *items):
+        return self._update_items("tool_feature_method", *items)
+
+    def update_parameter_value_lists(self, *items, **kwargs):
+        return self.update_items("parameter_value_list", *items, **kwargs)
+
+    def _update_parameter_value_lists(self, *items):
+        return self._update_items("parameter_value_list", *items)
+
+    def update_list_values(self, *items, **kwargs):
+        return self.update_items("list_value", *items, **kwargs)
+
+    def _update_list_values(self, *items):
+        return self._update_items("list_value", *items)
+
+    def update_metadata(self, *items, **kwargs):
+        return self.update_items("metadata", *items, **kwargs)
+
+    def _update_metadata(self, *items):
+        return self._update_items("metadata", *items)
+
+    def update_ext_entity_metadata(self, *items, check=True, strict=False, return_items=False, cache=None):
+        updated_items, errors = self._update_ext_item_metadata(
+            "entity_metadata", *items, check=check, strict=strict, cache=cache
+        )
+        if return_items:
+            return updated_items, errors
+        return {i["id"] for i in updated_items}, errors
+
+    def update_ext_parameter_value_metadata(self, *items, check=True, strict=False, return_items=False, cache=None):
+        updated_items, errors = self._update_ext_item_metadata(
+            "parameter_value_metadata", *items, check=check, strict=strict, cache=cache
+        )
+        if return_items:
+            return updated_items, errors
+        return {i["id"] for i in updated_items}, errors
+
+    def _update_ext_item_metadata(self, metadata_table, *items, check=True, strict=False, cache=None):
+        if cache is None:
+            cache = self.make_cache({"entity_metadata", "parameter_value_metadata", "metadata"})
+        metadata_ids = {}
+        for entry in cache.get("metadata", {}).values():
+            metadata_ids.setdefault(entry.name, {})[entry.value] = entry.id
+        item_metadata_cache = cache[metadata_table]
+        metadata_usage_counts = self._metadata_usage_counts(cache)
+        updatable_items = []
+        homeless_items = []
+        for item in items:
+            metadata_name = item["metadata_name"]
+            metadata_value = item["metadata_value"]
+            metadata_id = metadata_ids.get(metadata_name, {}).get(metadata_value)
+            if metadata_id is None:
+                homeless_items.append(item)
+                continue
+            item["metadata_id"] = metadata_id
+            previous_metadata_id = item_metadata_cache[item["id"]]["metadata_id"]
+            metadata_usage_counts[previous_metadata_id] -= 1
+            metadata_usage_counts[metadata_id] += 1
+            updatable_items.append(item)
+        homeless_item_metadata_usage_counts = Counter()
+        for item in homeless_items:
+            homeless_item_metadata_usage_counts[item_metadata_cache[item["id"]].metadata_id] += 1
+        updatable_metadata_items = []
+        future_metadata_ids = {}
+        for metadata_id, count in homeless_item_metadata_usage_counts.items():
+            if count == metadata_usage_counts[metadata_id]:
+                for cached_item in item_metadata_cache.values():
+                    if cached_item["metadata_id"] == metadata_id:
+                        found = False
+                        for item in homeless_items:
+                            if item["id"] == cached_item["id"]:
+                                metadata_name = item["metadata_name"]
+                                metadata_value = item["metadata_value"]
+                                updatable_metadata_items.append(
+                                    {"id": metadata_id, "name": metadata_name, "value": metadata_value}
+                                )
+                                future_metadata_ids.setdefault(metadata_name, {})[metadata_value] = metadata_id
+                                metadata_usage_counts[metadata_id] = 0
+                                found = True
+                                break
+                        if found:
+                            break
+        items_needing_new_metadata = []
+        for item in homeless_items:
+            metadata_name = item["metadata_name"]
+            metadata_value = item["metadata_value"]
+            metadata_id = future_metadata_ids.get(metadata_name, {}).get(metadata_value)
+            if metadata_id is None:
+                items_needing_new_metadata.append(item)
+                continue
+            if item_metadata_cache[item["id"]]["metadata_id"] == metadata_id:
+                continue
+            item["metadata_id"] = metadata_id
+            updatable_items.append(item)
+        all_items = []
+        errors = []
+        if updatable_metadata_items:
+            updated_metadata, errors = self.update_metadata(
+                *updatable_metadata_items, check=False, strict=strict, return_items=True, cache=cache
+            )
+            all_items += updated_metadata
+            if errors:
+                return all_items, errors
+        addable_metadata = [
+            {"name": i["metadata_name"], "value": i["metadata_value"]} for i in items_needing_new_metadata
+        ]
+        added_metadata = []
+        if addable_metadata:
+            added_metadata, metadata_add_errors = self.add_metadata(
+                *addable_metadata, check=False, strict=strict, return_items=True, cache=cache
+            )
+            all_items += added_metadata
+            errors += metadata_add_errors
+            if errors:
+                return all_items, errors
+        added_metadata_ids = {}
+        for item in added_metadata:
+            added_metadata_ids.setdefault(item["name"], {})[item["value"]] = item["id"]
+        for item in items_needing_new_metadata:
+            item["metadata_id"] = added_metadata_ids[item["metadata_name"]][item["metadata_value"]]
+            updatable_items.append(item)
+        if updatable_items:
+            # Force-clear cache before updating item metadata to ensure that added/updated metadata is found.
+            updated_item_metadata, item_metadata_errors = self.update_items(
+                metadata_table, *updatable_items, check=check, strict=strict, return_items=True
+            )
+            all_items += updated_item_metadata
+            errors += item_metadata_errors
+        return all_items, errors
+
+    def get_data_to_set_scenario_alternatives(self, *items, cache=None):
+        """Returns data to add and remove, in order to set wide scenario alternatives.
+
+        Args:
+            *items: One or more wide scenario :class:`dict` objects to set.
+                Each item must include the following keys:
+
+                - "id": integer scenario id
+                - "alternative_id_list": list of alternative ids for that scenario
+
+        Returns
+            list: narrow scenario_alternative :class:`dict` objects to add.
+            set: integer scenario_alternative ids to remove
+        """
+        if cache is None:
+            cache = self.make_cache("scenario_alternative", "scenario")
+        current_alternative_id_lists = {x.id: x.alternative_id_list for x in cache.get("scenario", {}).values()}
+        scenario_alternative_ids = {
+            (x.scenario_id, x.alternative_id): x.id for x in cache.get("scenario_alternative", {}).values()
+        }
+        items_to_add = list()
+        ids_to_remove = set()
+        for item in items:
+            scenario_id = item["id"]
+            alternative_id_list = item["alternative_id_list"]
+            current_alternative_id_list = current_alternative_id_lists[scenario_id]
+            for k, alternative_id in enumerate(alternative_id_list):
+                item_to_add = {"scenario_id": scenario_id, "alternative_id": alternative_id, "rank": k + 1}
+                items_to_add.append(item_to_add)
+            for alternative_id in current_alternative_id_list:
+                ids_to_remove.add(scenario_alternative_ids[scenario_id, alternative_id])
+        return items_to_add, ids_to_remove
```

### Comparing `spinedb_api-0.30.3/spinedb_api/diff_db_mapping.py` & `spinedb_api-0.30.4/spinedb_api/diff_db_mapping.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,178 +1,178 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Provides :class:`DiffDatabaseMapping`.
-
-"""
-
-from sqlalchemy.sql.expression import bindparam
-from sqlalchemy.exc import DBAPIError
-from .db_mapping_query_mixin import DatabaseMappingQueryMixin
-from .db_mapping_check_mixin import DatabaseMappingCheckMixin
-from .db_mapping_add_mixin import DatabaseMappingAddMixin
-from .db_mapping_update_mixin import DatabaseMappingUpdateMixin
-from .db_mapping_remove_mixin import DatabaseMappingRemoveMixin
-from .diff_db_mapping_commit_mixin import DiffDatabaseMappingCommitMixin
-from .diff_db_mapping_base import DiffDatabaseMappingBase
-from .filters.tools import apply_filter_stack, load_filters
-from .exception import SpineDBAPIError
-
-
-class DiffDatabaseMapping(
-    DatabaseMappingQueryMixin,
-    DatabaseMappingCheckMixin,
-    DatabaseMappingAddMixin,
-    DatabaseMappingUpdateMixin,
-    DatabaseMappingRemoveMixin,
-    DiffDatabaseMappingCommitMixin,
-    DiffDatabaseMappingBase,
-):
-    """A read-write database mapping.
-
-    Provides methods to *stage* any number of changes (namely, ``INSERT``, ``UPDATE`` and ``REMOVE`` operations)
-    over a Spine database, as well as to commit or rollback the batch of changes.
-
-    For convenience, querying this mapping return results *as if* all the staged changes were already committed.
-
-    :param str db_url: A database URL in RFC-1738 format pointing to the database to be mapped.
-    :param str username: A user name. If ``None``, it gets replaced by the string ``"anon"``.
-    :param bool upgrade: Whether or not the db at the given URL should be upgraded to the most recent version.
-    """
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._init_type_attributes()
-        if self._filter_configs is not None:
-            stack = load_filters(self._filter_configs)
-            apply_filter_stack(self, stack)
-
-    def _add_items(self, tablename, *items):
-        self._add_commit_id_and_ids(tablename, *items)
-        ids = {x["id"] for x in items}
-        for tablename_ in self._do_add_items(tablename, *items):
-            self.added_item_id[tablename_].update(ids)
-            self._clear_subqueries(tablename_)
-        return ids
-
-    def _readd_items(self, tablename, *items):
-        ids = set(x["id"] for x in items)
-        for tablename_ in self._do_add_items(tablename, *items):
-            self.added_item_id[tablename_].update(ids)
-            self._clear_subqueries(tablename_)
-
-    def _get_table_for_insert(self, tablename):
-        return self._diff_table(tablename)
-
-    def _get_items_for_update_and_insert(self, tablename, checked_items):
-        """Return lists of items for update and insert.
-        Items in the diff table should be updated, whereas items in the original table
-        should be marked as dirty and inserted into the corresponding diff table."""
-        items_for_update = list()
-        items_for_insert = list()
-        dirty_ids = set()
-        updated_ids = set()
-        id_field = self.table_ids.get(tablename, "id")
-        for item in checked_items:
-            id_ = item[id_field]
-            updated_ids.add(id_)
-            if id_ in self.added_item_id[tablename] | self.updated_item_id[tablename]:
-                items_for_update.append(item)
-            else:
-                items_for_insert.append(item)
-                dirty_ids.add(id_)
-        return items_for_update, items_for_insert, dirty_ids, updated_ids
-
-    def _do_update_items(self, tablename, *items):
-        items_for_update, items_for_insert, dirty_ids, updated_ids = self._get_items_for_update_and_insert(
-            tablename, items
-        )
-        if self.committing:
-            try:
-                self._update_and_insert_items(tablename, items_for_update, items_for_insert)
-                self._mark_as_dirty(tablename, dirty_ids)
-                self.updated_item_id[tablename].update(dirty_ids)
-            except DBAPIError as e:
-                msg = f"DBAPIError while updating {tablename} items: {e.orig.args}"
-                raise SpineDBAPIError(msg)
-        return updated_ids
-
-    def _update_and_insert_items(self, tablename, items_for_update, items_for_insert):
-        diff_table = self._diff_table(tablename)
-        if items_for_update:
-            upd = diff_table.update()
-            for k in self._get_primary_key(tablename):
-                upd = upd.where(getattr(diff_table.c, k) == bindparam(k))
-            upd = upd.values({key: bindparam(key) for key in diff_table.columns.keys() & items_for_update[0].keys()})
-            self._checked_execute(upd, [{**item} for item in items_for_update])
-        ins = diff_table.insert()
-        self._checked_execute(ins, [{**item} for item in items_for_insert])
-
-    def _update_wide_relationships(self, *items):
-        """Update relationships without checking integrity."""
-        items = self._items_with_type_id("relationship", *items)
-        ent_items = []
-        rel_ent_items = []
-        for item in items:
-            ent_item = item.copy()
-            object_class_id_list = ent_item.pop("object_class_id_list", [])
-            object_id_list = ent_item.pop("object_id_list", [])
-            ent_items.append(ent_item)
-            for dimension, (member_class_id, member_id) in enumerate(zip(object_class_id_list, object_id_list)):
-                rel_ent_item = ent_item.copy()
-                rel_ent_item["entity_class_id"] = rel_ent_item.pop("class_id", None)
-                rel_ent_item["entity_id"] = rel_ent_item.pop("id", None)
-                rel_ent_item["dimension"] = dimension
-                rel_ent_item["member_class_id"] = member_class_id
-                rel_ent_item["member_id"] = member_id
-                rel_ent_items.append(rel_ent_item)
-        try:
-            ents_for_update, ents_for_insert, dirty_ent_ids, updated_ent_ids = self._get_items_for_update_and_insert(
-                "entity", ent_items
-            )
-            (
-                rel_ents_for_update,
-                rel_ents_for_insert,
-                dirty_rel_ent_ids,
-                updated_rel_ent_ids,
-            ) = self._get_items_for_update_and_insert("relationship_entity", rel_ent_items)
-            self._update_and_insert_items("entity", ents_for_update, ents_for_insert)
-            self._mark_as_dirty("entity", dirty_ent_ids)
-            self.updated_item_id["entity"].update(dirty_ent_ids)
-            self._update_and_insert_items("relationship_entity", rel_ents_for_update, rel_ents_for_insert)
-            self._mark_as_dirty("relationship_entity", dirty_rel_ent_ids)
-            self.updated_item_id["relationship_entity"].update(dirty_rel_ent_ids)
-            return updated_ent_ids.union(updated_rel_ent_ids)
-        except DBAPIError as e:
-            msg = "DBAPIError while updating relationships: {}".format(e.orig.args)
-            raise SpineDBAPIError(msg)
-
-    def remove_items(self, **kwargs):
-        """Removes items by id, *not in cascade*.
-
-        Args:
-            **kwargs: keyword is table name, argument is list of ids to remove
-        """
-        if self.committing:
-            for tablename, ids in kwargs.items():
-                table_id = self.table_ids.get(tablename, "id")
-                diff_table = self._diff_table(tablename)
-                delete = diff_table.delete().where(self.in_(getattr(diff_table.c, table_id), ids))
-                try:
-                    self.connection.execute(delete)
-                except DBAPIError as e:
-                    msg = f"DBAPIError while removing {tablename} items: {e.orig.args}"
-                    raise SpineDBAPIError(msg)
-        for tablename, ids in kwargs.items():
-            self.added_item_id[tablename].difference_update(ids)
-            self.updated_item_id[tablename].difference_update(ids)
-            self.removed_item_id[tablename].update(ids)
-            self._mark_as_dirty(tablename, ids)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Provides :class:`DiffDatabaseMapping`.
+
+"""
+
+from sqlalchemy.sql.expression import bindparam
+from sqlalchemy.exc import DBAPIError
+from .db_mapping_query_mixin import DatabaseMappingQueryMixin
+from .db_mapping_check_mixin import DatabaseMappingCheckMixin
+from .db_mapping_add_mixin import DatabaseMappingAddMixin
+from .db_mapping_update_mixin import DatabaseMappingUpdateMixin
+from .db_mapping_remove_mixin import DatabaseMappingRemoveMixin
+from .diff_db_mapping_commit_mixin import DiffDatabaseMappingCommitMixin
+from .diff_db_mapping_base import DiffDatabaseMappingBase
+from .filters.tools import apply_filter_stack, load_filters
+from .exception import SpineDBAPIError
+
+
+class DiffDatabaseMapping(
+    DatabaseMappingQueryMixin,
+    DatabaseMappingCheckMixin,
+    DatabaseMappingAddMixin,
+    DatabaseMappingUpdateMixin,
+    DatabaseMappingRemoveMixin,
+    DiffDatabaseMappingCommitMixin,
+    DiffDatabaseMappingBase,
+):
+    """A read-write database mapping.
+
+    Provides methods to *stage* any number of changes (namely, ``INSERT``, ``UPDATE`` and ``REMOVE`` operations)
+    over a Spine database, as well as to commit or rollback the batch of changes.
+
+    For convenience, querying this mapping return results *as if* all the staged changes were already committed.
+
+    :param str db_url: A database URL in RFC-1738 format pointing to the database to be mapped.
+    :param str username: A user name. If ``None``, it gets replaced by the string ``"anon"``.
+    :param bool upgrade: Whether or not the db at the given URL should be upgraded to the most recent version.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._init_type_attributes()
+        if self._filter_configs is not None:
+            stack = load_filters(self._filter_configs)
+            apply_filter_stack(self, stack)
+
+    def _add_items(self, tablename, *items):
+        self._add_commit_id_and_ids(tablename, *items)
+        ids = {x["id"] for x in items}
+        for tablename_ in self._do_add_items(tablename, *items):
+            self.added_item_id[tablename_].update(ids)
+            self._clear_subqueries(tablename_)
+        return ids
+
+    def _readd_items(self, tablename, *items):
+        ids = set(x["id"] for x in items)
+        for tablename_ in self._do_add_items(tablename, *items):
+            self.added_item_id[tablename_].update(ids)
+            self._clear_subqueries(tablename_)
+
+    def _get_table_for_insert(self, tablename):
+        return self._diff_table(tablename)
+
+    def _get_items_for_update_and_insert(self, tablename, checked_items):
+        """Return lists of items for update and insert.
+        Items in the diff table should be updated, whereas items in the original table
+        should be marked as dirty and inserted into the corresponding diff table."""
+        items_for_update = list()
+        items_for_insert = list()
+        dirty_ids = set()
+        updated_ids = set()
+        id_field = self.table_ids.get(tablename, "id")
+        for item in checked_items:
+            id_ = item[id_field]
+            updated_ids.add(id_)
+            if id_ in self.added_item_id[tablename] | self.updated_item_id[tablename]:
+                items_for_update.append(item)
+            else:
+                items_for_insert.append(item)
+                dirty_ids.add(id_)
+        return items_for_update, items_for_insert, dirty_ids, updated_ids
+
+    def _do_update_items(self, tablename, *items):
+        items_for_update, items_for_insert, dirty_ids, updated_ids = self._get_items_for_update_and_insert(
+            tablename, items
+        )
+        if self.committing:
+            try:
+                self._update_and_insert_items(tablename, items_for_update, items_for_insert)
+                self.updated_item_id[tablename].update(dirty_ids)
+                self._mark_as_dirty(tablename, dirty_ids)
+            except DBAPIError as e:
+                msg = f"DBAPIError while updating {tablename} items: {e.orig.args}"
+                raise SpineDBAPIError(msg)
+        return updated_ids
+
+    def _update_and_insert_items(self, tablename, items_for_update, items_for_insert):
+        diff_table = self._diff_table(tablename)
+        if items_for_update:
+            upd = diff_table.update()
+            for k in self._get_primary_key(tablename):
+                upd = upd.where(getattr(diff_table.c, k) == bindparam(k))
+            upd = upd.values({key: bindparam(key) for key in diff_table.columns.keys() & items_for_update[0].keys()})
+            self._checked_execute(upd, [{**item} for item in items_for_update])
+        ins = diff_table.insert()
+        self._checked_execute(ins, [{**item} for item in items_for_insert])
+
+    def _update_wide_relationships(self, *items):
+        """Update relationships without checking integrity."""
+        items = self._items_with_type_id("relationship", *items)
+        ent_items = []
+        rel_ent_items = []
+        for item in items:
+            ent_item = item.copy()
+            object_class_id_list = ent_item.pop("object_class_id_list", [])
+            object_id_list = ent_item.pop("object_id_list", [])
+            ent_items.append(ent_item)
+            for dimension, (member_class_id, member_id) in enumerate(zip(object_class_id_list, object_id_list)):
+                rel_ent_item = ent_item.copy()
+                rel_ent_item["entity_class_id"] = rel_ent_item.pop("class_id", None)
+                rel_ent_item["entity_id"] = rel_ent_item.pop("id", None)
+                rel_ent_item["dimension"] = dimension
+                rel_ent_item["member_class_id"] = member_class_id
+                rel_ent_item["member_id"] = member_id
+                rel_ent_items.append(rel_ent_item)
+        try:
+            ents_for_update, ents_for_insert, dirty_ent_ids, updated_ent_ids = self._get_items_for_update_and_insert(
+                "entity", ent_items
+            )
+            (
+                rel_ents_for_update,
+                rel_ents_for_insert,
+                dirty_rel_ent_ids,
+                updated_rel_ent_ids,
+            ) = self._get_items_for_update_and_insert("relationship_entity", rel_ent_items)
+            self._update_and_insert_items("entity", ents_for_update, ents_for_insert)
+            self._mark_as_dirty("entity", dirty_ent_ids)
+            self.updated_item_id["entity"].update(dirty_ent_ids)
+            self._update_and_insert_items("relationship_entity", rel_ents_for_update, rel_ents_for_insert)
+            self._mark_as_dirty("relationship_entity", dirty_rel_ent_ids)
+            self.updated_item_id["relationship_entity"].update(dirty_rel_ent_ids)
+            return updated_ent_ids.union(updated_rel_ent_ids)
+        except DBAPIError as e:
+            msg = "DBAPIError while updating relationships: {}".format(e.orig.args)
+            raise SpineDBAPIError(msg)
+
+    def remove_items(self, **kwargs):
+        """Removes items by id, *not in cascade*.
+
+        Args:
+            **kwargs: keyword is table name, argument is list of ids to remove
+        """
+        if self.committing:
+            for tablename, ids in kwargs.items():
+                table_id = self.table_ids.get(tablename, "id")
+                diff_table = self._diff_table(tablename)
+                delete = diff_table.delete().where(self.in_(getattr(diff_table.c, table_id), ids))
+                try:
+                    self.connection.execute(delete)
+                except DBAPIError as e:
+                    msg = f"DBAPIError while removing {tablename} items: {e.orig.args}"
+                    raise SpineDBAPIError(msg)
+        for tablename, ids in kwargs.items():
+            self.added_item_id[tablename].difference_update(ids)
+            self.updated_item_id[tablename].difference_update(ids)
+            self.removed_item_id[tablename].update(ids)
+            self._mark_as_dirty(tablename, ids)
```

### Comparing `spinedb_api-0.30.3/spinedb_api/exception.py` & `spinedb_api-0.30.4/spinedb_api/exception.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,105 +1,105 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Classes to handle exceptions while using the Spine database API.
-
-"""
-
-
-class SpineDBAPIError(Exception):
-    """Basic exception for errors raised by the API."""
-
-    def __init__(self, msg=None):
-        super().__init__(msg)
-        self.msg = msg
-
-    def __str__(self):
-        return self.msg
-
-
-class SpineIntegrityError(SpineDBAPIError):
-    """Database integrity error while inserting/updating records.
-
-    Attributes:
-        msg (str): the message to be displayed
-        id (int): the id the instance that caused a unique violation
-    """
-
-    def __init__(self, msg=None, id=None):
-        super().__init__(msg)
-        self.id = id
-
-
-class SpineDBVersionError(SpineDBAPIError):
-    """Database version error."""
-
-    def __init__(self, url=None, current=None, expected=None, upgrade_available=True):
-        super().__init__(msg="The database at '{}' is not the expected version.".format(url))
-        self.url = url
-        self.current = current
-        self.expected = expected
-        self.upgrade_available = upgrade_available
-
-
-class SpineTableNotFoundError(SpineDBAPIError):
-    """Can't find one of the tables."""
-
-    def __init__(self, table, url=None):
-        super().__init__(msg="Table(s) '{}' couldn't be mapped from the database at '{}'.".format(table, url))
-        self.table = table
-
-
-class RecordNotFoundError(SpineDBAPIError):
-    """Can't find one record in one of the tables."""
-
-    def __init__(self, table, name=None, id=None):
-        super().__init__(msg="Unable to find item in table '{}'.".format(table))
-        self.table = table
-        self.name = name
-        self.id = id
-
-
-class ParameterValueError(SpineDBAPIError):
-    """The value given for a parameter does not fit the datatype."""
-
-    def __init__(self, value, data_type):
-        super().__init__(msg="The value {} does not fit the datatype '{}'.".format(value, data_type))
-        self.value = value
-        self.data_type = data_type
-
-
-class ParameterValueFormatError(SpineDBAPIError):
-    """
-    Failure in encoding/decoding a parameter value.
-
-    Attributes:
-        msg (str): an error message
-    """
-
-    def __init__(self, msg):
-        super().__init__(msg)
-
-
-class InvalidMapping(SpineDBAPIError):
-    """
-    Failure in import/export mapping
-    """
-
-    def __init__(self, msg):
-        super().__init__(msg)
-
-
-class InvalidMappingComponent(InvalidMapping):
-    def __init__(self, msg, rank=None, key=None):
-        super().__init__(msg)
-        self.rank = rank
-        self.key = key
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Classes to handle exceptions while using the Spine database API.
+
+"""
+
+
+class SpineDBAPIError(Exception):
+    """Basic exception for errors raised by the API."""
+
+    def __init__(self, msg=None):
+        super().__init__(msg)
+        self.msg = msg
+
+    def __str__(self):
+        return self.msg
+
+
+class SpineIntegrityError(SpineDBAPIError):
+    """Database integrity error while inserting/updating records.
+
+    Attributes:
+        msg (str): the message to be displayed
+        id (int): the id the instance that caused a unique violation
+    """
+
+    def __init__(self, msg=None, id=None):
+        super().__init__(msg)
+        self.id = id
+
+
+class SpineDBVersionError(SpineDBAPIError):
+    """Database version error."""
+
+    def __init__(self, url=None, current=None, expected=None, upgrade_available=True):
+        super().__init__(msg="The database at '{}' is not the expected version.".format(url))
+        self.url = url
+        self.current = current
+        self.expected = expected
+        self.upgrade_available = upgrade_available
+
+
+class SpineTableNotFoundError(SpineDBAPIError):
+    """Can't find one of the tables."""
+
+    def __init__(self, table, url=None):
+        super().__init__(msg="Table(s) '{}' couldn't be mapped from the database at '{}'.".format(table, url))
+        self.table = table
+
+
+class RecordNotFoundError(SpineDBAPIError):
+    """Can't find one record in one of the tables."""
+
+    def __init__(self, table, name=None, id=None):
+        super().__init__(msg="Unable to find item in table '{}'.".format(table))
+        self.table = table
+        self.name = name
+        self.id = id
+
+
+class ParameterValueError(SpineDBAPIError):
+    """The value given for a parameter does not fit the datatype."""
+
+    def __init__(self, value, data_type):
+        super().__init__(msg="The value {} does not fit the datatype '{}'.".format(value, data_type))
+        self.value = value
+        self.data_type = data_type
+
+
+class ParameterValueFormatError(SpineDBAPIError):
+    """
+    Failure in encoding/decoding a parameter value.
+
+    Attributes:
+        msg (str): an error message
+    """
+
+    def __init__(self, msg):
+        super().__init__(msg)
+
+
+class InvalidMapping(SpineDBAPIError):
+    """
+    Failure in import/export mapping
+    """
+
+    def __init__(self, msg):
+        super().__init__(msg)
+
+
+class InvalidMappingComponent(InvalidMapping):
+    def __init__(self, msg, rank=None, key=None):
+        super().__init__(msg)
+        self.rank = rank
+        self.key = key
```

### Comparing `spinedb_api-0.30.3/spinedb_api/export_functions.py` & `spinedb_api-0.30.4/spinedb_api/export_functions.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,313 +1,313 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Functions for exporting data from a Spine database using entity names as references.
-
-"""
-from operator import itemgetter
-
-from sqlalchemy.util import KeyedTuple
-from .parameter_value import from_database
-from .helpers import Asterisk
-
-
-def export_data(
-    db_map,
-    object_class_ids=Asterisk,
-    relationship_class_ids=Asterisk,
-    parameter_value_list_ids=Asterisk,
-    object_parameter_ids=Asterisk,
-    relationship_parameter_ids=Asterisk,
-    object_ids=Asterisk,
-    object_group_ids=Asterisk,
-    relationship_ids=Asterisk,
-    object_parameter_value_ids=Asterisk,
-    relationship_parameter_value_ids=Asterisk,
-    alternative_ids=Asterisk,
-    scenario_ids=Asterisk,
-    scenario_alternative_ids=Asterisk,
-    tool_ids=Asterisk,
-    feature_ids=Asterisk,
-    tool_feature_ids=Asterisk,
-    tool_feature_method_ids=Asterisk,
-    make_cache=None,
-    parse_value=from_database,
-):
-    """
-    Exports data from given database into a dictionary that can be splatted into keyword arguments for ``import_data``.
-
-    Args:
-        db_map (DiffDatabaseMapping): The db to pull stuff from.
-        object_class_ids (Iterable, optional): A collection of ids to pick from the database table
-        relationship_class_ids (Iterable, optional): A collection of ids to pick from the database table
-        parameter_value_list_ids (Iterable, optional): A collection of ids to pick from the database table
-        object_parameter_ids (Iterable, optional): A collection of ids to pick from the database table
-        relationship_parameter_ids (Iterable, optional): A collection of ids to pick from the database table
-        object_ids (Iterable, optional): A collection of ids to pick from the database table
-        relationship_ids (Iterable, optional): A collection of ids to pick from the database table
-        object_parameter_value_ids (Iterable, optional): A collection of ids to pick from the database table
-        relationship_parameter_value_ids (Iterable, optional): A collection of ids to pick from the database table
-        alternative_ids (Iterable, optional): A collection of ids to pick from the database table
-        scenario_ids (Iterable, optional): A collection of ids to pick from the database table
-        scenario_alternative_ids (Iterable, optional): A collection of ids to pick from the database table
-        tool_ids (Iterable, optional): A collection of ids to pick from the database table
-        feature_ids (Iterable, optional): A collection of ids to pick from the database table
-        tool_feature_ids (Iterable, optional): A collection of ids to pick from the database table
-        tool_feature_method_ids (Iterable, optional): A collection of ids to pick from the database table
-
-    Returns:
-        dict: exported data
-    """
-    data = {
-        "object_classes": export_object_classes(db_map, object_class_ids, make_cache=make_cache),
-        "relationship_classes": export_relationship_classes(db_map, relationship_class_ids, make_cache=make_cache),
-        "parameter_value_lists": export_parameter_value_lists(
-            db_map, parameter_value_list_ids, make_cache=make_cache, parse_value=parse_value
-        ),
-        "object_parameters": export_object_parameters(
-            db_map, object_parameter_ids, make_cache=make_cache, parse_value=parse_value
-        ),
-        "relationship_parameters": export_relationship_parameters(
-            db_map, relationship_parameter_ids, make_cache=make_cache, parse_value=parse_value
-        ),
-        "objects": export_objects(db_map, object_ids, make_cache=make_cache),
-        "relationships": export_relationships(db_map, relationship_ids, make_cache=make_cache),
-        "object_groups": export_object_groups(db_map, object_group_ids, make_cache=make_cache),
-        "object_parameter_values": export_object_parameter_values(
-            db_map, object_parameter_value_ids, make_cache=make_cache, parse_value=parse_value
-        ),
-        "relationship_parameter_values": export_relationship_parameter_values(
-            db_map, relationship_parameter_value_ids, make_cache=make_cache, parse_value=parse_value
-        ),
-        "alternatives": export_alternatives(db_map, alternative_ids, make_cache=make_cache),
-        "scenarios": export_scenarios(db_map, scenario_ids, make_cache=make_cache),
-        "scenario_alternatives": export_scenario_alternatives(db_map, scenario_alternative_ids, make_cache=make_cache),
-        "tools": export_tools(db_map, tool_ids, make_cache=make_cache),
-        "features": export_features(db_map, feature_ids, make_cache=make_cache),
-        "tool_features": export_tool_features(db_map, tool_feature_ids, make_cache=make_cache),
-        "tool_feature_methods": export_tool_feature_methods(
-            db_map, tool_feature_method_ids, make_cache=make_cache, parse_value=parse_value
-        ),
-    }
-    return {key: value for key, value in data.items() if value}
-
-
-def _get_items(db_map, tablename, ids, make_cache):
-    if not ids:
-        return ()
-    if make_cache is None:
-        make_cache = db_map.make_cache
-    cache = make_cache({tablename}, include_ancestors=True)
-    _process_item = _make_item_processor(tablename, make_cache)
-    for item in _get_items_from_cache(cache, tablename, ids):
-        yield from _process_item(item)
-
-
-def _get_items_from_cache(cache, tablename, ids):
-    items = cache.get(tablename, {})
-    if ids is Asterisk:
-        yield from items.values()
-        return
-    for id_ in ids:
-        item = items[id_]
-        if item.is_valid():
-            yield item
-
-
-def _make_item_processor(tablename, make_cache):
-    if tablename == "parameter_value_list":
-        return _ParameterValueListProcessor(make_cache)
-    return lambda item: (item,)
-
-
-class _ParameterValueListProcessor:
-    def __init__(self, make_cache):
-        self._cache = make_cache({"list_value"})
-
-    def __call__(self, item):
-        fields = ["name", "value", "type"]
-        if item.value_id_list is None:
-            yield KeyedTuple([item.name, None, None], fields)
-            return
-        for value_id in item.value_id_list:
-            val = self._cache["list_value"][value_id]
-            yield KeyedTuple([item.name, val.value, val.type], fields)
-
-
-def export_object_classes(db_map, ids=Asterisk, make_cache=None):
-    return sorted((x.name, x.description, x.display_icon) for x in _get_items(db_map, "object_class", ids, make_cache))
-
-
-def export_objects(db_map, ids=Asterisk, make_cache=None):
-    return sorted((x.class_name, x.name, x.description) for x in _get_items(db_map, "object", ids, make_cache))
-
-
-def export_relationship_classes(db_map, ids=Asterisk, make_cache=None):
-    return sorted(
-        (x.name, x.object_class_name_list, x.description, x.display_icon)
-        for x in _get_items(db_map, "relationship_class", ids, make_cache)
-    )
-
-
-def export_parameter_value_lists(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
-    return sorted(
-        ((x.name, parse_value(x.value, x.type)) for x in _get_items(db_map, "parameter_value_list", ids, make_cache)),
-        key=itemgetter(0),
-    )
-
-
-def export_object_parameters(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
-    return sorted(
-        (
-            x.object_class_name,
-            x.parameter_name,
-            parse_value(x.default_value, x.default_type),
-            x.value_list_name,
-            x.description,
-        )
-        for x in _get_items(db_map, "parameter_definition", ids, make_cache)
-        if x.object_class_id
-    )
-
-
-def export_relationship_parameters(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
-    return sorted(
-        (
-            x.relationship_class_name,
-            x.parameter_name,
-            parse_value(x.default_value, x.default_type),
-            x.value_list_name,
-            x.description,
-        )
-        for x in _get_items(db_map, "parameter_definition", ids, make_cache)
-        if x.relationship_class_id
-    )
-
-
-def export_relationships(db_map, ids=Asterisk, make_cache=None):
-    return sorted((x.class_name, x.object_name_list) for x in _get_items(db_map, "relationship", ids, make_cache))
-
-
-def export_object_groups(db_map, ids=Asterisk, make_cache=None):
-    return sorted(
-        (x.class_name, x.group_name, x.member_name)
-        for x in _get_items(db_map, "entity_group", ids, make_cache)
-        if x.object_class_id
-    )
-
-
-def export_object_parameter_values(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
-    return sorted(
-        (
-            (x.object_class_name, x.object_name, x.parameter_name, parse_value(x.value, x.type), x.alternative_name)
-            for x in _get_items(db_map, "parameter_value", ids, make_cache)
-            if x.object_id
-        ),
-        key=lambda x: x[:3] + (x[-1],),
-    )
-
-
-def export_relationship_parameter_values(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
-    return sorted(
-        (
-            (
-                x.relationship_class_name,
-                x.object_name_list,
-                x.parameter_name,
-                parse_value(x.value, x.type),
-                x.alternative_name,
-            )
-            for x in _get_items(db_map, "parameter_value", ids, make_cache)
-            if x.relationship_id
-        ),
-        key=lambda x: x[:3] + (x[-1],),
-    )
-
-
-def export_alternatives(db_map, ids=Asterisk, make_cache=None):
-    """
-    Exports alternatives from database.
-
-    The format is what :func:`import_alternatives` accepts as its input.
-
-    Args:
-        db_map (spinedb_api.DatabaseMapping or spinedb_api.DiffDatabaseMapping): a database map
-        ids (Iterable, optional): ids of the alternatives to export
-
-    Returns:
-        Iterable: tuples of two elements: name of alternative and description
-    """
-    return sorted((x.name, x.description) for x in _get_items(db_map, "alternative", ids, make_cache))
-
-
-def export_scenarios(db_map, ids=Asterisk, make_cache=None):
-    """
-    Exports scenarios from database.
-
-    The format is what :func:`import_scenarios` accepts as its input.
-
-    Args:
-        db_map (spinedb_api.DatabaseMapping or spinedb_api.DiffDatabaseMapping): a database map
-        ids (Iterable, optional): ids of the scenarios to export
-
-    Returns:
-        Iterable: tuples of two elements: name of scenario and description
-    """
-    return sorted((x.name, x.active, x.description) for x in _get_items(db_map, "scenario", ids, make_cache))
-
-
-def export_scenario_alternatives(db_map, ids=Asterisk, make_cache=None):
-    """
-    Exports scenario alternatives from database.
-
-    The format is what :func:`import_scenario_alternatives` accepts as its input.
-
-    Args:
-        db_map (spinedb_api.DatabaseMapping or spinedb_api.DiffDatabaseMapping): a database map
-        ids (Iterable, optional): ids of the scenario alternatives to export
-
-    Returns:
-        Iterable: tuples of three elements: name of scenario, tuple containing one alternative name,
-            and name of next alternative
-    """
-    return sorted(
-        (
-            (x.scenario_name, x.alternative_name, x.before_alternative_name)
-            for x in _get_items(db_map, "scenario_alternative", ids, make_cache)
-        ),
-        key=itemgetter(0),
-    )
-
-
-def export_tools(db_map, ids=Asterisk, make_cache=None):
-    return sorted((x.name, x.description) for x in _get_items(db_map, "tool", ids, make_cache))
-
-
-def export_features(db_map, ids=Asterisk, make_cache=None):
-    return sorted(
-        (x.entity_class_name, x.parameter_definition_name, x.parameter_value_list_name, x.description)
-        for x in _get_items(db_map, "feature", ids, make_cache)
-    )
-
-
-def export_tool_features(db_map, ids=Asterisk, make_cache=None):
-    return sorted(
-        (x.tool_name, x.entity_class_name, x.parameter_definition_name, x.required)
-        for x in _get_items(db_map, "tool_feature", ids, make_cache)
-    )
-
-
-def export_tool_feature_methods(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
-    return sorted(
-        (x.tool_name, x.entity_class_name, x.parameter_definition_name, parse_value(x.method, None))
-        for x in _get_items(db_map, "tool_feature_method", ids, make_cache)
-    )
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Functions for exporting data from a Spine database using entity names as references.
+
+"""
+from operator import itemgetter
+
+from sqlalchemy.util import KeyedTuple
+from .parameter_value import from_database
+from .helpers import Asterisk
+
+
+def export_data(
+    db_map,
+    object_class_ids=Asterisk,
+    relationship_class_ids=Asterisk,
+    parameter_value_list_ids=Asterisk,
+    object_parameter_ids=Asterisk,
+    relationship_parameter_ids=Asterisk,
+    object_ids=Asterisk,
+    object_group_ids=Asterisk,
+    relationship_ids=Asterisk,
+    object_parameter_value_ids=Asterisk,
+    relationship_parameter_value_ids=Asterisk,
+    alternative_ids=Asterisk,
+    scenario_ids=Asterisk,
+    scenario_alternative_ids=Asterisk,
+    tool_ids=Asterisk,
+    feature_ids=Asterisk,
+    tool_feature_ids=Asterisk,
+    tool_feature_method_ids=Asterisk,
+    make_cache=None,
+    parse_value=from_database,
+):
+    """
+    Exports data from given database into a dictionary that can be splatted into keyword arguments for ``import_data``.
+
+    Args:
+        db_map (DiffDatabaseMapping): The db to pull stuff from.
+        object_class_ids (Iterable, optional): A collection of ids to pick from the database table
+        relationship_class_ids (Iterable, optional): A collection of ids to pick from the database table
+        parameter_value_list_ids (Iterable, optional): A collection of ids to pick from the database table
+        object_parameter_ids (Iterable, optional): A collection of ids to pick from the database table
+        relationship_parameter_ids (Iterable, optional): A collection of ids to pick from the database table
+        object_ids (Iterable, optional): A collection of ids to pick from the database table
+        relationship_ids (Iterable, optional): A collection of ids to pick from the database table
+        object_parameter_value_ids (Iterable, optional): A collection of ids to pick from the database table
+        relationship_parameter_value_ids (Iterable, optional): A collection of ids to pick from the database table
+        alternative_ids (Iterable, optional): A collection of ids to pick from the database table
+        scenario_ids (Iterable, optional): A collection of ids to pick from the database table
+        scenario_alternative_ids (Iterable, optional): A collection of ids to pick from the database table
+        tool_ids (Iterable, optional): A collection of ids to pick from the database table
+        feature_ids (Iterable, optional): A collection of ids to pick from the database table
+        tool_feature_ids (Iterable, optional): A collection of ids to pick from the database table
+        tool_feature_method_ids (Iterable, optional): A collection of ids to pick from the database table
+
+    Returns:
+        dict: exported data
+    """
+    data = {
+        "object_classes": export_object_classes(db_map, object_class_ids, make_cache=make_cache),
+        "relationship_classes": export_relationship_classes(db_map, relationship_class_ids, make_cache=make_cache),
+        "parameter_value_lists": export_parameter_value_lists(
+            db_map, parameter_value_list_ids, make_cache=make_cache, parse_value=parse_value
+        ),
+        "object_parameters": export_object_parameters(
+            db_map, object_parameter_ids, make_cache=make_cache, parse_value=parse_value
+        ),
+        "relationship_parameters": export_relationship_parameters(
+            db_map, relationship_parameter_ids, make_cache=make_cache, parse_value=parse_value
+        ),
+        "objects": export_objects(db_map, object_ids, make_cache=make_cache),
+        "relationships": export_relationships(db_map, relationship_ids, make_cache=make_cache),
+        "object_groups": export_object_groups(db_map, object_group_ids, make_cache=make_cache),
+        "object_parameter_values": export_object_parameter_values(
+            db_map, object_parameter_value_ids, make_cache=make_cache, parse_value=parse_value
+        ),
+        "relationship_parameter_values": export_relationship_parameter_values(
+            db_map, relationship_parameter_value_ids, make_cache=make_cache, parse_value=parse_value
+        ),
+        "alternatives": export_alternatives(db_map, alternative_ids, make_cache=make_cache),
+        "scenarios": export_scenarios(db_map, scenario_ids, make_cache=make_cache),
+        "scenario_alternatives": export_scenario_alternatives(db_map, scenario_alternative_ids, make_cache=make_cache),
+        "tools": export_tools(db_map, tool_ids, make_cache=make_cache),
+        "features": export_features(db_map, feature_ids, make_cache=make_cache),
+        "tool_features": export_tool_features(db_map, tool_feature_ids, make_cache=make_cache),
+        "tool_feature_methods": export_tool_feature_methods(
+            db_map, tool_feature_method_ids, make_cache=make_cache, parse_value=parse_value
+        ),
+    }
+    return {key: value for key, value in data.items() if value}
+
+
+def _get_items(db_map, tablename, ids, make_cache):
+    if not ids:
+        return ()
+    if make_cache is None:
+        make_cache = db_map.make_cache
+    cache = make_cache({tablename}, include_ancestors=True)
+    _process_item = _make_item_processor(tablename, make_cache)
+    for item in _get_items_from_cache(cache, tablename, ids):
+        yield from _process_item(item)
+
+
+def _get_items_from_cache(cache, tablename, ids):
+    items = cache.get(tablename, {})
+    if ids is Asterisk:
+        yield from items.values()
+        return
+    for id_ in ids:
+        item = items[id_]
+        if item.is_valid():
+            yield item
+
+
+def _make_item_processor(tablename, make_cache):
+    if tablename == "parameter_value_list":
+        return _ParameterValueListProcessor(make_cache)
+    return lambda item: (item,)
+
+
+class _ParameterValueListProcessor:
+    def __init__(self, make_cache):
+        self._cache = make_cache({"list_value"})
+
+    def __call__(self, item):
+        fields = ["name", "value", "type"]
+        if item.value_id_list is None:
+            yield KeyedTuple([item.name, None, None], fields)
+            return
+        for value_id in item.value_id_list:
+            val = self._cache["list_value"][value_id]
+            yield KeyedTuple([item.name, val.value, val.type], fields)
+
+
+def export_object_classes(db_map, ids=Asterisk, make_cache=None):
+    return sorted((x.name, x.description, x.display_icon) for x in _get_items(db_map, "object_class", ids, make_cache))
+
+
+def export_objects(db_map, ids=Asterisk, make_cache=None):
+    return sorted((x.class_name, x.name, x.description) for x in _get_items(db_map, "object", ids, make_cache))
+
+
+def export_relationship_classes(db_map, ids=Asterisk, make_cache=None):
+    return sorted(
+        (x.name, x.object_class_name_list, x.description, x.display_icon)
+        for x in _get_items(db_map, "relationship_class", ids, make_cache)
+    )
+
+
+def export_parameter_value_lists(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
+    return sorted(
+        ((x.name, parse_value(x.value, x.type)) for x in _get_items(db_map, "parameter_value_list", ids, make_cache)),
+        key=itemgetter(0),
+    )
+
+
+def export_object_parameters(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
+    return sorted(
+        (
+            x.object_class_name,
+            x.parameter_name,
+            parse_value(x.default_value, x.default_type),
+            x.value_list_name,
+            x.description,
+        )
+        for x in _get_items(db_map, "parameter_definition", ids, make_cache)
+        if x.object_class_id
+    )
+
+
+def export_relationship_parameters(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
+    return sorted(
+        (
+            x.relationship_class_name,
+            x.parameter_name,
+            parse_value(x.default_value, x.default_type),
+            x.value_list_name,
+            x.description,
+        )
+        for x in _get_items(db_map, "parameter_definition", ids, make_cache)
+        if x.relationship_class_id
+    )
+
+
+def export_relationships(db_map, ids=Asterisk, make_cache=None):
+    return sorted((x.class_name, x.object_name_list) for x in _get_items(db_map, "relationship", ids, make_cache))
+
+
+def export_object_groups(db_map, ids=Asterisk, make_cache=None):
+    return sorted(
+        (x.class_name, x.group_name, x.member_name)
+        for x in _get_items(db_map, "entity_group", ids, make_cache)
+        if x.object_class_id
+    )
+
+
+def export_object_parameter_values(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
+    return sorted(
+        (
+            (x.object_class_name, x.object_name, x.parameter_name, parse_value(x.value, x.type), x.alternative_name)
+            for x in _get_items(db_map, "parameter_value", ids, make_cache)
+            if x.object_id
+        ),
+        key=lambda x: x[:3] + (x[-1],),
+    )
+
+
+def export_relationship_parameter_values(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
+    return sorted(
+        (
+            (
+                x.relationship_class_name,
+                x.object_name_list,
+                x.parameter_name,
+                parse_value(x.value, x.type),
+                x.alternative_name,
+            )
+            for x in _get_items(db_map, "parameter_value", ids, make_cache)
+            if x.relationship_id
+        ),
+        key=lambda x: x[:3] + (x[-1],),
+    )
+
+
+def export_alternatives(db_map, ids=Asterisk, make_cache=None):
+    """
+    Exports alternatives from database.
+
+    The format is what :func:`import_alternatives` accepts as its input.
+
+    Args:
+        db_map (spinedb_api.DatabaseMapping or spinedb_api.DiffDatabaseMapping): a database map
+        ids (Iterable, optional): ids of the alternatives to export
+
+    Returns:
+        Iterable: tuples of two elements: name of alternative and description
+    """
+    return sorted((x.name, x.description) for x in _get_items(db_map, "alternative", ids, make_cache))
+
+
+def export_scenarios(db_map, ids=Asterisk, make_cache=None):
+    """
+    Exports scenarios from database.
+
+    The format is what :func:`import_scenarios` accepts as its input.
+
+    Args:
+        db_map (spinedb_api.DatabaseMapping or spinedb_api.DiffDatabaseMapping): a database map
+        ids (Iterable, optional): ids of the scenarios to export
+
+    Returns:
+        Iterable: tuples of two elements: name of scenario and description
+    """
+    return sorted((x.name, x.active, x.description) for x in _get_items(db_map, "scenario", ids, make_cache))
+
+
+def export_scenario_alternatives(db_map, ids=Asterisk, make_cache=None):
+    """
+    Exports scenario alternatives from database.
+
+    The format is what :func:`import_scenario_alternatives` accepts as its input.
+
+    Args:
+        db_map (spinedb_api.DatabaseMapping or spinedb_api.DiffDatabaseMapping): a database map
+        ids (Iterable, optional): ids of the scenario alternatives to export
+
+    Returns:
+        Iterable: tuples of three elements: name of scenario, tuple containing one alternative name,
+            and name of next alternative
+    """
+    return sorted(
+        (
+            (x.scenario_name, x.alternative_name, x.before_alternative_name)
+            for x in _get_items(db_map, "scenario_alternative", ids, make_cache)
+        ),
+        key=itemgetter(0),
+    )
+
+
+def export_tools(db_map, ids=Asterisk, make_cache=None):
+    return sorted((x.name, x.description) for x in _get_items(db_map, "tool", ids, make_cache))
+
+
+def export_features(db_map, ids=Asterisk, make_cache=None):
+    return sorted(
+        (x.entity_class_name, x.parameter_definition_name, x.parameter_value_list_name, x.description)
+        for x in _get_items(db_map, "feature", ids, make_cache)
+    )
+
+
+def export_tool_features(db_map, ids=Asterisk, make_cache=None):
+    return sorted(
+        (x.tool_name, x.entity_class_name, x.parameter_definition_name, x.required)
+        for x in _get_items(db_map, "tool_feature", ids, make_cache)
+    )
+
+
+def export_tool_feature_methods(db_map, ids=Asterisk, make_cache=None, parse_value=from_database):
+    return sorted(
+        (x.tool_name, x.entity_class_name, x.parameter_definition_name, parse_value(x.method, None))
+        for x in _get_items(db_map, "tool_feature_method", ids, make_cache)
+    )
```

### Comparing `spinedb_api-0.30.3/spinedb_api/export_mapping/__init__.py` & `spinedb_api-0.30.4/spinedb_api/export_mapping/__init__.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-This package contains facilities to map a Spine database into tables.
-
-"""
-
-from .generator import rows, titles
-from .settings import (
-    alternative_export,
-    feature_export,
-    object_export,
-    object_group_export,
-    object_parameter_default_value_export,
-    object_parameter_export,
-    parameter_value_list_export,
-    relationship_export,
-    relationship_object_parameter_default_value_export,
-    relationship_object_parameter_export,
-    relationship_parameter_default_value_export,
-    relationship_parameter_export,
-    scenario_alternative_export,
-    scenario_export,
-    tool_export,
-    tool_feature_export,
-    tool_feature_method_export,
-)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+This package contains facilities to map a Spine database into tables.
+
+"""
+
+from .generator import rows, titles
+from .settings import (
+    alternative_export,
+    feature_export,
+    object_export,
+    object_group_export,
+    object_parameter_default_value_export,
+    object_parameter_export,
+    parameter_value_list_export,
+    relationship_export,
+    relationship_object_parameter_default_value_export,
+    relationship_object_parameter_export,
+    relationship_parameter_default_value_export,
+    relationship_parameter_export,
+    scenario_alternative_export,
+    scenario_export,
+    tool_export,
+    tool_feature_export,
+    tool_feature_method_export,
+)
```

### Comparing `spinedb_api-0.30.3/spinedb_api/export_mapping/export_mapping.py` & `spinedb_api-0.30.4/spinedb_api/export_mapping/export_mapping.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,1957 +1,1957 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Contains export mappings for database items such as entities, entity classes and parameter values.
-
-"""
-
-from dataclasses import dataclass
-from itertools import cycle, dropwhile, islice
-from sqlalchemy import and_
-from sqlalchemy.sql.expression import literal
-from ..parameter_value import (
-    from_database_to_single_value,
-    from_database,
-    IndexedValue,
-    from_database_to_dimension_count,
-    map_dimensions,
-    convert_containers_to_maps,
-)
-from ..mapping import Mapping, Position, is_pivoted, is_regular, unflatten
-from .group_functions import NoGroup
-
-
-def check_validity(root_mapping):
-    """Checks validity of a mapping hierarchy.
-
-    To check validity of individual mappings withing the hierarchy, use :func:`Mapping.check_validity`.
-
-    Args:
-        root_mapping (Mapping): root mapping
-
-    Returns:
-        list of str: a list of issue descriptions
-    """
-    issues = list()
-    flattened = root_mapping.flatten()
-    non_title_mappings = [m for m in flattened if m.position != Position.table_name]
-    if len(non_title_mappings) == 2 and is_pivoted(non_title_mappings[0].position):
-        issues.append("First mapping cannot be pivoted")
-    return issues
-
-
-class _MappingWithLeafMixin:
-    """Provides current_leaf field."""
-
-    current_leaf = None
-
-
-class ExportMapping(Mapping):
-    _TITLE_SEP = ","
-
-    def __init__(self, position, value=None, header="", filter_re=""):
-        """
-        Args:
-            position (int or Position): column index or Position
-            value (Any, optional): A fixed value
-            header (str); A string column header that's yielded as 'first row', if not empty.
-                The default is an empty string (so it's not yielded).
-            filter_re (str): A regular expression to filter the mapped values by
-        """
-        super().__init__(position, value, filter_re)
-        self._ignorable = False
-        self.header = header
-        self._convert_data = None
-
-    def __eq__(self, other):
-        if not isinstance(other, ExportMapping):
-            return NotImplemented
-        if not super().__eq__(other):
-            return False
-        return self._ignorable == other._ignorable and self.header == other.header
-
-    def check_validity(self):
-        """Checks if mapping is valid.
-
-        Returns:
-            list: a list of issues
-        """
-        issues = list()
-        if self.child is None:
-            is_effective_leaf = True
-        else:
-            is_effective_leaf = any(
-                child.position in (Position.hidden, Position.table_name) for child in self.child.flatten()
-            )
-        if is_effective_leaf and is_pivoted(self.position):
-            issues.append("Cannot be pivoted.")
-        return issues
-
-    def replace_data(self, data):
-        """
-        Replaces the data generated by this item by user given data.
-
-        If data is exhausted, it gets cycled again from the beginning.
-
-        Args:
-            data (Iterable): user data
-        """
-        data_iterator = cycle(data)
-        self._convert_data = lambda _: next(data_iterator)
-
-    @staticmethod
-    def is_buddy(parent):
-        """Checks if mapping uses a parent's state for its data.
-
-        Args:
-            parent (ExportMapping): a parent mapping
-
-        Returns:
-            bool: True if parent's state affects what a mapping yields
-        """
-        return False
-
-    def is_ignorable(self):
-        """Returns True if the mapping is ignorable, False otherwise.
-
-        Returns:
-            bool: True if mapping is ignorable, False otherwise
-        """
-        return self._ignorable
-
-    def set_ignorable(self, ignorable):
-        """
-        Sets mapping as ignorable.
-
-        Mappings that are ignorable map to None if there is no other data to yield.
-        This allows 'incomplete' rows if child mappings do not depend on the ignored mapping.
-
-        Args:
-            ignorable (bool): True to set mapping ignorable, False to unset
-        """
-        self._ignorable = ignorable
-
-    def to_dict(self):
-        """
-        Serializes mapping into dict.
-
-        Returns:
-            dict: serialized mapping
-        """
-        mapping_dict = super().to_dict()
-        if self._ignorable:
-            mapping_dict["ignorable"] = True
-        if self.header:
-            mapping_dict["header"] = self.header
-        return mapping_dict
-
-    @classmethod
-    def reconstruct(cls, position, value, header, filter_re, ignorable, mapping_dict):
-        """
-        Reconstructs mapping.
-
-        Args:
-            position (int or Position, optional): mapping's position
-            value (Any): fixed value
-            header (str, optional): column header
-            filter_re (str): filter regular expression
-            ignorable (bool): ignorable flag
-            mapping_dict (dict): mapping dict
-
-        Returns:
-            Mapping: reconstructed mapping
-        """
-        mapping = cls(position, value, header, filter_re)
-        mapping.set_ignorable(ignorable)
-        return mapping
-
-    def add_query_columns(self, db_map, query):
-        """Adds columns to the mapping query if needed, and returns the new query.
-
-        The base class implementation just returns the same query without adding any new columns.
-
-        Args:
-            db_map (DatabaseMappingBase)
-            query (Alias or dict)
-
-        Returns:
-            Alias: expanded query, or the same if nothing to add.
-        """
-        return query
-
-    def filter_query(self, db_map, query):
-        """Filters the mapping query if needed, and returns the new query.
-
-        The base class implementation just returns the same query without applying any new filters.
-
-        Args:
-            db_map (DatabaseMappingBase)
-            query (Alias or dict)
-
-        Returns:
-            Alias: filtered query, or the same if nothing to add.
-        """
-        return query
-
-    def filter_query_by_title(self, query, title_state):
-        """Filters the query according to the given title state.
-        Note that ``_build_query()`` does some default filtering on the title state after calling this method.
-        Therefore, if a subclass reimplements this method, it needs to delete the consumed keys from ``title_state``
-        so they aren't consumed again by ``_build_query()``.
-
-        The base class implementations just returns the unaltered query.
-
-        Args:
-            title_state (dict)
-
-        Returns:
-            Query or _FilteredQuery
-        """
-        return query
-
-    def _build_query(self, db_map, title_state):
-        """Builds and returns the query to run for this mapping hierarchy.
-
-        Args:
-            db_map (DatabaseMappingBase)
-            title_state (dict)
-
-        Returns:
-            Query
-        """
-        mappings = self.flatten()
-        # Start with empty query
-        qry = db_map.query(literal(None))
-        # Add columns
-        for m in mappings:
-            qry = m.add_query_columns(db_map, qry)
-        # Apply filters
-        for m in mappings:
-            qry = m.filter_query(db_map, qry)
-        # Apply special title filters (first, so we clean up the state)
-        for m in mappings:
-            qry = m.filter_query_by_title(qry, title_state)
-        # Apply standard title filters
-        if not title_state:
-            return qry
-        # Use a _FilteredQuery, since building a subquery to query it again leads to parser stack overflow
-        return _FilteredQuery(
-            qry, lambda db_row: all(getattr(db_row, key) == value for key, value in title_state.items())
-        )
-
-    def _build_title_query(self, db_map):
-        """Builds and returns the query to get titles for this mapping hierarchy.
-
-        Args:
-            db_map (DatabaseMappingBase): database mapping
-
-        Returns:
-            Alias: title query
-        """
-        mappings = self.flatten()
-        for _ in range(len(mappings)):
-            if mappings[-1].position == Position.table_name:
-                break
-            mappings.pop(-1)
-        # Start with empty query
-        qry = db_map.query(literal(None))
-        # Add columns
-        for m in mappings:
-            qry = m.add_query_columns(db_map, qry)
-        # Apply filters
-        for m in mappings:
-            qry = m.filter_query(db_map, qry)
-        return qry
-
-    def _build_header_query(self, db_map, title_state, buddies):
-        """Builds the header query for this mapping hierarchy.
-
-        Args:
-            db_map (DatabaseMappingBase): database mapping
-            title_state (dict): title state
-            buddies (list of tuple): pairs of buddy mappings
-
-        Returns:
-            Alias: header query
-        """
-        mappings = self.flatten()
-        flat_buddies = [b for pair in buddies for b in pair]
-        for _ in range(len(mappings)):
-            m = mappings[-1]
-            if m.position == Position.header or m.position == Position.table_name or m in flat_buddies:
-                break
-            mappings.pop(-1)
-        # Start with empty query
-        qry = db_map.query(literal(None))
-        # Add columns
-        for m in mappings:
-            qry = m.add_query_columns(db_map, qry)
-        # Apply filters
-        for m in mappings:
-            qry = m.filter_query(db_map, qry)
-        # Apply special title filters (first, so we clean up the state)
-        for m in mappings:
-            qry = m.filter_query_by_title(qry, title_state)
-        # Apply standard title filters
-        if not title_state:
-            return qry
-        # Use a _FilteredQuery, since building a subquery to query it again leads to parser stack overflow
-        return _FilteredQuery(
-            qry, lambda db_row: all(getattr(db_row, key) == value for key, value in title_state.items())
-        )
-
-    @staticmethod
-    def name_field():
-        """Returns the 'name' field associated to this mapping within the query.
-        Used to obtain the relevant data from a db row.
-
-        Returns:
-            str
-        """
-        raise NotImplementedError()
-
-    @staticmethod
-    def id_field():
-        """Returns the 'id' field associated to this mapping within the query.
-        Used to compose the title state.
-
-        Returns:
-            str
-        """
-        raise NotImplementedError()
-
-    def _data(self, db_row):  # pylint: disable=arguments-differ
-        """Returns the data relevant to this mapping from given database row.
-
-        The base class implementation returns the field given by ``name_field()``.
-
-        Args:
-            db_row (KeyedTuple)
-
-        Returns:
-            any
-        """
-        return getattr(db_row, self.name_field(), None)
-
-    def _expand_data(self, data):
-        """Takes data from an individual field in the db and yields all data generated by this mapping.
-
-        The base class implementation simply yields the given data.
-        Reimplement in subclasses that need to expand the data into multiple elements (e.g., indexed value mappings).
-
-        Args:
-            data (any)
-
-        Returns:
-            generator(any)
-        """
-        yield data
-
-    def _get_data_iterator(self, data):
-        """Applies regexp filtering and data conversion on the output of ``_expand_data()`` to produce the final data
-        iterator for this mapping.
-
-        Args:
-            data (any)
-
-        Returns:
-            generator(any)
-        """
-        data_iterator = self._expand_data(data)
-        if self._filter_re is not None:
-            data_iterator = (x for x in data_iterator if self._filter_re.search(str(x)))
-        if self._convert_data is not None:
-            data_iterator = (self._convert_data(x) for x in data_iterator)
-        return data_iterator
-
-    def _get_rows(self, db_row):
-        """Yields rows issued by this mapping for given database row.
-
-        Args:
-            db_row (KeyedTuple)
-
-        Returns:
-            generator(dict)
-        """
-        if self.position == Position.table_name:
-            yield {}
-            return
-        data = self._data(db_row)
-        if data is None and not self._ignorable:
-            return ()
-        data_iterator = self._get_data_iterator(data)
-        for data in data_iterator:
-            yield {self.position: data}
-
-    def get_rows_recursive(self, db_row):
-        """Takes a database row and yields rows issued by this mapping and its children combined.
-
-        Args:
-            db_row (KeyedTuple)
-
-        Returns:
-            generator(dict)
-        """
-        if self.child is None:
-            yield from self._get_rows(db_row)
-            return
-        for row in self._get_rows(db_row):
-            for child_row in self.child.get_rows_recursive(db_row):
-                row = row.copy()
-                row.update(child_row)
-                yield row
-
-    def rows(self, db_map, title_state):
-        """Yields rows issued by this mapping and its children combined.
-
-        Args:
-            db_map (DatabaseMappingBase)
-            title_state (dict)
-
-        Returns:
-            generator(dict)
-        """
-        qry = self._build_query(db_map, title_state)
-        for db_row in qry.yield_per(1000):
-            yield from self.get_rows_recursive(db_row)
-
-    def has_titles(self):
-        """Returns True if this mapping or one of its children generates titles.
-
-        Returns:
-            bool: True if mappings generate titles, False otherwise
-        """
-        if self.position == Position.table_name:
-            return True
-        if self.child is not None:
-            return self.child.has_titles()
-        return False
-
-    def _title_state(self, db_row):
-        """Returns the title state associated to this mapping from given database row.
-
-        The base class implementation returns a dict mapping the output of ``id_field()``
-        to the corresponding field from the row.
-
-        Args:
-            db_row (KeyedTuple)
-
-        Returns:
-            dict
-        """
-        id_field = self.id_field()
-        if id_field is None:
-            return {}
-        return {id_field: getattr(db_row, id_field)}
-
-    def _get_titles(self, db_row, limit=None):
-        """Yields pairs (title, title state) issued by this mapping for given database row.
-
-        Args:
-            db_row (KeyedTuple)
-            limit (int, optional): yield only this many items
-
-        Returns:
-            generator(str,dict)
-        """
-        if self.position != Position.table_name:
-            yield "", {}
-            return
-        data = self._data(db_row)
-        title_state = self._title_state(db_row)
-        data_iterator = self._get_data_iterator(data)
-        if limit is not None:
-            data_iterator = islice(data_iterator, limit)
-        for data in data_iterator:
-            if data is None:
-                data = ""
-            yield data, title_state
-
-    def get_titles_recursive(self, db_row, limit=None):
-        """Takes a database row and yields pairs (title, title state) issued by this mapping and its children combined.
-
-        Args:
-            db_row (KeyedTuple)
-            limit (int, optional): yield only this many items
-
-        Returns:
-            generator(str,dict)
-        """
-        if self.child is None:
-            yield from self._get_titles(db_row, limit=limit)
-            return
-        for title, title_state in self._get_titles(db_row, limit=limit):
-            for child_title, child_title_state in self.child.get_titles_recursive(db_row, limit=limit):
-                title_sep = self._TITLE_SEP if title and child_title else ""
-                final_title = title + title_sep + child_title
-                yield final_title, {**title_state, **child_title_state}
-
-    def _non_unique_titles(self, db_map, limit=None):
-        """Yields all titles, not necessarily unique, and associated state dictionaries.
-
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            limit (int, optional): yield only this many items
-
-        Yields:
-            tuple(str,dict): title, and associated title state dictionary
-        """
-        qry = self._build_title_query(db_map)
-        for db_row in qry.yield_per(1000):
-            yield from self.get_titles_recursive(db_row, limit=limit)
-
-    def titles(self, db_map, limit=None):
-        """Yields unique titles and associated state dictionaries.
-
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            limit (int, optional): yield only this many items
-
-        Yields:
-            tuple(str,dict): unique title, and associated title state dictionary
-        """
-        titles = {}
-        for title, title_state in self._non_unique_titles(db_map, limit=limit):
-            titles.setdefault(title, {}).update(title_state)
-        yield from titles.items()
-
-    def has_header(self):
-        """Recursively checks if mapping would create a header row.
-
-        Returns:
-            bool: True if make_header() would return something useful
-        """
-        if self.header or self.position == Position.header:
-            return True
-        if self.child is None:
-            return False
-        return self.child.has_header()
-
-    def make_header_recursive(self, query, buddies):
-        """Builds the header recursively.
-
-        Args:
-            build_header_query (callable): a function that any mapping in the hierarchy can call to get the query
-            db_map (DatabaseMappingBase): database map
-            title_state (dict): title state
-            buddies (list of tuple): buddy mappings
-
-        Returns
-            dict: a mapping from column index to string header
-        """
-        if self.child is None:
-            if not is_regular(self.position):
-                return {}
-            return {self.position: self.header}
-        header = self.child.make_header_recursive(query, buddies)
-        if self.position == Position.header:
-            buddy = find_my_buddy(self, buddies)
-            if buddy is not None:
-                query.rewind()
-                header[buddy.position] = next(
-                    (x for db_row in query for x in self._get_data_iterator(self._data(db_row))), ""
-                )
-        else:
-            header[self.position] = self.header
-        return header
-
-    def make_header(self, db_map, title_state, buddies):
-        """Returns the header for this mapping.
-
-        Args:
-            db_map (DatabaseMappingBase): database map
-            title_state (dict): title state
-            buddies (list of tuple): buddy mappings
-
-        Returns
-            dict: a mapping from column index to string header
-        """
-        query = _Rewindable(self._build_header_query(db_map, title_state, buddies).yield_per(1000))
-        return self.make_header_recursive(query, buddies)
-
-
-def drop_non_positioned_tail(root_mapping):
-    """Makes a modified mapping hierarchy without hidden tail mappings.
-
-    This enables pivot tables to work correctly in certain situations.
-
-    Args:
-        root_mapping (Mapping): root mapping
-
-    Returns:
-        Mapping: modified mapping hierarchy
-    """
-    mappings = root_mapping.flatten()
-    return unflatten(
-        reversed(list(dropwhile(lambda m: m.position == Position.hidden and not m.filter_re, reversed(mappings))))
-    )
-
-
-class FixedValueMapping(ExportMapping):
-    """Always yields a fixed value.
-
-    Can be used as the topmost mapping.
-
-    """
-
-    MAP_TYPE = "FixedValue"
-
-    def __init__(self, position, value, header="", filter_re=""):
-        """
-        Args:
-            position (int or Position, optional): mapping's position
-            value (Any): value to yield
-            header (str, optional); A string column header that's yielt as 'first row', if not empty.
-                The default is an empty string (so it's not yielt).
-            filter_re (str, optional): A regular expression to filter the mapped values by
-        """
-        super().__init__(position, value, header, filter_re)
-
-    @staticmethod
-    def name_field():
-        return None
-
-    @staticmethod
-    def id_field():
-        return None
-
-
-class ObjectClassMapping(ExportMapping):
-    """Maps object classes.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "ObjectClass"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.object_class_sq.c.id.label("object_class_id"),
-            db_map.object_class_sq.c.name.label("object_class_name"),
-        )
-
-    @staticmethod
-    def name_field():
-        return "object_class_name"
-
-    @staticmethod
-    def id_field():
-        # Use the class name here, for the sake of the standard excel export
-        return "object_class_name"
-
-
-class ObjectMapping(ExportMapping):
-    """Maps objects.
-
-    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectClassMapping`.
-    """
-
-    MAP_TYPE = "Object"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.object_sq.c.id.label("object_id"), db_map.object_sq.c.name.label("object_name"))
-
-    def filter_query(self, db_map, query):
-        return query.outerjoin(db_map.object_sq, db_map.object_sq.c.class_id == db_map.object_class_sq.c.id)
-
-    @staticmethod
-    def name_field():
-        return "object_name"
-
-    @staticmethod
-    def id_field():
-        return "object_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ObjectClassMapping)
-
-
-class ObjectGroupMapping(ExportMapping):
-    """Maps object groups.
-
-    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectClassMapping`.
-    """
-
-    MAP_TYPE = "ObjectGroup"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.ext_entity_group_sq.c.group_id, db_map.ext_entity_group_sq.c.group_name)
-
-    def filter_query(self, db_map, query):
-        return query.outerjoin(
-            db_map.ext_entity_group_sq, db_map.ext_entity_group_sq.c.class_id == db_map.object_class_sq.c.id
-        ).distinct()
-
-    @staticmethod
-    def name_field():
-        return "group_name"
-
-    @staticmethod
-    def id_field():
-        return "group_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ObjectClassMapping)
-
-
-class ObjectGroupObjectMapping(ExportMapping):
-    """Maps objects in object groups.
-
-    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectGroupMapping`.
-    """
-
-    MAP_TYPE = "ObjectGroupObject"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.object_sq.c.id.label("object_id"), db_map.object_sq.c.name.label("object_name"))
-
-    def filter_query(self, db_map, query):
-        return query.filter(db_map.ext_entity_group_sq.c.member_id == db_map.object_sq.c.id)
-
-    @staticmethod
-    def name_field():
-        return "object_name"
-
-    @staticmethod
-    def id_field():
-        return "object_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ObjectGroupMapping)
-
-
-class RelationshipClassMapping(ExportMapping):
-    """Maps relationships classes.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "RelationshipClass"
-
-    def __init__(self, position, value=None, header="", filter_re="", highlight_dimension=None):
-        super().__init__(position, value, header, filter_re)
-        self.highlight_dimension = highlight_dimension
-
-    def add_query_columns(self, db_map, query):
-        query = query.add_columns(
-            db_map.wide_relationship_class_sq.c.id.label("relationship_class_id"),
-            db_map.wide_relationship_class_sq.c.name.label("relationship_class_name"),
-            db_map.wide_relationship_class_sq.c.object_class_id_list,
-            db_map.wide_relationship_class_sq.c.object_class_name_list,
-        )
-        if self.highlight_dimension is not None:
-            query = query.add_columns(
-                db_map.ext_relationship_class_sq.c.object_class_id.label("highlighted_object_class_id")
-            )
-        return query
-
-    def filter_query(self, db_map, query):
-        if self.highlight_dimension is not None:
-            query = query.outerjoin(
-                db_map.ext_relationship_class_sq,
-                db_map.ext_relationship_class_sq.c.id == db_map.wide_relationship_class_sq.c.id,
-            ).filter(db_map.ext_relationship_class_sq.c.dimension == self.highlight_dimension)
-        return query
-
-    @staticmethod
-    def name_field():
-        return "relationship_class_name"
-
-    @staticmethod
-    def id_field():
-        # Use the class name here, for the sake of the standard excel export
-        return "relationship_class_name"
-
-    def query_parents(self, what):
-        if what == "dimension":
-            return -1
-        if what == "highlight_dimension":
-            return self.highlight_dimension
-        return super().query_parents(what)
-
-    def _title_state(self, db_row):
-        state = super()._title_state(db_row)
-        state["object_class_id_list"] = getattr(db_row, "object_class_id_list")
-        return state
-
-    def to_dict(self):
-        mapping_dict = super().to_dict()
-        if self.highlight_dimension is not None:
-            mapping_dict["highlight_dimension"] = self.highlight_dimension
-        return mapping_dict
-
-    @classmethod
-    def reconstruct(cls, position, value, header, filter_re, ignorable, mapping_dict):
-        highlight_dimension = mapping_dict.get("highlight_dimension")
-        mapping = cls(position, value, header, filter_re, highlight_dimension)
-        mapping.set_ignorable(ignorable)
-        return mapping
-
-
-class RelationshipClassObjectClassMapping(ExportMapping):
-    """Maps relationship class object classes.
-
-    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
-    """
-
-    MAP_TYPE = "RelationshipClassObjectClass"
-    _cached_dimension = None
-
-    @staticmethod
-    def name_field():
-        return "object_class_name_list"
-
-    @staticmethod
-    def id_field():
-        return "object_class_id_list"
-
-    def _data(self, db_row):
-        data = super()._data(db_row).split(",")
-        if self._cached_dimension is None:
-            self._cached_dimension = self.query_parents("dimension")
-        try:
-            return data[self._cached_dimension]
-        except IndexError:
-            return ""
-
-    def query_parents(self, what):
-        if what != "dimension":
-            return super().query_parents(what)
-        return self.parent.query_parents(what) + 1
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, RelationshipClassMapping)
-
-
-class RelationshipMapping(ExportMapping):
-    """Maps relationships.
-
-    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
-    """
-
-    MAP_TYPE = "Relationship"
-
-    def add_query_columns(self, db_map, query):
-        query = query.add_columns(
-            db_map.wide_relationship_sq.c.id.label("relationship_id"),
-            db_map.wide_relationship_sq.c.name.label("relationship_name"),
-            db_map.wide_relationship_sq.c.object_id_list,
-            db_map.wide_relationship_sq.c.object_name_list,
-        )
-        if self.query_parents("highlight_dimension") is not None:
-            query = query.add_columns(db_map.ext_relationship_sq.c.object_id.label("highlighted_object_id"))
-        return query
-
-    def filter_query(self, db_map, query):
-        query = query.outerjoin(
-            db_map.wide_relationship_sq,
-            db_map.wide_relationship_sq.c.class_id == db_map.wide_relationship_class_sq.c.id,
-        )
-        if (highlight_dimension := self.query_parents("highlight_dimension")) is not None:
-            query = query.outerjoin(
-                db_map.ext_relationship_sq, db_map.ext_relationship_sq.c.id == db_map.wide_relationship_sq.c.id
-            ).filter(db_map.ext_relationship_sq.c.dimension == highlight_dimension)
-        return query
-
-    @staticmethod
-    def name_field():
-        return "relationship_name"
-
-    @staticmethod
-    def id_field():
-        return "relationship_id"
-
-    def query_parents(self, what):
-        if what != "dimension":
-            return super().query_parents(what)
-        return -1
-
-    def _title_state(self, db_row):
-        state = super()._title_state(db_row)
-        state["object_id_list"] = getattr(db_row, "object_id_list")
-        return state
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, RelationshipClassMapping)
-
-
-class RelationshipObjectMapping(ExportMapping):
-    """Maps relationship's objects.
-
-    Cannot be used as the topmost mapping; must have :class:`RelationshipClassMapping` and :class:`RelationshipMapping`
-    as parents.
-    """
-
-    MAP_TYPE = "RelationshipObject"
-    _cached_dimension = None
-
-    @staticmethod
-    def name_field():
-        return "object_name_list"
-
-    @staticmethod
-    def id_field():
-        return "object_id_list"
-
-    def _data(self, db_row):
-        data = super()._data(db_row).split(",")
-        if self._cached_dimension is None:
-            self._cached_dimension = self.query_parents("dimension")
-        try:
-            return data[self._cached_dimension]
-        except IndexError:
-            return ""
-
-    def query_parents(self, what):
-        if what != "dimension":
-            return super().query_parents(what)
-        return self.parent.query_parents(what) + 1
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, RelationshipClassObjectClassMapping)
-
-
-class ParameterDefinitionMapping(ExportMapping):
-    """Maps parameter definitions.
-
-    Cannot be used as the topmost mapping; must have an entity class mapping as one of parents.
-    """
-
-    MAP_TYPE = "ParameterDefinition"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.parameter_definition_sq.c.id.label("parameter_definition_id"),
-            db_map.parameter_definition_sq.c.name.label("parameter_definition_name"),
-        )
-
-    def filter_query(self, db_map, query):
-        column_names = {c["name"] for c in query.column_descriptions}
-        if "object_class_id" in column_names:
-            return query.outerjoin(
-                db_map.parameter_definition_sq,
-                db_map.parameter_definition_sq.c.object_class_id == db_map.object_class_sq.c.id,
-            )
-        if "relationship_class_id" in column_names:
-            if self.query_parents("highlight_dimension") is not None:
-                return query.outerjoin(
-                    db_map.parameter_definition_sq,
-                    db_map.parameter_definition_sq.c.object_class_id
-                    == db_map.ext_relationship_class_sq.c.object_class_id,
-                )
-            return query.outerjoin(
-                db_map.parameter_definition_sq,
-                db_map.parameter_definition_sq.c.relationship_class_id == db_map.wide_relationship_class_sq.c.id,
-            )
-        raise RuntimeError("Logic error: this code should be unreachable.")
-
-    @staticmethod
-    def name_field():
-        return "parameter_definition_name"
-
-    @staticmethod
-    def id_field():
-        return "parameter_definition_id"
-
-
-class ParameterDefaultValueMapping(ExportMapping):
-    """Maps scalar (non-indexed) default values
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
-    """
-
-    MAP_TYPE = "ParameterDefaultValue"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.parameter_definition_sq.c.default_value, db_map.parameter_definition_sq.c.default_type
-        )
-
-    @staticmethod
-    def name_field():
-        return None
-
-    @staticmethod
-    def id_field():
-        return None
-
-    def _data(self, db_row):
-        return from_database_to_single_value(db_row.default_value, db_row.default_type)
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ParameterDefinitionMapping)
-
-
-class ParameterDefaultValueTypeMapping(ParameterDefaultValueMapping):
-    """Maps parameter value types.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
-    an :class:`AlternativeMapping` as parents.
-    """
-
-    MAP_TYPE = "ParameterDefaultValueType"
-
-    def _data(self, db_row):
-        type_ = db_row.default_type
-        if type_ == "map":
-            return f"{map_dimensions(from_database(db_row.default_value, type_))}d_map"
-        if type_ in ("time_series", "time_pattern", "array"):
-            return type_
-        return "single_value"
-
-    def _title_state(self, db_row):
-        return {
-            "type_and_dimensions": (
-                db_row.default_type,
-                from_database_to_dimension_count(db_row.default_value, db_row.default_type),
-            )
-        }
-
-    def filter_query_by_title(self, query, title_state):
-        pv = title_state.pop("type_and_dimensions", None)
-        if pv is None:
-            return query
-        if "default_value" not in {c["name"] for c in query.column_descriptions}:
-            return query
-        return _FilteredQuery(
-            query,
-            lambda db_row: (
-                db_row.default_type,
-                from_database_to_dimension_count(db_row.default_value, db_row.default_type) == pv,
-            ),
-        )
-
-
-class DefaultValueIndexNameMapping(_MappingWithLeafMixin, ParameterDefaultValueMapping):
-    """Maps parameter default value index names.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as a parent.
-    """
-
-    MAP_TYPE = "DefaultValueIndexName"
-
-    def _data(self, db_row):
-        return db_row.default_value, db_row.default_type
-
-    def _expand_data(self, data):
-        yield from _expand_index_names(data, self)
-
-
-class ParameterDefaultValueIndexMapping(_MappingWithLeafMixin, ExportMapping):
-    """Maps default value indexes.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
-    """
-
-    MAP_TYPE = "ParameterDefaultValueIndex"
-
-    def add_query_columns(self, db_map, query):
-        if "default_value" in {c["name"] for c in query.column_descriptions}:
-            return query
-        return query.add_columns(
-            db_map.parameter_definition_sq.c.default_value, db_map.parameter_definition_sq.c.default_type
-        )
-
-    def _expand_data(self, data):
-        yield from _expand_indexed_data(data, self)
-
-    @staticmethod
-    def name_field():
-        return None
-
-    @staticmethod
-    def id_field():
-        return None
-
-    def _data(self, db_row):
-        return db_row.default_value, db_row.default_type
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, DefaultValueIndexNameMapping)
-
-
-class ExpandedParameterDefaultValueMapping(ExportMapping):
-    """Maps indexed default values.
-
-    Whenever this mapping is a child of :class:`ParameterDefaultValueIndexMapping`, it maps individual values of
-    indexed parameters.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
-    """
-
-    MAP_TYPE = "ExpandedDefaultValue"
-
-    @staticmethod
-    def name_field():
-        return "default_value"
-
-    @staticmethod
-    def id_field():
-        return "default_value"
-
-    def _data(self, db_row):
-        value = self.parent.current_leaf
-        return value if not isinstance(value, IndexedValue) else value.VALUE_TYPE
-
-
-class ParameterValueMapping(ExportMapping):
-    """Maps scalar (non-indexed) parameter values.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
-    an :class:`AlternativeMapping` as parents.
-    """
-
-    MAP_TYPE = "ParameterValue"
-    _selects_value = False
-
-    def add_query_columns(self, db_map, query):
-        if "value" in {c["name"] for c in query.column_descriptions}:
-            return query
-        self._selects_value = True
-        return query.add_columns(db_map.parameter_value_sq.c.value, db_map.parameter_value_sq.c.type)
-
-    def filter_query(self, db_map, query):
-        if not self._selects_value:
-            return query
-        column_names = {c["name"] for c in query.column_descriptions}
-        if "object_id" in column_names:
-            return query.filter(
-                and_(
-                    db_map.parameter_value_sq.c.object_id == db_map.object_sq.c.id,
-                    db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
-                )
-            )
-        if "relationship_id" in column_names:
-            if self.query_parents("highlight_dimension") is not None:
-                return query.filter(
-                    and_(
-                        db_map.parameter_value_sq.c.object_id == db_map.ext_relationship_sq.c.object_id,
-                        db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
-                    )
-                )
-            return query.filter(
-                and_(
-                    db_map.parameter_value_sq.c.relationship_id == db_map.wide_relationship_sq.c.id,
-                    db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
-                )
-            )
-        raise RuntimeError("Logic error: this code should be unreachable.")
-
-    @staticmethod
-    def name_field():
-        return None
-
-    @staticmethod
-    def id_field():
-        return None
-
-    def _data(self, db_row):
-        return from_database_to_single_value(db_row.value, db_row.type)
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, (ParameterDefinitionMapping, ObjectMapping, RelationshipMapping, AlternativeMapping))
-
-
-class ParameterValueTypeMapping(ParameterValueMapping):
-    """Maps parameter value types.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
-    an :class:`AlternativeMapping` as parents.
-    """
-
-    MAP_TYPE = "ParameterValueType"
-
-    def _data(self, db_row):
-        type_ = db_row.type
-        if type_ == "map":
-            return f"{map_dimensions(from_database(db_row.value, type_))}d_map"
-        if type_ in ("time_series", "time_pattern", "array"):
-            return type_
-        return "single_value"
-
-    def _title_state(self, db_row):
-        return {"type_and_dimensions": (db_row.type, from_database_to_dimension_count(db_row.value, db_row.type))}
-
-    def filter_query_by_title(self, query, title_state):
-        pv = title_state.pop("type_and_dimensions", None)
-        if pv is None:
-            return query
-        if "value" not in {c["name"] for c in query.column_descriptions}:
-            return query
-        return _FilteredQuery(
-            query, lambda db_row: (db_row.type, from_database_to_dimension_count(db_row.value, db_row.type) == pv)
-        )
-
-
-class IndexNameMapping(_MappingWithLeafMixin, ParameterValueMapping):
-    """Maps parameter value index names.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
-    an :class:`AlternativeMapping` as parents.
-    """
-
-    MAP_TYPE = "IndexName"
-
-    def _data(self, db_row):
-        return db_row.value, db_row.type
-
-    def _expand_data(self, data):
-        yield from _expand_index_names(data, self)
-
-
-class ParameterValueIndexMapping(_MappingWithLeafMixin, ParameterValueMapping):
-    """Maps parameter value indexes.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
-    an :class:`AlternativeMapping` as parents.
-    """
-
-    MAP_TYPE = "ParameterValueIndex"
-
-    def _data(self, db_row):
-        return db_row.value, db_row.type
-
-    def _expand_data(self, data):
-        yield from _expand_indexed_data(data, self)
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, IndexNameMapping)
-
-
-class ExpandedParameterValueMapping(ExportMapping):
-    """Maps parameter values.
-
-    Whenever this mapping is a child of :class:`ParameterValueIndexMapping`, it maps individual values of indexed
-    parameters.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
-    an :class:`AlternativeMapping` as parents.
-    """
-
-    MAP_TYPE = "ExpandedValue"
-
-    @staticmethod
-    def name_field():
-        return "value"
-
-    @staticmethod
-    def id_field():
-        return "value"
-
-    def _data(self, db_row):
-        value = self.parent.current_leaf
-        return value if not isinstance(value, IndexedValue) else value.VALUE_TYPE
-
-
-class ParameterValueListMapping(ExportMapping):
-    """Maps parameter value list names.
-
-    Can be used as the topmost mapping; in case the mapping has a :class:`ParameterDefinitionMapping` as parent,
-    yields value list name for that parameter definition.
-    """
-
-    MAP_TYPE = "ParameterValueList"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.parameter_value_list_sq.c.id.label("parameter_value_list_id"),
-            db_map.parameter_value_list_sq.c.name.label("parameter_value_list_name"),
-        )
-
-    def filter_query(self, db_map, query):
-        if self.parent is None:
-            return query
-        return query.outerjoin(
-            db_map.parameter_value_list_sq,
-            db_map.parameter_value_list_sq.c.id == db_map.parameter_definition_sq.c.parameter_value_list_id,
-        )
-
-    @staticmethod
-    def name_field():
-        return "parameter_value_list_name"
-
-    @staticmethod
-    def id_field():
-        return "parameter_value_list_id"
-
-
-class ParameterValueListValueMapping(ExportMapping):
-    """Maps parameter value list values.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterValueListMapping` as parent.
-
-    """
-
-    MAP_TYPE = "ParameterValueListValue"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.ord_list_value_sq.c.value, db_map.ord_list_value_sq.c.type)
-
-    def filter_query(self, db_map, query):
-        return query.filter(db_map.ord_list_value_sq.c.parameter_value_list_id == db_map.parameter_value_list_sq.c.id)
-
-    @staticmethod
-    def name_field():
-        return None
-
-    @staticmethod
-    def id_field():
-        return None
-
-    def _data(self, db_row):
-        return from_database_to_single_value(db_row.value, db_row.type)
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ParameterValueListMapping)
-
-
-class AlternativeMapping(ExportMapping):
-    """Maps alternatives.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "Alternative"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.alternative_sq.c.id.label("alternative_id"),
-            db_map.alternative_sq.c.name.label("alternative_name"),
-            db_map.alternative_sq.c.description.label("description"),
-        )
-
-    def filter_query(self, db_map, query):
-        if self.parent is None:
-            return query
-        return query.filter(db_map.alternative_sq.c.id == db_map.parameter_value_sq.c.alternative_id)
-
-    @staticmethod
-    def name_field():
-        return "alternative_name"
-
-    @staticmethod
-    def id_field():
-        return "alternative_id"
-
-
-class ScenarioMapping(ExportMapping):
-    """Maps scenarios.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "Scenario"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.scenario_sq.c.id.label("scenario_id"),
-            db_map.scenario_sq.c.name.label("scenario_name"),
-            db_map.scenario_sq.c.description.label("description"),
-        )
-
-    @staticmethod
-    def name_field():
-        return "scenario_name"
-
-    @staticmethod
-    def id_field():
-        return "scenario_id"
-
-
-class ScenarioActiveFlagMapping(ExportMapping):
-    """Maps scenario active flags.
-
-    Cannot be used as the topmost mapping; must have a :class:`ScenarioMapping` as parent.
-    """
-
-    MAP_TYPE = "ScenarioActiveFlag"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.scenario_sq.c.active)
-
-    @staticmethod
-    def name_field():
-        return "active"
-
-    @staticmethod
-    def id_field():
-        return "active"
-
-
-class ScenarioAlternativeMapping(ExportMapping):
-    """Maps scenario alternatives.
-
-    Cannot be used as the topmost mapping; must have a :class:`ScenarioMapping` as parent.
-    """
-
-    MAP_TYPE = "ScenarioAlternative"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_linked_scenario_alternative_sq.c.alternative_id,
-            db_map.ext_linked_scenario_alternative_sq.c.alternative_name,
-        )
-
-    def filter_query(self, db_map, query):
-        return query.outerjoin(
-            db_map.ext_linked_scenario_alternative_sq,
-            db_map.ext_linked_scenario_alternative_sq.c.scenario_id == db_map.scenario_sq.c.id,
-        )
-
-    @staticmethod
-    def name_field():
-        return "alternative_name"
-
-    @staticmethod
-    def id_field():
-        return "alternative_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ScenarioMapping)
-
-
-class ScenarioBeforeAlternativeMapping(ExportMapping):
-    """Maps scenario 'before' alternatives.
-
-    Cannot be used as the topmost mapping; must have a :class:`ScenarioAlternativeMapping` as parent.
-    """
-
-    MAP_TYPE = "ScenarioBeforeAlternative"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_linked_scenario_alternative_sq.c.before_alternative_id,
-            db_map.ext_linked_scenario_alternative_sq.c.before_alternative_name,
-        )
-
-    @staticmethod
-    def name_field():
-        return "before_alternative_name"
-
-    @staticmethod
-    def id_field():
-        return "before_alternative_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ScenarioAlternativeMapping)
-
-
-class FeatureEntityClassMapping(ExportMapping):
-    """Maps feature entity classes.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "FeatureEntityClass"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.ext_feature_sq.c.entity_class_id, db_map.ext_feature_sq.c.entity_class_name)
-
-    @staticmethod
-    def name_field():
-        return "entity_class_name"
-
-    @staticmethod
-    def id_field():
-        return "entity_class_id"
-
-
-class FeatureParameterDefinitionMapping(ExportMapping):
-    """Maps feature parameter definitions.
-
-    Cannot be used as the topmost mapping; must have a :class:`FeatureEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "FeatureParameterDefinition"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_feature_sq.c.parameter_definition_id, db_map.ext_feature_sq.c.parameter_definition_name
-        )
-
-    @staticmethod
-    def name_field():
-        return "parameter_definition_name"
-
-    @staticmethod
-    def id_field():
-        return "parameter_definition_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, FeatureEntityClassMapping)
-
-
-class ToolMapping(ExportMapping):
-    """Maps tools.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "Tool"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.tool_sq.c.id.label("tool_id"), db_map.tool_sq.c.name.label("tool_name"))
-
-    @staticmethod
-    def name_field():
-        return "tool_name"
-
-    @staticmethod
-    def id_field():
-        return "tool_id"
-
-
-class ToolFeatureEntityClassMapping(ExportMapping):
-    """Maps tool feature entity classes.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureEntityClass"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_tool_feature_sq.c.entity_class_id, db_map.ext_tool_feature_sq.c.entity_class_name
-        )
-
-    def filter_query(self, db_map, query):
-        return query.outerjoin(db_map.ext_tool_feature_sq, db_map.ext_tool_feature_sq.c.tool_id == db_map.tool_sq.c.id)
-
-    @staticmethod
-    def name_field():
-        return "entity_class_name"
-
-    @staticmethod
-    def id_field():
-        return "entity_class_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ToolMapping)
-
-
-class ToolFeatureParameterDefinitionMapping(ExportMapping):
-    """Maps tool feature parameter definitions.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureParameterDefinition"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_tool_feature_sq.c.parameter_definition_id, db_map.ext_tool_feature_sq.c.parameter_definition_name
-        )
-
-    @staticmethod
-    def name_field():
-        return "parameter_definition_name"
-
-    @staticmethod
-    def id_field():
-        return "parameter_definition_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ToolFeatureEntityClassMapping)
-
-
-class ToolFeatureRequiredFlagMapping(ExportMapping):
-    """Maps tool feature required flags.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureRequiredFlag"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.ext_tool_feature_sq.c.required)
-
-    @staticmethod
-    def name_field():
-        return "required"
-
-    @staticmethod
-    def id_field():
-        return "required"
-
-
-class ToolFeatureMethodEntityClassMapping(ExportMapping):
-    """Maps tool feature method entity classes.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureMethodEntityClass"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_tool_feature_sq.c.entity_class_id, db_map.ext_tool_feature_sq.c.entity_class_name
-        )
-
-    def filter_query(self, db_map, query):
-        return query.outerjoin(db_map.ext_tool_feature_sq, db_map.ext_tool_feature_sq.c.tool_id == db_map.tool_sq.c.id)
-
-    @staticmethod
-    def name_field():
-        return "entity_class_name"
-
-    @staticmethod
-    def id_field():
-        return "entity_class_id"
-
-
-class ToolFeatureMethodParameterDefinitionMapping(ExportMapping):
-    """Maps tool feature method parameter definitions.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureMethodParameterDefinition"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_tool_feature_sq.c.parameter_definition_id, db_map.ext_tool_feature_sq.c.parameter_definition_name
-        )
-
-    @staticmethod
-    def name_field():
-        return "parameter_definition_name"
-
-    @staticmethod
-    def id_field():
-        return "parameter_definition_id"
-
-
-class ToolFeatureMethodMethodMapping(ExportMapping):
-    """Maps tool feature method methods.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureMethodMethod"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.ext_tool_feature_method_sq.c.method)
-
-    def filter_query(self, db_map, query):
-        return query.outerjoin(
-            db_map.ext_tool_feature_method_sq,
-            and_(
-                db_map.ext_tool_feature_method_sq.c.tool_id == db_map.ext_tool_feature_sq.c.tool_id,
-                db_map.ext_tool_feature_method_sq.c.feature_id == db_map.ext_tool_feature_sq.c.feature_id,
-            ),
-        )
-
-    @staticmethod
-    def name_field():
-        return "method"
-
-    @staticmethod
-    def id_field():
-        return "method"
-
-    def _data(self, db_row):
-        data = super()._data(db_row)
-        return from_database_to_single_value(data, None)
-
-
-class _DescriptionMappingBase(ExportMapping):
-    """Maps descriptions."""
-
-    MAP_TYPE = "Description"
-
-    @staticmethod
-    def name_field():
-        return "description"
-
-    @staticmethod
-    def id_field():
-        return "description"
-
-
-class AlternativeDescriptionMapping(_DescriptionMappingBase):
-    """Maps alternative descriptions.
-
-    Cannot be used as the topmost mapping; must have :class:`AlternativeMapping` as parent.
-    """
-
-    MAP_TYPE = "AlternativeDescription"
-
-
-class ScenarioDescriptionMapping(_DescriptionMappingBase):
-    """Maps scenario descriptions.
-
-    Cannot be used as the topmost mapping; must have :class:`ScenarioMapping` as parent.
-    """
-
-    MAP_TYPE = "ScenarioDescription"
-
-
-class _FilteredQuery:
-    """Helper class to define non-standard query filters.
-
-    It implements everything we use from the standard sqlalchemy's ``Query``.
-    """
-
-    def __init__(self, query, condition):
-        """
-        Args:
-            query (Query): a query to filter
-            condition (function): the filter condition
-        """
-        self._query = query
-        self._condition = condition
-
-    def yield_per(self, count):
-        return _FilteredQuery(self._query.yield_per(count), self._condition)
-
-    def filter(self, *args, **kwargs):
-        return _FilteredQuery(self._query.filter(*args, **kwargs), self._condition)
-
-    def __iter__(self):
-        for db_row in self._query:
-            if self._condition(db_row):
-                yield db_row
-
-
-class _Rewindable:
-    def __init__(self, it):
-        self._it = iter(it)
-        self._seen = []
-        self._seen_it = iter(self._seen)
-
-    def rewind(self):
-        self._seen_it = iter(self._seen)
-
-    def __next__(self):
-        try:
-            return next(self._seen_it)
-        except StopIteration:
-            pass
-        item = next(self._it)
-        self._seen.append(item)
-        return item
-
-    def __iter__(self):
-        return self
-
-
-def pair_header_buddies(root_mapping):
-    """Pairs mappings that have Position.header to their 'buddy' child mappings.
-
-    Args:
-        root_mapping (ExportMapping): root mapping
-
-    Returns:
-        list of tuple: pairs of parent mapping - buddy child mapping
-    """
-
-    @dataclass
-    class Pairable:
-        mapping: ExportMapping
-        paired: bool
-
-    pairables = [Pairable(m, False) for m in root_mapping.flatten()]
-    buddies = list()
-    for i, parent in enumerate(pairables):
-        if parent.mapping.position != Position.header:
-            continue
-        for child in pairables[i + 1 :]:
-            if child.mapping.is_buddy(parent.mapping) and not child.paired:
-                buddies.append((parent.mapping, child.mapping))
-                child.paired = True
-                break
-    return buddies
-
-
-def find_my_buddy(mapping, buddies):
-    """Finds mapping's buddy.
-
-    Args:
-        mapping (ExportMapping): a mapping
-        buddies (list of tuple): list of mapping - buddy mapping pairs
-
-    Returns:
-        ExportMapping: buddy mapping or None if not found
-    """
-    for parent, buddy in buddies:
-        if mapping is parent:
-            return buddy
-    return None
-
-
-def from_dict(serialized):
-    """
-    Deserializes mappings.
-
-    Args:
-        serialized (list): serialized mappings
-
-    Returns:
-        ExportMapping: root mapping
-    """
-    mappings = {
-        klass.MAP_TYPE: klass
-        for klass in (
-            AlternativeDescriptionMapping,
-            AlternativeMapping,
-            DefaultValueIndexNameMapping,
-            ExpandedParameterDefaultValueMapping,
-            ExpandedParameterValueMapping,
-            FeatureEntityClassMapping,
-            FeatureParameterDefinitionMapping,
-            FixedValueMapping,
-            IndexNameMapping,
-            ObjectClassMapping,
-            ObjectGroupMapping,
-            ObjectGroupObjectMapping,
-            ObjectMapping,
-            ParameterDefaultValueIndexMapping,
-            ParameterDefaultValueMapping,
-            ParameterDefaultValueTypeMapping,
-            ParameterDefinitionMapping,
-            ParameterValueIndexMapping,
-            ParameterValueListMapping,
-            ParameterValueListValueMapping,
-            ParameterValueMapping,
-            ParameterValueTypeMapping,
-            RelationshipClassMapping,
-            RelationshipClassObjectClassMapping,
-            RelationshipMapping,
-            RelationshipObjectMapping,
-            ScenarioActiveFlagMapping,
-            ScenarioAlternativeMapping,
-            ScenarioBeforeAlternativeMapping,
-            ScenarioDescriptionMapping,
-            ScenarioMapping,
-            ToolMapping,
-            ToolFeatureEntityClassMapping,
-            ToolFeatureParameterDefinitionMapping,
-            ToolFeatureRequiredFlagMapping,
-            ToolFeatureMethodEntityClassMapping,
-            ToolFeatureMethodParameterDefinitionMapping,
-        )
-    }
-    # Legacy
-    mappings["ParameterIndex"] = ParameterValueIndexMapping
-    if any(m["map_type"] == "RelationshipClassObjectHighlightingMapping" for m in serialized):
-        _upgrade_legacy_object_highlighting_mapping(serialized)
-    flattened = list()
-    for mapping_dict in serialized:
-        position = mapping_dict["position"]
-        if isinstance(position, str):
-            position = Position(position)
-        ignorable = mapping_dict.get("ignorable", False)
-        value = mapping_dict.get("value")
-        header = mapping_dict.get("header", "")
-        filter_re = mapping_dict.get("filter_re", "")
-        flattened.append(
-            mappings[mapping_dict["map_type"]].reconstruct(position, value, header, filter_re, ignorable, mapping_dict)
-        )
-    return unflatten(flattened)
-
-
-def legacy_group_fn_from_dict(serialized):
-    """Restores legacy group_fn attribute from serialized mappings.
-
-    group_fn has been removed from export mappings but this serves for backwards compatibility.
-
-    Args:
-        serialized (list): serialized mappings
-
-    Returns:
-        str: name of the first group_fn attribute that was found in the serialized mappings or NoGroup if not found
-    """
-    for mapping_dict in serialized:
-        group_fn = mapping_dict.get("group_fn")
-        if group_fn is not None:
-            return group_fn
-    return NoGroup.NAME
-
-
-def _upgrade_legacy_object_highlighting_mapping(serialized):
-    """Upgrades legacy object highlighting mappings in place.
-
-    ``RelationshipClassObjectHighlightingMapping`` and ``RelationshipObjectHighlightingMapping``
-    have been replaced by a ``highlight_dimension`` argument in ``RelationshipClassObjectClassMapping``.
-
-    Args:
-        serialized (list of dict): serialized mappings
-    """
-    for mapping_dict in serialized:
-        if mapping_dict["map_type"] == "RelationshipClassObjectHighlightingMapping":
-            mapping_dict["map_type"] = RelationshipClassMapping.MAP_TYPE
-        elif mapping_dict["map_type"] == "RelationshipObjectHighlightingMapping":
-            mapping_dict["map_type"] = RelationshipMapping.MAP_TYPE
-
-
-def _expand_indexed_data(data, mapping):
-    """Expands indexed data and updates the current_leaf attribute.
-
-    Args:
-        data (Any): data to expand
-        mapping (ExportMapping): mapping whose data is being expanded
-
-    Yields:
-        Any: parameter value index
-    """
-    if not isinstance(mapping.parent, _MappingWithLeafMixin):
-        # Get dict
-        current_leaf = from_database(data[0], data[1])
-        if data[1] == "map":
-            current_leaf = convert_containers_to_maps(current_leaf)
-    else:
-        # Get leaf from parent
-        current_leaf = mapping.parent.current_leaf
-    if not isinstance(current_leaf, IndexedValue):
-        # Nothing to expand. Set the current leaf so the child can find it
-        mapping.current_leaf = current_leaf
-        yield None
-        return
-    # Expand and set the current leaf so the child can find it
-    for index, value in zip(current_leaf.indexes, current_leaf.values):
-        mapping.current_leaf = value
-        yield index
-
-
-def _expand_index_names(data, mapping):
-    """Expands index names and updates the current_leaf attribute.
-
-    Args:
-        data (Any): data to expand
-        mapping (ExportMapping): mapping whose data is being expanded
-
-    Yields:
-        str: index name
-    """
-    if not isinstance(mapping.parent, _MappingWithLeafMixin):
-        current_leaf = from_database(data[0], data[1])
-        if data[1] == "map":
-            current_leaf = convert_containers_to_maps(current_leaf)
-    else:
-        current_leaf = mapping.parent.current_leaf
-    mapping.current_leaf = current_leaf
-    yield current_leaf.index_name if isinstance(current_leaf, IndexedValue) else None
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Contains export mappings for database items such as entities, entity classes and parameter values.
+
+"""
+
+from dataclasses import dataclass
+from itertools import cycle, dropwhile, islice
+from sqlalchemy import and_
+from sqlalchemy.sql.expression import literal
+from ..parameter_value import (
+    from_database_to_single_value,
+    from_database,
+    IndexedValue,
+    from_database_to_dimension_count,
+    map_dimensions,
+    convert_containers_to_maps,
+)
+from ..mapping import Mapping, Position, is_pivoted, is_regular, unflatten
+from .group_functions import NoGroup
+
+
+def check_validity(root_mapping):
+    """Checks validity of a mapping hierarchy.
+
+    To check validity of individual mappings withing the hierarchy, use :func:`Mapping.check_validity`.
+
+    Args:
+        root_mapping (Mapping): root mapping
+
+    Returns:
+        list of str: a list of issue descriptions
+    """
+    issues = list()
+    flattened = root_mapping.flatten()
+    non_title_mappings = [m for m in flattened if m.position != Position.table_name]
+    if len(non_title_mappings) == 2 and is_pivoted(non_title_mappings[0].position):
+        issues.append("First mapping cannot be pivoted")
+    return issues
+
+
+class _MappingWithLeafMixin:
+    """Provides current_leaf field."""
+
+    current_leaf = None
+
+
+class ExportMapping(Mapping):
+    _TITLE_SEP = ","
+
+    def __init__(self, position, value=None, header="", filter_re=""):
+        """
+        Args:
+            position (int or Position): column index or Position
+            value (Any, optional): A fixed value
+            header (str); A string column header that's yielded as 'first row', if not empty.
+                The default is an empty string (so it's not yielded).
+            filter_re (str): A regular expression to filter the mapped values by
+        """
+        super().__init__(position, value, filter_re)
+        self._ignorable = False
+        self.header = header
+        self._convert_data = None
+
+    def __eq__(self, other):
+        if not isinstance(other, ExportMapping):
+            return NotImplemented
+        if not super().__eq__(other):
+            return False
+        return self._ignorable == other._ignorable and self.header == other.header
+
+    def check_validity(self):
+        """Checks if mapping is valid.
+
+        Returns:
+            list: a list of issues
+        """
+        issues = list()
+        if self.child is None:
+            is_effective_leaf = True
+        else:
+            is_effective_leaf = any(
+                child.position in (Position.hidden, Position.table_name) for child in self.child.flatten()
+            )
+        if is_effective_leaf and is_pivoted(self.position):
+            issues.append("Cannot be pivoted.")
+        return issues
+
+    def replace_data(self, data):
+        """
+        Replaces the data generated by this item by user given data.
+
+        If data is exhausted, it gets cycled again from the beginning.
+
+        Args:
+            data (Iterable): user data
+        """
+        data_iterator = cycle(data)
+        self._convert_data = lambda _: next(data_iterator)
+
+    @staticmethod
+    def is_buddy(parent):
+        """Checks if mapping uses a parent's state for its data.
+
+        Args:
+            parent (ExportMapping): a parent mapping
+
+        Returns:
+            bool: True if parent's state affects what a mapping yields
+        """
+        return False
+
+    def is_ignorable(self):
+        """Returns True if the mapping is ignorable, False otherwise.
+
+        Returns:
+            bool: True if mapping is ignorable, False otherwise
+        """
+        return self._ignorable
+
+    def set_ignorable(self, ignorable):
+        """
+        Sets mapping as ignorable.
+
+        Mappings that are ignorable map to None if there is no other data to yield.
+        This allows 'incomplete' rows if child mappings do not depend on the ignored mapping.
+
+        Args:
+            ignorable (bool): True to set mapping ignorable, False to unset
+        """
+        self._ignorable = ignorable
+
+    def to_dict(self):
+        """
+        Serializes mapping into dict.
+
+        Returns:
+            dict: serialized mapping
+        """
+        mapping_dict = super().to_dict()
+        if self._ignorable:
+            mapping_dict["ignorable"] = True
+        if self.header:
+            mapping_dict["header"] = self.header
+        return mapping_dict
+
+    @classmethod
+    def reconstruct(cls, position, value, header, filter_re, ignorable, mapping_dict):
+        """
+        Reconstructs mapping.
+
+        Args:
+            position (int or Position, optional): mapping's position
+            value (Any): fixed value
+            header (str, optional): column header
+            filter_re (str): filter regular expression
+            ignorable (bool): ignorable flag
+            mapping_dict (dict): mapping dict
+
+        Returns:
+            Mapping: reconstructed mapping
+        """
+        mapping = cls(position, value, header, filter_re)
+        mapping.set_ignorable(ignorable)
+        return mapping
+
+    def add_query_columns(self, db_map, query):
+        """Adds columns to the mapping query if needed, and returns the new query.
+
+        The base class implementation just returns the same query without adding any new columns.
+
+        Args:
+            db_map (DatabaseMappingBase)
+            query (Alias or dict)
+
+        Returns:
+            Alias: expanded query, or the same if nothing to add.
+        """
+        return query
+
+    def filter_query(self, db_map, query):
+        """Filters the mapping query if needed, and returns the new query.
+
+        The base class implementation just returns the same query without applying any new filters.
+
+        Args:
+            db_map (DatabaseMappingBase)
+            query (Alias or dict)
+
+        Returns:
+            Alias: filtered query, or the same if nothing to add.
+        """
+        return query
+
+    def filter_query_by_title(self, query, title_state):
+        """Filters the query according to the given title state.
+        Note that ``_build_query()`` does some default filtering on the title state after calling this method.
+        Therefore, if a subclass reimplements this method, it needs to delete the consumed keys from ``title_state``
+        so they aren't consumed again by ``_build_query()``.
+
+        The base class implementations just returns the unaltered query.
+
+        Args:
+            title_state (dict)
+
+        Returns:
+            Query or _FilteredQuery
+        """
+        return query
+
+    def _build_query(self, db_map, title_state):
+        """Builds and returns the query to run for this mapping hierarchy.
+
+        Args:
+            db_map (DatabaseMappingBase)
+            title_state (dict)
+
+        Returns:
+            Query
+        """
+        mappings = self.flatten()
+        # Start with empty query
+        qry = db_map.query(literal(None))
+        # Add columns
+        for m in mappings:
+            qry = m.add_query_columns(db_map, qry)
+        # Apply filters
+        for m in mappings:
+            qry = m.filter_query(db_map, qry)
+        # Apply special title filters (first, so we clean up the state)
+        for m in mappings:
+            qry = m.filter_query_by_title(qry, title_state)
+        # Apply standard title filters
+        if not title_state:
+            return qry
+        # Use a _FilteredQuery, since building a subquery to query it again leads to parser stack overflow
+        return _FilteredQuery(
+            qry, lambda db_row: all(getattr(db_row, key) == value for key, value in title_state.items())
+        )
+
+    def _build_title_query(self, db_map):
+        """Builds and returns the query to get titles for this mapping hierarchy.
+
+        Args:
+            db_map (DatabaseMappingBase): database mapping
+
+        Returns:
+            Alias: title query
+        """
+        mappings = self.flatten()
+        for _ in range(len(mappings)):
+            if mappings[-1].position == Position.table_name:
+                break
+            mappings.pop(-1)
+        # Start with empty query
+        qry = db_map.query(literal(None))
+        # Add columns
+        for m in mappings:
+            qry = m.add_query_columns(db_map, qry)
+        # Apply filters
+        for m in mappings:
+            qry = m.filter_query(db_map, qry)
+        return qry
+
+    def _build_header_query(self, db_map, title_state, buddies):
+        """Builds the header query for this mapping hierarchy.
+
+        Args:
+            db_map (DatabaseMappingBase): database mapping
+            title_state (dict): title state
+            buddies (list of tuple): pairs of buddy mappings
+
+        Returns:
+            Alias: header query
+        """
+        mappings = self.flatten()
+        flat_buddies = [b for pair in buddies for b in pair]
+        for _ in range(len(mappings)):
+            m = mappings[-1]
+            if m.position == Position.header or m.position == Position.table_name or m in flat_buddies:
+                break
+            mappings.pop(-1)
+        # Start with empty query
+        qry = db_map.query(literal(None))
+        # Add columns
+        for m in mappings:
+            qry = m.add_query_columns(db_map, qry)
+        # Apply filters
+        for m in mappings:
+            qry = m.filter_query(db_map, qry)
+        # Apply special title filters (first, so we clean up the state)
+        for m in mappings:
+            qry = m.filter_query_by_title(qry, title_state)
+        # Apply standard title filters
+        if not title_state:
+            return qry
+        # Use a _FilteredQuery, since building a subquery to query it again leads to parser stack overflow
+        return _FilteredQuery(
+            qry, lambda db_row: all(getattr(db_row, key) == value for key, value in title_state.items())
+        )
+
+    @staticmethod
+    def name_field():
+        """Returns the 'name' field associated to this mapping within the query.
+        Used to obtain the relevant data from a db row.
+
+        Returns:
+            str
+        """
+        raise NotImplementedError()
+
+    @staticmethod
+    def id_field():
+        """Returns the 'id' field associated to this mapping within the query.
+        Used to compose the title state.
+
+        Returns:
+            str
+        """
+        raise NotImplementedError()
+
+    def _data(self, db_row):  # pylint: disable=arguments-differ
+        """Returns the data relevant to this mapping from given database row.
+
+        The base class implementation returns the field given by ``name_field()``.
+
+        Args:
+            db_row (KeyedTuple)
+
+        Returns:
+            any
+        """
+        return getattr(db_row, self.name_field(), None)
+
+    def _expand_data(self, data):
+        """Takes data from an individual field in the db and yields all data generated by this mapping.
+
+        The base class implementation simply yields the given data.
+        Reimplement in subclasses that need to expand the data into multiple elements (e.g., indexed value mappings).
+
+        Args:
+            data (any)
+
+        Returns:
+            generator(any)
+        """
+        yield data
+
+    def _get_data_iterator(self, data):
+        """Applies regexp filtering and data conversion on the output of ``_expand_data()`` to produce the final data
+        iterator for this mapping.
+
+        Args:
+            data (any)
+
+        Returns:
+            generator(any)
+        """
+        data_iterator = self._expand_data(data)
+        if self._filter_re is not None:
+            data_iterator = (x for x in data_iterator if self._filter_re.search(str(x)))
+        if self._convert_data is not None:
+            data_iterator = (self._convert_data(x) for x in data_iterator)
+        return data_iterator
+
+    def _get_rows(self, db_row):
+        """Yields rows issued by this mapping for given database row.
+
+        Args:
+            db_row (KeyedTuple)
+
+        Returns:
+            generator(dict)
+        """
+        if self.position == Position.table_name:
+            yield {}
+            return
+        data = self._data(db_row)
+        if data is None and not self._ignorable:
+            return ()
+        data_iterator = self._get_data_iterator(data)
+        for data in data_iterator:
+            yield {self.position: data}
+
+    def get_rows_recursive(self, db_row):
+        """Takes a database row and yields rows issued by this mapping and its children combined.
+
+        Args:
+            db_row (KeyedTuple)
+
+        Returns:
+            generator(dict)
+        """
+        if self.child is None:
+            yield from self._get_rows(db_row)
+            return
+        for row in self._get_rows(db_row):
+            for child_row in self.child.get_rows_recursive(db_row):
+                row = row.copy()
+                row.update(child_row)
+                yield row
+
+    def rows(self, db_map, title_state):
+        """Yields rows issued by this mapping and its children combined.
+
+        Args:
+            db_map (DatabaseMappingBase)
+            title_state (dict)
+
+        Returns:
+            generator(dict)
+        """
+        qry = self._build_query(db_map, title_state)
+        for db_row in qry.yield_per(1000):
+            yield from self.get_rows_recursive(db_row)
+
+    def has_titles(self):
+        """Returns True if this mapping or one of its children generates titles.
+
+        Returns:
+            bool: True if mappings generate titles, False otherwise
+        """
+        if self.position == Position.table_name:
+            return True
+        if self.child is not None:
+            return self.child.has_titles()
+        return False
+
+    def _title_state(self, db_row):
+        """Returns the title state associated to this mapping from given database row.
+
+        The base class implementation returns a dict mapping the output of ``id_field()``
+        to the corresponding field from the row.
+
+        Args:
+            db_row (KeyedTuple)
+
+        Returns:
+            dict
+        """
+        id_field = self.id_field()
+        if id_field is None:
+            return {}
+        return {id_field: getattr(db_row, id_field)}
+
+    def _get_titles(self, db_row, limit=None):
+        """Yields pairs (title, title state) issued by this mapping for given database row.
+
+        Args:
+            db_row (KeyedTuple)
+            limit (int, optional): yield only this many items
+
+        Returns:
+            generator(str,dict)
+        """
+        if self.position != Position.table_name:
+            yield "", {}
+            return
+        data = self._data(db_row)
+        title_state = self._title_state(db_row)
+        data_iterator = self._get_data_iterator(data)
+        if limit is not None:
+            data_iterator = islice(data_iterator, limit)
+        for data in data_iterator:
+            if data is None:
+                data = ""
+            yield data, title_state
+
+    def get_titles_recursive(self, db_row, limit=None):
+        """Takes a database row and yields pairs (title, title state) issued by this mapping and its children combined.
+
+        Args:
+            db_row (KeyedTuple)
+            limit (int, optional): yield only this many items
+
+        Returns:
+            generator(str,dict)
+        """
+        if self.child is None:
+            yield from self._get_titles(db_row, limit=limit)
+            return
+        for title, title_state in self._get_titles(db_row, limit=limit):
+            for child_title, child_title_state in self.child.get_titles_recursive(db_row, limit=limit):
+                title_sep = self._TITLE_SEP if title and child_title else ""
+                final_title = title + title_sep + child_title
+                yield final_title, {**title_state, **child_title_state}
+
+    def _non_unique_titles(self, db_map, limit=None):
+        """Yields all titles, not necessarily unique, and associated state dictionaries.
+
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            limit (int, optional): yield only this many items
+
+        Yields:
+            tuple(str,dict): title, and associated title state dictionary
+        """
+        qry = self._build_title_query(db_map)
+        for db_row in qry.yield_per(1000):
+            yield from self.get_titles_recursive(db_row, limit=limit)
+
+    def titles(self, db_map, limit=None):
+        """Yields unique titles and associated state dictionaries.
+
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            limit (int, optional): yield only this many items
+
+        Yields:
+            tuple(str,dict): unique title, and associated title state dictionary
+        """
+        titles = {}
+        for title, title_state in self._non_unique_titles(db_map, limit=limit):
+            titles.setdefault(title, {}).update(title_state)
+        yield from titles.items()
+
+    def has_header(self):
+        """Recursively checks if mapping would create a header row.
+
+        Returns:
+            bool: True if make_header() would return something useful
+        """
+        if self.header or self.position == Position.header:
+            return True
+        if self.child is None:
+            return False
+        return self.child.has_header()
+
+    def make_header_recursive(self, query, buddies):
+        """Builds the header recursively.
+
+        Args:
+            build_header_query (callable): a function that any mapping in the hierarchy can call to get the query
+            db_map (DatabaseMappingBase): database map
+            title_state (dict): title state
+            buddies (list of tuple): buddy mappings
+
+        Returns
+            dict: a mapping from column index to string header
+        """
+        if self.child is None:
+            if not is_regular(self.position):
+                return {}
+            return {self.position: self.header}
+        header = self.child.make_header_recursive(query, buddies)
+        if self.position == Position.header:
+            buddy = find_my_buddy(self, buddies)
+            if buddy is not None:
+                query.rewind()
+                header[buddy.position] = next(
+                    (x for db_row in query for x in self._get_data_iterator(self._data(db_row))), ""
+                )
+        else:
+            header[self.position] = self.header
+        return header
+
+    def make_header(self, db_map, title_state, buddies):
+        """Returns the header for this mapping.
+
+        Args:
+            db_map (DatabaseMappingBase): database map
+            title_state (dict): title state
+            buddies (list of tuple): buddy mappings
+
+        Returns
+            dict: a mapping from column index to string header
+        """
+        query = _Rewindable(self._build_header_query(db_map, title_state, buddies).yield_per(1000))
+        return self.make_header_recursive(query, buddies)
+
+
+def drop_non_positioned_tail(root_mapping):
+    """Makes a modified mapping hierarchy without hidden tail mappings.
+
+    This enables pivot tables to work correctly in certain situations.
+
+    Args:
+        root_mapping (Mapping): root mapping
+
+    Returns:
+        Mapping: modified mapping hierarchy
+    """
+    mappings = root_mapping.flatten()
+    return unflatten(
+        reversed(list(dropwhile(lambda m: m.position == Position.hidden and not m.filter_re, reversed(mappings))))
+    )
+
+
+class FixedValueMapping(ExportMapping):
+    """Always yields a fixed value.
+
+    Can be used as the topmost mapping.
+
+    """
+
+    MAP_TYPE = "FixedValue"
+
+    def __init__(self, position, value, header="", filter_re=""):
+        """
+        Args:
+            position (int or Position, optional): mapping's position
+            value (Any): value to yield
+            header (str, optional); A string column header that's yielt as 'first row', if not empty.
+                The default is an empty string (so it's not yielt).
+            filter_re (str, optional): A regular expression to filter the mapped values by
+        """
+        super().__init__(position, value, header, filter_re)
+
+    @staticmethod
+    def name_field():
+        return None
+
+    @staticmethod
+    def id_field():
+        return None
+
+
+class ObjectClassMapping(ExportMapping):
+    """Maps object classes.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "ObjectClass"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.object_class_sq.c.id.label("object_class_id"),
+            db_map.object_class_sq.c.name.label("object_class_name"),
+        )
+
+    @staticmethod
+    def name_field():
+        return "object_class_name"
+
+    @staticmethod
+    def id_field():
+        # Use the class name here, for the sake of the standard excel export
+        return "object_class_name"
+
+
+class ObjectMapping(ExportMapping):
+    """Maps objects.
+
+    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectClassMapping`.
+    """
+
+    MAP_TYPE = "Object"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(db_map.object_sq.c.id.label("object_id"), db_map.object_sq.c.name.label("object_name"))
+
+    def filter_query(self, db_map, query):
+        return query.outerjoin(db_map.object_sq, db_map.object_sq.c.class_id == db_map.object_class_sq.c.id)
+
+    @staticmethod
+    def name_field():
+        return "object_name"
+
+    @staticmethod
+    def id_field():
+        return "object_id"
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, ObjectClassMapping)
+
+
+class ObjectGroupMapping(ExportMapping):
+    """Maps object groups.
+
+    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectClassMapping`.
+    """
+
+    MAP_TYPE = "ObjectGroup"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(db_map.ext_entity_group_sq.c.group_id, db_map.ext_entity_group_sq.c.group_name)
+
+    def filter_query(self, db_map, query):
+        return query.outerjoin(
+            db_map.ext_entity_group_sq, db_map.ext_entity_group_sq.c.class_id == db_map.object_class_sq.c.id
+        ).distinct()
+
+    @staticmethod
+    def name_field():
+        return "group_name"
+
+    @staticmethod
+    def id_field():
+        return "group_id"
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, ObjectClassMapping)
+
+
+class ObjectGroupObjectMapping(ExportMapping):
+    """Maps objects in object groups.
+
+    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectGroupMapping`.
+    """
+
+    MAP_TYPE = "ObjectGroupObject"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(db_map.object_sq.c.id.label("object_id"), db_map.object_sq.c.name.label("object_name"))
+
+    def filter_query(self, db_map, query):
+        return query.filter(db_map.ext_entity_group_sq.c.member_id == db_map.object_sq.c.id)
+
+    @staticmethod
+    def name_field():
+        return "object_name"
+
+    @staticmethod
+    def id_field():
+        return "object_id"
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, ObjectGroupMapping)
+
+
+class RelationshipClassMapping(ExportMapping):
+    """Maps relationships classes.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "RelationshipClass"
+
+    def __init__(self, position, value=None, header="", filter_re="", highlight_dimension=None):
+        super().__init__(position, value, header, filter_re)
+        self.highlight_dimension = highlight_dimension
+
+    def add_query_columns(self, db_map, query):
+        query = query.add_columns(
+            db_map.wide_relationship_class_sq.c.id.label("relationship_class_id"),
+            db_map.wide_relationship_class_sq.c.name.label("relationship_class_name"),
+            db_map.wide_relationship_class_sq.c.object_class_id_list,
+            db_map.wide_relationship_class_sq.c.object_class_name_list,
+        )
+        if self.highlight_dimension is not None:
+            query = query.add_columns(
+                db_map.ext_relationship_class_sq.c.object_class_id.label("highlighted_object_class_id")
+            )
+        return query
+
+    def filter_query(self, db_map, query):
+        if self.highlight_dimension is not None:
+            query = query.outerjoin(
+                db_map.ext_relationship_class_sq,
+                db_map.ext_relationship_class_sq.c.id == db_map.wide_relationship_class_sq.c.id,
+            ).filter(db_map.ext_relationship_class_sq.c.dimension == self.highlight_dimension)
+        return query
+
+    @staticmethod
+    def name_field():
+        return "relationship_class_name"
+
+    @staticmethod
+    def id_field():
+        # Use the class name here, for the sake of the standard excel export
+        return "relationship_class_name"
+
+    def query_parents(self, what):
+        if what == "dimension":
+            return -1
+        if what == "highlight_dimension":
+            return self.highlight_dimension
+        return super().query_parents(what)
+
+    def _title_state(self, db_row):
+        state = super()._title_state(db_row)
+        state["object_class_id_list"] = getattr(db_row, "object_class_id_list")
+        return state
+
+    def to_dict(self):
+        mapping_dict = super().to_dict()
+        if self.highlight_dimension is not None:
+            mapping_dict["highlight_dimension"] = self.highlight_dimension
+        return mapping_dict
+
+    @classmethod
+    def reconstruct(cls, position, value, header, filter_re, ignorable, mapping_dict):
+        highlight_dimension = mapping_dict.get("highlight_dimension")
+        mapping = cls(position, value, header, filter_re, highlight_dimension)
+        mapping.set_ignorable(ignorable)
+        return mapping
+
+
+class RelationshipClassObjectClassMapping(ExportMapping):
+    """Maps relationship class object classes.
+
+    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
+    """
+
+    MAP_TYPE = "RelationshipClassObjectClass"
+    _cached_dimension = None
+
+    @staticmethod
+    def name_field():
+        return "object_class_name_list"
+
+    @staticmethod
+    def id_field():
+        return "object_class_id_list"
+
+    def _data(self, db_row):
+        data = super()._data(db_row).split(",")
+        if self._cached_dimension is None:
+            self._cached_dimension = self.query_parents("dimension")
+        try:
+            return data[self._cached_dimension]
+        except IndexError:
+            return ""
+
+    def query_parents(self, what):
+        if what != "dimension":
+            return super().query_parents(what)
+        return self.parent.query_parents(what) + 1
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, RelationshipClassMapping)
+
+
+class RelationshipMapping(ExportMapping):
+    """Maps relationships.
+
+    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
+    """
+
+    MAP_TYPE = "Relationship"
+
+    def add_query_columns(self, db_map, query):
+        query = query.add_columns(
+            db_map.wide_relationship_sq.c.id.label("relationship_id"),
+            db_map.wide_relationship_sq.c.name.label("relationship_name"),
+            db_map.wide_relationship_sq.c.object_id_list,
+            db_map.wide_relationship_sq.c.object_name_list,
+        )
+        if self.query_parents("highlight_dimension") is not None:
+            query = query.add_columns(db_map.ext_relationship_sq.c.object_id.label("highlighted_object_id"))
+        return query
+
+    def filter_query(self, db_map, query):
+        query = query.outerjoin(
+            db_map.wide_relationship_sq,
+            db_map.wide_relationship_sq.c.class_id == db_map.wide_relationship_class_sq.c.id,
+        )
+        if (highlight_dimension := self.query_parents("highlight_dimension")) is not None:
+            query = query.outerjoin(
+                db_map.ext_relationship_sq, db_map.ext_relationship_sq.c.id == db_map.wide_relationship_sq.c.id
+            ).filter(db_map.ext_relationship_sq.c.dimension == highlight_dimension)
+        return query
+
+    @staticmethod
+    def name_field():
+        return "relationship_name"
+
+    @staticmethod
+    def id_field():
+        return "relationship_id"
+
+    def query_parents(self, what):
+        if what != "dimension":
+            return super().query_parents(what)
+        return -1
+
+    def _title_state(self, db_row):
+        state = super()._title_state(db_row)
+        state["object_id_list"] = getattr(db_row, "object_id_list")
+        return state
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, RelationshipClassMapping)
+
+
+class RelationshipObjectMapping(ExportMapping):
+    """Maps relationship's objects.
+
+    Cannot be used as the topmost mapping; must have :class:`RelationshipClassMapping` and :class:`RelationshipMapping`
+    as parents.
+    """
+
+    MAP_TYPE = "RelationshipObject"
+    _cached_dimension = None
+
+    @staticmethod
+    def name_field():
+        return "object_name_list"
+
+    @staticmethod
+    def id_field():
+        return "object_id_list"
+
+    def _data(self, db_row):
+        data = super()._data(db_row).split(",")
+        if self._cached_dimension is None:
+            self._cached_dimension = self.query_parents("dimension")
+        try:
+            return data[self._cached_dimension]
+        except IndexError:
+            return ""
+
+    def query_parents(self, what):
+        if what != "dimension":
+            return super().query_parents(what)
+        return self.parent.query_parents(what) + 1
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, RelationshipClassObjectClassMapping)
+
+
+class ParameterDefinitionMapping(ExportMapping):
+    """Maps parameter definitions.
+
+    Cannot be used as the topmost mapping; must have an entity class mapping as one of parents.
+    """
+
+    MAP_TYPE = "ParameterDefinition"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.parameter_definition_sq.c.id.label("parameter_definition_id"),
+            db_map.parameter_definition_sq.c.name.label("parameter_definition_name"),
+        )
+
+    def filter_query(self, db_map, query):
+        column_names = {c["name"] for c in query.column_descriptions}
+        if "object_class_id" in column_names:
+            return query.outerjoin(
+                db_map.parameter_definition_sq,
+                db_map.parameter_definition_sq.c.object_class_id == db_map.object_class_sq.c.id,
+            )
+        if "relationship_class_id" in column_names:
+            if self.query_parents("highlight_dimension") is not None:
+                return query.outerjoin(
+                    db_map.parameter_definition_sq,
+                    db_map.parameter_definition_sq.c.object_class_id
+                    == db_map.ext_relationship_class_sq.c.object_class_id,
+                )
+            return query.outerjoin(
+                db_map.parameter_definition_sq,
+                db_map.parameter_definition_sq.c.relationship_class_id == db_map.wide_relationship_class_sq.c.id,
+            )
+        raise RuntimeError("Logic error: this code should be unreachable.")
+
+    @staticmethod
+    def name_field():
+        return "parameter_definition_name"
+
+    @staticmethod
+    def id_field():
+        return "parameter_definition_id"
+
+
+class ParameterDefaultValueMapping(ExportMapping):
+    """Maps scalar (non-indexed) default values
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
+    """
+
+    MAP_TYPE = "ParameterDefaultValue"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.parameter_definition_sq.c.default_value, db_map.parameter_definition_sq.c.default_type
+        )
+
+    @staticmethod
+    def name_field():
+        return None
+
+    @staticmethod
+    def id_field():
+        return None
+
+    def _data(self, db_row):
+        return from_database_to_single_value(db_row.default_value, db_row.default_type)
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, ParameterDefinitionMapping)
+
+
+class ParameterDefaultValueTypeMapping(ParameterDefaultValueMapping):
+    """Maps parameter value types.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
+    an :class:`AlternativeMapping` as parents.
+    """
+
+    MAP_TYPE = "ParameterDefaultValueType"
+
+    def _data(self, db_row):
+        type_ = db_row.default_type
+        if type_ == "map":
+            return f"{map_dimensions(from_database(db_row.default_value, type_))}d_map"
+        if type_ in ("time_series", "time_pattern", "array"):
+            return type_
+        return "single_value"
+
+    def _title_state(self, db_row):
+        return {
+            "type_and_dimensions": (
+                db_row.default_type,
+                from_database_to_dimension_count(db_row.default_value, db_row.default_type),
+            )
+        }
+
+    def filter_query_by_title(self, query, title_state):
+        pv = title_state.pop("type_and_dimensions", None)
+        if pv is None:
+            return query
+        if "default_value" not in {c["name"] for c in query.column_descriptions}:
+            return query
+        return _FilteredQuery(
+            query,
+            lambda db_row: (
+                db_row.default_type,
+                from_database_to_dimension_count(db_row.default_value, db_row.default_type) == pv,
+            ),
+        )
+
+
+class DefaultValueIndexNameMapping(_MappingWithLeafMixin, ParameterDefaultValueMapping):
+    """Maps parameter default value index names.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as a parent.
+    """
+
+    MAP_TYPE = "DefaultValueIndexName"
+
+    def _data(self, db_row):
+        return db_row.default_value, db_row.default_type
+
+    def _expand_data(self, data):
+        yield from _expand_index_names(data, self)
+
+
+class ParameterDefaultValueIndexMapping(_MappingWithLeafMixin, ExportMapping):
+    """Maps default value indexes.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
+    """
+
+    MAP_TYPE = "ParameterDefaultValueIndex"
+
+    def add_query_columns(self, db_map, query):
+        if "default_value" in {c["name"] for c in query.column_descriptions}:
+            return query
+        return query.add_columns(
+            db_map.parameter_definition_sq.c.default_value, db_map.parameter_definition_sq.c.default_type
+        )
+
+    def _expand_data(self, data):
+        yield from _expand_indexed_data(data, self)
+
+    @staticmethod
+    def name_field():
+        return None
+
+    @staticmethod
+    def id_field():
+        return None
+
+    def _data(self, db_row):
+        return db_row.default_value, db_row.default_type
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, DefaultValueIndexNameMapping)
+
+
+class ExpandedParameterDefaultValueMapping(ExportMapping):
+    """Maps indexed default values.
+
+    Whenever this mapping is a child of :class:`ParameterDefaultValueIndexMapping`, it maps individual values of
+    indexed parameters.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
+    """
+
+    MAP_TYPE = "ExpandedDefaultValue"
+
+    @staticmethod
+    def name_field():
+        return "default_value"
+
+    @staticmethod
+    def id_field():
+        return "default_value"
+
+    def _data(self, db_row):
+        value = self.parent.current_leaf
+        return value if not isinstance(value, IndexedValue) else value.VALUE_TYPE
+
+
+class ParameterValueMapping(ExportMapping):
+    """Maps scalar (non-indexed) parameter values.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
+    an :class:`AlternativeMapping` as parents.
+    """
+
+    MAP_TYPE = "ParameterValue"
+    _selects_value = False
+
+    def add_query_columns(self, db_map, query):
+        if "value" in {c["name"] for c in query.column_descriptions}:
+            return query
+        self._selects_value = True
+        return query.add_columns(db_map.parameter_value_sq.c.value, db_map.parameter_value_sq.c.type)
+
+    def filter_query(self, db_map, query):
+        if not self._selects_value:
+            return query
+        column_names = {c["name"] for c in query.column_descriptions}
+        if "object_id" in column_names:
+            return query.filter(
+                and_(
+                    db_map.parameter_value_sq.c.object_id == db_map.object_sq.c.id,
+                    db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
+                )
+            )
+        if "relationship_id" in column_names:
+            if self.query_parents("highlight_dimension") is not None:
+                return query.filter(
+                    and_(
+                        db_map.parameter_value_sq.c.object_id == db_map.ext_relationship_sq.c.object_id,
+                        db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
+                    )
+                )
+            return query.filter(
+                and_(
+                    db_map.parameter_value_sq.c.relationship_id == db_map.wide_relationship_sq.c.id,
+                    db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
+                )
+            )
+        raise RuntimeError("Logic error: this code should be unreachable.")
+
+    @staticmethod
+    def name_field():
+        return None
+
+    @staticmethod
+    def id_field():
+        return None
+
+    def _data(self, db_row):
+        return from_database_to_single_value(db_row.value, db_row.type)
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, (ParameterDefinitionMapping, ObjectMapping, RelationshipMapping, AlternativeMapping))
+
+
+class ParameterValueTypeMapping(ParameterValueMapping):
+    """Maps parameter value types.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
+    an :class:`AlternativeMapping` as parents.
+    """
+
+    MAP_TYPE = "ParameterValueType"
+
+    def _data(self, db_row):
+        type_ = db_row.type
+        if type_ == "map":
+            return f"{map_dimensions(from_database(db_row.value, type_))}d_map"
+        if type_ in ("time_series", "time_pattern", "array"):
+            return type_
+        return "single_value"
+
+    def _title_state(self, db_row):
+        return {"type_and_dimensions": (db_row.type, from_database_to_dimension_count(db_row.value, db_row.type))}
+
+    def filter_query_by_title(self, query, title_state):
+        pv = title_state.pop("type_and_dimensions", None)
+        if pv is None:
+            return query
+        if "value" not in {c["name"] for c in query.column_descriptions}:
+            return query
+        return _FilteredQuery(
+            query, lambda db_row: (db_row.type, from_database_to_dimension_count(db_row.value, db_row.type) == pv)
+        )
+
+
+class IndexNameMapping(_MappingWithLeafMixin, ParameterValueMapping):
+    """Maps parameter value index names.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
+    an :class:`AlternativeMapping` as parents.
+    """
+
+    MAP_TYPE = "IndexName"
+
+    def _data(self, db_row):
+        return db_row.value, db_row.type
+
+    def _expand_data(self, data):
+        yield from _expand_index_names(data, self)
+
+
+class ParameterValueIndexMapping(_MappingWithLeafMixin, ParameterValueMapping):
+    """Maps parameter value indexes.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
+    an :class:`AlternativeMapping` as parents.
+    """
+
+    MAP_TYPE = "ParameterValueIndex"
+
+    def _data(self, db_row):
+        return db_row.value, db_row.type
+
+    def _expand_data(self, data):
+        yield from _expand_indexed_data(data, self)
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, IndexNameMapping)
+
+
+class ExpandedParameterValueMapping(ExportMapping):
+    """Maps parameter values.
+
+    Whenever this mapping is a child of :class:`ParameterValueIndexMapping`, it maps individual values of indexed
+    parameters.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
+    an :class:`AlternativeMapping` as parents.
+    """
+
+    MAP_TYPE = "ExpandedValue"
+
+    @staticmethod
+    def name_field():
+        return "value"
+
+    @staticmethod
+    def id_field():
+        return "value"
+
+    def _data(self, db_row):
+        value = self.parent.current_leaf
+        return value if not isinstance(value, IndexedValue) else value.VALUE_TYPE
+
+
+class ParameterValueListMapping(ExportMapping):
+    """Maps parameter value list names.
+
+    Can be used as the topmost mapping; in case the mapping has a :class:`ParameterDefinitionMapping` as parent,
+    yields value list name for that parameter definition.
+    """
+
+    MAP_TYPE = "ParameterValueList"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.parameter_value_list_sq.c.id.label("parameter_value_list_id"),
+            db_map.parameter_value_list_sq.c.name.label("parameter_value_list_name"),
+        )
+
+    def filter_query(self, db_map, query):
+        if self.parent is None:
+            return query
+        return query.outerjoin(
+            db_map.parameter_value_list_sq,
+            db_map.parameter_value_list_sq.c.id == db_map.parameter_definition_sq.c.parameter_value_list_id,
+        )
+
+    @staticmethod
+    def name_field():
+        return "parameter_value_list_name"
+
+    @staticmethod
+    def id_field():
+        return "parameter_value_list_id"
+
+
+class ParameterValueListValueMapping(ExportMapping):
+    """Maps parameter value list values.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterValueListMapping` as parent.
+
+    """
+
+    MAP_TYPE = "ParameterValueListValue"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(db_map.ord_list_value_sq.c.value, db_map.ord_list_value_sq.c.type)
+
+    def filter_query(self, db_map, query):
+        return query.filter(db_map.ord_list_value_sq.c.parameter_value_list_id == db_map.parameter_value_list_sq.c.id)
+
+    @staticmethod
+    def name_field():
+        return None
+
+    @staticmethod
+    def id_field():
+        return None
+
+    def _data(self, db_row):
+        return from_database_to_single_value(db_row.value, db_row.type)
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, ParameterValueListMapping)
+
+
+class AlternativeMapping(ExportMapping):
+    """Maps alternatives.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "Alternative"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.alternative_sq.c.id.label("alternative_id"),
+            db_map.alternative_sq.c.name.label("alternative_name"),
+            db_map.alternative_sq.c.description.label("description"),
+        )
+
+    def filter_query(self, db_map, query):
+        if self.parent is None:
+            return query
+        return query.filter(db_map.alternative_sq.c.id == db_map.parameter_value_sq.c.alternative_id)
+
+    @staticmethod
+    def name_field():
+        return "alternative_name"
+
+    @staticmethod
+    def id_field():
+        return "alternative_id"
+
+
+class ScenarioMapping(ExportMapping):
+    """Maps scenarios.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "Scenario"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.scenario_sq.c.id.label("scenario_id"),
+            db_map.scenario_sq.c.name.label("scenario_name"),
+            db_map.scenario_sq.c.description.label("description"),
+        )
+
+    @staticmethod
+    def name_field():
+        return "scenario_name"
+
+    @staticmethod
+    def id_field():
+        return "scenario_id"
+
+
+class ScenarioActiveFlagMapping(ExportMapping):
+    """Maps scenario active flags.
+
+    Cannot be used as the topmost mapping; must have a :class:`ScenarioMapping` as parent.
+    """
+
+    MAP_TYPE = "ScenarioActiveFlag"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(db_map.scenario_sq.c.active)
+
+    @staticmethod
+    def name_field():
+        return "active"
+
+    @staticmethod
+    def id_field():
+        return "active"
+
+
+class ScenarioAlternativeMapping(ExportMapping):
+    """Maps scenario alternatives.
+
+    Cannot be used as the topmost mapping; must have a :class:`ScenarioMapping` as parent.
+    """
+
+    MAP_TYPE = "ScenarioAlternative"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.ext_linked_scenario_alternative_sq.c.alternative_id,
+            db_map.ext_linked_scenario_alternative_sq.c.alternative_name,
+        )
+
+    def filter_query(self, db_map, query):
+        return query.outerjoin(
+            db_map.ext_linked_scenario_alternative_sq,
+            db_map.ext_linked_scenario_alternative_sq.c.scenario_id == db_map.scenario_sq.c.id,
+        )
+
+    @staticmethod
+    def name_field():
+        return "alternative_name"
+
+    @staticmethod
+    def id_field():
+        return "alternative_id"
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, ScenarioMapping)
+
+
+class ScenarioBeforeAlternativeMapping(ExportMapping):
+    """Maps scenario 'before' alternatives.
+
+    Cannot be used as the topmost mapping; must have a :class:`ScenarioAlternativeMapping` as parent.
+    """
+
+    MAP_TYPE = "ScenarioBeforeAlternative"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.ext_linked_scenario_alternative_sq.c.before_alternative_id,
+            db_map.ext_linked_scenario_alternative_sq.c.before_alternative_name,
+        )
+
+    @staticmethod
+    def name_field():
+        return "before_alternative_name"
+
+    @staticmethod
+    def id_field():
+        return "before_alternative_id"
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, ScenarioAlternativeMapping)
+
+
+class FeatureEntityClassMapping(ExportMapping):
+    """Maps feature entity classes.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "FeatureEntityClass"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(db_map.ext_feature_sq.c.entity_class_id, db_map.ext_feature_sq.c.entity_class_name)
+
+    @staticmethod
+    def name_field():
+        return "entity_class_name"
+
+    @staticmethod
+    def id_field():
+        return "entity_class_id"
+
+
+class FeatureParameterDefinitionMapping(ExportMapping):
+    """Maps feature parameter definitions.
+
+    Cannot be used as the topmost mapping; must have a :class:`FeatureEntityClassMapping` as parent.
+    """
+
+    MAP_TYPE = "FeatureParameterDefinition"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.ext_feature_sq.c.parameter_definition_id, db_map.ext_feature_sq.c.parameter_definition_name
+        )
+
+    @staticmethod
+    def name_field():
+        return "parameter_definition_name"
+
+    @staticmethod
+    def id_field():
+        return "parameter_definition_id"
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, FeatureEntityClassMapping)
+
+
+class ToolMapping(ExportMapping):
+    """Maps tools.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "Tool"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(db_map.tool_sq.c.id.label("tool_id"), db_map.tool_sq.c.name.label("tool_name"))
+
+    @staticmethod
+    def name_field():
+        return "tool_name"
+
+    @staticmethod
+    def id_field():
+        return "tool_id"
+
+
+class ToolFeatureEntityClassMapping(ExportMapping):
+    """Maps tool feature entity classes.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureEntityClass"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.ext_tool_feature_sq.c.entity_class_id, db_map.ext_tool_feature_sq.c.entity_class_name
+        )
+
+    def filter_query(self, db_map, query):
+        return query.outerjoin(db_map.ext_tool_feature_sq, db_map.ext_tool_feature_sq.c.tool_id == db_map.tool_sq.c.id)
+
+    @staticmethod
+    def name_field():
+        return "entity_class_name"
+
+    @staticmethod
+    def id_field():
+        return "entity_class_id"
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, ToolMapping)
+
+
+class ToolFeatureParameterDefinitionMapping(ExportMapping):
+    """Maps tool feature parameter definitions.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureParameterDefinition"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.ext_tool_feature_sq.c.parameter_definition_id, db_map.ext_tool_feature_sq.c.parameter_definition_name
+        )
+
+    @staticmethod
+    def name_field():
+        return "parameter_definition_name"
+
+    @staticmethod
+    def id_field():
+        return "parameter_definition_id"
+
+    @staticmethod
+    def is_buddy(parent):
+        return isinstance(parent, ToolFeatureEntityClassMapping)
+
+
+class ToolFeatureRequiredFlagMapping(ExportMapping):
+    """Maps tool feature required flags.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureRequiredFlag"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(db_map.ext_tool_feature_sq.c.required)
+
+    @staticmethod
+    def name_field():
+        return "required"
+
+    @staticmethod
+    def id_field():
+        return "required"
+
+
+class ToolFeatureMethodEntityClassMapping(ExportMapping):
+    """Maps tool feature method entity classes.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureMethodEntityClass"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.ext_tool_feature_sq.c.entity_class_id, db_map.ext_tool_feature_sq.c.entity_class_name
+        )
+
+    def filter_query(self, db_map, query):
+        return query.outerjoin(db_map.ext_tool_feature_sq, db_map.ext_tool_feature_sq.c.tool_id == db_map.tool_sq.c.id)
+
+    @staticmethod
+    def name_field():
+        return "entity_class_name"
+
+    @staticmethod
+    def id_field():
+        return "entity_class_id"
+
+
+class ToolFeatureMethodParameterDefinitionMapping(ExportMapping):
+    """Maps tool feature method parameter definitions.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureMethodParameterDefinition"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(
+            db_map.ext_tool_feature_sq.c.parameter_definition_id, db_map.ext_tool_feature_sq.c.parameter_definition_name
+        )
+
+    @staticmethod
+    def name_field():
+        return "parameter_definition_name"
+
+    @staticmethod
+    def id_field():
+        return "parameter_definition_id"
+
+
+class ToolFeatureMethodMethodMapping(ExportMapping):
+    """Maps tool feature method methods.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureMethodMethod"
+
+    def add_query_columns(self, db_map, query):
+        return query.add_columns(db_map.ext_tool_feature_method_sq.c.method)
+
+    def filter_query(self, db_map, query):
+        return query.outerjoin(
+            db_map.ext_tool_feature_method_sq,
+            and_(
+                db_map.ext_tool_feature_method_sq.c.tool_id == db_map.ext_tool_feature_sq.c.tool_id,
+                db_map.ext_tool_feature_method_sq.c.feature_id == db_map.ext_tool_feature_sq.c.feature_id,
+            ),
+        )
+
+    @staticmethod
+    def name_field():
+        return "method"
+
+    @staticmethod
+    def id_field():
+        return "method"
+
+    def _data(self, db_row):
+        data = super()._data(db_row)
+        return from_database_to_single_value(data, None)
+
+
+class _DescriptionMappingBase(ExportMapping):
+    """Maps descriptions."""
+
+    MAP_TYPE = "Description"
+
+    @staticmethod
+    def name_field():
+        return "description"
+
+    @staticmethod
+    def id_field():
+        return "description"
+
+
+class AlternativeDescriptionMapping(_DescriptionMappingBase):
+    """Maps alternative descriptions.
+
+    Cannot be used as the topmost mapping; must have :class:`AlternativeMapping` as parent.
+    """
+
+    MAP_TYPE = "AlternativeDescription"
+
+
+class ScenarioDescriptionMapping(_DescriptionMappingBase):
+    """Maps scenario descriptions.
+
+    Cannot be used as the topmost mapping; must have :class:`ScenarioMapping` as parent.
+    """
+
+    MAP_TYPE = "ScenarioDescription"
+
+
+class _FilteredQuery:
+    """Helper class to define non-standard query filters.
+
+    It implements everything we use from the standard sqlalchemy's ``Query``.
+    """
+
+    def __init__(self, query, condition):
+        """
+        Args:
+            query (Query): a query to filter
+            condition (function): the filter condition
+        """
+        self._query = query
+        self._condition = condition
+
+    def yield_per(self, count):
+        return _FilteredQuery(self._query.yield_per(count), self._condition)
+
+    def filter(self, *args, **kwargs):
+        return _FilteredQuery(self._query.filter(*args, **kwargs), self._condition)
+
+    def __iter__(self):
+        for db_row in self._query:
+            if self._condition(db_row):
+                yield db_row
+
+
+class _Rewindable:
+    def __init__(self, it):
+        self._it = iter(it)
+        self._seen = []
+        self._seen_it = iter(self._seen)
+
+    def rewind(self):
+        self._seen_it = iter(self._seen)
+
+    def __next__(self):
+        try:
+            return next(self._seen_it)
+        except StopIteration:
+            pass
+        item = next(self._it)
+        self._seen.append(item)
+        return item
+
+    def __iter__(self):
+        return self
+
+
+def pair_header_buddies(root_mapping):
+    """Pairs mappings that have Position.header to their 'buddy' child mappings.
+
+    Args:
+        root_mapping (ExportMapping): root mapping
+
+    Returns:
+        list of tuple: pairs of parent mapping - buddy child mapping
+    """
+
+    @dataclass
+    class Pairable:
+        mapping: ExportMapping
+        paired: bool
+
+    pairables = [Pairable(m, False) for m in root_mapping.flatten()]
+    buddies = list()
+    for i, parent in enumerate(pairables):
+        if parent.mapping.position != Position.header:
+            continue
+        for child in pairables[i + 1 :]:
+            if child.mapping.is_buddy(parent.mapping) and not child.paired:
+                buddies.append((parent.mapping, child.mapping))
+                child.paired = True
+                break
+    return buddies
+
+
+def find_my_buddy(mapping, buddies):
+    """Finds mapping's buddy.
+
+    Args:
+        mapping (ExportMapping): a mapping
+        buddies (list of tuple): list of mapping - buddy mapping pairs
+
+    Returns:
+        ExportMapping: buddy mapping or None if not found
+    """
+    for parent, buddy in buddies:
+        if mapping is parent:
+            return buddy
+    return None
+
+
+def from_dict(serialized):
+    """
+    Deserializes mappings.
+
+    Args:
+        serialized (list): serialized mappings
+
+    Returns:
+        ExportMapping: root mapping
+    """
+    mappings = {
+        klass.MAP_TYPE: klass
+        for klass in (
+            AlternativeDescriptionMapping,
+            AlternativeMapping,
+            DefaultValueIndexNameMapping,
+            ExpandedParameterDefaultValueMapping,
+            ExpandedParameterValueMapping,
+            FeatureEntityClassMapping,
+            FeatureParameterDefinitionMapping,
+            FixedValueMapping,
+            IndexNameMapping,
+            ObjectClassMapping,
+            ObjectGroupMapping,
+            ObjectGroupObjectMapping,
+            ObjectMapping,
+            ParameterDefaultValueIndexMapping,
+            ParameterDefaultValueMapping,
+            ParameterDefaultValueTypeMapping,
+            ParameterDefinitionMapping,
+            ParameterValueIndexMapping,
+            ParameterValueListMapping,
+            ParameterValueListValueMapping,
+            ParameterValueMapping,
+            ParameterValueTypeMapping,
+            RelationshipClassMapping,
+            RelationshipClassObjectClassMapping,
+            RelationshipMapping,
+            RelationshipObjectMapping,
+            ScenarioActiveFlagMapping,
+            ScenarioAlternativeMapping,
+            ScenarioBeforeAlternativeMapping,
+            ScenarioDescriptionMapping,
+            ScenarioMapping,
+            ToolMapping,
+            ToolFeatureEntityClassMapping,
+            ToolFeatureParameterDefinitionMapping,
+            ToolFeatureRequiredFlagMapping,
+            ToolFeatureMethodEntityClassMapping,
+            ToolFeatureMethodParameterDefinitionMapping,
+        )
+    }
+    # Legacy
+    mappings["ParameterIndex"] = ParameterValueIndexMapping
+    if any(m["map_type"] == "RelationshipClassObjectHighlightingMapping" for m in serialized):
+        _upgrade_legacy_object_highlighting_mapping(serialized)
+    flattened = list()
+    for mapping_dict in serialized:
+        position = mapping_dict["position"]
+        if isinstance(position, str):
+            position = Position(position)
+        ignorable = mapping_dict.get("ignorable", False)
+        value = mapping_dict.get("value")
+        header = mapping_dict.get("header", "")
+        filter_re = mapping_dict.get("filter_re", "")
+        flattened.append(
+            mappings[mapping_dict["map_type"]].reconstruct(position, value, header, filter_re, ignorable, mapping_dict)
+        )
+    return unflatten(flattened)
+
+
+def legacy_group_fn_from_dict(serialized):
+    """Restores legacy group_fn attribute from serialized mappings.
+
+    group_fn has been removed from export mappings but this serves for backwards compatibility.
+
+    Args:
+        serialized (list): serialized mappings
+
+    Returns:
+        str: name of the first group_fn attribute that was found in the serialized mappings or NoGroup if not found
+    """
+    for mapping_dict in serialized:
+        group_fn = mapping_dict.get("group_fn")
+        if group_fn is not None:
+            return group_fn
+    return NoGroup.NAME
+
+
+def _upgrade_legacy_object_highlighting_mapping(serialized):
+    """Upgrades legacy object highlighting mappings in place.
+
+    ``RelationshipClassObjectHighlightingMapping`` and ``RelationshipObjectHighlightingMapping``
+    have been replaced by a ``highlight_dimension`` argument in ``RelationshipClassObjectClassMapping``.
+
+    Args:
+        serialized (list of dict): serialized mappings
+    """
+    for mapping_dict in serialized:
+        if mapping_dict["map_type"] == "RelationshipClassObjectHighlightingMapping":
+            mapping_dict["map_type"] = RelationshipClassMapping.MAP_TYPE
+        elif mapping_dict["map_type"] == "RelationshipObjectHighlightingMapping":
+            mapping_dict["map_type"] = RelationshipMapping.MAP_TYPE
+
+
+def _expand_indexed_data(data, mapping):
+    """Expands indexed data and updates the current_leaf attribute.
+
+    Args:
+        data (Any): data to expand
+        mapping (ExportMapping): mapping whose data is being expanded
+
+    Yields:
+        Any: parameter value index
+    """
+    if not isinstance(mapping.parent, _MappingWithLeafMixin):
+        # Get dict
+        current_leaf = from_database(data[0], data[1])
+        if data[1] == "map":
+            current_leaf = convert_containers_to_maps(current_leaf)
+    else:
+        # Get leaf from parent
+        current_leaf = mapping.parent.current_leaf
+    if not isinstance(current_leaf, IndexedValue):
+        # Nothing to expand. Set the current leaf so the child can find it
+        mapping.current_leaf = current_leaf
+        yield None
+        return
+    # Expand and set the current leaf so the child can find it
+    for index, value in zip(current_leaf.indexes, current_leaf.values):
+        mapping.current_leaf = value
+        yield index
+
+
+def _expand_index_names(data, mapping):
+    """Expands index names and updates the current_leaf attribute.
+
+    Args:
+        data (Any): data to expand
+        mapping (ExportMapping): mapping whose data is being expanded
+
+    Yields:
+        str: index name
+    """
+    if not isinstance(mapping.parent, _MappingWithLeafMixin):
+        current_leaf = from_database(data[0], data[1])
+        if data[1] == "map":
+            current_leaf = convert_containers_to_maps(current_leaf)
+    else:
+        current_leaf = mapping.parent.current_leaf
+    mapping.current_leaf = current_leaf
+    yield current_leaf.index_name if isinstance(current_leaf, IndexedValue) else None
```

### Comparing `spinedb_api-0.30.3/spinedb_api/export_mapping/group_functions.py` & `spinedb_api-0.30.4/spinedb_api/export_mapping/group_functions.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,142 +1,142 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Contains functions to group values in pivot tables with hidden columns or rows.
-
-"""
-import numpy as np
-
-
-class GroupFunction:
-    NAME = NotImplemented
-    DISPLAY_NAME = NotImplemented
-
-    def __call__(self, items):
-        """Performs the grouping. Reduces the given list of items into a single value.
-
-        Args:
-            items (list or None)
-
-        Returns:
-            Any
-        """
-        raise NotImplementedError
-
-
-class GroupSum(GroupFunction):
-    NAME = "sum"
-    DISPLAY_NAME = "sum"
-
-    def __call__(self, items):
-        if not items:
-            return np.nan
-        try:
-            return sum(items)
-        except TypeError:
-            return np.nan
-
-
-class GroupMean(GroupFunction):
-    NAME = "mean"
-    DISPLAY_NAME = "mean"
-
-    def __call__(self, items):
-        if not items:
-            return np.nan
-        try:
-            return np.mean(items)
-        except TypeError:
-            return np.nan
-
-
-class GroupMin(GroupFunction):
-    NAME = "min"
-    DISPLAY_NAME = "min"
-
-    def __call__(self, items):
-        if not items:
-            return np.nan
-        try:
-            return min(items)
-        except TypeError:
-            return np.nan
-
-
-class GroupMax(GroupFunction):
-    NAME = "max"
-    DISPLAY_NAME = "max"
-
-    def __call__(self, items):
-        if not items:
-            return np.nan
-        try:
-            return max(items)
-        except TypeError:
-            return np.nan
-
-
-class GroupConcat(GroupFunction):
-    NAME = "concat"
-    DISPLAY_NAME = "concatenate"
-
-    def __call__(self, items):
-        if not items:
-            return ""
-        return ",".join([str(x) for x in items])
-
-
-class GroupOneOrNone(GroupFunction):
-    NAME = "one_or_none"
-    DISPLAY_NAME = "one or none"
-
-    def __call__(self, items):
-        if not items or len(items) != 1:
-            return None
-        return items[0]
-
-
-class NoGroup(GroupFunction):
-    NAME = "no_group"
-    DISPLAY_NAME = "do not group"
-
-    def __call__(self, items):
-        if items is None:
-            return None
-        # The items are always in a list, even if not grouping, because we want to use the same code
-        # for grouping and not grouping. If not grouping, the list will contain exactly one element.
-        return items[0]
-
-
-_classes = (NoGroup, GroupSum, GroupMean, GroupMin, GroupMax, GroupConcat, GroupOneOrNone)
-
-GROUP_FUNCTION_DISPLAY_NAMES = [klass.DISPLAY_NAME for klass in _classes]
-
-
-def group_function_name_from_display(display_name):
-    return {klass.DISPLAY_NAME: klass.NAME for klass in _classes}.get(display_name, NoGroup.NAME)
-
-
-def group_function_display_from_name(name):
-    return {klass.NAME: klass.DISPLAY_NAME for klass in _classes}.get(name, NoGroup.DISPLAY_NAME)
-
-
-def from_str(name):
-    """
-    Creates group function from name.
-
-    Args:
-        name (str, NoneType): group function name or None if no aggregation wanted.
-
-    Returns:
-        GroupFunction or NoneType
-    """
-    constructor = {klass.NAME: klass for klass in _classes}.get(name, NoGroup)
-    return constructor()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Contains functions to group values in pivot tables with hidden columns or rows.
+
+"""
+import numpy as np
+
+
+class GroupFunction:
+    NAME = NotImplemented
+    DISPLAY_NAME = NotImplemented
+
+    def __call__(self, items):
+        """Performs the grouping. Reduces the given list of items into a single value.
+
+        Args:
+            items (list or None)
+
+        Returns:
+            Any
+        """
+        raise NotImplementedError
+
+
+class GroupSum(GroupFunction):
+    NAME = "sum"
+    DISPLAY_NAME = "sum"
+
+    def __call__(self, items):
+        if not items:
+            return np.nan
+        try:
+            return sum(items)
+        except TypeError:
+            return np.nan
+
+
+class GroupMean(GroupFunction):
+    NAME = "mean"
+    DISPLAY_NAME = "mean"
+
+    def __call__(self, items):
+        if not items:
+            return np.nan
+        try:
+            return np.mean(items)
+        except TypeError:
+            return np.nan
+
+
+class GroupMin(GroupFunction):
+    NAME = "min"
+    DISPLAY_NAME = "min"
+
+    def __call__(self, items):
+        if not items:
+            return np.nan
+        try:
+            return min(items)
+        except TypeError:
+            return np.nan
+
+
+class GroupMax(GroupFunction):
+    NAME = "max"
+    DISPLAY_NAME = "max"
+
+    def __call__(self, items):
+        if not items:
+            return np.nan
+        try:
+            return max(items)
+        except TypeError:
+            return np.nan
+
+
+class GroupConcat(GroupFunction):
+    NAME = "concat"
+    DISPLAY_NAME = "concatenate"
+
+    def __call__(self, items):
+        if not items:
+            return ""
+        return ",".join([str(x) for x in items])
+
+
+class GroupOneOrNone(GroupFunction):
+    NAME = "one_or_none"
+    DISPLAY_NAME = "one or none"
+
+    def __call__(self, items):
+        if not items or len(items) != 1:
+            return None
+        return items[0]
+
+
+class NoGroup(GroupFunction):
+    NAME = "no_group"
+    DISPLAY_NAME = "do not group"
+
+    def __call__(self, items):
+        if items is None:
+            return None
+        # The items are always in a list, even if not grouping, because we want to use the same code
+        # for grouping and not grouping. If not grouping, the list will contain exactly one element.
+        return items[0]
+
+
+_classes = (NoGroup, GroupSum, GroupMean, GroupMin, GroupMax, GroupConcat, GroupOneOrNone)
+
+GROUP_FUNCTION_DISPLAY_NAMES = [klass.DISPLAY_NAME for klass in _classes]
+
+
+def group_function_name_from_display(display_name):
+    return {klass.DISPLAY_NAME: klass.NAME for klass in _classes}.get(display_name, NoGroup.NAME)
+
+
+def group_function_display_from_name(name):
+    return {klass.NAME: klass.DISPLAY_NAME for klass in _classes}.get(name, NoGroup.DISPLAY_NAME)
+
+
+def from_str(name):
+    """
+    Creates group function from name.
+
+    Args:
+        name (str, NoneType): group function name or None if no aggregation wanted.
+
+    Returns:
+        GroupFunction or NoneType
+    """
+    constructor = {klass.NAME: klass for klass in _classes}.get(name, NoGroup)
+    return constructor()
```

### Comparing `spinedb_api-0.30.3/spinedb_api/export_mapping/pivot.py` & `spinedb_api-0.30.4/spinedb_api/export_mapping/pivot.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,238 +1,238 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Contains functions and methods to turn a regular export table into a pivot table
-
-"""
-from copy import deepcopy
-
-from .export_mapping import RelationshipMapping
-from ..mapping import is_regular, is_pivoted, Position, unflatten, value_index
-from .group_functions import from_str as group_function_from_str, NoGroup
-
-
-def make_pivot(
-    table, header, value_column, regular_columns, hidden_columns, pivot_columns, group_fn=None, empty_data_header=True
-):
-    """Turns a regular table into a pivot table.
-
-    Args:
-        table (list of list): table to convert
-        header (list, optional): header row
-        value_column (int): index of data column in ``table``
-        regular_columns (Iterable of int): indexes of non-pivoted columns in ``table``
-        hidden_columns (Iterable of int): indexes of columns that will not show on the pivot table
-        pivot_columns (Iterable of int): indexes of pivoted columns in ``table``
-        group_fn (str, optional): grouping function's name
-        empty_data_header (bool): True to yield at least header rows even if there is no data, False to yield nothing
-
-    Yields:
-        list: pivoted table row
-    """
-
-    def leaf(nested, keys):
-        """Returns leaf element from nested dict or None if not found
-
-        Args:
-            nested (dict): a nested dictionary
-            keys (Sequence): dictionary keys that identify the leaf element
-
-        Returns:
-            Any: the leaf element, or None if not found
-        """
-        if not keys:
-            return nested
-        v = nested.get(keys[0])
-        if v is None:
-            return None
-        if isinstance(v, dict):
-            return leaf(v, keys[1:])
-        return v
-
-    def make_regular_rows():
-        """Creates pivot table's 'left' side rows and non pivoted keys.
-
-        Returns:
-            dict: mapping non-pivoted keys to regular rows
-        """
-        regular_rows = dict()
-        for row in table:
-            regular_key = tuple(row[c] for c in key_columns)
-            regular_row = [row[i] for i in range(regular_column_width)]
-            regular_rows[regular_key] = regular_row
-        return regular_rows
-
-    def value_tree():
-        """Indexes pivot values.
-
-        Returns:
-            dict: a nested dictionary mapping keys to pivot values
-        """
-        tree = dict()
-        for row in table:
-            branch = tree
-            for c in key_columns + pivot_columns[:-1]:
-                branch = branch.setdefault(row[c], dict())
-            # If not grouping, the list below will have exactly one element
-            # If grouping, it will have all the elements that need to be grouped
-            values = branch.setdefault(row[pivot_columns[-1]], list())
-            values.append(row[value_column])
-        return tree
-
-    def half_pivot():
-        """Builds a 'half' pivot table that is missing the left columns.
-
-        Yields:
-            list: table row
-        """
-        for i in range(len(pivot_columns)):
-            row = [pivot_header[i]] if pivot_header is not None else []
-            row += list(k[i] for k in pivot_keys)
-            yield row
-        values = dict()
-        for row in table:
-            branch = values
-            for c in pivot_columns[:-1]:
-                branch = branch.setdefault(row[c], dict())
-            branch.setdefault(row[pivot_columns[-1]], list()).append(row[value_column])
-        height = max(len(leaf(values, key)) for key in pivot_keys) if pivot_keys else 0
-        for i in range(height):
-            row = [None] if pivot_header is not None else []
-            for key in pivot_keys:
-                v = leaf(values, key)
-                if i < len(v):
-                    row.append(v[i])
-                else:
-                    row.append(None)
-            yield row
-
-    def put_pivot_header(row, header):
-        """Puts the given pivot header into the given regular row.
-
-        Args:
-            row (list)
-            header (str or None)
-        """
-        if row:
-            if not row[-1]:
-                row[-1] = header
-        else:
-            row.append(header)
-
-    if (not table and (not empty_data_header or not header)) or not pivot_columns:
-        return
-    pivot_keys = sorted({tuple(row[i] for i in pivot_columns) for row in table}, key=_convert_elements_to_strings)
-    pivot_header = tuple(header[i] for i in pivot_columns) if header is not None else None
-    if regular_columns or hidden_columns:
-        regular_column_width = max(regular_columns) + 1 if regular_columns else 0
-        regular_header = [header[i] for i in range(regular_column_width)] if header is not None else None
-        # If grouping, key columns are the 'visible' regular columns
-        # If not grouping, we add the hidden columns
-        key_columns = regular_columns
-        group_fn = group_function_from_str(group_fn)
-        if isinstance(group_fn, NoGroup):
-            key_columns += hidden_columns
-        # Yield pivot rows (all but last)
-        for i in range(len(pivot_columns) - 1):
-            row = regular_column_width * [None]
-            if pivot_header is not None:
-                put_pivot_header(row, pivot_header[i])
-            row += list(k[i] for k in pivot_keys)
-            yield row
-        # Yield last pivot row. This one has the regular header (if any) at the beginning
-        if pivot_columns:
-            if regular_header is not None:
-                last_pivot_row = regular_header
-            else:
-                last_pivot_row = regular_column_width * [None]
-            # Note that the last regular header and the last pivot header would end up in the same cell.
-            # This is an arbitrary decision so the tables are more compact; otherwise we'd have an empty row or column
-            # at the last header position.
-            # To solve the conflict, we take the regular header if not None or empty, and the pivot header otherwise.
-            if pivot_header is not None and pivot_header[-1]:
-                put_pivot_header(last_pivot_row, pivot_header[-1])
-            last_pivot_row += list(k[-1] for k in pivot_keys)
-            yield last_pivot_row
-        # Yield regular rows
-        regular_rows = make_regular_rows()
-        values = value_tree()
-        if not any(regular_rows.values()) and pivot_header and table:
-            # Need a padding column for pivot header.
-            for row_key in sorted(regular_rows.keys()):
-                pivot_branch = leaf(values, row_key)
-                yield [None] + [group_fn(leaf(pivot_branch, column_key)) for column_key in pivot_keys]
-        else:
-            for row_key in sorted(regular_rows.keys(), key=_convert_elements_to_strings):
-                pivot_branch = leaf(values, row_key)
-                row = regular_rows[row_key]
-                row += [group_fn(leaf(pivot_branch, column_key)) for column_key in pivot_keys]
-                yield row
-    else:
-        for row in half_pivot():
-            yield row
-
-
-def _convert_elements_to_strings(key):
-    """Converts tuple's elements to strings replacing Nones with empty strings.
-
-    Args:
-        key (tuple): tuple to convert
-
-    Returns:
-        tuple of str: sortable tuple
-    """
-    return tuple(map(lambda x: str(x) if x is not None else "", key))
-
-
-def make_regular(root_mapping):
-    """
-    Makes a regular (non-pivoted) table out of pivoted mapping by giving column indexes to pivoted and hidden positions.
-
-    Useful when preparing data for pivoting.
-
-    Args:
-        root_mapping (Mapping): root mapping
-
-    Returns:
-        tuple: non-pivoted root mapping, value column, regular columns, hidden columns, pivot columns
-    """
-    mappings = deepcopy(root_mapping).flatten()
-    value_i = value_index(mappings)
-    regular_columns = [m.position for m in mappings[:value_i] if is_regular(m.position)]
-    regular_column_count = max(regular_columns) + 1 if regular_columns else 0
-    pivoted_positions = sorted((m.position for m in mappings[:value_i] if is_pivoted(m.position)), reverse=True)
-    pivot_position_to_row = {position: i for i, position in enumerate(pivoted_positions)}
-    pivot_column_count = len(pivot_position_to_row)
-    pivot_column_base = regular_column_count
-    pivot_columns = list()
-    hidden_column_base = pivot_column_base + pivot_column_count
-    current_hidden_column = 0
-    hidden_columns = list()
-    for mapping in mappings[:value_i]:
-        position = mapping.position
-        if is_pivoted(position):
-            mapping.position = pivot_column_base + pivot_position_to_row[mapping.position]
-            pivot_columns.append(mapping.position)
-        elif position == Position.hidden and _is_unhiddable(mapping):
-            column = hidden_column_base + current_hidden_column
-            mapping.position = column
-            hidden_columns.append(column)
-            current_hidden_column += 1
-    value_column = hidden_column_base + current_hidden_column + 1
-    for i, mapping in enumerate(mappings[value_i + 1 :]):
-        mapping.position = value_column + i + 1
-    mappings[value_i].position = value_column
-    return unflatten(mappings), value_column, regular_columns, hidden_columns, sorted(pivot_columns)
-
-
-def _is_unhiddable(mapping):
-    """Returns True if mapping uhiddable for pivoting purposes."""
-    return not isinstance(mapping, RelationshipMapping)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Contains functions and methods to turn a regular export table into a pivot table
+
+"""
+from copy import deepcopy
+
+from .export_mapping import RelationshipMapping
+from ..mapping import is_regular, is_pivoted, Position, unflatten, value_index
+from .group_functions import from_str as group_function_from_str, NoGroup
+
+
+def make_pivot(
+    table, header, value_column, regular_columns, hidden_columns, pivot_columns, group_fn=None, empty_data_header=True
+):
+    """Turns a regular table into a pivot table.
+
+    Args:
+        table (list of list): table to convert
+        header (list, optional): header row
+        value_column (int): index of data column in ``table``
+        regular_columns (Iterable of int): indexes of non-pivoted columns in ``table``
+        hidden_columns (Iterable of int): indexes of columns that will not show on the pivot table
+        pivot_columns (Iterable of int): indexes of pivoted columns in ``table``
+        group_fn (str, optional): grouping function's name
+        empty_data_header (bool): True to yield at least header rows even if there is no data, False to yield nothing
+
+    Yields:
+        list: pivoted table row
+    """
+
+    def leaf(nested, keys):
+        """Returns leaf element from nested dict or None if not found
+
+        Args:
+            nested (dict): a nested dictionary
+            keys (Sequence): dictionary keys that identify the leaf element
+
+        Returns:
+            Any: the leaf element, or None if not found
+        """
+        if not keys:
+            return nested
+        v = nested.get(keys[0])
+        if v is None:
+            return None
+        if isinstance(v, dict):
+            return leaf(v, keys[1:])
+        return v
+
+    def make_regular_rows():
+        """Creates pivot table's 'left' side rows and non pivoted keys.
+
+        Returns:
+            dict: mapping non-pivoted keys to regular rows
+        """
+        regular_rows = dict()
+        for row in table:
+            regular_key = tuple(row[c] for c in key_columns)
+            regular_row = [row[i] for i in range(regular_column_width)]
+            regular_rows[regular_key] = regular_row
+        return regular_rows
+
+    def value_tree():
+        """Indexes pivot values.
+
+        Returns:
+            dict: a nested dictionary mapping keys to pivot values
+        """
+        tree = dict()
+        for row in table:
+            branch = tree
+            for c in key_columns + pivot_columns[:-1]:
+                branch = branch.setdefault(row[c], dict())
+            # If not grouping, the list below will have exactly one element
+            # If grouping, it will have all the elements that need to be grouped
+            values = branch.setdefault(row[pivot_columns[-1]], list())
+            values.append(row[value_column])
+        return tree
+
+    def half_pivot():
+        """Builds a 'half' pivot table that is missing the left columns.
+
+        Yields:
+            list: table row
+        """
+        for i in range(len(pivot_columns)):
+            row = [pivot_header[i]] if pivot_header is not None else []
+            row += list(k[i] for k in pivot_keys)
+            yield row
+        values = dict()
+        for row in table:
+            branch = values
+            for c in pivot_columns[:-1]:
+                branch = branch.setdefault(row[c], dict())
+            branch.setdefault(row[pivot_columns[-1]], list()).append(row[value_column])
+        height = max(len(leaf(values, key)) for key in pivot_keys) if pivot_keys else 0
+        for i in range(height):
+            row = [None] if pivot_header is not None else []
+            for key in pivot_keys:
+                v = leaf(values, key)
+                if i < len(v):
+                    row.append(v[i])
+                else:
+                    row.append(None)
+            yield row
+
+    def put_pivot_header(row, header):
+        """Puts the given pivot header into the given regular row.
+
+        Args:
+            row (list)
+            header (str or None)
+        """
+        if row:
+            if not row[-1]:
+                row[-1] = header
+        else:
+            row.append(header)
+
+    if (not table and (not empty_data_header or not header)) or not pivot_columns:
+        return
+    pivot_keys = sorted({tuple(row[i] for i in pivot_columns) for row in table}, key=_convert_elements_to_strings)
+    pivot_header = tuple(header[i] for i in pivot_columns) if header is not None else None
+    if regular_columns or hidden_columns:
+        regular_column_width = max(regular_columns) + 1 if regular_columns else 0
+        regular_header = [header[i] for i in range(regular_column_width)] if header is not None else None
+        # If grouping, key columns are the 'visible' regular columns
+        # If not grouping, we add the hidden columns
+        key_columns = regular_columns
+        group_fn = group_function_from_str(group_fn)
+        if isinstance(group_fn, NoGroup):
+            key_columns += hidden_columns
+        # Yield pivot rows (all but last)
+        for i in range(len(pivot_columns) - 1):
+            row = regular_column_width * [None]
+            if pivot_header is not None:
+                put_pivot_header(row, pivot_header[i])
+            row += list(k[i] for k in pivot_keys)
+            yield row
+        # Yield last pivot row. This one has the regular header (if any) at the beginning
+        if pivot_columns:
+            if regular_header is not None:
+                last_pivot_row = regular_header
+            else:
+                last_pivot_row = regular_column_width * [None]
+            # Note that the last regular header and the last pivot header would end up in the same cell.
+            # This is an arbitrary decision so the tables are more compact; otherwise we'd have an empty row or column
+            # at the last header position.
+            # To solve the conflict, we take the regular header if not None or empty, and the pivot header otherwise.
+            if pivot_header is not None and pivot_header[-1]:
+                put_pivot_header(last_pivot_row, pivot_header[-1])
+            last_pivot_row += list(k[-1] for k in pivot_keys)
+            yield last_pivot_row
+        # Yield regular rows
+        regular_rows = make_regular_rows()
+        values = value_tree()
+        if not any(regular_rows.values()) and pivot_header and table:
+            # Need a padding column for pivot header.
+            for row_key in sorted(regular_rows.keys()):
+                pivot_branch = leaf(values, row_key)
+                yield [None] + [group_fn(leaf(pivot_branch, column_key)) for column_key in pivot_keys]
+        else:
+            for row_key in sorted(regular_rows.keys(), key=_convert_elements_to_strings):
+                pivot_branch = leaf(values, row_key)
+                row = regular_rows[row_key]
+                row += [group_fn(leaf(pivot_branch, column_key)) for column_key in pivot_keys]
+                yield row
+    else:
+        for row in half_pivot():
+            yield row
+
+
+def _convert_elements_to_strings(key):
+    """Converts tuple's elements to strings replacing Nones with empty strings.
+
+    Args:
+        key (tuple): tuple to convert
+
+    Returns:
+        tuple of str: sortable tuple
+    """
+    return tuple(map(lambda x: str(x) if x is not None else "", key))
+
+
+def make_regular(root_mapping):
+    """
+    Makes a regular (non-pivoted) table out of pivoted mapping by giving column indexes to pivoted and hidden positions.
+
+    Useful when preparing data for pivoting.
+
+    Args:
+        root_mapping (Mapping): root mapping
+
+    Returns:
+        tuple: non-pivoted root mapping, value column, regular columns, hidden columns, pivot columns
+    """
+    mappings = deepcopy(root_mapping).flatten()
+    value_i = value_index(mappings)
+    regular_columns = [m.position for m in mappings[:value_i] if is_regular(m.position)]
+    regular_column_count = max(regular_columns) + 1 if regular_columns else 0
+    pivoted_positions = sorted((m.position for m in mappings[:value_i] if is_pivoted(m.position)), reverse=True)
+    pivot_position_to_row = {position: i for i, position in enumerate(pivoted_positions)}
+    pivot_column_count = len(pivot_position_to_row)
+    pivot_column_base = regular_column_count
+    pivot_columns = list()
+    hidden_column_base = pivot_column_base + pivot_column_count
+    current_hidden_column = 0
+    hidden_columns = list()
+    for mapping in mappings[:value_i]:
+        position = mapping.position
+        if is_pivoted(position):
+            mapping.position = pivot_column_base + pivot_position_to_row[mapping.position]
+            pivot_columns.append(mapping.position)
+        elif position == Position.hidden and _is_unhiddable(mapping):
+            column = hidden_column_base + current_hidden_column
+            mapping.position = column
+            hidden_columns.append(column)
+            current_hidden_column += 1
+    value_column = hidden_column_base + current_hidden_column + 1
+    for i, mapping in enumerate(mappings[value_i + 1 :]):
+        mapping.position = value_column + i + 1
+    mappings[value_i].position = value_column
+    return unflatten(mappings), value_column, regular_columns, hidden_columns, sorted(pivot_columns)
+
+
+def _is_unhiddable(mapping):
+    """Returns True if mapping uhiddable for pivoting purposes."""
+    return not isinstance(mapping, RelationshipMapping)
```

### Comparing `spinedb_api-0.30.3/spinedb_api/filters/__init__.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Init file for spine_io package. Intentionally empty.
+
+"""
```

### Comparing `spinedb_api-0.30.3/spinedb_api/filters/alternative_filter.py` & `spinedb_api-0.30.4/spinedb_api/filters/alternative_filter.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,177 +1,177 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Provides functions to apply filtering based on alternatives to parameter value subqueries.
-
-"""
-from functools import partial
-from ..exception import SpineDBAPIError
-
-
-ALTERNATIVE_FILTER_TYPE = "alternative_filter"
-ALTERNATIVE_FILTER_SHORTHAND_TAG = "alternatives"
-
-
-def apply_alternative_filter_to_parameter_value_sq(db_map, alternatives):
-    """
-    Replaces parameter value subquery properties in ``db_map`` such that they return only values of given alternatives.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map to alter
-        alternatives (Iterable of str or int, optional): alternative names or ids;
-    """
-    state = _AlternativeFilterState(db_map, alternatives)
-    filtering = partial(_make_alternative_filtered_parameter_value_sq, state=state)
-    db_map.override_parameter_value_sq_maker(filtering)
-
-
-def alternative_filter_config(alternatives):
-    """
-    Creates a config dict for alternative filter.
-
-    Args:
-        alternatives (Iterable of str): alternative names
-
-    Returns:
-        dict: filter configuration
-    """
-    return {"type": ALTERNATIVE_FILTER_TYPE, "alternatives": list(alternatives)}
-
-
-def alternative_filter_from_dict(db_map, config):
-    """
-    Applies alternative filter to given database map.
-
-    Args:
-        db_map (DatabaseMappingBase): target database map
-        config (dict): alternative filter configuration
-    """
-    apply_alternative_filter_to_parameter_value_sq(db_map, config["alternatives"])
-
-
-def alternative_filter_config_to_shorthand(config):
-    """
-    Makes a shorthand string from alternative filter configuration.
-
-    Args:
-        config (dict): alternative filter configuration
-
-    Returns:
-        str: a shorthand string
-    """
-    shorthand = ""
-    for alternative in config["alternatives"]:
-        shorthand = shorthand + f":'{alternative}'"
-    return ALTERNATIVE_FILTER_SHORTHAND_TAG + shorthand
-
-
-def alternative_names_from_dict(config):
-    """
-    Returns alternatives' names from filter config.
-
-    Args:
-        config (dict): alternative filter configuration
-
-    Returns:
-        list: list of alternative names or None if ``config`` is not a valid alternative filter configuration
-    """
-    if not config["type"] == ALTERNATIVE_FILTER_TYPE:
-        return None
-    return config["alternatives"]
-
-
-def alternative_filter_shorthand_to_config(shorthand):
-    """
-    Makes configuration dictionary out of a shorthand string.
-
-    Args:
-        shorthand (str): a shorthand string
-
-    Returns:
-        dict: alternative filter configuration
-    """
-    filter_type, separator, tokens = shorthand.partition(":'")
-    alternatives = tokens.split("':'")
-    alternatives[-1] = alternatives[-1][:-1]
-    return alternative_filter_config(alternatives)
-
-
-class _AlternativeFilterState:
-    """
-    Internal state for :func:`_make_alternative_filtered_parameter_value_sq`
-
-    Attributes:
-        original_parameter_value_sq (Alias): previous ``parameter_value_sq``
-        alternatives (Iterable of int): ids of alternatives
-    """
-
-    def __init__(self, db_map, alternatives):
-        """
-        Args:
-            db_map (DatabaseMappingBase): database the state applies to
-            alternatives (Iterable of str or int): alternative names or ids;
-        """
-        self.original_parameter_value_sq = db_map.parameter_value_sq
-        self.alternatives = self._alternative_ids(db_map, alternatives) if alternatives is not None else None
-
-    @staticmethod
-    def _alternative_ids(db_map, alternatives):
-        """
-        Finds ids for given alternatives.
-
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            alternatives (Iterable): alternative names or ids
-
-        Returns:
-            list of int: alternative ids
-        """
-        alternative_names = [name for name in alternatives if isinstance(name, str)]
-        ids_from_db = (
-            db_map.query(db_map.alternative_sq.c.id, db_map.alternative_sq.c.name)
-            .filter(db_map.in_(db_map.alternative_sq.c.name, alternative_names))
-            .all()
-        )
-        names_in_db = [i.name for i in ids_from_db]
-        if len(alternative_names) != len(names_in_db):
-            missing_names = tuple(name for name in alternative_names if name not in names_in_db)
-            raise SpineDBAPIError(f"Alternative(s) {missing_names} not found")
-        ids = [i.id for i in ids_from_db]
-        alternative_ids = [id_ for id_ in alternatives if isinstance(id_, int)]
-        ids_from_db = (
-            db_map.query(db_map.alternative_sq.c.id)
-            .filter(db_map.in_(db_map.alternative_sq.c.id, alternative_ids))
-            .all()
-        )
-        ids_in_db = [i.id for i in ids_from_db]
-        if len(alternative_ids) != len(ids_from_db):
-            missing_ids = tuple(i for i in alternative_ids if i not in ids_in_db)
-            raise SpineDBAPIError(f"Alternative id(s) {missing_ids} not found")
-        ids += ids_in_db
-        return ids
-
-
-def _make_alternative_filtered_parameter_value_sq(db_map, state):
-    """
-    Returns an alternative filtering subquery similar to :func:`DatabaseMappingBase.parameter_value_sq`.
-
-    This function can be used as replacement for parameter value subquery maker in :class:`DatabaseMappingBase`.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        state (_AlternativeFilterState): a state bound to ``db_map``
-
-    Returns:
-        Alias: a subquery for parameter value filtered by selected alternatives
-    """
-    subquery = state.original_parameter_value_sq
-    return db_map.query(subquery).filter(db_map.in_(subquery.c.alternative_id, state.alternatives)).subquery()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Provides functions to apply filtering based on alternatives to parameter value subqueries.
+
+"""
+from functools import partial
+from ..exception import SpineDBAPIError
+
+
+ALTERNATIVE_FILTER_TYPE = "alternative_filter"
+ALTERNATIVE_FILTER_SHORTHAND_TAG = "alternatives"
+
+
+def apply_alternative_filter_to_parameter_value_sq(db_map, alternatives):
+    """
+    Replaces parameter value subquery properties in ``db_map`` such that they return only values of given alternatives.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map to alter
+        alternatives (Iterable of str or int, optional): alternative names or ids;
+    """
+    state = _AlternativeFilterState(db_map, alternatives)
+    filtering = partial(_make_alternative_filtered_parameter_value_sq, state=state)
+    db_map.override_parameter_value_sq_maker(filtering)
+
+
+def alternative_filter_config(alternatives):
+    """
+    Creates a config dict for alternative filter.
+
+    Args:
+        alternatives (Iterable of str): alternative names
+
+    Returns:
+        dict: filter configuration
+    """
+    return {"type": ALTERNATIVE_FILTER_TYPE, "alternatives": list(alternatives)}
+
+
+def alternative_filter_from_dict(db_map, config):
+    """
+    Applies alternative filter to given database map.
+
+    Args:
+        db_map (DatabaseMappingBase): target database map
+        config (dict): alternative filter configuration
+    """
+    apply_alternative_filter_to_parameter_value_sq(db_map, config["alternatives"])
+
+
+def alternative_filter_config_to_shorthand(config):
+    """
+    Makes a shorthand string from alternative filter configuration.
+
+    Args:
+        config (dict): alternative filter configuration
+
+    Returns:
+        str: a shorthand string
+    """
+    shorthand = ""
+    for alternative in config["alternatives"]:
+        shorthand = shorthand + f":'{alternative}'"
+    return ALTERNATIVE_FILTER_SHORTHAND_TAG + shorthand
+
+
+def alternative_names_from_dict(config):
+    """
+    Returns alternatives' names from filter config.
+
+    Args:
+        config (dict): alternative filter configuration
+
+    Returns:
+        list: list of alternative names or None if ``config`` is not a valid alternative filter configuration
+    """
+    if not config["type"] == ALTERNATIVE_FILTER_TYPE:
+        return None
+    return config["alternatives"]
+
+
+def alternative_filter_shorthand_to_config(shorthand):
+    """
+    Makes configuration dictionary out of a shorthand string.
+
+    Args:
+        shorthand (str): a shorthand string
+
+    Returns:
+        dict: alternative filter configuration
+    """
+    filter_type, separator, tokens = shorthand.partition(":'")
+    alternatives = tokens.split("':'")
+    alternatives[-1] = alternatives[-1][:-1]
+    return alternative_filter_config(alternatives)
+
+
+class _AlternativeFilterState:
+    """
+    Internal state for :func:`_make_alternative_filtered_parameter_value_sq`
+
+    Attributes:
+        original_parameter_value_sq (Alias): previous ``parameter_value_sq``
+        alternatives (Iterable of int): ids of alternatives
+    """
+
+    def __init__(self, db_map, alternatives):
+        """
+        Args:
+            db_map (DatabaseMappingBase): database the state applies to
+            alternatives (Iterable of str or int): alternative names or ids;
+        """
+        self.original_parameter_value_sq = db_map.parameter_value_sq
+        self.alternatives = self._alternative_ids(db_map, alternatives) if alternatives is not None else None
+
+    @staticmethod
+    def _alternative_ids(db_map, alternatives):
+        """
+        Finds ids for given alternatives.
+
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            alternatives (Iterable): alternative names or ids
+
+        Returns:
+            list of int: alternative ids
+        """
+        alternative_names = [name for name in alternatives if isinstance(name, str)]
+        ids_from_db = (
+            db_map.query(db_map.alternative_sq.c.id, db_map.alternative_sq.c.name)
+            .filter(db_map.in_(db_map.alternative_sq.c.name, alternative_names))
+            .all()
+        )
+        names_in_db = [i.name for i in ids_from_db]
+        if len(alternative_names) != len(names_in_db):
+            missing_names = tuple(name for name in alternative_names if name not in names_in_db)
+            raise SpineDBAPIError(f"Alternative(s) {missing_names} not found")
+        ids = [i.id for i in ids_from_db]
+        alternative_ids = [id_ for id_ in alternatives if isinstance(id_, int)]
+        ids_from_db = (
+            db_map.query(db_map.alternative_sq.c.id)
+            .filter(db_map.in_(db_map.alternative_sq.c.id, alternative_ids))
+            .all()
+        )
+        ids_in_db = [i.id for i in ids_from_db]
+        if len(alternative_ids) != len(ids_from_db):
+            missing_ids = tuple(i for i in alternative_ids if i not in ids_in_db)
+            raise SpineDBAPIError(f"Alternative id(s) {missing_ids} not found")
+        ids += ids_in_db
+        return ids
+
+
+def _make_alternative_filtered_parameter_value_sq(db_map, state):
+    """
+    Returns an alternative filtering subquery similar to :func:`DatabaseMappingBase.parameter_value_sq`.
+
+    This function can be used as replacement for parameter value subquery maker in :class:`DatabaseMappingBase`.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        state (_AlternativeFilterState): a state bound to ``db_map``
+
+    Returns:
+        Alias: a subquery for parameter value filtered by selected alternatives
+    """
+    subquery = state.original_parameter_value_sq
+    return db_map.query(subquery).filter(db_map.in_(subquery.c.alternative_id, state.alternatives)).subquery()
```

### Comparing `spinedb_api-0.30.3/spinedb_api/filters/renamer.py` & `spinedb_api-0.30.4/spinedb_api/filters/renamer.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,285 +1,285 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Provides a database query manipulator that renames database items.
-
-"""
-from functools import partial
-from sqlalchemy import case
-
-
-ENTITY_CLASS_RENAMER_TYPE = "entity_class_renamer"
-ENTITY_CLASS_RENAMER_SHORTHAND_TAG = "entity_class_rename"
-PARAMETER_RENAMER_TYPE = "parameter_renamer"
-PARAMETER_RENAMER_SHORTHAND_TAG = "parameter_rename"
-
-
-def apply_renaming_to_entity_class_sq(db_map, name_map):
-    """
-    Applies renaming to entity class subquery.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        name_map (dict): a map from old name to new name
-    """
-    state = _EntityClassRenamerState(db_map, name_map)
-    renaming = partial(_make_renaming_entity_class_sq, state=state)
-    db_map.override_entity_class_sq_maker(renaming)
-
-
-def entity_class_renamer_config(**renames):
-    """
-    Creates a config dict for renamer.
-
-    Args:
-        **renames: keyword is the old name, value is the new name
-
-    Returns:
-        dict: renamer configuration
-    """
-    return {"type": ENTITY_CLASS_RENAMER_TYPE, "name_map": dict(renames)}
-
-
-def entity_class_renamer_from_dict(db_map, config):
-    """
-    Applies entity class renamer manipulator to given database map.
-
-    Args:
-        db_map (DatabaseMappingBase): target database map
-        config (dict): renamer configuration
-    """
-    apply_renaming_to_entity_class_sq(db_map, config["name_map"])
-
-
-def entity_class_renamer_config_to_shorthand(config):
-    """
-    Makes a shorthand string from renamer configuration.
-
-    Args:
-        config (dict): renamer configuration
-
-    Returns:
-        str: a shorthand string
-    """
-    shorthand = ""
-    for old_name, new_name in config["name_map"].items():
-        shorthand = shorthand + ":" + old_name + ":" + new_name
-    return ENTITY_CLASS_RENAMER_SHORTHAND_TAG + shorthand
-
-
-def entity_class_renamer_shorthand_to_config(shorthand):
-    """
-    Makes configuration dictionary out of a shorthand string.
-
-    Args:
-        shorthand (str): a shorthand string
-
-    Returns:
-        dict: renamer configuration
-    """
-    names = shorthand.split(":")
-    name_map = {}
-    for old_name, new_name in zip(names[1::2], names[2::2]):
-        name_map[old_name] = new_name
-    return entity_class_renamer_config(**name_map)
-
-
-def apply_renaming_to_parameter_definition_sq(db_map, name_map):
-    """
-    Applies renaming to parameter definition subquery.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        name_map (dict): a map from old name to new name
-    """
-    state = _ParameterRenamerState(db_map, name_map)
-    renaming = partial(_make_renaming_parameter_definition_sq, state=state)
-    db_map.override_parameter_definition_sq_maker(renaming)
-
-
-def parameter_renamer_config(renames):
-    """
-    Creates a config dict for renamer.
-
-    Args:
-        renames (dict): mapping from entity class name to mapping from parameter name to new name
-
-    Returns:
-        dict: renamer configuration
-    """
-    return {"type": PARAMETER_RENAMER_TYPE, "name_map": renames}
-
-
-def parameter_renamer_from_dict(db_map, config):
-    """
-    Applies parameter renamer manipulator to given database map.
-
-    Args:
-        db_map (DatabaseMappingBase): target database map
-        config (dict): renamer configuration
-    """
-    apply_renaming_to_parameter_definition_sq(db_map, config["name_map"])
-
-
-def parameter_renamer_config_to_shorthand(config):
-    """
-    Makes a shorthand string from renamer configuration.
-
-    Args:
-        config (dict): renamer configuration
-
-    Returns:
-        str: a shorthand string
-    """
-    shorthand = ""
-    for class_name, renaming in config["name_map"].items():
-        for old_name, new_name in renaming.items():
-            shorthand = shorthand + ":" + class_name + ":" + old_name + ":" + new_name
-    return PARAMETER_RENAMER_SHORTHAND_TAG + shorthand
-
-
-def parameter_renamer_shorthand_to_config(shorthand):
-    """
-    Makes configuration dictionary out of a shorthand string.
-
-    Args:
-        shorthand (str): a shorthand string
-
-    Returns:
-        dict: renamer configuration
-    """
-    names = shorthand.split(":")
-    name_map = {}
-    for class_name, old_name, new_name in zip(names[1::3], names[2::3], names[3::3]):
-        name_map.setdefault(class_name, {})[old_name] = new_name
-    return parameter_renamer_config(name_map)
-
-
-class _EntityClassRenamerState:
-    def __init__(self, db_map, name_map):
-        """
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            name_map (dict): a mapping from original name to a new name.
-        """
-        name_map = {old: new for old, new in name_map.items() if old != new}
-        self.id_to_name = self._ids(db_map, name_map)
-        self.original_entity_class_sq = db_map.entity_class_sq
-
-    @staticmethod
-    def _ids(db_map, name_map):
-        """
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            name_map (dict): a mapping from original name to a new name
-
-        Returns:
-            dict: a mapping from entity class id to a new name
-        """
-        names = set(name_map.keys())
-        return {
-            class_row.id: name_map[class_row.name]
-            for class_row in db_map.query(db_map.entity_class_sq).filter(db_map.entity_class_sq.c.name.in_(names)).all()
-        }
-
-
-def _make_renaming_entity_class_sq(db_map, state):
-    """
-    Returns an entity class subquery which renames classes.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        state (_EntityClassRenamerState):
-
-    Returns:
-        Alias: a renaming entity class subquery
-    """
-    subquery = state.original_entity_class_sq
-    if not state.id_to_name:
-        return subquery
-    cases = [(subquery.c.id == id, new_name) for id, new_name in state.id_to_name.items()]
-    new_class_name = case(cases, else_=subquery.c.name)  # if not in the name map, just keep the original name
-    entity_class_sq = db_map.query(
-        subquery.c.id,
-        subquery.c.type_id,
-        new_class_name.label("name"),
-        subquery.c.description,
-        subquery.c.display_order,
-        subquery.c.display_icon,
-        subquery.c.hidden,
-        subquery.c.commit_id,
-    ).subquery()
-    return entity_class_sq
-
-
-class _ParameterRenamerState:
-    def __init__(self, db_map, name_map):
-        """
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            name_map (dict): mapping from entity class name to mapping from parameter name to new name
-        """
-        self.id_to_name = self._ids(db_map, name_map)
-        self.original_parameter_definition_sq = db_map.parameter_definition_sq
-
-    @staticmethod
-    def _ids(db_map, name_map):
-        """
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            name_map (dict): a mapping from original name to a new name
-
-        Returns:
-            dict: a mapping from entity class id to a new name
-        """
-        class_names = set(name_map.keys())
-        param_names = set(old_name for renaming in name_map.values() for old_name in renaming)
-        id_to_names = {
-            (definition_row.entity_class_name, definition_row.parameter_name): definition_row.id
-            for definition_row in db_map.query(db_map.entity_parameter_definition_sq).filter(
-                db_map.entity_parameter_definition_sq.c.entity_class_name.in_(class_names)
-                & db_map.entity_parameter_definition_sq.c.parameter_name.in_(param_names)
-            )
-        }
-        return {id_: name_map[path[0]][path[1]] for path, id_ in id_to_names.items() if path[1] in name_map[path[0]]}
-
-
-def _make_renaming_parameter_definition_sq(db_map, state):
-    """
-    Returns an entity class subquery which renames parameters.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        state (_ParameterRenamerState):
-
-    Returns:
-        Alias: a renaming parameter definition subquery
-    """
-    subquery = state.original_parameter_definition_sq
-    if not state.id_to_name:
-        return subquery
-    cases = [(subquery.c.id == id, new_name) for id, new_name in state.id_to_name.items()]
-    new_parameter_name = case(cases, else_=subquery.c.name)  # if not in the name map, just keep the original name
-    parameter_definition_sq = db_map.query(
-        subquery.c.id,
-        new_parameter_name.label("name"),
-        subquery.c.description,
-        subquery.c.entity_class_id,
-        subquery.c.object_class_id,
-        subquery.c.relationship_class_id,
-        subquery.c.default_value,
-        subquery.c.default_type,
-        subquery.c.list_value_id,
-        subquery.c.commit_id,
-        subquery.c.parameter_value_list_id,
-    ).subquery()
-    return parameter_definition_sq
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Provides a database query manipulator that renames database items.
+
+"""
+from functools import partial
+from sqlalchemy import case
+
+
+ENTITY_CLASS_RENAMER_TYPE = "entity_class_renamer"
+ENTITY_CLASS_RENAMER_SHORTHAND_TAG = "entity_class_rename"
+PARAMETER_RENAMER_TYPE = "parameter_renamer"
+PARAMETER_RENAMER_SHORTHAND_TAG = "parameter_rename"
+
+
+def apply_renaming_to_entity_class_sq(db_map, name_map):
+    """
+    Applies renaming to entity class subquery.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        name_map (dict): a map from old name to new name
+    """
+    state = _EntityClassRenamerState(db_map, name_map)
+    renaming = partial(_make_renaming_entity_class_sq, state=state)
+    db_map.override_entity_class_sq_maker(renaming)
+
+
+def entity_class_renamer_config(**renames):
+    """
+    Creates a config dict for renamer.
+
+    Args:
+        **renames: keyword is the old name, value is the new name
+
+    Returns:
+        dict: renamer configuration
+    """
+    return {"type": ENTITY_CLASS_RENAMER_TYPE, "name_map": dict(renames)}
+
+
+def entity_class_renamer_from_dict(db_map, config):
+    """
+    Applies entity class renamer manipulator to given database map.
+
+    Args:
+        db_map (DatabaseMappingBase): target database map
+        config (dict): renamer configuration
+    """
+    apply_renaming_to_entity_class_sq(db_map, config["name_map"])
+
+
+def entity_class_renamer_config_to_shorthand(config):
+    """
+    Makes a shorthand string from renamer configuration.
+
+    Args:
+        config (dict): renamer configuration
+
+    Returns:
+        str: a shorthand string
+    """
+    shorthand = ""
+    for old_name, new_name in config["name_map"].items():
+        shorthand = shorthand + ":" + old_name + ":" + new_name
+    return ENTITY_CLASS_RENAMER_SHORTHAND_TAG + shorthand
+
+
+def entity_class_renamer_shorthand_to_config(shorthand):
+    """
+    Makes configuration dictionary out of a shorthand string.
+
+    Args:
+        shorthand (str): a shorthand string
+
+    Returns:
+        dict: renamer configuration
+    """
+    names = shorthand.split(":")
+    name_map = {}
+    for old_name, new_name in zip(names[1::2], names[2::2]):
+        name_map[old_name] = new_name
+    return entity_class_renamer_config(**name_map)
+
+
+def apply_renaming_to_parameter_definition_sq(db_map, name_map):
+    """
+    Applies renaming to parameter definition subquery.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        name_map (dict): a map from old name to new name
+    """
+    state = _ParameterRenamerState(db_map, name_map)
+    renaming = partial(_make_renaming_parameter_definition_sq, state=state)
+    db_map.override_parameter_definition_sq_maker(renaming)
+
+
+def parameter_renamer_config(renames):
+    """
+    Creates a config dict for renamer.
+
+    Args:
+        renames (dict): mapping from entity class name to mapping from parameter name to new name
+
+    Returns:
+        dict: renamer configuration
+    """
+    return {"type": PARAMETER_RENAMER_TYPE, "name_map": renames}
+
+
+def parameter_renamer_from_dict(db_map, config):
+    """
+    Applies parameter renamer manipulator to given database map.
+
+    Args:
+        db_map (DatabaseMappingBase): target database map
+        config (dict): renamer configuration
+    """
+    apply_renaming_to_parameter_definition_sq(db_map, config["name_map"])
+
+
+def parameter_renamer_config_to_shorthand(config):
+    """
+    Makes a shorthand string from renamer configuration.
+
+    Args:
+        config (dict): renamer configuration
+
+    Returns:
+        str: a shorthand string
+    """
+    shorthand = ""
+    for class_name, renaming in config["name_map"].items():
+        for old_name, new_name in renaming.items():
+            shorthand = shorthand + ":" + class_name + ":" + old_name + ":" + new_name
+    return PARAMETER_RENAMER_SHORTHAND_TAG + shorthand
+
+
+def parameter_renamer_shorthand_to_config(shorthand):
+    """
+    Makes configuration dictionary out of a shorthand string.
+
+    Args:
+        shorthand (str): a shorthand string
+
+    Returns:
+        dict: renamer configuration
+    """
+    names = shorthand.split(":")
+    name_map = {}
+    for class_name, old_name, new_name in zip(names[1::3], names[2::3], names[3::3]):
+        name_map.setdefault(class_name, {})[old_name] = new_name
+    return parameter_renamer_config(name_map)
+
+
+class _EntityClassRenamerState:
+    def __init__(self, db_map, name_map):
+        """
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            name_map (dict): a mapping from original name to a new name.
+        """
+        name_map = {old: new for old, new in name_map.items() if old != new}
+        self.id_to_name = self._ids(db_map, name_map)
+        self.original_entity_class_sq = db_map.entity_class_sq
+
+    @staticmethod
+    def _ids(db_map, name_map):
+        """
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            name_map (dict): a mapping from original name to a new name
+
+        Returns:
+            dict: a mapping from entity class id to a new name
+        """
+        names = set(name_map.keys())
+        return {
+            class_row.id: name_map[class_row.name]
+            for class_row in db_map.query(db_map.entity_class_sq).filter(db_map.entity_class_sq.c.name.in_(names)).all()
+        }
+
+
+def _make_renaming_entity_class_sq(db_map, state):
+    """
+    Returns an entity class subquery which renames classes.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        state (_EntityClassRenamerState):
+
+    Returns:
+        Alias: a renaming entity class subquery
+    """
+    subquery = state.original_entity_class_sq
+    if not state.id_to_name:
+        return subquery
+    cases = [(subquery.c.id == id, new_name) for id, new_name in state.id_to_name.items()]
+    new_class_name = case(cases, else_=subquery.c.name)  # if not in the name map, just keep the original name
+    entity_class_sq = db_map.query(
+        subquery.c.id,
+        subquery.c.type_id,
+        new_class_name.label("name"),
+        subquery.c.description,
+        subquery.c.display_order,
+        subquery.c.display_icon,
+        subquery.c.hidden,
+        subquery.c.commit_id,
+    ).subquery()
+    return entity_class_sq
+
+
+class _ParameterRenamerState:
+    def __init__(self, db_map, name_map):
+        """
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            name_map (dict): mapping from entity class name to mapping from parameter name to new name
+        """
+        self.id_to_name = self._ids(db_map, name_map)
+        self.original_parameter_definition_sq = db_map.parameter_definition_sq
+
+    @staticmethod
+    def _ids(db_map, name_map):
+        """
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            name_map (dict): a mapping from original name to a new name
+
+        Returns:
+            dict: a mapping from entity class id to a new name
+        """
+        class_names = set(name_map.keys())
+        param_names = set(old_name for renaming in name_map.values() for old_name in renaming)
+        id_to_names = {
+            (definition_row.entity_class_name, definition_row.parameter_name): definition_row.id
+            for definition_row in db_map.query(db_map.entity_parameter_definition_sq).filter(
+                db_map.entity_parameter_definition_sq.c.entity_class_name.in_(class_names)
+                & db_map.entity_parameter_definition_sq.c.parameter_name.in_(param_names)
+            )
+        }
+        return {id_: name_map[path[0]][path[1]] for path, id_ in id_to_names.items() if path[1] in name_map[path[0]]}
+
+
+def _make_renaming_parameter_definition_sq(db_map, state):
+    """
+    Returns an entity class subquery which renames parameters.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        state (_ParameterRenamerState):
+
+    Returns:
+        Alias: a renaming parameter definition subquery
+    """
+    subquery = state.original_parameter_definition_sq
+    if not state.id_to_name:
+        return subquery
+    cases = [(subquery.c.id == id, new_name) for id, new_name in state.id_to_name.items()]
+    new_parameter_name = case(cases, else_=subquery.c.name)  # if not in the name map, just keep the original name
+    parameter_definition_sq = db_map.query(
+        subquery.c.id,
+        new_parameter_name.label("name"),
+        subquery.c.description,
+        subquery.c.entity_class_id,
+        subquery.c.object_class_id,
+        subquery.c.relationship_class_id,
+        subquery.c.default_value,
+        subquery.c.default_type,
+        subquery.c.list_value_id,
+        subquery.c.commit_id,
+        subquery.c.parameter_value_list_id,
+    ).subquery()
+    return parameter_definition_sq
```

### Comparing `spinedb_api-0.30.3/spinedb_api/filters/scenario_filter.py` & `spinedb_api-0.30.4/spinedb_api/filters/scenario_filter.py`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,266 +1,266 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Provides functions to apply filtering based on scenarios to subqueries.
-
-"""
-
-from functools import partial
-from sqlalchemy import desc, func
-from ..exception import SpineDBAPIError
-
-SCENARIO_FILTER_TYPE = "scenario_filter"
-SCENARIO_SHORTHAND_TAG = "scenario"
-
-
-def apply_scenario_filter_to_subqueries(db_map, scenario):
-    """
-    Replaces affected subqueries in ``db_map`` such that they return only values of given scenario.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map to alter
-        scenario (str or int): scenario name or id
-    """
-    state = _ScenarioFilterState(db_map, scenario)
-    make_parameter_value_sq = partial(_make_scenario_filtered_parameter_value_sq, state=state)
-    db_map.override_parameter_value_sq_maker(make_parameter_value_sq)
-    make_alternative_sq = partial(_make_scenario_filtered_alternative_sq, state=state)
-    db_map.override_alternative_sq_maker(make_alternative_sq)
-    make_scenario_sq = partial(_make_scenario_filtered_scenario_sq, state=state)
-    db_map.override_scenario_sq_maker(make_scenario_sq)
-    make_scenario_alternative_sq = partial(_make_scenario_filtered_scenario_alternative_sq, state=state)
-    db_map.override_scenario_alternative_sq_maker(make_scenario_alternative_sq)
-
-
-def scenario_filter_config(scenario):
-    """
-    Creates a config dict for scenario filter.
-
-    Args:
-        scenario (str): scenario name
-
-    Returns:
-        dict: filter configuration
-    """
-    return {"type": SCENARIO_FILTER_TYPE, "scenario": scenario}
-
-
-def scenario_filter_from_dict(db_map, config):
-    """
-    Applies scenario filter to given database map.
-
-    Args:
-        db_map (DatabaseMappingBase): target database map
-        config (dict): scenario filter configuration
-    """
-    apply_scenario_filter_to_subqueries(db_map, config["scenario"])
-
-
-def scenario_name_from_dict(config):
-    """
-    Returns scenario's name from filter config.
-
-    Args:
-        config (dict): scenario filter configuration
-
-    Returns:
-        str: scenario name or None if ``config`` is not a valid scenario filter configuration
-    """
-    if config["type"] != SCENARIO_FILTER_TYPE:
-        return None
-    return config["scenario"]
-
-
-def scenario_filter_config_to_shorthand(config):
-    """
-    Makes a shorthand string from scenario filter configuration.
-
-    Args:
-        config (dict): scenario filter configuration
-
-    Returns:
-        str: a shorthand string
-    """
-    return SCENARIO_SHORTHAND_TAG + ":" + config["scenario"]
-
-
-def scenario_filter_shorthand_to_config(shorthand):
-    """
-    Makes configuration dictionary out of a shorthand string.
-
-    Args:
-        shorthand (str): a shorthand string
-
-    Returns:
-        dict: scenario filter configuration
-    """
-    _, _, scenario = shorthand.partition(":")
-    return scenario_filter_config(scenario)
-
-
-class _ScenarioFilterState:
-    """
-    Internal state for :func:`_make_scenario_filtered_parameter_value_sq`.
-
-    Attributes:
-        original_alternative_sq (Alias): previous ``alternative_sq``
-        original_parameter_value_sq (Alias): previous ``parameter_value_sq``
-        original_scenario_alternative_sq (Alias): previous ``scenario_alternative_sq``
-        original_scenario_sq (Alias): previous ``scenario_sq``
-        scenario_alternative_ids (list of int): ids of selected scenario's alternatives
-        scenario_id (int): id of selected scenario
-    """
-
-    def __init__(self, db_map, scenario):
-        """
-        Args:
-            db_map (DatabaseMappingBase): database the state applies to
-            scenario (str or int): scenario name or ids
-        """
-        self.original_parameter_value_sq = db_map.parameter_value_sq
-        self.original_scenario_sq = db_map.scenario_sq
-        self.original_scenario_alternative_sq = db_map.scenario_alternative_sq
-        self.original_alternative_sq = db_map.alternative_sq
-        self.scenario_id = self._scenario_id(db_map, scenario)
-        self.scenario_alternative_ids, self.alternative_ids = self._scenario_alternative_ids(db_map)
-
-    @staticmethod
-    def _scenario_id(db_map, scenario):
-        """
-        Finds id for given scenario.
-
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            scenario (str or int): scenario name or id
-
-        Returns:
-            int: scenario's id
-        """
-        if isinstance(scenario, str):
-            scenario_name = scenario
-            scenario_id = (
-                db_map.query(db_map.scenario_sq.c.id).filter(db_map.scenario_sq.c.name == scenario_name).scalar()
-            )
-            if scenario_id is None:
-                raise SpineDBAPIError(f"Scenario '{scenario_name}' not found.")
-            return scenario_id
-        scenario_id = scenario
-        scenario = db_map.query(db_map.scenario_sq).filter(db_map.scenario_sq.c.id == scenario_id).one_or_none()
-        if scenario is None:
-            raise SpineDBAPIError(f"Scenario id {scenario_id} not found.")
-        return scenario_id
-
-    def _scenario_alternative_ids(self, db_map):
-        """
-        Finds scenario alternative and alternative ids of current scenario.
-
-        Args:
-            db_map (DatabaseMappingBase): a database map
-
-        Returns:
-            tuple: scenario alternative ids and alternative ids
-        """
-        alternative_ids = []
-        scenario_alternative_ids = []
-        for row in db_map.query(db_map.scenario_alternative_sq).filter(
-            db_map.scenario_alternative_sq.c.scenario_id == self.scenario_id
-        ):
-            scenario_alternative_ids.append(row.id)
-            alternative_ids.append(row.alternative_id)
-        return scenario_alternative_ids, alternative_ids
-
-
-def _make_scenario_filtered_parameter_value_sq(db_map, state):
-    """
-    Returns a scenario filtering subquery similar to :func:`DatabaseMappingBase.parameter_value_sq`.
-
-    This function can be used as replacement for parameter value subquery maker in :class:`DatabaseMappingBase`.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        state (_ScenarioFilterState): a state bound to ``db_map``
-
-    Returns:
-        Alias: a subquery for parameter value filtered by selected scenario
-    """
-    ext_parameter_value_sq = (
-        db_map.query(
-            state.original_parameter_value_sq,
-            func.row_number()
-            .over(
-                partition_by=[
-                    state.original_parameter_value_sq.c.parameter_definition_id,
-                    state.original_parameter_value_sq.c.entity_id,
-                ],
-                order_by=desc(db_map.scenario_alternative_sq.c.rank),
-            )
-            .label("max_rank_row_number"),
-        )
-        .filter(state.original_parameter_value_sq.c.alternative_id == db_map.scenario_alternative_sq.c.alternative_id)
-        .filter(db_map.scenario_alternative_sq.c.scenario_id == state.scenario_id)
-    ).subquery()
-    return db_map.query(ext_parameter_value_sq).filter(ext_parameter_value_sq.c.max_rank_row_number == 1).subquery()
-
-
-def _make_scenario_filtered_alternative_sq(db_map, state):
-    """
-    Returns an alternative filtering subquery similar to :func:`DatabaseMappingBase.alternative_sq`.
-
-    This function can be used as replacement for alternative subquery maker in :class:`DatabaseMappingBase`.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        state (_ScenarioFilterState): a state bound to ``db_map``
-
-    Returns:
-        Alias: a subquery for alternative filtered by selected scenario
-    """
-    alternative_sq = state.original_alternative_sq
-    return db_map.query(alternative_sq).filter(alternative_sq.c.id.in_(state.alternative_ids)).subquery()
-
-
-def _make_scenario_filtered_scenario_sq(db_map, state):
-    """
-    Returns a scenario filtering subquery similar to :func:`DatabaseMappingBase.scenario_sq`.
-
-    This function can be used as replacement for scenario subquery maker in :class:`DatabaseMappingBase`.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        state (_ScenarioFilterState): a state bound to ``db_map``
-
-    Returns:
-        Alias: a subquery for scenario filtered by selected scenario
-    """
-    scenario_sq = state.original_scenario_sq
-    return db_map.query(scenario_sq).filter(scenario_sq.c.id == state.scenario_id).subquery()
-
-
-def _make_scenario_filtered_scenario_alternative_sq(db_map, state):
-    """
-    Returns a scenario alternative filtering subquery similar to :func:`DatabaseMappingBase.scenario_alternative_sq`.
-
-    This function can be used as replacement for scenario alternative subquery maker in :class:`DatabaseMappingBase`.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        state (_ScenarioFilterState): a state bound to ``db_map``
-
-    Returns:
-        Alias: a subquery for scenario alternative filtered by selected scenario
-    """
-    scenario_alternative_sq = state.original_scenario_alternative_sq
-    return (
-        db_map.query(scenario_alternative_sq)
-        .filter(scenario_alternative_sq.c.id.in_(state.scenario_alternative_ids))
-        .subquery()
-    )
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Provides functions to apply filtering based on scenarios to subqueries.
+
+"""
+
+from functools import partial
+from sqlalchemy import desc, func
+from ..exception import SpineDBAPIError
+
+SCENARIO_FILTER_TYPE = "scenario_filter"
+SCENARIO_SHORTHAND_TAG = "scenario"
+
+
+def apply_scenario_filter_to_subqueries(db_map, scenario):
+    """
+    Replaces affected subqueries in ``db_map`` such that they return only values of given scenario.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map to alter
+        scenario (str or int): scenario name or id
+    """
+    state = _ScenarioFilterState(db_map, scenario)
+    make_parameter_value_sq = partial(_make_scenario_filtered_parameter_value_sq, state=state)
+    db_map.override_parameter_value_sq_maker(make_parameter_value_sq)
+    make_alternative_sq = partial(_make_scenario_filtered_alternative_sq, state=state)
+    db_map.override_alternative_sq_maker(make_alternative_sq)
+    make_scenario_sq = partial(_make_scenario_filtered_scenario_sq, state=state)
+    db_map.override_scenario_sq_maker(make_scenario_sq)
+    make_scenario_alternative_sq = partial(_make_scenario_filtered_scenario_alternative_sq, state=state)
+    db_map.override_scenario_alternative_sq_maker(make_scenario_alternative_sq)
+
+
+def scenario_filter_config(scenario):
+    """
+    Creates a config dict for scenario filter.
+
+    Args:
+        scenario (str): scenario name
+
+    Returns:
+        dict: filter configuration
+    """
+    return {"type": SCENARIO_FILTER_TYPE, "scenario": scenario}
+
+
+def scenario_filter_from_dict(db_map, config):
+    """
+    Applies scenario filter to given database map.
+
+    Args:
+        db_map (DatabaseMappingBase): target database map
+        config (dict): scenario filter configuration
+    """
+    apply_scenario_filter_to_subqueries(db_map, config["scenario"])
+
+
+def scenario_name_from_dict(config):
+    """
+    Returns scenario's name from filter config.
+
+    Args:
+        config (dict): scenario filter configuration
+
+    Returns:
+        str: scenario name or None if ``config`` is not a valid scenario filter configuration
+    """
+    if config["type"] != SCENARIO_FILTER_TYPE:
+        return None
+    return config["scenario"]
+
+
+def scenario_filter_config_to_shorthand(config):
+    """
+    Makes a shorthand string from scenario filter configuration.
+
+    Args:
+        config (dict): scenario filter configuration
+
+    Returns:
+        str: a shorthand string
+    """
+    return SCENARIO_SHORTHAND_TAG + ":" + config["scenario"]
+
+
+def scenario_filter_shorthand_to_config(shorthand):
+    """
+    Makes configuration dictionary out of a shorthand string.
+
+    Args:
+        shorthand (str): a shorthand string
+
+    Returns:
+        dict: scenario filter configuration
+    """
+    _, _, scenario = shorthand.partition(":")
+    return scenario_filter_config(scenario)
+
+
+class _ScenarioFilterState:
+    """
+    Internal state for :func:`_make_scenario_filtered_parameter_value_sq`.
+
+    Attributes:
+        original_alternative_sq (Alias): previous ``alternative_sq``
+        original_parameter_value_sq (Alias): previous ``parameter_value_sq``
+        original_scenario_alternative_sq (Alias): previous ``scenario_alternative_sq``
+        original_scenario_sq (Alias): previous ``scenario_sq``
+        scenario_alternative_ids (list of int): ids of selected scenario's alternatives
+        scenario_id (int): id of selected scenario
+    """
+
+    def __init__(self, db_map, scenario):
+        """
+        Args:
+            db_map (DatabaseMappingBase): database the state applies to
+            scenario (str or int): scenario name or ids
+        """
+        self.original_parameter_value_sq = db_map.parameter_value_sq
+        self.original_scenario_sq = db_map.scenario_sq
+        self.original_scenario_alternative_sq = db_map.scenario_alternative_sq
+        self.original_alternative_sq = db_map.alternative_sq
+        self.scenario_id = self._scenario_id(db_map, scenario)
+        self.scenario_alternative_ids, self.alternative_ids = self._scenario_alternative_ids(db_map)
+
+    @staticmethod
+    def _scenario_id(db_map, scenario):
+        """
+        Finds id for given scenario.
+
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            scenario (str or int): scenario name or id
+
+        Returns:
+            int: scenario's id
+        """
+        if isinstance(scenario, str):
+            scenario_name = scenario
+            scenario_id = (
+                db_map.query(db_map.scenario_sq.c.id).filter(db_map.scenario_sq.c.name == scenario_name).scalar()
+            )
+            if scenario_id is None:
+                raise SpineDBAPIError(f"Scenario '{scenario_name}' not found.")
+            return scenario_id
+        scenario_id = scenario
+        scenario = db_map.query(db_map.scenario_sq).filter(db_map.scenario_sq.c.id == scenario_id).one_or_none()
+        if scenario is None:
+            raise SpineDBAPIError(f"Scenario id {scenario_id} not found.")
+        return scenario_id
+
+    def _scenario_alternative_ids(self, db_map):
+        """
+        Finds scenario alternative and alternative ids of current scenario.
+
+        Args:
+            db_map (DatabaseMappingBase): a database map
+
+        Returns:
+            tuple: scenario alternative ids and alternative ids
+        """
+        alternative_ids = []
+        scenario_alternative_ids = []
+        for row in db_map.query(db_map.scenario_alternative_sq).filter(
+            db_map.scenario_alternative_sq.c.scenario_id == self.scenario_id
+        ):
+            scenario_alternative_ids.append(row.id)
+            alternative_ids.append(row.alternative_id)
+        return scenario_alternative_ids, alternative_ids
+
+
+def _make_scenario_filtered_parameter_value_sq(db_map, state):
+    """
+    Returns a scenario filtering subquery similar to :func:`DatabaseMappingBase.parameter_value_sq`.
+
+    This function can be used as replacement for parameter value subquery maker in :class:`DatabaseMappingBase`.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        state (_ScenarioFilterState): a state bound to ``db_map``
+
+    Returns:
+        Alias: a subquery for parameter value filtered by selected scenario
+    """
+    ext_parameter_value_sq = (
+        db_map.query(
+            state.original_parameter_value_sq,
+            func.row_number()
+            .over(
+                partition_by=[
+                    state.original_parameter_value_sq.c.parameter_definition_id,
+                    state.original_parameter_value_sq.c.entity_id,
+                ],
+                order_by=desc(db_map.scenario_alternative_sq.c.rank),
+            )
+            .label("max_rank_row_number"),
+        )
+        .filter(state.original_parameter_value_sq.c.alternative_id == db_map.scenario_alternative_sq.c.alternative_id)
+        .filter(db_map.scenario_alternative_sq.c.scenario_id == state.scenario_id)
+    ).subquery()
+    return db_map.query(ext_parameter_value_sq).filter(ext_parameter_value_sq.c.max_rank_row_number == 1).subquery()
+
+
+def _make_scenario_filtered_alternative_sq(db_map, state):
+    """
+    Returns an alternative filtering subquery similar to :func:`DatabaseMappingBase.alternative_sq`.
+
+    This function can be used as replacement for alternative subquery maker in :class:`DatabaseMappingBase`.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        state (_ScenarioFilterState): a state bound to ``db_map``
+
+    Returns:
+        Alias: a subquery for alternative filtered by selected scenario
+    """
+    alternative_sq = state.original_alternative_sq
+    return db_map.query(alternative_sq).filter(alternative_sq.c.id.in_(state.alternative_ids)).subquery()
+
+
+def _make_scenario_filtered_scenario_sq(db_map, state):
+    """
+    Returns a scenario filtering subquery similar to :func:`DatabaseMappingBase.scenario_sq`.
+
+    This function can be used as replacement for scenario subquery maker in :class:`DatabaseMappingBase`.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        state (_ScenarioFilterState): a state bound to ``db_map``
+
+    Returns:
+        Alias: a subquery for scenario filtered by selected scenario
+    """
+    scenario_sq = state.original_scenario_sq
+    return db_map.query(scenario_sq).filter(scenario_sq.c.id == state.scenario_id).subquery()
+
+
+def _make_scenario_filtered_scenario_alternative_sq(db_map, state):
+    """
+    Returns a scenario alternative filtering subquery similar to :func:`DatabaseMappingBase.scenario_alternative_sq`.
+
+    This function can be used as replacement for scenario alternative subquery maker in :class:`DatabaseMappingBase`.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        state (_ScenarioFilterState): a state bound to ``db_map``
+
+    Returns:
+        Alias: a subquery for scenario alternative filtered by selected scenario
+    """
+    scenario_alternative_sq = state.original_scenario_alternative_sq
+    return (
+        db_map.query(scenario_alternative_sq)
+        .filter(scenario_alternative_sq.c.id.in_(state.scenario_alternative_ids))
+        .subquery()
+    )
```

### Comparing `spinedb_api-0.30.3/spinedb_api/filters/tool_filter.py` & `spinedb_api-0.30.4/spinedb_api/filters/tool_filter.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,289 +1,293 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Provides functions to apply filtering based on tools to entity subqueries.
-
-"""
-from functools import partial
-from uuid import uuid4
-from sqlalchemy import and_, or_, case, func, Table, Column, ForeignKey
-from ..exception import SpineDBAPIError
-
-
-TOOL_FILTER_TYPE = "tool_filter"
-TOOL_SHORTHAND_TAG = "tool"
-
-
-def apply_tool_filter_to_entity_sq(db_map, tool):
-    """
-    Replaces entity subquery properties in ``db_map`` such that they return only values of given tool.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map to alter
-        tool (str or int): tool name or id
-    """
-    state = _ToolFilterState(db_map, tool)
-    filtering = partial(_make_tool_filtered_entity_sq, state=state)
-    db_map.override_entity_sq_maker(filtering)
-
-
-def tool_filter_config(tool):
-    """
-    Creates a config dict for tool filter.
-
-    Args:
-        tool (str): tool name
-
-    Returns:
-        dict: filter configuration
-    """
-    return {"type": TOOL_FILTER_TYPE, "tool": tool}
-
-
-def tool_filter_from_dict(db_map, config):
-    """
-    Applies tool filter to given database map.
-
-    Args:
-        db_map (DatabaseMappingBase): target database map
-        config (dict): tool filter configuration
-    """
-    apply_tool_filter_to_entity_sq(db_map, config["tool"])
-
-
-def tool_name_from_dict(config):
-    """
-    Returns tool's name from filter config.
-
-    Args:
-        config (dict): tool filter configuration
-
-    Returns:
-        str: tool name or None if ``config`` is not a valid tool filter configuration
-    """
-    if config["type"] != TOOL_FILTER_TYPE:
-        return None
-    return config["tool"]
-
-
-def tool_filter_config_to_shorthand(config):
-    """
-    Makes a shorthand string from tool filter configuration.
-
-    Args:
-        config (dict): tool filter configuration
-
-    Returns:
-        str: a shorthand string
-    """
-    return TOOL_SHORTHAND_TAG + ":" + config["tool"]
-
-
-def tool_filter_shorthand_to_config(shorthand):
-    """
-    Makes configuration dictionary out of a shorthand string.
-
-    Args:
-        shorthand (str): a shorthand string
-
-    Returns:
-        dict: tool filter configuration
-    """
-    _, _, tool = shorthand.partition(":")
-    return tool_filter_config(tool)
-
-
-class _ToolFilterState:
-    """
-    Internal state for :func:`_make_tool_filtered_entity_sq`
-
-    Attributes:
-        original_entity_sq (Alias): previous ``entity_sq``
-        table (Table): temporary table containing cached entity ids that passed the filter
-    """
-
-    def __init__(self, db_map, tool):
-        """
-        Args:
-            db_map (DatabaseMappingBase): database the state applies to
-            tool (str or int): tool name or id
-        """
-        self.original_entity_sq = db_map.entity_sq
-        tool_id = self._tool_id(db_map, tool)
-        table_name = "tool_filter_cache_" + uuid4().hex
-        column = Column("entity_id", ForeignKey("entity.id"))
-        self.table = db_map.make_temporary_table(table_name, column)
-        statement = self.table.insert().from_select(["entity_id"], self.active_entity_id_sq(db_map, tool_id))
-        db_map.connection.execute(statement)
-
-    @staticmethod
-    def _tool_id(db_map, tool):
-        """
-        Finds id for given tool.
-
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            tool (str or int): tool name or id
-
-        Returns:
-            int or NoneType: tool id
-        """
-        if isinstance(tool, str):
-            tool_name = tool
-            tool_id = db_map.query(db_map.tool_sq.c.id).filter(db_map.tool_sq.c.name == tool_name).scalar()
-            if tool_id is None:
-                raise SpineDBAPIError(f"Tool '{tool_name}' not found.")
-            return tool_id
-        tool_id = tool
-        tool = db_map.query(db_map.tool_sq).filter(db_map.tool_sq.c.id == tool_id).one_or_none()
-        if tool is None:
-            raise SpineDBAPIError(f"Tool id {tool_id} not found.")
-        return tool_id
-
-    @staticmethod
-    def active_entity_id_sq(db_map, tool_id):
-        """
-        Creates a subquery that returns entity ids that pass the tool filter.
-
-        Args:
-            db_map (DatabaseMappingBase): database mapping
-            tool_id (int): tool identifier
-
-        Returns:
-            Alias: subquery
-        """
-        tool_feature_method_sq = _make_ext_tool_feature_method_sq(db_map, tool_id)
-
-        method_filter = _make_method_filter(
-            tool_feature_method_sq, db_map.parameter_value_sq, db_map.parameter_definition_sq
-        )
-        required_filter = _make_required_filter(tool_feature_method_sq, db_map.parameter_value_sq)
-
-        return (
-            db_map.query(db_map.entity_sq.c.id)
-            .outerjoin(
-                db_map.parameter_definition_sq,
-                db_map.parameter_definition_sq.c.entity_class_id == db_map.entity_sq.c.class_id,
-            )
-            .outerjoin(
-                db_map.parameter_value_sq,
-                and_(
-                    db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
-                    db_map.parameter_value_sq.c.entity_id == db_map.entity_sq.c.id,
-                ),
-            )
-            .outerjoin(
-                tool_feature_method_sq,
-                tool_feature_method_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
-            )
-            .group_by(db_map.entity_sq.c.id)
-            .having(and_(func.min(method_filter).is_(True), func.min(required_filter).is_(True)))
-        ).subquery()
-
-
-def _make_ext_tool_feature_method_sq(db_map, tool_id):
-    """
-    Returns an extended tool_feature_method subquery that has ``None`` whenever no method is specified.
-    Used by ``_make_tool_filtered_entity_sq``
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        tool_id (int): tool id
-
-    Returns:
-        Alias: a subquery for tool_feature_method
-    """
-    return (
-        db_map.query(
-            db_map.ext_tool_feature_sq.c.tool_id,
-            db_map.ext_tool_feature_sq.c.parameter_definition_id,
-            db_map.ext_tool_feature_sq.c.required,
-            db_map.list_value_sq.c.id.label("method_list_value_id"),
-        )
-        .outerjoin(
-            db_map.tool_feature_method_sq,
-            db_map.tool_feature_method_sq.c.tool_feature_id == db_map.ext_tool_feature_sq.c.id,
-        )
-        .outerjoin(
-            db_map.list_value_sq,
-            and_(
-                db_map.ext_tool_feature_sq.c.parameter_value_list_id == db_map.list_value_sq.c.parameter_value_list_id,
-                db_map.tool_feature_method_sq.c.method_index == db_map.list_value_sq.c.index,
-            ),
-        )
-        .filter(db_map.ext_tool_feature_sq.c.tool_id == tool_id)
-    ).subquery()
-
-
-def _make_method_filter(tool_feature_method_sq, parameter_value_sq, parameter_definition_sq):
-    # Filter passes if either:
-    # 1) parameter definition is not a feature for the tool
-    # 2) method is not specified
-    # 3) value is equal to method
-    # 4) value is not specified, but default value is equal to method
-    return case(
-        [
-            (
-                or_(
-                    tool_feature_method_sq.c.parameter_definition_id.is_(None),
-                    tool_feature_method_sq.c.method_list_value_id.is_(None),
-                    parameter_value_sq.c.list_value_id == tool_feature_method_sq.c.method_list_value_id,
-                    and_(
-                        parameter_value_sq.c.value.is_(None),
-                        parameter_definition_sq.c.list_value_id == tool_feature_method_sq.c.method_list_value_id,
-                    ),
-                ),
-                True,
-            )
-        ],
-        else_=False,
-    )
-
-
-def _make_required_filter(tool_feature_method_sq, parameter_value_sq):
-    # Filter passes if either:
-    # 1) parameter definition is not a feature for the tool
-    # 2) value is specified
-    # 3) method is not required
-    return case(
-        [
-            (
-                or_(
-                    tool_feature_method_sq.c.parameter_definition_id.is_(None),
-                    parameter_value_sq.c.value.isnot(None),
-                    tool_feature_method_sq.c.required.is_(False),
-                ),
-                True,
-            )
-        ],
-        else_=False,
-    )
-
-
-def _make_tool_filtered_entity_sq(db_map, state):
-    """
-    Returns a tool filtering subquery similar to :func:`DatabaseMappingBase.entity_sq`.
-
-    This function can be used as replacement for entity subquery maker in :class:`DatabaseMappingBase`.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        state (_ScenarioFilterState): a state bound to ``db_map``
-
-    Returns:
-        Alias: a subquery for entity filtered by selected tool
-    """
-    return (
-        db_map.query(state.original_entity_sq)
-        .join(state.table, state.original_entity_sq.c.id == state.table.c.entity_id)
-        .subquery()
-    )
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Provides functions to apply filtering based on tools to entity subqueries.
+
+"""
+from functools import partial
+from uuid import uuid4
+from sqlalchemy import and_, or_, case, func, Table, Column, ForeignKey
+from ..exception import SpineDBAPIError
+from sqlalchemy.orm import Session
+
+
+TOOL_FILTER_TYPE = "tool_filter"
+TOOL_SHORTHAND_TAG = "tool"
+
+
+def apply_tool_filter_to_entity_sq(db_map, tool):
+    """
+    Replaces entity subquery properties in ``db_map`` such that they return only values of given tool.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map to alter
+        tool (str or int): tool name or id
+    """
+    state = _ToolFilterState(db_map, tool)
+    filtering = partial(_make_tool_filtered_entity_sq, state=state)
+    db_map.override_entity_sq_maker(filtering)
+
+
+def tool_filter_config(tool):
+    """
+    Creates a config dict for tool filter.
+
+    Args:
+        tool (str): tool name
+
+    Returns:
+        dict: filter configuration
+    """
+    return {"type": TOOL_FILTER_TYPE, "tool": tool}
+
+
+def tool_filter_from_dict(db_map, config):
+    """
+    Applies tool filter to given database map.
+
+    Args:
+        db_map (DatabaseMappingBase): target database map
+        config (dict): tool filter configuration
+    """
+    apply_tool_filter_to_entity_sq(db_map, config["tool"])
+
+
+def tool_name_from_dict(config):
+    """
+    Returns tool's name from filter config.
+
+    Args:
+        config (dict): tool filter configuration
+
+    Returns:
+        str: tool name or None if ``config`` is not a valid tool filter configuration
+    """
+    if config["type"] != TOOL_FILTER_TYPE:
+        return None
+    return config["tool"]
+
+
+def tool_filter_config_to_shorthand(config):
+    """
+    Makes a shorthand string from tool filter configuration.
+
+    Args:
+        config (dict): tool filter configuration
+
+    Returns:
+        str: a shorthand string
+    """
+    return TOOL_SHORTHAND_TAG + ":" + config["tool"]
+
+
+def tool_filter_shorthand_to_config(shorthand):
+    """
+    Makes configuration dictionary out of a shorthand string.
+
+    Args:
+        shorthand (str): a shorthand string
+
+    Returns:
+        dict: tool filter configuration
+    """
+    _, _, tool = shorthand.partition(":")
+    return tool_filter_config(tool)
+
+
+class _ToolFilterState:
+    """
+    Internal state for :func:`_make_tool_filtered_entity_sq`
+
+    Attributes:
+        original_entity_sq (Alias): previous ``entity_sq``
+        table (Table): temporary table containing cached entity ids that passed the filter
+    """
+
+    def __init__(self, db_map, tool):
+        """
+        Args:
+            db_map (DatabaseMappingBase): database the state applies to
+            tool (str or int): tool name or id
+        """
+        self.original_entity_sq = db_map.entity_sq
+        tool_id = self._tool_id(db_map, tool)
+        table_name = "tool_filter_cache_" + uuid4().hex
+        column = Column("entity_id", ForeignKey("entity.id"))
+        self.table = db_map.make_temporary_table(table_name, column)
+        statement = self.table.insert().from_select(["entity_id"], self.active_entity_id_sq(db_map, tool_id))
+        db_map.connection.execute(statement)
+
+    @staticmethod
+    def _tool_id(db_map, tool):
+        """
+        Finds id for given tool.
+
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            tool (str or int): tool name or id
+
+        Returns:
+            int or NoneType: tool id
+        """
+        if isinstance(tool, str):
+            tool_name = tool
+            tool_id = db_map.query(db_map.tool_sq.c.id).filter(db_map.tool_sq.c.name == tool_name).scalar()
+            if tool_id is None:
+                raise SpineDBAPIError(f"Tool '{tool_name}' not found.")
+            return tool_id
+        tool_id = tool
+        tool = db_map.query(db_map.tool_sq).filter(db_map.tool_sq.c.id == tool_id).one_or_none()
+        if tool is None:
+            raise SpineDBAPIError(f"Tool id {tool_id} not found.")
+        return tool_id
+
+    @staticmethod
+    def active_entity_id_sq(db_map, tool_id):
+        """
+        Creates a subquery that returns entity ids that pass the tool filter.
+
+        Args:
+            db_map (DatabaseMappingBase): database mapping
+            tool_id (int): tool identifier
+
+        Returns:
+            Alias: subquery
+        """
+        tool_feature_method_sq = _make_ext_tool_feature_method_sq(db_map, tool_id)
+
+        method_filter = _make_method_filter(
+            tool_feature_method_sq, db_map.parameter_value_sq, db_map.parameter_definition_sq
+        )
+        required_filter = _make_required_filter(tool_feature_method_sq, db_map.parameter_value_sq)
+
+        return (
+            db_map.query(db_map.entity_sq.c.id)
+            .outerjoin(
+                db_map.parameter_definition_sq,
+                db_map.parameter_definition_sq.c.entity_class_id == db_map.entity_sq.c.class_id,
+            )
+            .outerjoin(
+                db_map.parameter_value_sq,
+                and_(
+                    db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
+                    db_map.parameter_value_sq.c.entity_id == db_map.entity_sq.c.id,
+                ),
+            )
+            .outerjoin(
+                tool_feature_method_sq,
+                tool_feature_method_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
+            )
+            .group_by(db_map.entity_sq.c.id)
+            .having(and_(func.min(method_filter).is_(True), func.min(required_filter).is_(True)))
+        ).subquery()
+
+
+def _make_ext_tool_feature_method_sq(db_map, tool_id):
+    """
+    Returns an extended tool_feature_method subquery that has ``None`` whenever no method is specified.
+    Used by ``_make_tool_filtered_entity_sq``
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        tool_id (int): tool id
+
+    Returns:
+        Alias: a subquery for tool_feature_method
+    """
+    return (
+        db_map.query(
+            db_map.ext_tool_feature_sq.c.tool_id,
+            db_map.ext_tool_feature_sq.c.parameter_definition_id,
+            db_map.ext_tool_feature_sq.c.required,
+            db_map.list_value_sq.c.id.label("method_list_value_id"),
+        )
+        .outerjoin(
+            db_map.tool_feature_method_sq,
+            db_map.tool_feature_method_sq.c.tool_feature_id == db_map.ext_tool_feature_sq.c.id,
+        )
+        .outerjoin(
+            db_map.list_value_sq,
+            and_(
+                db_map.ext_tool_feature_sq.c.parameter_value_list_id == db_map.list_value_sq.c.parameter_value_list_id,
+                db_map.tool_feature_method_sq.c.method_index == db_map.list_value_sq.c.index,
+            ),
+        )
+        .filter(db_map.ext_tool_feature_sq.c.tool_id == tool_id)
+    ).subquery()
+
+
+def _make_method_filter(tool_feature_method_sq, parameter_value_sq, parameter_definition_sq):
+    # Filter passes if either:
+    # 1) parameter definition is not a feature for the tool
+    # 2) method is not specified
+    # 3) value is contained in the listed values of the method
+    # 4) value is not specified, but default value is contained in the listed values of the method
+
+    method_value_id_list = Session().query(tool_feature_method_sq.c.method_list_value_id).subquery()
+
+    return case(
+        [
+            (
+                or_(
+                    tool_feature_method_sq.c.parameter_definition_id.is_(None),
+                    tool_feature_method_sq.c.method_list_value_id.is_(None),
+                    parameter_value_sq.c.list_value_id.in_(method_value_id_list),
+                    and_(
+                        parameter_value_sq.c.value.is_(None),
+                        parameter_definition_sq.c.list_value_id.in_(method_value_id_list),
+                    ),
+                ),
+                True,
+            )
+        ],
+        else_=False,
+    )
+
+
+def _make_required_filter(tool_feature_method_sq, parameter_value_sq):
+    # Filter passes if either:
+    # 1) parameter definition is not a feature for the tool
+    # 2) value is specified
+    # 3) method is not required
+    return case(
+        [
+            (
+                or_(
+                    tool_feature_method_sq.c.parameter_definition_id.is_(None),
+                    parameter_value_sq.c.value.isnot(None),
+                    tool_feature_method_sq.c.required.is_(False),
+                ),
+                True,
+            )
+        ],
+        else_=False,
+    )
+
+
+def _make_tool_filtered_entity_sq(db_map, state):
+    """
+    Returns a tool filtering subquery similar to :func:`DatabaseMappingBase.entity_sq`.
+
+    This function can be used as replacement for entity subquery maker in :class:`DatabaseMappingBase`.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        state (_ScenarioFilterState): a state bound to ``db_map``
+
+    Returns:
+        Alias: a subquery for entity filtered by selected tool
+    """
+    return (
+        db_map.query(state.original_entity_sq)
+        .join(state.table, state.original_entity_sq.c.id == state.table.c.entity_id)
+        .subquery()
+    )
```

### Comparing `spinedb_api-0.30.3/spinedb_api/filters/tools.py` & `spinedb_api-0.30.4/spinedb_api/filters/tools.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,334 +1,334 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Tools and utilities to work with filters, manipulators and database URLs.
-
-"""
-from json import dump, load
-from urllib.parse import parse_qs, urlencode, urlparse, urlunparse
-from .alternative_filter import (
-    ALTERNATIVE_FILTER_SHORTHAND_TAG,
-    ALTERNATIVE_FILTER_TYPE,
-    alternative_filter_config,
-    alternative_filter_from_dict,
-    alternative_filter_config_to_shorthand,
-    alternative_filter_shorthand_to_config,
-    alternative_names_from_dict,
-)
-from .renamer import (
-    ENTITY_CLASS_RENAMER_SHORTHAND_TAG,
-    ENTITY_CLASS_RENAMER_TYPE,
-    PARAMETER_RENAMER_SHORTHAND_TAG,
-    PARAMETER_RENAMER_TYPE,
-    entity_class_renamer_config_to_shorthand,
-    entity_class_renamer_from_dict,
-    entity_class_renamer_shorthand_to_config,
-    parameter_renamer_config_to_shorthand,
-    parameter_renamer_from_dict,
-    parameter_renamer_shorthand_to_config,
-)
-from .scenario_filter import (
-    SCENARIO_FILTER_TYPE,
-    SCENARIO_SHORTHAND_TAG,
-    scenario_filter_config,
-    scenario_filter_config_to_shorthand,
-    scenario_filter_from_dict,
-    scenario_filter_shorthand_to_config,
-    scenario_name_from_dict,
-)
-from .tool_filter import (
-    TOOL_SHORTHAND_TAG,
-    TOOL_FILTER_TYPE,
-    tool_filter_config,
-    tool_filter_config_to_shorthand,
-    tool_filter_from_dict,
-    tool_filter_shorthand_to_config,
-    tool_name_from_dict,
-)
-from .value_transformer import (
-    VALUE_TRANSFORMER_SHORTHAND_TAG,
-    VALUE_TRANSFORMER_TYPE,
-    value_transformer_shorthand_to_config,
-    value_transformer_from_dict,
-    value_transformer_config_to_shorthand,
-)
-from .execution_filter import (
-    EXECUTION_SHORTHAND_TAG,
-    EXECUTION_FILTER_TYPE,
-    execution_filter_config,
-    execution_filter_config_to_shorthand,
-    execution_filter_from_dict,
-    execution_filter_shorthand_to_config,
-)
-
-FILTER_IDENTIFIER = "spinedbfilter"
-SHORTHAND_TAG = "cfg:"
-
-
-def apply_filter_stack(db_map, stack):
-    """
-    Applies stack of filters and manipulator to given database map.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        stack (list): a stack of database filters and manipulators
-    """
-    appliers = {
-        ALTERNATIVE_FILTER_TYPE: alternative_filter_from_dict,
-        ENTITY_CLASS_RENAMER_TYPE: entity_class_renamer_from_dict,
-        EXECUTION_FILTER_TYPE: execution_filter_from_dict,
-        PARAMETER_RENAMER_TYPE: parameter_renamer_from_dict,
-        SCENARIO_FILTER_TYPE: scenario_filter_from_dict,
-        TOOL_FILTER_TYPE: tool_filter_from_dict,
-        VALUE_TRANSFORMER_TYPE: value_transformer_from_dict,
-    }
-    for filter_ in stack:
-        appliers[filter_["type"]](db_map, filter_)
-
-
-def load_filters(configs):
-    """
-    Loads filter configurations from disk as needed and constructs a filter stack.
-
-    Args:
-        configs (list): list of filter config dicts and paths to filter configuration files
-
-    Returns:
-        list of dict: filter stack
-    """
-    stack = list()
-    for config in configs:
-        if isinstance(config, str):
-            with open(config) as config_file:
-                stack.append(load(config_file))
-        else:
-            stack.append(config)
-    return stack
-
-
-def store_filter(config, out):
-    """
-    Writes filter config to an output stream.
-
-    Args:
-        config (dict): filter config to write
-        out (TextIOBase): a file-like object that supports writing
-    """
-    dump(config, out)
-
-
-def filter_config(filter_type, value):
-    """
-    Creates a config dict for filter of given type.
-
-    Args:
-        filter_type (str): the filter type (e.g. "scenario_filter")
-        value (object): the filter value (e.g. scenario name)
-
-    Returns:
-        dict: filter configuration
-    """
-    return {
-        SCENARIO_FILTER_TYPE: scenario_filter_config,
-        TOOL_FILTER_TYPE: tool_filter_config,
-        ALTERNATIVE_FILTER_TYPE: alternative_filter_config,
-        EXECUTION_FILTER_TYPE: execution_filter_config,
-    }[filter_type](value)
-
-
-def append_filter_config(url, config):
-    """
-    Appends a filter config to given url.
-
-    ``config`` can either be a configuration dictionary or a path to a JSON file that contains the dictionary.
-
-    Args:
-        url (str): base URL
-        config (str or dict): path to the config file or config as a ``dict``.
-
-    Returns:
-        str: the modified URL
-    """
-    url = urlparse(url)
-    query = parse_qs(url.query)
-    filters = query.setdefault(FILTER_IDENTIFIER, list())
-    if isinstance(config, dict):
-        config = config_to_shorthand(config)
-    if config not in filters:
-        filters.append(config)
-    url = url._replace(query=urlencode(query, doseq=True))
-    if not url.hostname:
-        url = url._replace(path="//" + url.path)
-    return url.geturl()
-
-
-def filter_configs(url):
-    """
-    Returns filter configs or file paths from given URL.
-
-    Args:
-        url (str): a URL
-
-    Returns:
-        list: a list of filter configs
-    """
-    parsed = urlparse(url)
-    query = parse_qs(parsed.query)
-    try:
-        filters = query[FILTER_IDENTIFIER]
-    except KeyError:
-        return []
-    parsed_filters = list()
-    for filter_ in filters:
-        if filter_.startswith(SHORTHAND_TAG):
-            parsed_filters.append(_parse_shorthand(filter_[len(SHORTHAND_TAG) :]))
-        else:
-            parsed_filters.append(filter_)
-    return parsed_filters
-
-
-def pop_filter_configs(url):
-    """
-    Pops filter config files and dicts from URL's query part.
-
-    Args:
-        url (str): a URL
-
-    Returns:
-        tuple: a list of filter configs and the 'popped from' URL
-    """
-    parsed = urlparse(url)
-    query = parse_qs(parsed.query)
-    try:
-        filters = query.pop(FILTER_IDENTIFIER)
-    except KeyError:
-        return [], url
-    parsed_filters = list()
-    for filter_ in filters:
-        if filter_.startswith(SHORTHAND_TAG):
-            parsed_filters.append(_parse_shorthand(filter_[len(SHORTHAND_TAG) :]))
-        else:
-            parsed_filters.append(filter_)
-    parsed = parsed._replace(query=urlencode(query, doseq=True))
-    if not parsed.hostname:
-        parsed = parsed._replace(path="//" + parsed.path)
-    return parsed_filters, urlunparse(parsed)
-
-
-def clear_filter_configs(url):
-    """
-    Removes filter configuration queries from given URL.
-
-    Args:
-        url (str): a URL
-
-    Returns:
-        str: a cleared URL
-    """
-    parsed = urlparse(url)
-    query = parse_qs(parsed.query)
-    try:
-        del query[FILTER_IDENTIFIER]
-    except KeyError:
-        return url
-    parsed = parsed._replace(query=urlencode(query, doseq=True), path="//" + parsed.path)
-    return urlunparse(parsed)
-
-
-def ensure_filtering(url, fallback_alternative=None):
-    """
-    Appends fallback filters to given url if it does not contain corresponding filter already.
-
-    Args:
-        url (str): database URL
-        fallback_alternative (str, optional): fallback alternative if URL does not contain scenario or alternative filters
-
-    Returns:
-        str: database URL
-    """
-    configs = filter_configs(url)
-    stack = load_filters(configs)
-    if fallback_alternative is not None:
-        alternative_found = False
-        for config in stack:
-            scenario = scenario_name_from_dict(config)
-            if scenario is not None:
-                alternative_found = True
-                break
-            alternatives = alternative_names_from_dict(config)
-            if alternatives:
-                alternative_found = True
-                break
-        if not alternative_found:
-            return append_filter_config(url, alternative_filter_config([fallback_alternative]))
-    return url
-
-
-def config_to_shorthand(config):
-    """
-    Converts a filter config dictionary to shorthand.
-
-    Args:
-        config (dict): filter configuration
-
-    Returns:
-        str: config shorthand
-    """
-    shorthands = {
-        ALTERNATIVE_FILTER_TYPE: alternative_filter_config_to_shorthand,
-        ENTITY_CLASS_RENAMER_TYPE: entity_class_renamer_config_to_shorthand,
-        PARAMETER_RENAMER_TYPE: parameter_renamer_config_to_shorthand,
-        SCENARIO_FILTER_TYPE: scenario_filter_config_to_shorthand,
-        TOOL_FILTER_TYPE: tool_filter_config_to_shorthand,
-        EXECUTION_FILTER_TYPE: execution_filter_config_to_shorthand,
-        VALUE_TRANSFORMER_TYPE: value_transformer_config_to_shorthand,
-    }
-    return SHORTHAND_TAG + shorthands[config["type"]](config)
-
-
-def _parse_shorthand(shorthand):
-    """
-    Converts shorthand filter config into configuration dictionary.
-
-    Args:
-        shorthand (str): a shorthand config string
-
-    Returns:
-        dict: filter configuration dictionary
-    """
-    shorthand_parsers = {
-        ALTERNATIVE_FILTER_SHORTHAND_TAG: alternative_filter_shorthand_to_config,
-        ENTITY_CLASS_RENAMER_SHORTHAND_TAG: entity_class_renamer_shorthand_to_config,
-        PARAMETER_RENAMER_SHORTHAND_TAG: parameter_renamer_shorthand_to_config,
-        SCENARIO_SHORTHAND_TAG: scenario_filter_shorthand_to_config,
-        TOOL_SHORTHAND_TAG: tool_filter_shorthand_to_config,
-        EXECUTION_SHORTHAND_TAG: execution_filter_shorthand_to_config,
-        VALUE_TRANSFORMER_SHORTHAND_TAG: value_transformer_shorthand_to_config,
-    }
-    tag, _, _ = shorthand.partition(":")
-    return shorthand_parsers[tag](shorthand)
-
-
-def name_from_dict(config):
-    """
-    Returns scenario or tool name from filter config.
-
-    Args:
-        config (dict): filter configuration
-
-    Returns:
-        str: name or None if ``config`` is not a valid 'name' filter configuration
-    """
-    func = {SCENARIO_FILTER_TYPE: scenario_name_from_dict, TOOL_FILTER_TYPE: tool_name_from_dict}.get(config["type"])
-    if func is None:
-        return None
-    return func(config)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Tools and utilities to work with filters, manipulators and database URLs.
+
+"""
+from json import dump, load
+from urllib.parse import parse_qs, urlencode, urlparse, urlunparse
+from .alternative_filter import (
+    ALTERNATIVE_FILTER_SHORTHAND_TAG,
+    ALTERNATIVE_FILTER_TYPE,
+    alternative_filter_config,
+    alternative_filter_from_dict,
+    alternative_filter_config_to_shorthand,
+    alternative_filter_shorthand_to_config,
+    alternative_names_from_dict,
+)
+from .renamer import (
+    ENTITY_CLASS_RENAMER_SHORTHAND_TAG,
+    ENTITY_CLASS_RENAMER_TYPE,
+    PARAMETER_RENAMER_SHORTHAND_TAG,
+    PARAMETER_RENAMER_TYPE,
+    entity_class_renamer_config_to_shorthand,
+    entity_class_renamer_from_dict,
+    entity_class_renamer_shorthand_to_config,
+    parameter_renamer_config_to_shorthand,
+    parameter_renamer_from_dict,
+    parameter_renamer_shorthand_to_config,
+)
+from .scenario_filter import (
+    SCENARIO_FILTER_TYPE,
+    SCENARIO_SHORTHAND_TAG,
+    scenario_filter_config,
+    scenario_filter_config_to_shorthand,
+    scenario_filter_from_dict,
+    scenario_filter_shorthand_to_config,
+    scenario_name_from_dict,
+)
+from .tool_filter import (
+    TOOL_SHORTHAND_TAG,
+    TOOL_FILTER_TYPE,
+    tool_filter_config,
+    tool_filter_config_to_shorthand,
+    tool_filter_from_dict,
+    tool_filter_shorthand_to_config,
+    tool_name_from_dict,
+)
+from .value_transformer import (
+    VALUE_TRANSFORMER_SHORTHAND_TAG,
+    VALUE_TRANSFORMER_TYPE,
+    value_transformer_shorthand_to_config,
+    value_transformer_from_dict,
+    value_transformer_config_to_shorthand,
+)
+from .execution_filter import (
+    EXECUTION_SHORTHAND_TAG,
+    EXECUTION_FILTER_TYPE,
+    execution_filter_config,
+    execution_filter_config_to_shorthand,
+    execution_filter_from_dict,
+    execution_filter_shorthand_to_config,
+)
+
+FILTER_IDENTIFIER = "spinedbfilter"
+SHORTHAND_TAG = "cfg:"
+
+
+def apply_filter_stack(db_map, stack):
+    """
+    Applies stack of filters and manipulator to given database map.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        stack (list): a stack of database filters and manipulators
+    """
+    appliers = {
+        ALTERNATIVE_FILTER_TYPE: alternative_filter_from_dict,
+        ENTITY_CLASS_RENAMER_TYPE: entity_class_renamer_from_dict,
+        EXECUTION_FILTER_TYPE: execution_filter_from_dict,
+        PARAMETER_RENAMER_TYPE: parameter_renamer_from_dict,
+        SCENARIO_FILTER_TYPE: scenario_filter_from_dict,
+        TOOL_FILTER_TYPE: tool_filter_from_dict,
+        VALUE_TRANSFORMER_TYPE: value_transformer_from_dict,
+    }
+    for filter_ in stack:
+        appliers[filter_["type"]](db_map, filter_)
+
+
+def load_filters(configs):
+    """
+    Loads filter configurations from disk as needed and constructs a filter stack.
+
+    Args:
+        configs (list): list of filter config dicts and paths to filter configuration files
+
+    Returns:
+        list of dict: filter stack
+    """
+    stack = list()
+    for config in configs:
+        if isinstance(config, str):
+            with open(config) as config_file:
+                stack.append(load(config_file))
+        else:
+            stack.append(config)
+    return stack
+
+
+def store_filter(config, out):
+    """
+    Writes filter config to an output stream.
+
+    Args:
+        config (dict): filter config to write
+        out (TextIOBase): a file-like object that supports writing
+    """
+    dump(config, out)
+
+
+def filter_config(filter_type, value):
+    """
+    Creates a config dict for filter of given type.
+
+    Args:
+        filter_type (str): the filter type (e.g. "scenario_filter")
+        value (object): the filter value (e.g. scenario name)
+
+    Returns:
+        dict: filter configuration
+    """
+    return {
+        SCENARIO_FILTER_TYPE: scenario_filter_config,
+        TOOL_FILTER_TYPE: tool_filter_config,
+        ALTERNATIVE_FILTER_TYPE: alternative_filter_config,
+        EXECUTION_FILTER_TYPE: execution_filter_config,
+    }[filter_type](value)
+
+
+def append_filter_config(url, config):
+    """
+    Appends a filter config to given url.
+
+    ``config`` can either be a configuration dictionary or a path to a JSON file that contains the dictionary.
+
+    Args:
+        url (str): base URL
+        config (str or dict): path to the config file or config as a ``dict``.
+
+    Returns:
+        str: the modified URL
+    """
+    url = urlparse(url)
+    query = parse_qs(url.query)
+    filters = query.setdefault(FILTER_IDENTIFIER, list())
+    if isinstance(config, dict):
+        config = config_to_shorthand(config)
+    if config not in filters:
+        filters.append(config)
+    url = url._replace(query=urlencode(query, doseq=True))
+    if not url.hostname:
+        url = url._replace(path="//" + url.path)
+    return url.geturl()
+
+
+def filter_configs(url):
+    """
+    Returns filter configs or file paths from given URL.
+
+    Args:
+        url (str): a URL
+
+    Returns:
+        list: a list of filter configs
+    """
+    parsed = urlparse(url)
+    query = parse_qs(parsed.query)
+    try:
+        filters = query[FILTER_IDENTIFIER]
+    except KeyError:
+        return []
+    parsed_filters = list()
+    for filter_ in filters:
+        if filter_.startswith(SHORTHAND_TAG):
+            parsed_filters.append(_parse_shorthand(filter_[len(SHORTHAND_TAG) :]))
+        else:
+            parsed_filters.append(filter_)
+    return parsed_filters
+
+
+def pop_filter_configs(url):
+    """
+    Pops filter config files and dicts from URL's query part.
+
+    Args:
+        url (str): a URL
+
+    Returns:
+        tuple: a list of filter configs and the 'popped from' URL
+    """
+    parsed = urlparse(url)
+    query = parse_qs(parsed.query)
+    try:
+        filters = query.pop(FILTER_IDENTIFIER)
+    except KeyError:
+        return [], url
+    parsed_filters = list()
+    for filter_ in filters:
+        if filter_.startswith(SHORTHAND_TAG):
+            parsed_filters.append(_parse_shorthand(filter_[len(SHORTHAND_TAG) :]))
+        else:
+            parsed_filters.append(filter_)
+    parsed = parsed._replace(query=urlencode(query, doseq=True))
+    if not parsed.hostname:
+        parsed = parsed._replace(path="//" + parsed.path)
+    return parsed_filters, urlunparse(parsed)
+
+
+def clear_filter_configs(url):
+    """
+    Removes filter configuration queries from given URL.
+
+    Args:
+        url (str): a URL
+
+    Returns:
+        str: a cleared URL
+    """
+    parsed = urlparse(url)
+    query = parse_qs(parsed.query)
+    try:
+        del query[FILTER_IDENTIFIER]
+    except KeyError:
+        return url
+    parsed = parsed._replace(query=urlencode(query, doseq=True), path="//" + parsed.path)
+    return urlunparse(parsed)
+
+
+def ensure_filtering(url, fallback_alternative=None):
+    """
+    Appends fallback filters to given url if it does not contain corresponding filter already.
+
+    Args:
+        url (str): database URL
+        fallback_alternative (str, optional): fallback alternative if URL does not contain scenario or alternative filters
+
+    Returns:
+        str: database URL
+    """
+    configs = filter_configs(url)
+    stack = load_filters(configs)
+    if fallback_alternative is not None:
+        alternative_found = False
+        for config in stack:
+            scenario = scenario_name_from_dict(config)
+            if scenario is not None:
+                alternative_found = True
+                break
+            alternatives = alternative_names_from_dict(config)
+            if alternatives:
+                alternative_found = True
+                break
+        if not alternative_found:
+            return append_filter_config(url, alternative_filter_config([fallback_alternative]))
+    return url
+
+
+def config_to_shorthand(config):
+    """
+    Converts a filter config dictionary to shorthand.
+
+    Args:
+        config (dict): filter configuration
+
+    Returns:
+        str: config shorthand
+    """
+    shorthands = {
+        ALTERNATIVE_FILTER_TYPE: alternative_filter_config_to_shorthand,
+        ENTITY_CLASS_RENAMER_TYPE: entity_class_renamer_config_to_shorthand,
+        PARAMETER_RENAMER_TYPE: parameter_renamer_config_to_shorthand,
+        SCENARIO_FILTER_TYPE: scenario_filter_config_to_shorthand,
+        TOOL_FILTER_TYPE: tool_filter_config_to_shorthand,
+        EXECUTION_FILTER_TYPE: execution_filter_config_to_shorthand,
+        VALUE_TRANSFORMER_TYPE: value_transformer_config_to_shorthand,
+    }
+    return SHORTHAND_TAG + shorthands[config["type"]](config)
+
+
+def _parse_shorthand(shorthand):
+    """
+    Converts shorthand filter config into configuration dictionary.
+
+    Args:
+        shorthand (str): a shorthand config string
+
+    Returns:
+        dict: filter configuration dictionary
+    """
+    shorthand_parsers = {
+        ALTERNATIVE_FILTER_SHORTHAND_TAG: alternative_filter_shorthand_to_config,
+        ENTITY_CLASS_RENAMER_SHORTHAND_TAG: entity_class_renamer_shorthand_to_config,
+        PARAMETER_RENAMER_SHORTHAND_TAG: parameter_renamer_shorthand_to_config,
+        SCENARIO_SHORTHAND_TAG: scenario_filter_shorthand_to_config,
+        TOOL_SHORTHAND_TAG: tool_filter_shorthand_to_config,
+        EXECUTION_SHORTHAND_TAG: execution_filter_shorthand_to_config,
+        VALUE_TRANSFORMER_SHORTHAND_TAG: value_transformer_shorthand_to_config,
+    }
+    tag, _, _ = shorthand.partition(":")
+    return shorthand_parsers[tag](shorthand)
+
+
+def name_from_dict(config):
+    """
+    Returns scenario or tool name from filter config.
+
+    Args:
+        config (dict): filter configuration
+
+    Returns:
+        str: name or None if ``config`` is not a valid 'name' filter configuration
+    """
+    func = {SCENARIO_FILTER_TYPE: scenario_name_from_dict, TOOL_FILTER_TYPE: tool_name_from_dict}.get(config["type"])
+    if func is None:
+        return None
+    return func(config)
```

### Comparing `spinedb_api-0.30.3/spinedb_api/filters/value_transformer.py` & `spinedb_api-0.30.4/spinedb_api/filters/value_transformer.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,329 +1,329 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Provides a database query manipulator that applies mathematical transformations to parameter values.
-
-"""
-from functools import partial
-from numbers import Number
-from sqlalchemy import case, literal, Integer, LargeBinary, String
-from sqlalchemy.sql.expression import label, select, cast, union_all
-
-from ..exception import SpineDBAPIError
-from ..helpers import LONGTEXT_LENGTH
-from ..parameter_value import from_database, IndexedValue, to_database, Map
-
-VALUE_TRANSFORMER_TYPE = "value_transformer"
-VALUE_TRANSFORMER_SHORTHAND_TAG = "value_transform"
-
-
-def apply_value_transform_to_parameter_value_sq(db_map, instructions):
-    """
-    Applies renaming to parameter definition subquery.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        instructions (dict): mapping from entity class name to mapping from parameter name to list of
-            instructions
-    """
-    state = _ValueTransformerState(db_map, instructions)
-    transform = partial(_make_parameter_value_transforming_sq, state=state)
-    db_map.override_parameter_value_sq_maker(transform)
-
-
-def value_transformer_config(instructions):
-    """
-    Creates a config dict for transformer.
-
-    Args:
-        instructions (dict): mapping from entity class name to mapping from parameter name to list of
-            instructions
-
-    Returns:
-        dict: transformer configuration
-    """
-    return {"type": VALUE_TRANSFORMER_TYPE, "instructions": instructions}
-
-
-def value_transformer_from_dict(db_map, config):
-    """
-    Applies value transformer manipulator to given database map.
-
-    Args:
-        db_map (DatabaseMappingBase): target database map
-        config (dict): transformer configuration
-    """
-    apply_value_transform_to_parameter_value_sq(db_map, config["instructions"])
-
-
-def value_transformer_config_to_shorthand(config):
-    """
-    Makes a shorthand string from transformer configuration.
-
-    Args:
-        config (dict): transformer configuration
-
-    Returns:
-        str: a shorthand string
-    """
-    shorthand = ""
-    instructions = config["instructions"]
-    for class_name, param_instructions in instructions.items():
-        for param_name, instruction_list in param_instructions.items():
-            for instruction in instruction_list:
-                shorthand = shorthand + ":" + class_name
-                shorthand = shorthand + ":" + param_name
-                operation = instruction["operation"]
-                shorthand = shorthand + ":" + operation
-                if operation == "multiply":
-                    shorthand = shorthand + ":" + str(instruction["rhs"])
-                elif operation == "generate_index":
-                    shorthand = shorthand + ":" + instruction["expression"]
-    return VALUE_TRANSFORMER_SHORTHAND_TAG + shorthand
-
-
-def value_transformer_shorthand_to_config(shorthand):
-    """
-    Makes configuration dictionary out of a shorthand string.
-
-    Args:
-        shorthand (str): a shorthand string
-
-    Returns:
-        dict: value transformer configuration
-    """
-    tokens = shorthand.split(":")[1:]
-    instructions = dict()
-    while tokens:
-        class_name = tokens.pop(0)
-        param_name = tokens.pop(0)
-        operation = tokens.pop(0)
-        instruction = {"operation": operation}
-        if operation == "multiply":
-            instruction["rhs"] = float(tokens.pop(0))
-        elif operation == "generate_index":
-            instruction["expression"] = tokens.pop(0)
-        instructions.setdefault(class_name, {}).setdefault(param_name, []).append(instruction)
-    return value_transformer_config(instructions)
-
-
-class _ValueTransformerState:
-    def __init__(self, db_map, instructions):
-        """
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            instructions (dict): mapping from entity class name to parameter name to list of instructions
-        """
-        self.original_parameter_value_sq = db_map.parameter_value_sq
-        self.transformed = self._transform(db_map, instructions)
-
-    @staticmethod
-    def _transform(db_map, instructions):
-        """Transforms applicable parameter values for caching.
-
-        Args:
-            db_map (DatabaseMappingBase): a database map
-            instructions (dict): mapping from entity class name to parameter name to list of instructions
-
-        Returns:
-            dict: mapping from parameter value ids to transformed values
-        """
-        class_names = set(instructions.keys())
-        param_names = set(name for class_instructions in instructions.values() for name in class_instructions)
-        definition_ids = {
-            definition_row.id
-            for definition_row in db_map.query(db_map.entity_parameter_definition_sq).filter(
-                db_map.entity_parameter_definition_sq.c.entity_class_name.in_(class_names)
-                & db_map.entity_parameter_definition_sq.c.parameter_name.in_(param_names)
-            )
-        }
-        transformed = dict()
-        for value_row in db_map.query(db_map.entity_parameter_value_sq).filter(
-            db_map.entity_parameter_value_sq.c.parameter_id.in_(definition_ids)
-        ):
-            # definition_ids may contain class-parameter name combinations that don't exist in instructions.
-            param_instructions = instructions[value_row.entity_class_name].get(value_row.parameter_name)
-            if param_instructions is not None:
-                transformed[value_row.id] = to_database(
-                    _transform(from_database(value_row.value, value_row.type), param_instructions)
-                )
-        return transformed
-
-
-def _make_parameter_value_transforming_sq(db_map, state):
-    """
-    Returns subquery which applies transformations to parameter values.
-
-    Args:
-        db_map (DatabaseMappingBase): a database map
-        state (_ValueTransformerState): state
-
-    Returns:
-        Alias: a value transforming parameter value subquery
-    """
-    subquery = state.original_parameter_value_sq
-    if not state.transformed:
-        return subquery
-    transformed_rows = [(id_, value, type_) for id_, (value, type_) in state.transformed.items()]
-    # Little optimization: SqlAlchemy can infer types from the first row, so we need to use cast only on that.
-    statements = [
-        select(
-            [
-                cast(literal(transformed_rows[0][0]), Integer()).label("id"),
-                cast(literal(transformed_rows[0][1]), LargeBinary(LONGTEXT_LENGTH)).label("transformed_value"),
-                cast(literal(transformed_rows[0][2]), String()).label("transformed_type"),
-            ]
-        )
-    ]
-    statements += [select([literal(i), literal(v), literal(t)]) for i, v, t in transformed_rows[1:]]
-    temp_sq = union_all(*statements).alias("transformed_values")
-    new_value = case([(temp_sq.c.transformed_value != None, temp_sq.c.transformed_value)], else_=subquery.c.value)
-    new_type = case([(temp_sq.c.transformed_type != None, temp_sq.c.transformed_type)], else_=subquery.c.type)
-    object_class_case = case(
-        [(db_map.entity_class_sq.c.type_id == db_map.object_class_type, subquery.c.entity_class_id)], else_=None
-    )
-    rel_class_case = case(
-        [(db_map.entity_class_sq.c.type_id == db_map.relationship_class_type, subquery.c.entity_class_id)], else_=None
-    )
-    object_entity_case = case(
-        [(db_map.entity_sq.c.type_id == db_map.object_entity_type, subquery.c.entity_id)], else_=None
-    )
-    rel_entity_case = case(
-        [(db_map.entity_sq.c.type_id == db_map.relationship_entity_type, subquery.c.entity_id)], else_=None
-    )
-    parameter_value_sq = (
-        db_map.query(
-            subquery.c.id.label("id"),
-            subquery.c.parameter_definition_id,
-            subquery.c.entity_class_id,
-            subquery.c.entity_id,
-            label("object_class_id", object_class_case),
-            label("relationship_class_id", rel_class_case),
-            label("object_id", object_entity_case),
-            label("relationship_id", rel_entity_case),
-            new_value.label("value"),
-            new_type.label("type"),
-            subquery.c.list_value_id,
-            subquery.c.alternative_id,
-            subquery.c.commit_id.label("commit_id"),
-        )
-        .join(temp_sq, subquery.c.id == temp_sq.c.id, isouter=True)
-        .join(db_map.entity_sq, db_map.entity_sq.c.id == subquery.c.entity_id)
-        .join(db_map.entity_class_sq, db_map.entity_class_sq.c.id == subquery.c.entity_class_id)
-        .subquery()
-    )
-    return parameter_value_sq
-
-
-def _transform(value, instructions):
-    """Transforms a value according to instructions.
-
-    Args:
-        value (Any): value to transform
-        instructions (list of dict): transformation instructions
-
-    Returns:
-        Any: transformed value
-    """
-
-    for instruction in instructions:
-        operation = instruction["operation"]
-        value = _VALUE_TRANSFORMS[operation](value, instruction)
-    return value
-
-
-def _negate(value, instruction):
-    """Negates a value.
-
-    Args:
-        value (Any): value to negate
-        instruction (dict): instruction for the operation
-
-    Returns:
-        Any: negated value
-    """
-    if isinstance(value, Number):
-        return -value
-    if isinstance(value, IndexedValue):
-        for i, element in enumerate(value.values):
-            value.values[i] = _negate(element, instruction)
-        return value
-    return value
-
-
-def _invert(value, instruction):
-    """Calculates the reciprocal of a value.
-
-    Args:
-        value (Any): value to invert
-        instruction (dict): instruction for the operation
-
-    Returns:
-        Any: reciprocal of value
-    """
-    if isinstance(value, Number):
-        return 1.0 / value
-    if isinstance(value, IndexedValue):
-        for i, element in enumerate(value.values):
-            value.values[i] = _invert(element, instruction)
-        return value
-    return value
-
-
-def _multiply(value, instruction):
-    """Multiplies a value.
-
-    Args:
-        value (Any): value to multiply
-        instruction (dict): instruction for the operation
-
-    Returns:
-        Any: multiplied value
-    """
-    if isinstance(value, Number):
-        return instruction["rhs"] * value
-    if isinstance(value, IndexedValue):
-        for i, element in enumerate(value.values):
-            value.values[i] = _multiply(element, instruction)
-        return value
-    return value
-
-
-def _generate_index(value, instruction):
-    """Converts value to Map and generates new indexes for the first dimension.
-
-    Args:
-        value (IndexedValue): value to modify
-        instruction (dict): rename instruction
-
-    Returns:
-        Map or Any: modified value or value as-is if it does not have indexes
-    """
-    if not isinstance(value, IndexedValue):
-        return value
-    if len(value) == 0:
-        return Map([], [], str)
-    try:
-        compiled = compile(instruction["expression"], "<string>", "eval")
-    except (SyntaxError, ValueError):
-        raise SpineDBAPIError("Failed to compile index generator expression.")
-    generate_index = partial(eval, compiled, {})
-    try:
-        indexes = [generate_index({"i": i}) for i in range(1, len(value) + 1)]  # pylint: disable=eval-used
-    except (AttributeError, NameError, ValueError):
-        raise SpineDBAPIError("Failed to evaluate index generator expression.")
-    if len(indexes) != len(set(indexes)):
-        raise SpineDBAPIError(f"Expression '{instruction['expression']}' does not generate unique indexes.")
-    return Map(indexes, value.values)
-
-
-_VALUE_TRANSFORMS = {"generate_index": _generate_index, "invert": _invert, "multiply": _multiply, "negate": _negate}
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Provides a database query manipulator that applies mathematical transformations to parameter values.
+
+"""
+from functools import partial
+from numbers import Number
+from sqlalchemy import case, literal, Integer, LargeBinary, String
+from sqlalchemy.sql.expression import label, select, cast, union_all
+
+from ..exception import SpineDBAPIError
+from ..helpers import LONGTEXT_LENGTH
+from ..parameter_value import from_database, IndexedValue, to_database, Map
+
+VALUE_TRANSFORMER_TYPE = "value_transformer"
+VALUE_TRANSFORMER_SHORTHAND_TAG = "value_transform"
+
+
+def apply_value_transform_to_parameter_value_sq(db_map, instructions):
+    """
+    Applies renaming to parameter definition subquery.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        instructions (dict): mapping from entity class name to mapping from parameter name to list of
+            instructions
+    """
+    state = _ValueTransformerState(db_map, instructions)
+    transform = partial(_make_parameter_value_transforming_sq, state=state)
+    db_map.override_parameter_value_sq_maker(transform)
+
+
+def value_transformer_config(instructions):
+    """
+    Creates a config dict for transformer.
+
+    Args:
+        instructions (dict): mapping from entity class name to mapping from parameter name to list of
+            instructions
+
+    Returns:
+        dict: transformer configuration
+    """
+    return {"type": VALUE_TRANSFORMER_TYPE, "instructions": instructions}
+
+
+def value_transformer_from_dict(db_map, config):
+    """
+    Applies value transformer manipulator to given database map.
+
+    Args:
+        db_map (DatabaseMappingBase): target database map
+        config (dict): transformer configuration
+    """
+    apply_value_transform_to_parameter_value_sq(db_map, config["instructions"])
+
+
+def value_transformer_config_to_shorthand(config):
+    """
+    Makes a shorthand string from transformer configuration.
+
+    Args:
+        config (dict): transformer configuration
+
+    Returns:
+        str: a shorthand string
+    """
+    shorthand = ""
+    instructions = config["instructions"]
+    for class_name, param_instructions in instructions.items():
+        for param_name, instruction_list in param_instructions.items():
+            for instruction in instruction_list:
+                shorthand = shorthand + ":" + class_name
+                shorthand = shorthand + ":" + param_name
+                operation = instruction["operation"]
+                shorthand = shorthand + ":" + operation
+                if operation == "multiply":
+                    shorthand = shorthand + ":" + str(instruction["rhs"])
+                elif operation == "generate_index":
+                    shorthand = shorthand + ":" + instruction["expression"]
+    return VALUE_TRANSFORMER_SHORTHAND_TAG + shorthand
+
+
+def value_transformer_shorthand_to_config(shorthand):
+    """
+    Makes configuration dictionary out of a shorthand string.
+
+    Args:
+        shorthand (str): a shorthand string
+
+    Returns:
+        dict: value transformer configuration
+    """
+    tokens = shorthand.split(":")[1:]
+    instructions = dict()
+    while tokens:
+        class_name = tokens.pop(0)
+        param_name = tokens.pop(0)
+        operation = tokens.pop(0)
+        instruction = {"operation": operation}
+        if operation == "multiply":
+            instruction["rhs"] = float(tokens.pop(0))
+        elif operation == "generate_index":
+            instruction["expression"] = tokens.pop(0)
+        instructions.setdefault(class_name, {}).setdefault(param_name, []).append(instruction)
+    return value_transformer_config(instructions)
+
+
+class _ValueTransformerState:
+    def __init__(self, db_map, instructions):
+        """
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            instructions (dict): mapping from entity class name to parameter name to list of instructions
+        """
+        self.original_parameter_value_sq = db_map.parameter_value_sq
+        self.transformed = self._transform(db_map, instructions)
+
+    @staticmethod
+    def _transform(db_map, instructions):
+        """Transforms applicable parameter values for caching.
+
+        Args:
+            db_map (DatabaseMappingBase): a database map
+            instructions (dict): mapping from entity class name to parameter name to list of instructions
+
+        Returns:
+            dict: mapping from parameter value ids to transformed values
+        """
+        class_names = set(instructions.keys())
+        param_names = set(name for class_instructions in instructions.values() for name in class_instructions)
+        definition_ids = {
+            definition_row.id
+            for definition_row in db_map.query(db_map.entity_parameter_definition_sq).filter(
+                db_map.entity_parameter_definition_sq.c.entity_class_name.in_(class_names)
+                & db_map.entity_parameter_definition_sq.c.parameter_name.in_(param_names)
+            )
+        }
+        transformed = dict()
+        for value_row in db_map.query(db_map.entity_parameter_value_sq).filter(
+            db_map.entity_parameter_value_sq.c.parameter_id.in_(definition_ids)
+        ):
+            # definition_ids may contain class-parameter name combinations that don't exist in instructions.
+            param_instructions = instructions[value_row.entity_class_name].get(value_row.parameter_name)
+            if param_instructions is not None:
+                transformed[value_row.id] = to_database(
+                    _transform(from_database(value_row.value, value_row.type), param_instructions)
+                )
+        return transformed
+
+
+def _make_parameter_value_transforming_sq(db_map, state):
+    """
+    Returns subquery which applies transformations to parameter values.
+
+    Args:
+        db_map (DatabaseMappingBase): a database map
+        state (_ValueTransformerState): state
+
+    Returns:
+        Alias: a value transforming parameter value subquery
+    """
+    subquery = state.original_parameter_value_sq
+    if not state.transformed:
+        return subquery
+    transformed_rows = [(id_, value, type_) for id_, (value, type_) in state.transformed.items()]
+    # Little optimization: SqlAlchemy can infer types from the first row, so we need to use cast only on that.
+    statements = [
+        select(
+            [
+                cast(literal(transformed_rows[0][0]), Integer()).label("id"),
+                cast(literal(transformed_rows[0][1]), LargeBinary(LONGTEXT_LENGTH)).label("transformed_value"),
+                cast(literal(transformed_rows[0][2]), String()).label("transformed_type"),
+            ]
+        )
+    ]
+    statements += [select([literal(i), literal(v), literal(t)]) for i, v, t in transformed_rows[1:]]
+    temp_sq = union_all(*statements).alias("transformed_values")
+    new_value = case([(temp_sq.c.transformed_value != None, temp_sq.c.transformed_value)], else_=subquery.c.value)
+    new_type = case([(temp_sq.c.transformed_type != None, temp_sq.c.transformed_type)], else_=subquery.c.type)
+    object_class_case = case(
+        [(db_map.entity_class_sq.c.type_id == db_map.object_class_type, subquery.c.entity_class_id)], else_=None
+    )
+    rel_class_case = case(
+        [(db_map.entity_class_sq.c.type_id == db_map.relationship_class_type, subquery.c.entity_class_id)], else_=None
+    )
+    object_entity_case = case(
+        [(db_map.entity_sq.c.type_id == db_map.object_entity_type, subquery.c.entity_id)], else_=None
+    )
+    rel_entity_case = case(
+        [(db_map.entity_sq.c.type_id == db_map.relationship_entity_type, subquery.c.entity_id)], else_=None
+    )
+    parameter_value_sq = (
+        db_map.query(
+            subquery.c.id.label("id"),
+            subquery.c.parameter_definition_id,
+            subquery.c.entity_class_id,
+            subquery.c.entity_id,
+            label("object_class_id", object_class_case),
+            label("relationship_class_id", rel_class_case),
+            label("object_id", object_entity_case),
+            label("relationship_id", rel_entity_case),
+            new_value.label("value"),
+            new_type.label("type"),
+            subquery.c.list_value_id,
+            subquery.c.alternative_id,
+            subquery.c.commit_id.label("commit_id"),
+        )
+        .join(temp_sq, subquery.c.id == temp_sq.c.id, isouter=True)
+        .join(db_map.entity_sq, db_map.entity_sq.c.id == subquery.c.entity_id)
+        .join(db_map.entity_class_sq, db_map.entity_class_sq.c.id == subquery.c.entity_class_id)
+        .subquery()
+    )
+    return parameter_value_sq
+
+
+def _transform(value, instructions):
+    """Transforms a value according to instructions.
+
+    Args:
+        value (Any): value to transform
+        instructions (list of dict): transformation instructions
+
+    Returns:
+        Any: transformed value
+    """
+
+    for instruction in instructions:
+        operation = instruction["operation"]
+        value = _VALUE_TRANSFORMS[operation](value, instruction)
+    return value
+
+
+def _negate(value, instruction):
+    """Negates a value.
+
+    Args:
+        value (Any): value to negate
+        instruction (dict): instruction for the operation
+
+    Returns:
+        Any: negated value
+    """
+    if isinstance(value, Number):
+        return -value
+    if isinstance(value, IndexedValue):
+        for i, element in enumerate(value.values):
+            value.values[i] = _negate(element, instruction)
+        return value
+    return value
+
+
+def _invert(value, instruction):
+    """Calculates the reciprocal of a value.
+
+    Args:
+        value (Any): value to invert
+        instruction (dict): instruction for the operation
+
+    Returns:
+        Any: reciprocal of value
+    """
+    if isinstance(value, Number):
+        return 1.0 / value
+    if isinstance(value, IndexedValue):
+        for i, element in enumerate(value.values):
+            value.values[i] = _invert(element, instruction)
+        return value
+    return value
+
+
+def _multiply(value, instruction):
+    """Multiplies a value.
+
+    Args:
+        value (Any): value to multiply
+        instruction (dict): instruction for the operation
+
+    Returns:
+        Any: multiplied value
+    """
+    if isinstance(value, Number):
+        return instruction["rhs"] * value
+    if isinstance(value, IndexedValue):
+        for i, element in enumerate(value.values):
+            value.values[i] = _multiply(element, instruction)
+        return value
+    return value
+
+
+def _generate_index(value, instruction):
+    """Converts value to Map and generates new indexes for the first dimension.
+
+    Args:
+        value (IndexedValue): value to modify
+        instruction (dict): rename instruction
+
+    Returns:
+        Map or Any: modified value or value as-is if it does not have indexes
+    """
+    if not isinstance(value, IndexedValue):
+        return value
+    if len(value) == 0:
+        return Map([], [], str)
+    try:
+        compiled = compile(instruction["expression"], "<string>", "eval")
+    except (SyntaxError, ValueError):
+        raise SpineDBAPIError("Failed to compile index generator expression.")
+    generate_index = partial(eval, compiled, {})
+    try:
+        indexes = [generate_index({"i": i}) for i in range(1, len(value) + 1)]  # pylint: disable=eval-used
+    except (AttributeError, NameError, ValueError):
+        raise SpineDBAPIError("Failed to evaluate index generator expression.")
+    if len(indexes) != len(set(indexes)):
+        raise SpineDBAPIError(f"Expression '{instruction['expression']}' does not generate unique indexes.")
+    return Map(indexes, value.values)
+
+
+_VALUE_TRANSFORMS = {"generate_index": _generate_index, "invert": _invert, "multiply": _multiply, "negate": _negate}
```

### Comparing `spinedb_api-0.30.3/spinedb_api/graph_layout_generator.py` & `spinedb_api-0.30.4/spinedb_api/graph_layout_generator.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,161 +1,161 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Engine.
-# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains the GraphLayoutGenerator class.
-
-"""
-
-import math
-import numpy as np
-from numpy import atleast_1d as arr
-from scipy.sparse.csgraph import dijkstra
-
-
-class GraphLayoutGenerator:
-    """Computes the layout for the Entity Graph View."""
-
-    def __init__(
-        self,
-        vertex_count,
-        src_inds=(),
-        dst_inds=(),
-        spread=0,
-        heavy_positions=None,
-        max_iters=12,
-        weight_exp=-2,
-        is_stopped=lambda: False,
-        preview_available=lambda x, y: None,
-        layout_available=lambda x, y: None,
-        layout_progressed=lambda iter: None,
-        message_available=lambda msg: None,
-    ):
-        super().__init__()
-        if vertex_count == 0:
-            vertex_count = 1
-        if heavy_positions is None:
-            heavy_positions = dict()
-        self.vertex_count = vertex_count
-        self.src_inds = src_inds
-        self.dst_inds = dst_inds
-        self.spread = spread
-        self.heavy_positions = heavy_positions
-        self.max_iters = max(3, round(max_iters * (1 - len(heavy_positions) / self.vertex_count)))
-        self.weight_exp = weight_exp
-        self.initial_diameter = (self.vertex_count ** (0.5)) * self.spread
-        self._is_stopped = is_stopped
-        self._preview_available = preview_available
-        self._layout_available = layout_available
-        self._layout_progressed = layout_progressed
-        self._message_available = message_available
-
-    def shortest_path_matrix(self):
-        """Returns the shortest-path matrix."""
-        if not self.src_inds:
-            # Graph with no edges, just vertices. Introduce fake pair of edges to help 'spreadness'.
-            self.src_inds = [self.vertex_count, self.vertex_count]
-            self.dst_inds = [np.random.randint(0, self.vertex_count), np.random.randint(0, self.vertex_count)]
-            self.vertex_count += 1
-        dist = np.zeros((self.vertex_count, self.vertex_count))
-        src_inds = arr(self.src_inds)
-        dst_inds = arr(self.dst_inds)
-        try:
-            dist[src_inds, dst_inds] = dist[dst_inds, src_inds] = self.spread
-        except IndexError:
-            pass
-        start = 0
-        slices = []
-        iteration = 0
-        self._message_available("Step 1 of 2: Computing shortest-path matrix...")
-        while start < self.vertex_count:
-            if self._is_stopped():
-                return None
-            self._layout_progressed(iteration)
-            stop = min(self.vertex_count, start + math.ceil(self.vertex_count / 10))
-            slice_ = dijkstra(dist, directed=False, indices=range(start, stop))
-            slices.append(slice_)
-            start = stop
-            iteration += 1
-        matrix = np.vstack(slices)
-        # Remove infinites and zeros
-        matrix[matrix == np.inf] = self.spread * self.vertex_count ** (0.5)
-        matrix[matrix == 0] = self.spread * 1e-6
-        return matrix
-
-    def sets(self):
-        """Returns sets of vertex pairs indices."""
-        sets = []
-        for n in range(1, self.vertex_count):
-            pairs = np.zeros((self.vertex_count - n, 2), int)  # pairs on diagonal n
-            pairs[:, 0] = np.arange(self.vertex_count - n)
-            pairs[:, 1] = pairs[:, 0] + n
-            mask = np.mod(range(self.vertex_count - n), 2 * n) < n
-            s1 = pairs[mask]
-            s2 = pairs[~mask]
-            if s1.any():
-                sets.append(s1)
-            if s2.any():
-                sets.append(s2)
-        return sets
-
-    def compute_layout(self):
-        """Computes and returns x and y coordinates for each vertex in the graph, using VSGD-MS."""
-        if self.vertex_count <= 1:
-            x, y = np.array([0.0]), np.array([0.0])
-            self._layout_available(x, y)
-            return
-        matrix = self.shortest_path_matrix()
-        if matrix is None:
-            return
-        mask = np.ones((self.vertex_count, self.vertex_count)) == 1 - np.tril(
-            np.ones((self.vertex_count, self.vertex_count))
-        )  # Upper triangular except diagonal
-        np.random.seed(0)
-        layout = np.random.rand(self.vertex_count, 2) * self.initial_diameter - self.initial_diameter / 2
-        heavy_ind_list = list()
-        heavy_pos_list = list()
-        for ind, pos in self.heavy_positions.items():
-            heavy_ind_list.append(ind)
-            heavy_pos_list.append([pos["x"], pos["y"]])
-        heavy_ind = arr(heavy_ind_list)
-        heavy_pos = arr(heavy_pos_list)
-        if heavy_ind.any():
-            layout[heavy_ind, :] = heavy_pos
-        weights = matrix ** self.weight_exp  # bus-pair weights (lower for distant buses)
-        maxstep = 1 / np.min(weights[mask])
-        minstep = 1 / np.max(weights[mask])
-        lambda_ = np.log(minstep / maxstep) / (self.max_iters - 1)  # exponential decay of allowed adjustment
-        sets = self.sets()  # construct sets of bus pairs
-        self._message_available("Step 2 of 2: Generating layout...")
-        for iteration in range(self.max_iters):
-            if self._is_stopped():
-                break
-            x, y = layout[:, 0], layout[:, 1]
-            self._preview_available(x, y)
-            self._layout_progressed(iteration)
-            # FIXME
-            step = maxstep * np.exp(lambda_ * iteration)  # how big adjustments are allowed?
-            rand_order = np.random.permutation(
-                self.vertex_count
-            )  # we don't want to use the same pair order each iteration
-            for s in sets:
-                v1, v2 = rand_order[s[:, 0]], rand_order[s[:, 1]]  # arrays of vertex1 and vertex2
-                # current distance (possibly accounting for system rescaling)
-                dist = ((layout[v1, 0] - layout[v2, 0]) ** 2 + (layout[v1, 1] - layout[v2, 1]) ** 2) ** 0.5
-                r = (matrix[v1, v2] - dist)[:, None] * (layout[v1] - layout[v2]) / dist[:, None] / 2  # desired change
-                dx1 = r * np.minimum(1, weights[v1, v2] * step)[:, None]
-                dx2 = -dx1
-                layout[v1, :] += dx1  # update position
-                layout[v2, :] += dx2
-                if heavy_ind.any():
-                    layout[heavy_ind, :] = heavy_pos
-        x, y = layout[:, 0], layout[:, 1]
-        self._layout_available(x, y)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Engine.
+# Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains the GraphLayoutGenerator class.
+
+"""
+
+import math
+import numpy as np
+from numpy import atleast_1d as arr
+from scipy.sparse.csgraph import dijkstra
+
+
+class GraphLayoutGenerator:
+    """Computes the layout for the Entity Graph View."""
+
+    def __init__(
+        self,
+        vertex_count,
+        src_inds=(),
+        dst_inds=(),
+        spread=0,
+        heavy_positions=None,
+        max_iters=12,
+        weight_exp=-2,
+        is_stopped=lambda: False,
+        preview_available=lambda x, y: None,
+        layout_available=lambda x, y: None,
+        layout_progressed=lambda iter: None,
+        message_available=lambda msg: None,
+    ):
+        super().__init__()
+        if vertex_count == 0:
+            vertex_count = 1
+        if heavy_positions is None:
+            heavy_positions = dict()
+        self.vertex_count = vertex_count
+        self.src_inds = src_inds
+        self.dst_inds = dst_inds
+        self.spread = spread
+        self.heavy_positions = heavy_positions
+        self.max_iters = max(3, round(max_iters * (1 - len(heavy_positions) / self.vertex_count)))
+        self.weight_exp = weight_exp
+        self.initial_diameter = (self.vertex_count ** (0.5)) * self.spread
+        self._is_stopped = is_stopped
+        self._preview_available = preview_available
+        self._layout_available = layout_available
+        self._layout_progressed = layout_progressed
+        self._message_available = message_available
+
+    def shortest_path_matrix(self):
+        """Returns the shortest-path matrix."""
+        if not self.src_inds:
+            # Graph with no edges, just vertices. Introduce fake pair of edges to help 'spreadness'.
+            self.src_inds = [self.vertex_count, self.vertex_count]
+            self.dst_inds = [np.random.randint(0, self.vertex_count), np.random.randint(0, self.vertex_count)]
+            self.vertex_count += 1
+        dist = np.zeros((self.vertex_count, self.vertex_count))
+        src_inds = arr(self.src_inds)
+        dst_inds = arr(self.dst_inds)
+        try:
+            dist[src_inds, dst_inds] = dist[dst_inds, src_inds] = self.spread
+        except IndexError:
+            pass
+        start = 0
+        slices = []
+        iteration = 0
+        self._message_available("Step 1 of 2: Computing shortest-path matrix...")
+        while start < self.vertex_count:
+            if self._is_stopped():
+                return None
+            self._layout_progressed(iteration)
+            stop = min(self.vertex_count, start + math.ceil(self.vertex_count / 10))
+            slice_ = dijkstra(dist, directed=False, indices=range(start, stop))
+            slices.append(slice_)
+            start = stop
+            iteration += 1
+        matrix = np.vstack(slices)
+        # Remove infinites and zeros
+        matrix[matrix == np.inf] = self.spread * self.vertex_count ** (0.5)
+        matrix[matrix == 0] = self.spread * 1e-6
+        return matrix
+
+    def sets(self):
+        """Returns sets of vertex pairs indices."""
+        sets = []
+        for n in range(1, self.vertex_count):
+            pairs = np.zeros((self.vertex_count - n, 2), int)  # pairs on diagonal n
+            pairs[:, 0] = np.arange(self.vertex_count - n)
+            pairs[:, 1] = pairs[:, 0] + n
+            mask = np.mod(range(self.vertex_count - n), 2 * n) < n
+            s1 = pairs[mask]
+            s2 = pairs[~mask]
+            if s1.any():
+                sets.append(s1)
+            if s2.any():
+                sets.append(s2)
+        return sets
+
+    def compute_layout(self):
+        """Computes and returns x and y coordinates for each vertex in the graph, using VSGD-MS."""
+        if self.vertex_count <= 1:
+            x, y = np.array([0.0]), np.array([0.0])
+            self._layout_available(x, y)
+            return
+        matrix = self.shortest_path_matrix()
+        if matrix is None:
+            return
+        mask = np.ones((self.vertex_count, self.vertex_count)) == 1 - np.tril(
+            np.ones((self.vertex_count, self.vertex_count))
+        )  # Upper triangular except diagonal
+        np.random.seed(0)
+        layout = np.random.rand(self.vertex_count, 2) * self.initial_diameter - self.initial_diameter / 2
+        heavy_ind_list = list()
+        heavy_pos_list = list()
+        for ind, pos in self.heavy_positions.items():
+            heavy_ind_list.append(ind)
+            heavy_pos_list.append([pos["x"], pos["y"]])
+        heavy_ind = arr(heavy_ind_list)
+        heavy_pos = arr(heavy_pos_list)
+        if heavy_ind.any():
+            layout[heavy_ind, :] = heavy_pos
+        weights = matrix ** self.weight_exp  # bus-pair weights (lower for distant buses)
+        maxstep = 1 / np.min(weights[mask])
+        minstep = 1 / np.max(weights[mask])
+        lambda_ = np.log(minstep / maxstep) / (self.max_iters - 1)  # exponential decay of allowed adjustment
+        sets = self.sets()  # construct sets of bus pairs
+        self._message_available("Step 2 of 2: Generating layout...")
+        for iteration in range(self.max_iters):
+            if self._is_stopped():
+                break
+            x, y = layout[:, 0], layout[:, 1]
+            self._preview_available(x, y)
+            self._layout_progressed(iteration)
+            # FIXME
+            step = maxstep * np.exp(lambda_ * iteration)  # how big adjustments are allowed?
+            rand_order = np.random.permutation(
+                self.vertex_count
+            )  # we don't want to use the same pair order each iteration
+            for s in sets:
+                v1, v2 = rand_order[s[:, 0]], rand_order[s[:, 1]]  # arrays of vertex1 and vertex2
+                # current distance (possibly accounting for system rescaling)
+                dist = ((layout[v1, 0] - layout[v2, 0]) ** 2 + (layout[v1, 1] - layout[v2, 1]) ** 2) ** 0.5
+                r = (matrix[v1, v2] - dist)[:, None] * (layout[v1] - layout[v2]) / dist[:, None] / 2  # desired change
+                dx1 = r * np.minimum(1, weights[v1, v2] * step)[:, None]
+                dx2 = -dx1
+                layout[v1, :] += dx1  # update position
+                layout[v2, :] += dx2
+                if heavy_ind.any():
+                    layout[heavy_ind, :] = heavy_pos
+        x, y = layout[:, 0], layout[:, 1]
+        self._layout_available(x, y)
```

### Comparing `spinedb_api-0.30.3/spinedb_api/helpers.py` & `spinedb_api-0.30.4/spinedb_api/helpers.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,976 +1,976 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-General helper functions and classes.
-
-"""
-
-import os
-import json
-import warnings
-from operator import itemgetter
-from urllib.parse import urlparse, urlunparse
-from sqlalchemy import (
-    Boolean,
-    BigInteger,
-    CheckConstraint,
-    Column,
-    DateTime,
-    Float,
-    ForeignKey,
-    ForeignKeyConstraint,
-    Integer,
-    MetaData,
-    PrimaryKeyConstraint,
-    String,
-    Table,
-    Text,
-    LargeBinary,
-    UniqueConstraint,
-    create_engine,
-    false,
-    func,
-    inspect,
-    null,
-    select,
-)
-from sqlalchemy.ext.automap import generate_relationship
-from sqlalchemy.ext.compiler import compiles
-from sqlalchemy.exc import DatabaseError, IntegrityError, OperationalError
-from sqlalchemy.dialects.mysql import TINYINT, DOUBLE
-from sqlalchemy.sql.expression import FunctionElement, bindparam, cast
-from alembic.config import Config
-from alembic.script import ScriptDirectory
-from alembic.migration import MigrationContext
-from alembic.environment import EnvironmentContext
-from .exception import SpineDBAPIError, SpineDBVersionError
-
-# Supported dialects and recommended dbapi. Restricted to mysql and sqlite for now:
-# - sqlite works
-# - mysql is trying to work
-SUPPORTED_DIALECTS = {
-    "mysql": "pymysql",
-    "sqlite": "sqlite3",
-}
-
-UNSUPPORTED_DIALECTS = {
-    "mssql": "pyodbc",
-    "postgresql": "psycopg2",
-    "oracle": "cx_oracle",
-}
-
-naming_convention = {
-    "pk": "pk_%(table_name)s",
-    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
-    "uq": "uq_%(table_name)s_%(column_0N_name)s",
-    "ck": "ck_%(table_name)s_%(constraint_name)s",
-}
-
-model_meta = MetaData(naming_convention=naming_convention)
-
-LONGTEXT_LENGTH = 2 ** 32 - 1
-
-
-# NOTE: Deactivated since foreign keys are too difficult to get right in the diff tables.
-# For example, the diff_object table would need a `class_id` field and a `diff_class_id` field,
-# plus a CHECK constraint that at least one of the two is NOT NULL.
-# @event.listens_for(Engine, "connect")
-def set_sqlite_pragma(dbapi_connection, connection_record):
-    module_name = dbapi_connection.__class__.__module__
-    if not module_name.lower().startswith("sqlite"):
-        return
-    cursor = dbapi_connection.cursor()
-    cursor.execute("PRAGMA foreign_keys=ON")
-    cursor.close()
-
-
-@compiles(TINYINT, "sqlite")
-def compile_TINYINT_mysql_sqlite(element, compiler, **kw):
-    """Handles mysql TINYINT datatype as INTEGER in sqlite."""
-    return compiler.visit_INTEGER(element, **kw)
-
-
-@compiles(DOUBLE, "sqlite")
-def compile_DOUBLE_mysql_sqlite(element, compiler, **kw):
-    """Handles mysql DOUBLE datatype as REAL in sqlite."""
-    return compiler.visit_REAL(element, **kw)
-
-
-class group_concat(FunctionElement):
-    type = String()
-    name = 'group_concat'
-
-
-def _parse_group_concat_clauses(clauses):
-    keys = ("group_concat_column", "order_by_column", "separator")
-    d = dict(zip(keys, clauses))
-    return d["group_concat_column"], d.get("order_by_column"), d.get("separator", bindparam("sep", ","))
-
-
-@compiles(group_concat, "sqlite")
-def compile_group_concat_sqlite(element, compiler, **kw):
-    group_concat_column, _, separator = _parse_group_concat_clauses(element.clauses)
-    return compiler.process(func.group_concat(group_concat_column, separator), **kw)
-
-
-@compiles(group_concat, "mysql")
-def compile_group_concat_mysql(element, compiler, **kw):
-    group_concat_column, order_by_column, separator = _parse_group_concat_clauses(element.clauses)
-    str_group_concat_column = cast(group_concat_column, String)
-    if order_by_column is not None:
-        str_group_concat_column = str_group_concat_column.op("ORDER BY")(order_by_column)
-    return "group_concat(%s separator %s)" % (
-        compiler.process(str_group_concat_column, **kw),
-        compiler.process(separator, **kw),
-    )
-
-
-def _parse_metadata_fallback(metadata):
-    yield ("unnamed", str(metadata))
-
-
-def _parse_metadata(metadata):
-    try:
-        parsed = json.loads(metadata)
-    except json.decoder.JSONDecodeError:
-        yield from _parse_metadata_fallback(metadata)
-        return
-    if not isinstance(parsed, dict):
-        yield from _parse_metadata_fallback(metadata)
-        return
-    for key, value in parsed.items():
-        if isinstance(value, list):
-            for val in value:
-                yield (key, str(val))
-            continue
-        yield (key, str(value))
-
-
-def is_head(db_url, upgrade=False):
-    """Check whether or not db_url is head.
-
-    Args:
-        db_url (str): database url
-        upgrade (Bool): if True, upgrade db to head
-    """
-    engine = create_engine(db_url)
-    return is_head_engine(engine, upgrade=upgrade)
-
-
-def is_head_engine(engine, upgrade=False):
-    """Check whether or not engine is head.
-
-    Args:
-        engine (Engine): database engine
-        upgrade (Bool): if True, upgrade db to head
-    """
-    config = Config()
-    config.set_main_option("script_location", "spinedb_api:alembic")
-    script = ScriptDirectory.from_config(config)
-    head = script.get_current_head()
-    with engine.connect() as connection:
-        migration_context = MigrationContext.configure(connection)
-        current_rev = migration_context.get_current_revision()
-        if current_rev == head:
-            return True
-        if not upgrade:
-            return False
-
-        # Upgrade function
-        def fn(rev, context):
-            return script._upgrade_revs("head", rev)
-
-        with EnvironmentContext(
-            config, script, fn=fn, as_sql=False, starting_rev=None, destination_rev="head", tag=None
-        ) as environment_context:
-            environment_context.configure(connection=connection, target_metadata=model_meta)
-            with environment_context.begin_transaction():
-                environment_context.run_migrations()
-    return True
-
-
-def copy_database(dest_url, source_url, overwrite=True, upgrade=False, only_tables=(), skip_tables=()):
-    """Copy the database from source_url into dest_url."""
-    if not is_head(source_url, upgrade=upgrade):
-        raise SpineDBVersionError(url=source_url)
-    source_engine = create_engine(source_url)
-    dest_engine = create_engine(dest_url)
-    copy_database_bind(
-        dest_engine,
-        source_engine,
-        overwrite=overwrite,
-        upgrade=upgrade,
-        only_tables=only_tables,
-        skip_tables=skip_tables,
-    )
-
-
-def copy_database_bind(dest_bind, source_bind, overwrite=True, upgrade=False, only_tables=(), skip_tables=()):
-    source_meta = MetaData(bind=source_bind)
-    source_meta.reflect()
-    if inspect(dest_bind).get_table_names():
-        if not overwrite:
-            raise SpineDBAPIError(
-                f"The database at '{dest_bind}' is not empty. "
-                "If you want to overwrite it, please pass the argument `overwrite=True` "
-                "to the function call."
-            )
-        source_meta.drop_all(dest_bind)
-    dest_meta = MetaData(bind=dest_bind)
-    for source_table in source_meta.sorted_tables:
-        # Create table in dest
-        source_table.create(dest_bind)
-        if source_table.name not in ("alembic_version", "next_id"):
-            # Skip tables according to `only_tables` and `skip_tables`
-            if only_tables and source_table.name not in only_tables:
-                continue
-            if source_table.name in skip_tables:
-                continue
-        dest_table = Table(source_table, dest_meta, autoload=True)
-        sel = select([source_table])
-        result = source_bind.execute(sel)
-        # Insert data from source into destination
-        data = result.fetchall()
-        if not data:
-            continue
-        ins = dest_table.insert()
-        try:
-            dest_bind.execute(ins, data)
-        except IntegrityError as e:
-            warnings.warn("Skipping table {0}: {1}".format(source_table.name, e.orig.args))
-
-
-def custom_generate_relationship(base, direction, return_fn, attrname, local_cls, referred_cls, **kw):
-    """Make all relationships view only to avoid warnings."""
-    kw["viewonly"] = True
-    kw["cascade"] = ""
-    kw["passive_deletes"] = False
-    kw["sync_backref"] = False
-    return generate_relationship(base, direction, return_fn, attrname, local_cls, referred_cls, **kw)
-
-
-def is_unlocked(db_url, timeout=0):
-    """Return True if the SQLite db_url is unlocked, after waiting at most timeout seconds.
-    Otherwise return False."""
-    if not db_url.startswith("sqlite"):
-        return False
-    try:
-        engine = create_engine(db_url, connect_args={"timeout": timeout})
-        engine.execute("BEGIN IMMEDIATE")
-        return True
-    except OperationalError:
-        return False
-
-
-def compare_schemas(left_engine, right_engine):
-    """Whether or not the left and right engine have the same schema."""
-    left_insp = inspect(left_engine)
-    right_insp = inspect(right_engine)
-    left_dict = schema_dict(left_insp)
-    right_dict = schema_dict(right_insp)
-    return str(left_dict) == str(right_dict)
-
-
-def schema_dict(insp):
-    return {
-        table_name: {
-            "columns": sorted(insp.get_columns(table_name), key=itemgetter("name")),
-            "pk_constraint": insp.get_pk_constraint(table_name),
-            "foreign_keys": sorted(insp.get_foreign_keys(table_name), key=lambda x: x["name"] or ""),
-            "check_constraints": insp.get_check_constraints(table_name),
-        }
-        for table_name in insp.get_table_names()
-    }
-
-
-def is_empty(db_url):
-    try:
-        engine = create_engine(db_url)
-    except DatabaseError as e:
-        raise SpineDBAPIError("Could not connect to '{}': {}".format(db_url, e.orig.args)) from None
-    insp = inspect(engine)
-    if insp.get_table_names():
-        return False
-    return True
-
-
-def create_spine_metadata():
-    meta = MetaData(naming_convention=naming_convention)
-    Table(
-        "commit",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("comment", String(255), nullable=False),
-        Column("date", DateTime, nullable=False),
-        Column("user", String(45)),
-    )
-    Table(
-        "alternative",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(255), nullable=False),
-        Column("description", Text(), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("name"),
-    )
-    Table(
-        "scenario",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(255), nullable=False),
-        Column("description", Text(), server_default=null()),
-        Column("active", Boolean(name="active"), server_default=false(), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("name"),
-    )
-    Table(
-        "scenario_alternative",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column(
-            "scenario_id", Integer, ForeignKey("scenario.id", onupdate="CASCADE", ondelete="CASCADE"), nullable=False
-        ),
-        Column(
-            "alternative_id",
-            Integer,
-            ForeignKey("alternative.id", onupdate="CASCADE", ondelete="CASCADE"),
-            nullable=False,
-        ),
-        Column("rank", Integer, nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("scenario_id", "rank"),
-        UniqueConstraint("scenario_id", "alternative_id"),
-    )
-    Table(
-        "entity_class_type",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(255), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "entity_type",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(255), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "entity_class",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column(
-            "type_id",
-            Integer,
-            ForeignKey("entity_class_type.id", onupdate="CASCADE", ondelete="CASCADE"),
-            nullable=False,
-        ),
-        Column("name", String(255), nullable=False),
-        Column("description", Text(), server_default=null()),
-        Column("display_order", Integer, server_default="99"),
-        Column("display_icon", BigInteger, server_default=null()),
-        Column("hidden", Integer, server_default="0"),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("id", "type_id"),
-        UniqueConstraint("type_id", "name"),
-    )
-    Table(
-        "object_class",
-        meta,
-        Column("entity_class_id", Integer, primary_key=True),
-        Column("type_id", Integer, nullable=False),
-        ForeignKeyConstraint(
-            ("entity_class_id", "type_id"), ("entity_class.id", "entity_class.type_id"), ondelete="CASCADE"
-        ),
-        CheckConstraint("`type_id` = 1", name="type_id"),  # make sure object class can only have object type
-    )
-    Table(
-        "relationship_class",
-        meta,
-        Column("entity_class_id", Integer, primary_key=True),
-        Column("type_id", Integer, nullable=False),
-        ForeignKeyConstraint(
-            ("entity_class_id", "type_id"), ("entity_class.id", "entity_class.type_id"), ondelete="CASCADE"
-        ),
-        CheckConstraint("`type_id` = 2", name="type_id"),
-    )
-    Table(
-        "relationship_entity_class",
-        meta,
-        Column(
-            "entity_class_id",
-            Integer,
-            ForeignKey("relationship_class.entity_class_id", onupdate="CASCADE", ondelete="CASCADE"),
-            primary_key=True,
-        ),
-        Column("dimension", Integer, primary_key=True),
-        Column("member_class_id", Integer, nullable=False),
-        Column("member_class_type_id", Integer, nullable=False),
-        UniqueConstraint("entity_class_id", "dimension", "member_class_id", name="uq_relationship_entity_class"),
-        ForeignKeyConstraint(("member_class_id", "member_class_type_id"), ("entity_class.id", "entity_class.type_id")),
-        CheckConstraint("`member_class_type_id` != 2", name="member_class_type_id"),
-    )
-    Table(
-        "entity",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("type_id", Integer, ForeignKey("entity_type.id", onupdate="CASCADE", ondelete="CASCADE")),
-        Column("class_id", Integer, ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE")),
-        Column("name", String(255), nullable=False),
-        Column("description", Text(), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("id", "class_id"),
-        UniqueConstraint("id", "type_id", "class_id"),
-        UniqueConstraint("class_id", "name"),
-    )
-    Table(
-        "object",
-        meta,
-        Column("entity_id", Integer, primary_key=True),
-        Column("type_id", Integer, nullable=False),
-        ForeignKeyConstraint(("entity_id", "type_id"), ("entity.id", "entity.type_id"), ondelete="CASCADE"),
-        CheckConstraint("`type_id` = 1", name="type_id"),  # make sure object can only have object type
-    )
-    Table(
-        "relationship",
-        meta,
-        Column("entity_id", Integer, primary_key=True),
-        Column("entity_class_id", Integer, nullable=False),
-        Column("type_id", Integer, nullable=False),
-        UniqueConstraint("entity_id", "entity_class_id"),
-        ForeignKeyConstraint(("entity_id", "type_id"), ("entity.id", "entity.type_id"), ondelete="CASCADE"),
-        CheckConstraint("`type_id` = 2", name="type_id"),
-    )
-    Table(
-        "relationship_entity",
-        meta,
-        Column("entity_id", Integer, primary_key=True),
-        Column("entity_class_id", Integer, nullable=False),
-        Column("dimension", Integer, primary_key=True),
-        Column("member_id", Integer, nullable=False),
-        Column("member_class_id", Integer, nullable=False),
-        ForeignKeyConstraint(
-            ("member_id", "member_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
-        ),
-        ForeignKeyConstraint(
-            ("entity_class_id", "dimension", "member_class_id"),
-            (
-                "relationship_entity_class.entity_class_id",
-                "relationship_entity_class.dimension",
-                "relationship_entity_class.member_class_id",
-            ),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-        ForeignKeyConstraint(
-            ("entity_id", "entity_class_id"),
-            ("relationship.entity_id", "relationship.entity_class_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    Table(
-        "entity_group",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("entity_id", Integer, nullable=False),
-        Column("entity_class_id", Integer, nullable=False),
-        Column("member_id", Integer, nullable=False),
-        UniqueConstraint("entity_id", "member_id"),
-        ForeignKeyConstraint(
-            ("entity_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
-        ),
-        ForeignKeyConstraint(
-            ("member_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
-        ),
-    )
-    Table(
-        "parameter_definition",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column(
-            "entity_class_id",
-            Integer,
-            ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE"),
-            nullable=False,
-        ),
-        Column("name", String(155), nullable=False),
-        Column("description", Text(), server_default=null()),
-        Column("default_type", String(255)),
-        Column("default_value", LargeBinary(LONGTEXT_LENGTH), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        Column("parameter_value_list_id", Integer),
-        UniqueConstraint("id", "entity_class_id"),
-        UniqueConstraint("entity_class_id", "name"),
-        UniqueConstraint("id", "parameter_value_list_id"),
-    )
-    Table(
-        "parameter_tag",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("tag", String(155), nullable=False, unique=True),
-        Column("description", Text(), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "parameter_definition_tag",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column(
-            "parameter_definition_id",
-            Integer,
-            ForeignKey("parameter_definition.id", name="fk_parameter_tag_parameter_definition"),
-            nullable=False,
-        ),
-        Column("parameter_tag_id", Integer, ForeignKey("parameter_tag.id"), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("parameter_definition_id", "parameter_tag_id", name="uq_parameter_definition_tag"),
-    )
-    Table(
-        "parameter_value",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("parameter_definition_id", Integer, nullable=False),
-        Column("entity_id", Integer, nullable=False),
-        Column("entity_class_id", Integer, nullable=False),
-        Column("type", String(255)),
-        Column("value", LargeBinary(LONGTEXT_LENGTH), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        Column("alternative_id", Integer, ForeignKey("alternative.id"), nullable=False),
-        UniqueConstraint("parameter_definition_id", "entity_id", "alternative_id", name="uq_parameter_value"),
-        ForeignKeyConstraint(
-            ("entity_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
-        ),
-        ForeignKeyConstraint(
-            ("parameter_definition_id", "entity_class_id"),
-            ("parameter_definition.id", "parameter_definition.entity_class_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    Table(
-        "parameter_value_list",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(155), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "list_value",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("parameter_value_list_id", Integer, ForeignKey("parameter_value_list.id"), nullable=False),
-        Column("index", Integer, nullable=False),
-        Column("type", String(255)),
-        Column("value", LargeBinary(LONGTEXT_LENGTH), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("parameter_value_list_id", "index"),
-    )
-    Table(
-        "tool",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(155), nullable=False),
-        Column("description", Text(), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "feature",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("parameter_definition_id", Integer, nullable=False),
-        Column("parameter_value_list_id", Integer, nullable=False),
-        Column("description", Text(), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("parameter_definition_id", "parameter_value_list_id"),
-        UniqueConstraint("id", "parameter_value_list_id"),
-        ForeignKeyConstraint(
-            ("parameter_definition_id", "parameter_value_list_id"),
-            ("parameter_definition.id", "parameter_definition.parameter_value_list_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    Table(
-        "tool_feature",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("tool_id", Integer, ForeignKey("tool.id")),
-        Column("feature_id", Integer, nullable=False),
-        Column("parameter_value_list_id", Integer, nullable=False),
-        Column("required", Boolean(name="required"), server_default=false(), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("tool_id", "feature_id"),
-        UniqueConstraint("id", "parameter_value_list_id"),
-        ForeignKeyConstraint(
-            ("feature_id", "parameter_value_list_id"),
-            ("feature.id", "feature.parameter_value_list_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    Table(
-        "tool_feature_method",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("tool_feature_id", Integer, nullable=False),
-        Column("parameter_value_list_id", Integer, nullable=False),
-        Column("method_index", Integer),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("tool_feature_id", "method_index"),
-        ForeignKeyConstraint(
-            ("tool_feature_id", "parameter_value_list_id"),
-            ("tool_feature.id", "tool_feature.parameter_value_list_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-        ForeignKeyConstraint(
-            ("parameter_value_list_id", "method_index"),
-            ("list_value.parameter_value_list_id", "list_value.index"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    Table(
-        "metadata",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(155), nullable=False),
-        Column("value", String(255), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("name", "value"),
-    )
-    Table(
-        "parameter_value_metadata",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column(
-            "parameter_value_id",
-            Integer,
-            ForeignKey("parameter_value.id", onupdate="CASCADE", ondelete="CASCADE"),
-            nullable=False,
-        ),
-        Column(
-            "metadata_id", Integer, ForeignKey("metadata.id", onupdate="CASCADE", ondelete="CASCADE"), nullable=False
-        ),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("parameter_value_id", "metadata_id"),
-    )
-    Table(
-        "entity_metadata",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("entity_id", Integer, ForeignKey("entity.id", onupdate="CASCADE", ondelete="CASCADE"), nullable=False),
-        Column(
-            "metadata_id", Integer, ForeignKey("metadata.id", onupdate="CASCADE", ondelete="CASCADE"), nullable=False
-        ),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("entity_id", "metadata_id"),
-    )
-    Table(
-        "alembic_version",
-        meta,
-        Column("version_num", String(32), nullable=False),
-        PrimaryKeyConstraint("version_num", name="alembic_version_pkc"),
-    )
-    return meta
-
-
-def create_new_spine_database(db_url):
-    """Create a new Spine database at the given url."""
-    try:
-        engine = create_engine(db_url)
-    except DatabaseError as e:
-        raise SpineDBAPIError("Could not connect to '{}': {}".format(db_url, e.orig.args)) from None
-    # Drop existing tables. This is a Spine db now...
-    meta = MetaData(engine)
-    meta.reflect()
-    meta.drop_all()
-    # Create new tables
-    meta = create_spine_metadata()
-    try:
-        meta.create_all(engine)
-        engine.execute("INSERT INTO `commit` VALUES (1, 'Create the database', CURRENT_TIMESTAMP, 'spinedb_api')")
-        engine.execute("INSERT INTO alternative VALUES (1, 'Base', 'Base alternative', 1)")
-        engine.execute("INSERT INTO entity_class_type VALUES (1, 'object', 1), (2, 'relationship', 1)")
-        engine.execute("INSERT INTO entity_type VALUES (1, 'object', 1), (2, 'relationship', 1)")
-        engine.execute("INSERT INTO alembic_version VALUES ('989fccf80441')")
-    except DatabaseError as e:
-        raise SpineDBAPIError("Unable to create Spine database: {}".format(e)) from None
-    return engine
-
-
-def _create_first_spine_database(db_url):
-    """Creates a Spine database with the very first version at the given url.
-    Used internally.
-    """
-    try:
-        engine = create_engine(db_url)
-    except DatabaseError as e:
-        raise SpineDBAPIError("Could not connect to '{}': {}".format(db_url, e.orig.args)) from None
-    # Drop existing tables. This is a Spine db now...
-    meta = MetaData(engine)
-    meta.reflect()
-    meta.drop_all(engine)
-    # Create new tables
-    meta = MetaData(naming_convention=naming_convention)
-    Table(
-        "commit",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("comment", String(255), nullable=False),
-        Column("date", DateTime, nullable=False),
-        Column("user", String(45)),
-    )
-    Table(
-        "object_class_category",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(255), nullable=False, unique=True),
-        Column("description", String(255), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "object_class",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(255), nullable=False, unique=True),
-        Column("description", String(255), server_default=null()),
-        Column("category_id", Integer, ForeignKey("object_class_category.id"), server_default=null()),
-        Column("display_order", Integer, server_default="99"),
-        Column("display_icon", BigInteger, server_default=null()),
-        Column("hidden", Integer, server_default="0"),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "object_category",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("object_class_id", Integer, ForeignKey("object_class.id")),
-        Column("name", String(255), nullable=False, unique=True),
-        Column("description", String(255), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "object",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("class_id", Integer, ForeignKey("object_class.id", onupdate="CASCADE", ondelete="CASCADE")),
-        Column("name", String(255), nullable=False, unique=True),
-        Column("description", String(255), server_default=null()),
-        Column("category_id", Integer, ForeignKey("object_category.id")),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "relationship_class",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("dimension", Integer, primary_key=True),
-        Column("object_class_id", Integer, ForeignKey("object_class.id")),
-        Column("name", String(255), nullable=False),
-        Column("hidden", Integer, server_default="0"),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("dimension", "name"),
-    )
-    Table(
-        "relationship",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("dimension", Integer, primary_key=True),
-        Column("object_id", Integer, ForeignKey("object.id")),
-        Column("class_id", Integer, nullable=False),
-        Column("name", String(255), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("dimension", "name"),
-        ForeignKeyConstraint(
-            ("class_id", "dimension"),
-            ("relationship_class.id", "relationship_class.dimension"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    Table(
-        "parameter",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(155), nullable=False, unique=True),
-        Column("description", String(155), server_default=null()),
-        Column("data_type", String(155), server_default="NUMERIC"),
-        Column("relationship_class_id", Integer, default=null()),
-        Column(
-            "object_class_id",
-            Integer,
-            ForeignKey("object_class.id", onupdate="CASCADE", ondelete="CASCADE"),
-            server_default=null(),
-        ),
-        Column("can_have_time_series", Integer, server_default="0"),
-        Column("can_have_time_pattern", Integer, server_default="1"),
-        Column("can_be_stochastic", Integer, server_default="0"),
-        Column("default_value", String(155), server_default="0"),
-        Column("is_mandatory", Integer, server_default="0"),
-        Column("precision", Integer, server_default="2"),
-        Column("unit", String(155), server_default=null()),
-        Column("minimum_value", Float, server_default=null()),
-        Column("maximum_value", Float, server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        CheckConstraint(
-            "`relationship_class_id` IS NOT NULL OR `object_class_id` IS NOT NULL",
-            name="obj_or_rel_class_id_is_not_null",
-        ),
-    )
-    Table(
-        "parameter_value",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("parameter_id", Integer, ForeignKey("parameter.id", onupdate="CASCADE", ondelete="CASCADE")),
-        Column("relationship_id", Integer, server_default=null()),
-        Column("dummy_relationship_dimension", Integer, server_default="0"),
-        Column(
-            "object_id", Integer, ForeignKey("object.id", onupdate="CASCADE", ondelete="CASCADE"), server_default=null()
-        ),
-        Column("index", Integer, server_default="1"),
-        Column("value", String(155), server_default=null()),
-        Column("json", String(255), server_default=null()),
-        Column("expression", String(155), server_default=null()),
-        Column("time_pattern", String(155), server_default=null()),
-        Column("time_series_id", String(155), server_default=null()),
-        Column("stochastic_model_id", String(155), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        CheckConstraint("`relationship_id` IS NOT NULL OR `object_id` IS NOT NULL", name="obj_or_rel_id_is_not_null"),
-        UniqueConstraint("parameter_id", "object_id"),
-        UniqueConstraint("parameter_id", "relationship_id"),
-        ForeignKeyConstraint(
-            ("relationship_id", "dummy_relationship_dimension"),
-            ("relationship.id", "relationship.dimension"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    try:
-        meta.create_all(engine)
-    except DatabaseError as e:
-        raise SpineDBAPIError("Unable to create Spine database: {}".format(e.orig.args))
-    return engine
-
-
-def forward_sweep(root, fn):
-    """Recursively visit, using `get_children()`, the given sqlalchemy object.
-    Apply `fn` on every visited node."""
-    current = root
-    parent = {}
-    children = {current: iter(current.get_children(column_collections=False))}
-    while True:
-        fn(current)
-        # Try and visit next children
-        next_ = next(children[current], None)
-        if next_ is not None:
-            parent[next_] = current
-            children[next_] = iter(next_.get_children(column_collections=False))
-            current = next_
-            continue
-        # No (more) children, try and visit next sibling
-        current_parent = parent[current]
-        next_ = next(children[current_parent], None)
-        if next_ is not None:
-            parent[next_] = current_parent
-            children[next_] = iter(next_.get_children(column_collections=False))
-            current = next_
-            continue
-        # No (more) siblings, go back to parent
-        current = current_parent
-        if current == root:
-            break
-
-
-def get_relationship_entity_class_items(item, object_class_type):
-    return [
-        {
-            "entity_class_id": item["id"],
-            "dimension": dimension,
-            "member_class_id": object_class_id,
-            "member_class_type_id": object_class_type,
-        }
-        for dimension, object_class_id in enumerate(item["object_class_id_list"])
-    ]
-
-
-def get_relationship_entity_items(item, relationship_entity_type, object_entity_type):
-    return [
-        {
-            "entity_id": item["id"],
-            "type_id": relationship_entity_type,
-            "entity_class_id": item["class_id"],
-            "dimension": dimension,
-            "member_id": object_id,
-            "member_class_type_id": object_entity_type,
-            "member_class_id": object_class_id,
-        }
-        for dimension, (object_id, object_class_id) in enumerate(
-            zip(item["object_id_list"], item["object_class_id_list"])
-        )
-    ]
-
-
-def labelled_columns(table):
-    return [c.label(c.name) for c in table.columns]
-
-
-class AsteriskType:
-    def __repr__(self):
-        return "Asterisk"
-
-
-Asterisk = AsteriskType()
-
-
-def fix_name_ambiguity(input_list, offset=0, prefix=""):
-    """Modify repeated entries in name list by appending an increasing integer."""
-    result = []
-    ocurrences = {}
-    for item in input_list:
-        n_ocurrences = input_list.count(item)
-        if n_ocurrences > 1:
-            ocurrence = ocurrences.get(item, 1)
-            ocurrences[item] = ocurrence + 1
-            item += prefix + str(offset + ocurrence)
-        result.append(item)
-    return result
-
-
-def vacuum(url):
-    engine = create_engine(url)
-    if not engine.url.drivername.startswith("sqlite"):
-        return 0, "bytes"
-    size_before = os.path.getsize(engine.url.database)
-    engine.execute("vacuum")
-    freed = size_before - os.path.getsize(engine.url.database)
-    k = 0
-    units = ("bytes", "KB", "MB", "GB", "TB")
-    while freed > 1e3 and k < len(units):
-        freed /= 1e3
-        k += 1
-    return freed, units[k]
-
-
-def remove_credentials_from_url(url):
-    """Removes username and password information from URLs.
-
-    Args:
-        url (str): URL
-
-    Returns:
-        str: sanitized URL
-    """
-    parsed = urlparse(url)
-    if parsed.username is None:
-        return url
-    return urlunparse(parsed._replace(netloc=parsed.netloc.partition("@")[-1]))
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+General helper functions and classes.
+
+"""
+
+import os
+import json
+import warnings
+from operator import itemgetter
+from urllib.parse import urlparse, urlunparse
+from sqlalchemy import (
+    Boolean,
+    BigInteger,
+    CheckConstraint,
+    Column,
+    DateTime,
+    Float,
+    ForeignKey,
+    ForeignKeyConstraint,
+    Integer,
+    MetaData,
+    PrimaryKeyConstraint,
+    String,
+    Table,
+    Text,
+    LargeBinary,
+    UniqueConstraint,
+    create_engine,
+    false,
+    func,
+    inspect,
+    null,
+    select,
+)
+from sqlalchemy.ext.automap import generate_relationship
+from sqlalchemy.ext.compiler import compiles
+from sqlalchemy.exc import DatabaseError, IntegrityError, OperationalError
+from sqlalchemy.dialects.mysql import TINYINT, DOUBLE
+from sqlalchemy.sql.expression import FunctionElement, bindparam, cast
+from alembic.config import Config
+from alembic.script import ScriptDirectory
+from alembic.migration import MigrationContext
+from alembic.environment import EnvironmentContext
+from .exception import SpineDBAPIError, SpineDBVersionError
+
+# Supported dialects and recommended dbapi. Restricted to mysql and sqlite for now:
+# - sqlite works
+# - mysql is trying to work
+SUPPORTED_DIALECTS = {
+    "mysql": "pymysql",
+    "sqlite": "sqlite3",
+}
+
+UNSUPPORTED_DIALECTS = {
+    "mssql": "pyodbc",
+    "postgresql": "psycopg2",
+    "oracle": "cx_oracle",
+}
+
+naming_convention = {
+    "pk": "pk_%(table_name)s",
+    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
+    "uq": "uq_%(table_name)s_%(column_0N_name)s",
+    "ck": "ck_%(table_name)s_%(constraint_name)s",
+}
+
+model_meta = MetaData(naming_convention=naming_convention)
+
+LONGTEXT_LENGTH = 2 ** 32 - 1
+
+
+# NOTE: Deactivated since foreign keys are too difficult to get right in the diff tables.
+# For example, the diff_object table would need a `class_id` field and a `diff_class_id` field,
+# plus a CHECK constraint that at least one of the two is NOT NULL.
+# @event.listens_for(Engine, "connect")
+def set_sqlite_pragma(dbapi_connection, connection_record):
+    module_name = dbapi_connection.__class__.__module__
+    if not module_name.lower().startswith("sqlite"):
+        return
+    cursor = dbapi_connection.cursor()
+    cursor.execute("PRAGMA foreign_keys=ON")
+    cursor.close()
+
+
+@compiles(TINYINT, "sqlite")
+def compile_TINYINT_mysql_sqlite(element, compiler, **kw):
+    """Handles mysql TINYINT datatype as INTEGER in sqlite."""
+    return compiler.visit_INTEGER(element, **kw)
+
+
+@compiles(DOUBLE, "sqlite")
+def compile_DOUBLE_mysql_sqlite(element, compiler, **kw):
+    """Handles mysql DOUBLE datatype as REAL in sqlite."""
+    return compiler.visit_REAL(element, **kw)
+
+
+class group_concat(FunctionElement):
+    type = String()
+    name = 'group_concat'
+
+
+def _parse_group_concat_clauses(clauses):
+    keys = ("group_concat_column", "order_by_column", "separator")
+    d = dict(zip(keys, clauses))
+    return d["group_concat_column"], d.get("order_by_column"), d.get("separator", bindparam("sep", ","))
+
+
+@compiles(group_concat, "sqlite")
+def compile_group_concat_sqlite(element, compiler, **kw):
+    group_concat_column, _, separator = _parse_group_concat_clauses(element.clauses)
+    return compiler.process(func.group_concat(group_concat_column, separator), **kw)
+
+
+@compiles(group_concat, "mysql")
+def compile_group_concat_mysql(element, compiler, **kw):
+    group_concat_column, order_by_column, separator = _parse_group_concat_clauses(element.clauses)
+    str_group_concat_column = cast(group_concat_column, String)
+    if order_by_column is not None:
+        str_group_concat_column = str_group_concat_column.op("ORDER BY")(order_by_column)
+    return "group_concat(%s separator %s)" % (
+        compiler.process(str_group_concat_column, **kw),
+        compiler.process(separator, **kw),
+    )
+
+
+def _parse_metadata_fallback(metadata):
+    yield ("unnamed", str(metadata))
+
+
+def _parse_metadata(metadata):
+    try:
+        parsed = json.loads(metadata)
+    except json.decoder.JSONDecodeError:
+        yield from _parse_metadata_fallback(metadata)
+        return
+    if not isinstance(parsed, dict):
+        yield from _parse_metadata_fallback(metadata)
+        return
+    for key, value in parsed.items():
+        if isinstance(value, list):
+            for val in value:
+                yield (key, str(val))
+            continue
+        yield (key, str(value))
+
+
+def is_head(db_url, upgrade=False):
+    """Check whether or not db_url is head.
+
+    Args:
+        db_url (str): database url
+        upgrade (Bool): if True, upgrade db to head
+    """
+    engine = create_engine(db_url)
+    return is_head_engine(engine, upgrade=upgrade)
+
+
+def is_head_engine(engine, upgrade=False):
+    """Check whether or not engine is head.
+
+    Args:
+        engine (Engine): database engine
+        upgrade (Bool): if True, upgrade db to head
+    """
+    config = Config()
+    config.set_main_option("script_location", "spinedb_api:alembic")
+    script = ScriptDirectory.from_config(config)
+    head = script.get_current_head()
+    with engine.connect() as connection:
+        migration_context = MigrationContext.configure(connection)
+        current_rev = migration_context.get_current_revision()
+        if current_rev == head:
+            return True
+        if not upgrade:
+            return False
+
+        # Upgrade function
+        def fn(rev, context):
+            return script._upgrade_revs("head", rev)
+
+        with EnvironmentContext(
+            config, script, fn=fn, as_sql=False, starting_rev=None, destination_rev="head", tag=None
+        ) as environment_context:
+            environment_context.configure(connection=connection, target_metadata=model_meta)
+            with environment_context.begin_transaction():
+                environment_context.run_migrations()
+    return True
+
+
+def copy_database(dest_url, source_url, overwrite=True, upgrade=False, only_tables=(), skip_tables=()):
+    """Copy the database from source_url into dest_url."""
+    if not is_head(source_url, upgrade=upgrade):
+        raise SpineDBVersionError(url=source_url)
+    source_engine = create_engine(source_url)
+    dest_engine = create_engine(dest_url)
+    copy_database_bind(
+        dest_engine,
+        source_engine,
+        overwrite=overwrite,
+        upgrade=upgrade,
+        only_tables=only_tables,
+        skip_tables=skip_tables,
+    )
+
+
+def copy_database_bind(dest_bind, source_bind, overwrite=True, upgrade=False, only_tables=(), skip_tables=()):
+    source_meta = MetaData(bind=source_bind)
+    source_meta.reflect()
+    if inspect(dest_bind).get_table_names():
+        if not overwrite:
+            raise SpineDBAPIError(
+                f"The database at '{dest_bind}' is not empty. "
+                "If you want to overwrite it, please pass the argument `overwrite=True` "
+                "to the function call."
+            )
+        source_meta.drop_all(dest_bind)
+    dest_meta = MetaData(bind=dest_bind)
+    for source_table in source_meta.sorted_tables:
+        # Create table in dest
+        source_table.create(dest_bind)
+        if source_table.name not in ("alembic_version", "next_id"):
+            # Skip tables according to `only_tables` and `skip_tables`
+            if only_tables and source_table.name not in only_tables:
+                continue
+            if source_table.name in skip_tables:
+                continue
+        dest_table = Table(source_table, dest_meta, autoload=True)
+        sel = select([source_table])
+        result = source_bind.execute(sel)
+        # Insert data from source into destination
+        data = result.fetchall()
+        if not data:
+            continue
+        ins = dest_table.insert()
+        try:
+            dest_bind.execute(ins, data)
+        except IntegrityError as e:
+            warnings.warn("Skipping table {0}: {1}".format(source_table.name, e.orig.args))
+
+
+def custom_generate_relationship(base, direction, return_fn, attrname, local_cls, referred_cls, **kw):
+    """Make all relationships view only to avoid warnings."""
+    kw["viewonly"] = True
+    kw["cascade"] = ""
+    kw["passive_deletes"] = False
+    kw["sync_backref"] = False
+    return generate_relationship(base, direction, return_fn, attrname, local_cls, referred_cls, **kw)
+
+
+def is_unlocked(db_url, timeout=0):
+    """Return True if the SQLite db_url is unlocked, after waiting at most timeout seconds.
+    Otherwise return False."""
+    if not db_url.startswith("sqlite"):
+        return False
+    try:
+        engine = create_engine(db_url, connect_args={"timeout": timeout})
+        engine.execute("BEGIN IMMEDIATE")
+        return True
+    except OperationalError:
+        return False
+
+
+def compare_schemas(left_engine, right_engine):
+    """Whether or not the left and right engine have the same schema."""
+    left_insp = inspect(left_engine)
+    right_insp = inspect(right_engine)
+    left_dict = schema_dict(left_insp)
+    right_dict = schema_dict(right_insp)
+    return str(left_dict) == str(right_dict)
+
+
+def schema_dict(insp):
+    return {
+        table_name: {
+            "columns": sorted(insp.get_columns(table_name), key=itemgetter("name")),
+            "pk_constraint": insp.get_pk_constraint(table_name),
+            "foreign_keys": sorted(insp.get_foreign_keys(table_name), key=lambda x: x["name"] or ""),
+            "check_constraints": insp.get_check_constraints(table_name),
+        }
+        for table_name in insp.get_table_names()
+    }
+
+
+def is_empty(db_url):
+    try:
+        engine = create_engine(db_url)
+    except DatabaseError as e:
+        raise SpineDBAPIError("Could not connect to '{}': {}".format(db_url, e.orig.args)) from None
+    insp = inspect(engine)
+    if insp.get_table_names():
+        return False
+    return True
+
+
+def create_spine_metadata():
+    meta = MetaData(naming_convention=naming_convention)
+    Table(
+        "commit",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("comment", String(255), nullable=False),
+        Column("date", DateTime, nullable=False),
+        Column("user", String(45)),
+    )
+    Table(
+        "alternative",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("name", String(255), nullable=False),
+        Column("description", Text(), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("name"),
+    )
+    Table(
+        "scenario",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("name", String(255), nullable=False),
+        Column("description", Text(), server_default=null()),
+        Column("active", Boolean(name="active"), server_default=false(), nullable=False),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("name"),
+    )
+    Table(
+        "scenario_alternative",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column(
+            "scenario_id", Integer, ForeignKey("scenario.id", onupdate="CASCADE", ondelete="CASCADE"), nullable=False
+        ),
+        Column(
+            "alternative_id",
+            Integer,
+            ForeignKey("alternative.id", onupdate="CASCADE", ondelete="CASCADE"),
+            nullable=False,
+        ),
+        Column("rank", Integer, nullable=False),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("scenario_id", "rank"),
+        UniqueConstraint("scenario_id", "alternative_id"),
+    )
+    Table(
+        "entity_class_type",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("name", String(255), nullable=False),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+    )
+    Table(
+        "entity_type",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("name", String(255), nullable=False),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+    )
+    Table(
+        "entity_class",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column(
+            "type_id",
+            Integer,
+            ForeignKey("entity_class_type.id", onupdate="CASCADE", ondelete="CASCADE"),
+            nullable=False,
+        ),
+        Column("name", String(255), nullable=False),
+        Column("description", Text(), server_default=null()),
+        Column("display_order", Integer, server_default="99"),
+        Column("display_icon", BigInteger, server_default=null()),
+        Column("hidden", Integer, server_default="0"),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("id", "type_id"),
+        UniqueConstraint("type_id", "name"),
+    )
+    Table(
+        "object_class",
+        meta,
+        Column("entity_class_id", Integer, primary_key=True),
+        Column("type_id", Integer, nullable=False),
+        ForeignKeyConstraint(
+            ("entity_class_id", "type_id"), ("entity_class.id", "entity_class.type_id"), ondelete="CASCADE"
+        ),
+        CheckConstraint("`type_id` = 1", name="type_id"),  # make sure object class can only have object type
+    )
+    Table(
+        "relationship_class",
+        meta,
+        Column("entity_class_id", Integer, primary_key=True),
+        Column("type_id", Integer, nullable=False),
+        ForeignKeyConstraint(
+            ("entity_class_id", "type_id"), ("entity_class.id", "entity_class.type_id"), ondelete="CASCADE"
+        ),
+        CheckConstraint("`type_id` = 2", name="type_id"),
+    )
+    Table(
+        "relationship_entity_class",
+        meta,
+        Column(
+            "entity_class_id",
+            Integer,
+            ForeignKey("relationship_class.entity_class_id", onupdate="CASCADE", ondelete="CASCADE"),
+            primary_key=True,
+        ),
+        Column("dimension", Integer, primary_key=True),
+        Column("member_class_id", Integer, nullable=False),
+        Column("member_class_type_id", Integer, nullable=False),
+        UniqueConstraint("entity_class_id", "dimension", "member_class_id", name="uq_relationship_entity_class"),
+        ForeignKeyConstraint(("member_class_id", "member_class_type_id"), ("entity_class.id", "entity_class.type_id")),
+        CheckConstraint("`member_class_type_id` != 2", name="member_class_type_id"),
+    )
+    Table(
+        "entity",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("type_id", Integer, ForeignKey("entity_type.id", onupdate="CASCADE", ondelete="CASCADE")),
+        Column("class_id", Integer, ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE")),
+        Column("name", String(255), nullable=False),
+        Column("description", Text(), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("id", "class_id"),
+        UniqueConstraint("id", "type_id", "class_id"),
+        UniqueConstraint("class_id", "name"),
+    )
+    Table(
+        "object",
+        meta,
+        Column("entity_id", Integer, primary_key=True),
+        Column("type_id", Integer, nullable=False),
+        ForeignKeyConstraint(("entity_id", "type_id"), ("entity.id", "entity.type_id"), ondelete="CASCADE"),
+        CheckConstraint("`type_id` = 1", name="type_id"),  # make sure object can only have object type
+    )
+    Table(
+        "relationship",
+        meta,
+        Column("entity_id", Integer, primary_key=True),
+        Column("entity_class_id", Integer, nullable=False),
+        Column("type_id", Integer, nullable=False),
+        UniqueConstraint("entity_id", "entity_class_id"),
+        ForeignKeyConstraint(("entity_id", "type_id"), ("entity.id", "entity.type_id"), ondelete="CASCADE"),
+        CheckConstraint("`type_id` = 2", name="type_id"),
+    )
+    Table(
+        "relationship_entity",
+        meta,
+        Column("entity_id", Integer, primary_key=True),
+        Column("entity_class_id", Integer, nullable=False),
+        Column("dimension", Integer, primary_key=True),
+        Column("member_id", Integer, nullable=False),
+        Column("member_class_id", Integer, nullable=False),
+        ForeignKeyConstraint(
+            ("member_id", "member_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
+        ),
+        ForeignKeyConstraint(
+            ("entity_class_id", "dimension", "member_class_id"),
+            (
+                "relationship_entity_class.entity_class_id",
+                "relationship_entity_class.dimension",
+                "relationship_entity_class.member_class_id",
+            ),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+        ForeignKeyConstraint(
+            ("entity_id", "entity_class_id"),
+            ("relationship.entity_id", "relationship.entity_class_id"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+    Table(
+        "entity_group",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("entity_id", Integer, nullable=False),
+        Column("entity_class_id", Integer, nullable=False),
+        Column("member_id", Integer, nullable=False),
+        UniqueConstraint("entity_id", "member_id"),
+        ForeignKeyConstraint(
+            ("entity_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
+        ),
+        ForeignKeyConstraint(
+            ("member_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
+        ),
+    )
+    Table(
+        "parameter_definition",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column(
+            "entity_class_id",
+            Integer,
+            ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE"),
+            nullable=False,
+        ),
+        Column("name", String(155), nullable=False),
+        Column("description", Text(), server_default=null()),
+        Column("default_type", String(255)),
+        Column("default_value", LargeBinary(LONGTEXT_LENGTH), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        Column("parameter_value_list_id", Integer),
+        UniqueConstraint("id", "entity_class_id"),
+        UniqueConstraint("entity_class_id", "name"),
+        UniqueConstraint("id", "parameter_value_list_id"),
+    )
+    Table(
+        "parameter_tag",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("tag", String(155), nullable=False, unique=True),
+        Column("description", Text(), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+    )
+    Table(
+        "parameter_definition_tag",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column(
+            "parameter_definition_id",
+            Integer,
+            ForeignKey("parameter_definition.id", name="fk_parameter_tag_parameter_definition"),
+            nullable=False,
+        ),
+        Column("parameter_tag_id", Integer, ForeignKey("parameter_tag.id"), nullable=False),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("parameter_definition_id", "parameter_tag_id", name="uq_parameter_definition_tag"),
+    )
+    Table(
+        "parameter_value",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("parameter_definition_id", Integer, nullable=False),
+        Column("entity_id", Integer, nullable=False),
+        Column("entity_class_id", Integer, nullable=False),
+        Column("type", String(255)),
+        Column("value", LargeBinary(LONGTEXT_LENGTH), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        Column("alternative_id", Integer, ForeignKey("alternative.id"), nullable=False),
+        UniqueConstraint("parameter_definition_id", "entity_id", "alternative_id", name="uq_parameter_value"),
+        ForeignKeyConstraint(
+            ("entity_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
+        ),
+        ForeignKeyConstraint(
+            ("parameter_definition_id", "entity_class_id"),
+            ("parameter_definition.id", "parameter_definition.entity_class_id"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+    Table(
+        "parameter_value_list",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("name", String(155), nullable=False),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+    )
+    Table(
+        "list_value",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("parameter_value_list_id", Integer, ForeignKey("parameter_value_list.id"), nullable=False),
+        Column("index", Integer, nullable=False),
+        Column("type", String(255)),
+        Column("value", LargeBinary(LONGTEXT_LENGTH), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("parameter_value_list_id", "index"),
+    )
+    Table(
+        "tool",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("name", String(155), nullable=False),
+        Column("description", Text(), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+    )
+    Table(
+        "feature",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("parameter_definition_id", Integer, nullable=False),
+        Column("parameter_value_list_id", Integer, nullable=False),
+        Column("description", Text(), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("parameter_definition_id", "parameter_value_list_id"),
+        UniqueConstraint("id", "parameter_value_list_id"),
+        ForeignKeyConstraint(
+            ("parameter_definition_id", "parameter_value_list_id"),
+            ("parameter_definition.id", "parameter_definition.parameter_value_list_id"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+    Table(
+        "tool_feature",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("tool_id", Integer, ForeignKey("tool.id")),
+        Column("feature_id", Integer, nullable=False),
+        Column("parameter_value_list_id", Integer, nullable=False),
+        Column("required", Boolean(name="required"), server_default=false(), nullable=False),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("tool_id", "feature_id"),
+        UniqueConstraint("id", "parameter_value_list_id"),
+        ForeignKeyConstraint(
+            ("feature_id", "parameter_value_list_id"),
+            ("feature.id", "feature.parameter_value_list_id"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+    Table(
+        "tool_feature_method",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("tool_feature_id", Integer, nullable=False),
+        Column("parameter_value_list_id", Integer, nullable=False),
+        Column("method_index", Integer),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("tool_feature_id", "method_index"),
+        ForeignKeyConstraint(
+            ("tool_feature_id", "parameter_value_list_id"),
+            ("tool_feature.id", "tool_feature.parameter_value_list_id"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+        ForeignKeyConstraint(
+            ("parameter_value_list_id", "method_index"),
+            ("list_value.parameter_value_list_id", "list_value.index"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+    Table(
+        "metadata",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("name", String(155), nullable=False),
+        Column("value", String(255), nullable=False),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("name", "value"),
+    )
+    Table(
+        "parameter_value_metadata",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column(
+            "parameter_value_id",
+            Integer,
+            ForeignKey("parameter_value.id", onupdate="CASCADE", ondelete="CASCADE"),
+            nullable=False,
+        ),
+        Column(
+            "metadata_id", Integer, ForeignKey("metadata.id", onupdate="CASCADE", ondelete="CASCADE"), nullable=False
+        ),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("parameter_value_id", "metadata_id"),
+    )
+    Table(
+        "entity_metadata",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("entity_id", Integer, ForeignKey("entity.id", onupdate="CASCADE", ondelete="CASCADE"), nullable=False),
+        Column(
+            "metadata_id", Integer, ForeignKey("metadata.id", onupdate="CASCADE", ondelete="CASCADE"), nullable=False
+        ),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("entity_id", "metadata_id"),
+    )
+    Table(
+        "alembic_version",
+        meta,
+        Column("version_num", String(32), nullable=False),
+        PrimaryKeyConstraint("version_num", name="alembic_version_pkc"),
+    )
+    return meta
+
+
+def create_new_spine_database(db_url):
+    """Create a new Spine database at the given url."""
+    try:
+        engine = create_engine(db_url)
+    except DatabaseError as e:
+        raise SpineDBAPIError("Could not connect to '{}': {}".format(db_url, e.orig.args)) from None
+    # Drop existing tables. This is a Spine db now...
+    meta = MetaData(engine)
+    meta.reflect()
+    meta.drop_all()
+    # Create new tables
+    meta = create_spine_metadata()
+    try:
+        meta.create_all(engine)
+        engine.execute("INSERT INTO `commit` VALUES (1, 'Create the database', CURRENT_TIMESTAMP, 'spinedb_api')")
+        engine.execute("INSERT INTO alternative VALUES (1, 'Base', 'Base alternative', 1)")
+        engine.execute("INSERT INTO entity_class_type VALUES (1, 'object', 1), (2, 'relationship', 1)")
+        engine.execute("INSERT INTO entity_type VALUES (1, 'object', 1), (2, 'relationship', 1)")
+        engine.execute("INSERT INTO alembic_version VALUES ('989fccf80441')")
+    except DatabaseError as e:
+        raise SpineDBAPIError("Unable to create Spine database: {}".format(e)) from None
+    return engine
+
+
+def _create_first_spine_database(db_url):
+    """Creates a Spine database with the very first version at the given url.
+    Used internally.
+    """
+    try:
+        engine = create_engine(db_url)
+    except DatabaseError as e:
+        raise SpineDBAPIError("Could not connect to '{}': {}".format(db_url, e.orig.args)) from None
+    # Drop existing tables. This is a Spine db now...
+    meta = MetaData(engine)
+    meta.reflect()
+    meta.drop_all(engine)
+    # Create new tables
+    meta = MetaData(naming_convention=naming_convention)
+    Table(
+        "commit",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("comment", String(255), nullable=False),
+        Column("date", DateTime, nullable=False),
+        Column("user", String(45)),
+    )
+    Table(
+        "object_class_category",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("name", String(255), nullable=False, unique=True),
+        Column("description", String(255), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+    )
+    Table(
+        "object_class",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("name", String(255), nullable=False, unique=True),
+        Column("description", String(255), server_default=null()),
+        Column("category_id", Integer, ForeignKey("object_class_category.id"), server_default=null()),
+        Column("display_order", Integer, server_default="99"),
+        Column("display_icon", BigInteger, server_default=null()),
+        Column("hidden", Integer, server_default="0"),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+    )
+    Table(
+        "object_category",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("object_class_id", Integer, ForeignKey("object_class.id")),
+        Column("name", String(255), nullable=False, unique=True),
+        Column("description", String(255), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+    )
+    Table(
+        "object",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("class_id", Integer, ForeignKey("object_class.id", onupdate="CASCADE", ondelete="CASCADE")),
+        Column("name", String(255), nullable=False, unique=True),
+        Column("description", String(255), server_default=null()),
+        Column("category_id", Integer, ForeignKey("object_category.id")),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+    )
+    Table(
+        "relationship_class",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("dimension", Integer, primary_key=True),
+        Column("object_class_id", Integer, ForeignKey("object_class.id")),
+        Column("name", String(255), nullable=False),
+        Column("hidden", Integer, server_default="0"),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("dimension", "name"),
+    )
+    Table(
+        "relationship",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("dimension", Integer, primary_key=True),
+        Column("object_id", Integer, ForeignKey("object.id")),
+        Column("class_id", Integer, nullable=False),
+        Column("name", String(255), nullable=False),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("dimension", "name"),
+        ForeignKeyConstraint(
+            ("class_id", "dimension"),
+            ("relationship_class.id", "relationship_class.dimension"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+    Table(
+        "parameter",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("name", String(155), nullable=False, unique=True),
+        Column("description", String(155), server_default=null()),
+        Column("data_type", String(155), server_default="NUMERIC"),
+        Column("relationship_class_id", Integer, default=null()),
+        Column(
+            "object_class_id",
+            Integer,
+            ForeignKey("object_class.id", onupdate="CASCADE", ondelete="CASCADE"),
+            server_default=null(),
+        ),
+        Column("can_have_time_series", Integer, server_default="0"),
+        Column("can_have_time_pattern", Integer, server_default="1"),
+        Column("can_be_stochastic", Integer, server_default="0"),
+        Column("default_value", String(155), server_default="0"),
+        Column("is_mandatory", Integer, server_default="0"),
+        Column("precision", Integer, server_default="2"),
+        Column("unit", String(155), server_default=null()),
+        Column("minimum_value", Float, server_default=null()),
+        Column("maximum_value", Float, server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        CheckConstraint(
+            "`relationship_class_id` IS NOT NULL OR `object_class_id` IS NOT NULL",
+            name="obj_or_rel_class_id_is_not_null",
+        ),
+    )
+    Table(
+        "parameter_value",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("parameter_id", Integer, ForeignKey("parameter.id", onupdate="CASCADE", ondelete="CASCADE")),
+        Column("relationship_id", Integer, server_default=null()),
+        Column("dummy_relationship_dimension", Integer, server_default="0"),
+        Column(
+            "object_id", Integer, ForeignKey("object.id", onupdate="CASCADE", ondelete="CASCADE"), server_default=null()
+        ),
+        Column("index", Integer, server_default="1"),
+        Column("value", String(155), server_default=null()),
+        Column("json", String(255), server_default=null()),
+        Column("expression", String(155), server_default=null()),
+        Column("time_pattern", String(155), server_default=null()),
+        Column("time_series_id", String(155), server_default=null()),
+        Column("stochastic_model_id", String(155), server_default=null()),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        CheckConstraint("`relationship_id` IS NOT NULL OR `object_id` IS NOT NULL", name="obj_or_rel_id_is_not_null"),
+        UniqueConstraint("parameter_id", "object_id"),
+        UniqueConstraint("parameter_id", "relationship_id"),
+        ForeignKeyConstraint(
+            ("relationship_id", "dummy_relationship_dimension"),
+            ("relationship.id", "relationship.dimension"),
+            onupdate="CASCADE",
+            ondelete="CASCADE",
+        ),
+    )
+    try:
+        meta.create_all(engine)
+    except DatabaseError as e:
+        raise SpineDBAPIError("Unable to create Spine database: {}".format(e.orig.args))
+    return engine
+
+
+def forward_sweep(root, fn):
+    """Recursively visit, using `get_children()`, the given sqlalchemy object.
+    Apply `fn` on every visited node."""
+    current = root
+    parent = {}
+    children = {current: iter(current.get_children(column_collections=False))}
+    while True:
+        fn(current)
+        # Try and visit next children
+        next_ = next(children[current], None)
+        if next_ is not None:
+            parent[next_] = current
+            children[next_] = iter(next_.get_children(column_collections=False))
+            current = next_
+            continue
+        # No (more) children, try and visit next sibling
+        current_parent = parent[current]
+        next_ = next(children[current_parent], None)
+        if next_ is not None:
+            parent[next_] = current_parent
+            children[next_] = iter(next_.get_children(column_collections=False))
+            current = next_
+            continue
+        # No (more) siblings, go back to parent
+        current = current_parent
+        if current == root:
+            break
+
+
+def get_relationship_entity_class_items(item, object_class_type):
+    return [
+        {
+            "entity_class_id": item["id"],
+            "dimension": dimension,
+            "member_class_id": object_class_id,
+            "member_class_type_id": object_class_type,
+        }
+        for dimension, object_class_id in enumerate(item["object_class_id_list"])
+    ]
+
+
+def get_relationship_entity_items(item, relationship_entity_type, object_entity_type):
+    return [
+        {
+            "entity_id": item["id"],
+            "type_id": relationship_entity_type,
+            "entity_class_id": item["class_id"],
+            "dimension": dimension,
+            "member_id": object_id,
+            "member_class_type_id": object_entity_type,
+            "member_class_id": object_class_id,
+        }
+        for dimension, (object_id, object_class_id) in enumerate(
+            zip(item["object_id_list"], item["object_class_id_list"])
+        )
+    ]
+
+
+def labelled_columns(table):
+    return [c.label(c.name) for c in table.columns]
+
+
+class AsteriskType:
+    def __repr__(self):
+        return "Asterisk"
+
+
+Asterisk = AsteriskType()
+
+
+def fix_name_ambiguity(input_list, offset=0, prefix=""):
+    """Modify repeated entries in name list by appending an increasing integer."""
+    result = []
+    ocurrences = {}
+    for item in input_list:
+        n_ocurrences = input_list.count(item)
+        if n_ocurrences > 1:
+            ocurrence = ocurrences.get(item, 1)
+            ocurrences[item] = ocurrence + 1
+            item += prefix + str(offset + ocurrence)
+        result.append(item)
+    return result
+
+
+def vacuum(url):
+    engine = create_engine(url)
+    if not engine.url.drivername.startswith("sqlite"):
+        return 0, "bytes"
+    size_before = os.path.getsize(engine.url.database)
+    engine.execute("vacuum")
+    freed = size_before - os.path.getsize(engine.url.database)
+    k = 0
+    units = ("bytes", "KB", "MB", "GB", "TB")
+    while freed > 1e3 and k < len(units):
+        freed /= 1e3
+        k += 1
+    return freed, units[k]
+
+
+def remove_credentials_from_url(url):
+    """Removes username and password information from URLs.
+
+    Args:
+        url (str): URL
+
+    Returns:
+        str: sanitized URL
+    """
+    parsed = urlparse(url)
+    if parsed.username is None:
+        return url
+    return urlunparse(parsed._replace(netloc=parsed.netloc.partition("@")[-1]))
```

### Comparing `spinedb_api-0.30.3/spinedb_api/import_functions.py` & `spinedb_api-0.30.4/spinedb_api/import_functions.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,2004 +1,2004 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Functions for importing data into a Spine database using entity names as references.
-
-"""
-
-import uuid
-from collections import defaultdict
-from contextlib import suppress
-
-from .exception import SpineIntegrityError, SpineDBAPIError
-from .check_functions import (
-    check_tool,
-    check_feature,
-    check_tool_feature,
-    check_tool_feature_method,
-    check_alternative,
-    check_object_class,
-    check_object,
-    check_wide_relationship_class,
-    check_wide_relationship,
-    check_entity_group,
-    check_parameter_definition,
-    check_parameter_value,
-    check_scenario,
-    check_parameter_value_list,
-    check_list_value,
-)
-from .parameter_value import to_database, from_database, fix_conflict
-from .helpers import _parse_metadata
-
-# TODO: update docstrings
-
-
-class ImportErrorLogItem:
-    """Class to hold log data for import errors"""
-
-    def __init__(self, msg="", db_type="", imported_from="", other=""):
-        self.msg = msg
-        self.db_type = db_type
-        self.imported_from = imported_from
-        self.other = other
-
-    def __repr__(self):
-        return self.msg
-
-
-def import_data(db_map, make_cache=None, unparse_value=to_database, on_conflict="merge", **kwargs):
-    """Imports data into a Spine database using name references (rather than id references).
-
-    Example::
-
-            object_c = ['example_class', 'other_class']
-            obj_parameters = [['example_class', 'example_parameter']]
-            relationship_c = [['example_rel_class', ['example_class', 'other_class']]]
-            rel_parameters = [['example_rel_class', 'rel_parameter']]
-            objects = [['example_class', 'example_object'],
-                       ['other_class', 'other_object']]
-            object_p_values = [['example_object_class', 'example_object', 'example_parameter', 3.14]]
-            relationships = [['example_rel_class', ['example_object', 'other_object']]]
-            rel_p_values = [['example_rel_class', ['example_object', 'other_object'], 'rel_parameter', 2.718]]
-            object_groups = [['object_class_name', 'object_group_name', ['member_name', 'another_member_name']]]
-            alternatives = [['example_alternative', 'An example']]
-            scenarios = [['example_scenario', 'An example']]
-            scenario_alternatives = [('scenario', 'alternative1'), ('scenario', 'alternative0', 'alternative1')]
-            tools = [('tool1', 'Tool one description'), ('tool2', 'Tool two description']]
-
-            import_data(db_map,
-                        object_classes=object_c,
-                        relationship_classes=relationship_c,
-                        object_parameters=obj_parameters,
-                        relationship_parameters=rel_parameters,
-                        objects=objects,
-                        relationships=relationships,
-                        object_groups=object_groups,
-                        object_parameter_values=object_p_values,
-                        relationship_parameter_values=rel_p_values,
-                        alternatives=alternatives,
-                        scenarios=scenarios,
-                        scenario_alternatives=scenario_alternatives
-                        tools=tools)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): database mapping
-        on_conflict (str): Conflict resolution strategy for ``parameter_value.fix_conflict``
-        object_classes (List[str]): List of object class names
-        relationship_classes (List[List[str, List(str)]):
-            List of lists with relationship class names and list of object class names
-        object_parameters (List[List[str, str]]):
-            list of lists with object class name and parameter name
-        relationship_parameters (List[List[str, str]]):
-            list of lists with relationship class name and parameter name
-        objects (List[List[str, str]]):
-            list of lists with object class name and object name
-        relationships: (List[List[str,List(String)]]):
-            list of lists with relationship class name and list of object names
-        object_groups (List[List/Tuple]): list/set/iterable of lists/tuples with object class name, group name,
-            and member name
-        object_parameter_values (List[List[str, str, str|numeric]]):
-            list of lists with object name, parameter name, parameter value
-        relationship_parameter_values (List[List[str, List(str), str, str|numeric]]):
-            list of lists with relationship class name, list of object names, parameter name, parameter value
-        alternatives (Iterable): alternative names or lists of two elements: alternative name and description
-        scenarios (Iterable): scenario names or lists of two elements: scenario name and description
-        scenario_alternatives (Iterable): lists of two elements: scenario name and a list of names of alternatives
-
-    Returns:
-        tuple: number of inserted/changed entities and list of ImportErrorLogItem with
-            any import errors
-    """
-    add_items_by_tablename = {
-        "alternative": db_map._add_alternatives,
-        "scenario": db_map._add_scenarios,
-        "scenario_alternative": db_map._add_scenario_alternatives,
-        "object_class": db_map._add_object_classes,
-        "relationship_class": db_map._add_wide_relationship_classes,
-        "parameter_value_list": db_map._add_parameter_value_lists,
-        "list_value": db_map._add_list_values,
-        "parameter_definition": db_map._add_parameter_definitions,
-        "feature": db_map._add_features,
-        "tool": db_map._add_tools,
-        "tool_feature": db_map._add_tool_features,
-        "tool_feature_method": db_map._add_tool_feature_methods,
-        "object": db_map._add_objects,
-        "relationship": db_map._add_wide_relationships,
-        "entity_group": db_map._add_entity_groups,
-        "parameter_value": db_map._add_parameter_values,
-        "metadata": db_map._add_metadata,
-        "entity_metadata": db_map._add_entity_metadata,
-        "parameter_value_metadata": db_map._add_parameter_value_metadata,
-    }
-    update_items_by_tablename = {
-        "alternative": db_map._update_alternatives,
-        "scenario": db_map._update_scenarios,
-        "scenario_alternative": db_map._update_scenario_alternatives,
-        "object_class": db_map._update_object_classes,
-        "relationship_class": db_map._update_wide_relationship_classes,
-        "parameter_value_list": db_map._update_parameter_value_lists,
-        "list_value": db_map._update_list_values,
-        "parameter_definition": db_map._update_parameter_definitions,
-        "feature": db_map._update_features,
-        "tool": db_map._update_tools,
-        "tool_feature": db_map._update_tool_features,
-        "object": db_map._update_objects,
-        "parameter_value": db_map._update_parameter_values,
-    }
-    error_log = []
-    num_imports = 0
-    for tablename, (to_add, to_update, errors) in get_data_for_import(
-        db_map, make_cache=make_cache, unparse_value=unparse_value, on_conflict=on_conflict, **kwargs
-    ):
-        update_items = update_items_by_tablename.get(tablename, lambda *args, **kwargs: ())
-        try:
-            updated = update_items(*to_update)
-        except SpineDBAPIError as error:
-            updated = []
-            error_log.append(ImportErrorLogItem(msg=str(error), db_type=tablename))
-        add_items = add_items_by_tablename[tablename]
-        try:
-            added = add_items(*to_add)
-        except SpineDBAPIError as error:
-            added = []
-            error_log.append(ImportErrorLogItem(msg=str(error), db_type=tablename))
-        num_imports += len(added) + len(updated)
-        error_log.extend(errors)
-    return num_imports, error_log
-
-
-def get_data_for_import(
-    db_map,
-    make_cache=None,
-    unparse_value=to_database,
-    on_conflict="merge",
-    object_classes=(),
-    relationship_classes=(),
-    parameter_value_lists=(),
-    object_parameters=(),
-    relationship_parameters=(),
-    objects=(),
-    relationships=(),
-    object_groups=(),
-    object_parameter_values=(),
-    relationship_parameter_values=(),
-    alternatives=(),
-    scenarios=(),
-    scenario_alternatives=(),
-    features=(),
-    tools=(),
-    tool_features=(),
-    tool_feature_methods=(),
-    metadata=(),
-    object_metadata=(),
-    relationship_metadata=(),
-    object_parameter_value_metadata=(),
-    relationship_parameter_value_metadata=(),
-):
-    """Returns an iterator of data for import, that the user can call instead of `import_data`
-    if they want to add and update the data by themselves.
-    Especially intended to be used with the toolbox undo/redo functionality.
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): database mapping
-        on_conflict (str): Conflict resolution strategy for ``parameter_value.fix_conflict``
-        object_classes (List[str]): List of object class names
-        relationship_classes (List[List[str, List(str)]):
-            List of lists with relationship class names and list of object class names
-        object_parameters (List[List[str, str]]):
-            list of lists with object class name and parameter name
-        relationship_parameters (List[List[str, str]]):
-            list of lists with relationship class name and parameter name
-        objects (List[List[str, str]]):
-            list of lists with object class name and object name
-        relationships: (List[List[str,List(String)]]):
-            list of lists with relationship class name and list of object names
-        object_groups (List[List/Tuple]): list/set/iterable of lists/tuples with object class name, group name,
-            and member name
-        object_parameter_values (List[List[str, str, str|numeric]]):
-            list of lists with object name, parameter name, parameter value
-        relationship_parameter_values (List[List[str, List(str), str, str|numeric]]):
-            list of lists with relationship class name, list of object names, parameter name,
-            parameter value
-
-    Returns:
-        dict(str, list)
-    """
-    if make_cache is None:
-        make_cache = db_map.make_cache
-    # NOTE: The order is important, because of references. E.g., we want to import alternatives before parameter_values
-    if alternatives:
-        yield ("alternative", _get_alternatives_for_import(alternatives, make_cache))
-    if scenarios:
-        yield ("scenario", _get_scenarios_for_import(scenarios, make_cache))
-    if scenario_alternatives:
-        if not scenarios:
-            scenarios = (item[0] for item in scenario_alternatives)
-            yield ("scenario", _get_scenarios_for_import(scenarios, make_cache))
-        if not alternatives:
-            alternatives = (item[1] for item in scenario_alternatives)
-            yield ("alternative", _get_alternatives_for_import(alternatives, make_cache))
-        yield ("scenario_alternative", _get_scenario_alternatives_for_import(scenario_alternatives, make_cache))
-    if object_classes:
-        yield ("object_class", _get_object_classes_for_import(db_map, object_classes, make_cache))
-    if relationship_classes:
-        yield ("relationship_class", _get_relationship_classes_for_import(db_map, relationship_classes, make_cache))
-    if parameter_value_lists:
-        yield ("parameter_value_list", _get_parameter_value_lists_for_import(db_map, parameter_value_lists, make_cache))
-        yield ("list_value", _get_list_values_for_import(db_map, parameter_value_lists, make_cache, unparse_value))
-    if object_parameters:
-        yield (
-            "parameter_definition",
-            _get_object_parameters_for_import(db_map, object_parameters, make_cache, unparse_value),
-        )
-    if relationship_parameters:
-        yield (
-            "parameter_definition",
-            _get_relationship_parameters_for_import(db_map, relationship_parameters, make_cache, unparse_value),
-        )
-    if features:
-        yield ("feature", _get_features_for_import(db_map, features, make_cache))
-    if tools:
-        yield ("tool", _get_tools_for_import(db_map, tools, make_cache))
-    if tool_features:
-        yield ("tool_feature", _get_tool_features_for_import(db_map, tool_features, make_cache))
-    if tool_feature_methods:
-        yield (
-            "tool_feature_method",
-            _get_tool_feature_methods_for_import(db_map, tool_feature_methods, make_cache, unparse_value),
-        )
-    if objects:
-        yield ("object", _get_objects_for_import(db_map, objects, make_cache))
-    if relationships:
-        yield ("relationship", _get_relationships_for_import(db_map, relationships, make_cache))
-    if object_groups:
-        yield ("entity_group", _get_object_groups_for_import(db_map, object_groups, make_cache))
-    if object_parameter_values:
-        yield (
-            "parameter_value",
-            _get_object_parameter_values_for_import(
-                db_map, object_parameter_values, make_cache, unparse_value, on_conflict
-            ),
-        )
-    if relationship_parameter_values:
-        yield (
-            "parameter_value",
-            _get_relationship_parameter_values_for_import(
-                db_map, relationship_parameter_values, make_cache, unparse_value, on_conflict
-            ),
-        )
-    if metadata:
-        yield ("metadata", _get_metadata_for_import(db_map, metadata, make_cache))
-    if object_metadata:
-        yield ("entity_metadata", _get_object_metadata_for_import(db_map, object_metadata, make_cache))
-    if relationship_metadata:
-        yield ("entity_metadata", _get_relationship_metadata_for_import(db_map, relationship_metadata, make_cache))
-    if object_parameter_value_metadata:
-        yield (
-            "parameter_value_metadata",
-            _get_object_parameter_value_metadata_for_import(db_map, object_parameter_value_metadata, make_cache),
-        )
-    if relationship_parameter_value_metadata:
-        yield (
-            "parameter_value_metadata",
-            _get_relationship_parameter_value_metadata_for_import(
-                db_map, relationship_parameter_value_metadata, make_cache
-            ),
-        )
-
-
-def import_features(db_map, data, make_cache=None):
-    """
-    Imports features.
-
-    Example:
-
-        data = [('class', 'parameter'), ('another_class', 'another_parameter', 'description')]
-        import_features(db_map, data)
-
-    Args:
-        db_map (DiffDatabaseMapping): mapping for database to insert into
-        data (Iterable): an iterable of lists/tuples with class name, parameter name, and optionally description
-
-    Returns:
-        tuple of int and list: Number of successfully inserted features, list of errors
-    """
-    return import_data(db_map, features=data, make_cache=make_cache)
-
-
-def _get_features_for_import(db_map, data, make_cache):
-    cache = make_cache({"feature"}, include_ancestors=True)
-    feature_ids = {x.parameter_definition_id: x.id for x in cache.get("feature", {}).values()}
-    parameter_ids = {
-        (x.entity_class_name, x.parameter_name): (x.id, x.value_list_id)
-        for x in cache.get("parameter_definition", {}).values()
-    }
-    parameter_definitions = {
-        x.id: {
-            "name": x.parameter_name,
-            "entity_class_id": x.entity_class_id,
-            "parameter_value_list_id": x.value_list_id,
-        }
-        for x in cache.get("parameter_definition", {}).values()
-    }
-    checked = set()
-    to_add = []
-    to_update = []
-    error_log = []
-    for class_name, parameter_name, *optionals in data:
-        parameter_definition_id, parameter_value_list_id = parameter_ids.get((class_name, parameter_name), (None, None))
-        if parameter_definition_id in checked:
-            continue
-        feature_id = feature_ids.pop(parameter_definition_id, None)
-        item = (
-            cache["feature"][feature_id]._asdict()
-            if feature_id is not None
-            else {
-                "parameter_definition_id": parameter_definition_id,
-                "parameter_value_list_id": parameter_value_list_id,
-                "description": None,
-            }
-        )
-        item.update(dict(zip(("description",), optionals)))
-        try:
-            check_feature(item, feature_ids, parameter_definitions)
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(
-                    msg=f"Could not import feature '{class_name, parameter_name}': {e.msg}", db_type="feature"
-                )
-            )
-            continue
-        finally:
-            if feature_id is not None:
-                feature_ids[parameter_definition_id] = feature_id
-        checked.add(parameter_definition_id)
-        if feature_id is not None:
-            item["id"] = feature_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_tools(db_map, data, make_cache=None):
-    """
-    Imports tools.
-
-    Example:
-
-        data = ['tool', ('another_tool', 'description')]
-        import_tools(db_map, data)
-
-    Args:
-        db_map (DiffDatabaseMapping): mapping for database to insert into
-        data (Iterable): an iterable of tool names,
-            or of lists/tuples with tool names and optional descriptions
-
-    Returns:
-        tuple of int and list: Number of successfully inserted tools, list of errors
-    """
-    return import_data(db_map, tools=data, make_cache=make_cache)
-
-
-def _get_tools_for_import(db_map, data, make_cache):
-    cache = make_cache({"tool"}, include_ancestors=True)
-    tool_ids = {tool.name: tool.id for tool in cache.get("tool", {}).values()}
-    checked = set()
-    to_add = []
-    to_update = []
-    error_log = []
-    for tool in data:
-        if isinstance(tool, str):
-            tool = (tool,)
-        name, *optionals = tool
-        if name in checked:
-            continue
-        tool_id = tool_ids.pop(name, None)
-        item = cache["tool"][tool_id]._asdict() if tool_id is not None else {"name": name, "description": None}
-        item.update(dict(zip(("description",), optionals)))
-        try:
-            check_tool(item, tool_ids)
-        except SpineIntegrityError as e:
-            error_log.append(ImportErrorLogItem(msg=f"Could not import tool '{name}': {e.msg}", db_type="tool"))
-            continue
-        finally:
-            if tool_id is not None:
-                tool_ids[name] = tool_id
-        checked.add(name)
-        if tool_id is not None:
-            item["id"] = tool_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_tool_features(db_map, data, make_cache=None):
-    """
-    Imports tool features.
-
-    Example:
-
-        data = [('tool', 'class', 'parameter'), ('another_tool', 'another_class', 'another_parameter', 'required')]
-        import_tool_features(db_map, data)
-
-    Args:
-        db_map (DiffDatabaseMapping): mapping for database to insert into
-        data (Iterable): an iterable of lists/tuples with tool name, class name, parameter name,
-            and optionally description
-
-    Returns:
-        tuple of int and list: Number of successfully inserted tool features, list of errors
-    """
-    return import_data(db_map, tool_features=data, make_cache=make_cache)
-
-
-def _get_tool_features_for_import(db_map, data, make_cache):
-    cache = make_cache({"tool_feature"}, include_ancestors=True)
-    tool_feature_ids = {(x.tool_id, x.feature_id): x.id for x in cache.get("tool_feature", {}).values()}
-    tool_ids = {x.name: x.id for x in cache.get("tool", {}).values()}
-    feature_ids = {
-        (x.entity_class_name, x.parameter_definition_name): (x.id, x.parameter_value_list_id)
-        for x in cache.get("feature", {}).values()
-    }
-    tools = {x.id: x._asdict() for x in cache.get("tool", {}).values()}
-    features = {
-        x.id: {
-            "name": x.entity_class_name + "/" + x.parameter_definition_name,
-            "parameter_value_list_id": x.parameter_value_list_id,
-        }
-        for x in cache.get("feature", {}).values()
-    }
-    checked = set()
-    to_add = []
-    to_update = []
-    error_log = []
-    for tool_name, class_name, parameter_name, *optionals in data:
-        tool_id = tool_ids.get(tool_name)
-        feature_id, parameter_value_list_id = feature_ids.get((class_name, parameter_name), (None, None))
-        if (tool_id, feature_id) in checked:
-            continue
-        tool_feature_id = tool_feature_ids.pop((tool_id, feature_id), None)
-        item = (
-            cache["tool_feature"][tool_feature_id]._asdict()
-            if tool_feature_id is not None
-            else {
-                "tool_id": tool_id,
-                "feature_id": feature_id,
-                "parameter_value_list_id": parameter_value_list_id,
-                "required": False,
-            }
-        )
-        item.update(dict(zip(("required",), optionals)))
-        try:
-            check_tool_feature(item, tool_feature_ids, tools, features)
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(
-                    msg=f"Could not import tool feature '{tool_name, class_name, parameter_name}': {e.msg}",
-                    db_type="tool_feature",
-                )
-            )
-            continue
-        finally:
-            if tool_feature_id is not None:
-                tool_feature_ids[tool_id, feature_id] = tool_feature_id
-        checked.add((tool_id, feature_id))
-        if tool_feature_id is not None:
-            item["id"] = tool_feature_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_tool_feature_methods(db_map, data, make_cache=None, unparse_value=to_database):
-    """
-    Imports tool feature methods.
-
-    Example:
-
-        data = [('tool', 'class', 'parameter', 'method'), ('another_tool', 'another_class', 'another_parameter', 'another_method')]
-        import_tool_features(db_map, data)
-
-    Args:
-        db_map (DiffDatabaseMapping): mapping for database to insert into
-        data (Iterable): an iterable of lists/tuples with tool name, class name, parameter name, and method
-
-    Returns:
-        tuple of int and list: Number of successfully inserted tool features, list of errors
-    """
-    return import_data(db_map, tool_feature_methods=data, make_cache=make_cache, unparse_value=unparse_value)
-
-
-def _get_tool_feature_methods_for_import(db_map, data, make_cache, unparse_value):
-    cache = make_cache({"tool_feature_method"}, include_ancestors=True)
-    tool_feature_method_ids = {
-        (x.tool_feature_id, x.method_index): x.id for x in cache.get("tool_feature_method", {}).values()
-    }
-    tool_feature_ids = {
-        (x.tool_name, x.entity_class_name, x.parameter_definition_name): (x.id, x.parameter_value_list_id)
-        for x in cache.get("tool_feature", {}).values()
-    }
-    tool_features = {x.id: x._asdict() for x in cache.get("tool_feature", {}).values()}
-    parameter_value_lists = {
-        x.id: {"name": x.name, "value_index_list": x.value_index_list}
-        for x in cache.get("parameter_value_list", {}).values()
-    }
-    list_values = {
-        (x.parameter_value_list_id, x.index): from_database(x.value, x.type)
-        for x in cache.get("list_value", {}).values()
-    }
-    seen = set()
-    to_add = []
-    error_log = []
-    for tool_name, class_name, parameter_name, method in data:
-        tool_feature_id, parameter_value_list_id = tool_feature_ids.get(
-            (tool_name, class_name, parameter_name), (None, None)
-        )
-        parameter_value_list = parameter_value_lists.get(parameter_value_list_id, {})
-        value_index_list = parameter_value_list.get("value_index_list", [])
-        method = from_database(*unparse_value(method))
-        method_index = next(
-            iter(index for index in value_index_list if list_values.get((parameter_value_list_id, index)) == method),
-            None,
-        )
-        if (tool_feature_id, method_index) in seen | tool_feature_method_ids.keys():
-            continue
-        item = {
-            "tool_feature_id": tool_feature_id,
-            "parameter_value_list_id": parameter_value_list_id,
-            "method_index": method_index,
-        }
-        try:
-            check_tool_feature_method(item, tool_feature_method_ids, tool_features, parameter_value_lists)
-            to_add.append(item)
-            seen.add((tool_feature_id, method_index))
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(
-                    msg=(
-                        f"Could not import tool feature method '{tool_name, class_name, parameter_name, method}':"
-                        f" {e.msg}"
-                    ),
-                    db_type="tool_feature_method",
-                )
-            )
-    return to_add, [], error_log
-
-
-def import_alternatives(db_map, data, make_cache=None):
-    """
-    Imports alternatives.
-
-    Example:
-
-        data = ['new_alternative', ('another_alternative', 'description')]
-        import_alternatives(db_map, data)
-
-    Args:
-        db_map (DiffDatabaseMapping): mapping for database to insert into
-        data (Iterable): an iterable of alternative names,
-            or of lists/tuples with alternative names and optional descriptions
-
-    Returns:
-        tuple of int and list: Number of successfully inserted alternatives, list of errors
-    """
-    return import_data(db_map, alternatives=data, make_cache=make_cache)
-
-
-def _get_alternatives_for_import(data, make_cache):
-    cache = make_cache({"alternative"}, include_ancestors=True)
-    alternative_ids = {alternative.name: alternative.id for alternative in cache.get("alternative", {}).values()}
-    checked = set()
-    to_add = []
-    to_update = []
-    error_log = []
-    for alternative in data:
-        if isinstance(alternative, str):
-            alternative = (alternative,)
-        name, *optionals = alternative
-        if name in checked:
-            continue
-        alternative_id = alternative_ids.pop(name, None)
-        item = (
-            cache["alternative"][alternative_id]._asdict()
-            if alternative_id is not None
-            else {"name": name, "description": None}
-        )
-        item.update(dict(zip(("description",), optionals)))
-        try:
-            check_alternative(item, alternative_ids)
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(msg=f"Could not import alternative '{name}': {e.msg}", db_type="alternative")
-            )
-            continue
-        finally:
-            if alternative_id is not None:
-                alternative_ids[name] = alternative_id
-        checked.add(name)
-        if alternative_id is not None:
-            item["id"] = alternative_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_scenarios(db_map, data, make_cache=None):
-    """
-    Imports scenarios.
-
-    Example:
-
-        second_active = True
-        third_active = False
-        data = ['scenario', ('second_scenario', second_active), ('third_scenario', third_active, 'description')]
-        import_scenarios(db_map, data)
-
-    Args:
-        db_map (DiffDatabaseMapping): mapping for database to insert into
-        data (Iterable): an iterable of scenario names,
-            or of lists/tuples with scenario names and optional descriptions
-
-    Returns:
-        tuple of int and list: Number of successfully inserted scenarios, list of errors
-    """
-    return import_data(db_map, scenarios=data, make_cache=make_cache)
-
-
-def _get_scenarios_for_import(data, make_cache):
-    cache = make_cache({"scenario"}, include_ancestors=True)
-    scenario_ids = {scenario.name: scenario.id for scenario in cache.get("scenario", {}).values()}
-    checked = set()
-    to_add = []
-    to_update = []
-    error_log = []
-    for scenario in data:
-        if isinstance(scenario, str):
-            scenario = (scenario,)
-        name, *optionals = scenario
-        if name in checked:
-            continue
-        scenario_id = scenario_ids.pop(name, None)
-        item = (
-            cache["scenario"][scenario_id]._asdict()
-            if scenario_id is not None
-            else {"name": name, "active": False, "description": None}
-        )
-        item.update(dict(zip(("active", "description"), optionals)))
-        try:
-            check_scenario(item, scenario_ids)
-        except SpineIntegrityError as e:
-            error_log.append(ImportErrorLogItem(msg=f"Could not import scenario '{name}': {e.msg}", db_type="scenario"))
-            continue
-        finally:
-            if scenario_id is not None:
-                scenario_ids[name] = scenario_id
-        checked.add(name)
-        if scenario_id is not None:
-            item["id"] = scenario_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_scenario_alternatives(db_map, data, make_cache=None):
-    """
-    Imports scenario alternatives.
-
-    Example:
-
-        data = [('scenario', 'bottom_alternative'), ('another_scenario', 'top_alternative', 'bottom_alternative')]
-        import_scenario_alternatives(db_map, data)
-
-    Args:
-        db_map (DiffDatabaseMapping): mapping for database to insert into
-        data (Iterable): an iterable of (scenario name, alternative name,
-            and optionally, 'before' alternative name).
-            Alternatives are inserted before the 'before' alternative,
-            or at the end if not given.
-
-    Returns:
-        tuple of int and list: Number of successfully inserted scenario alternatives, list of errors
-    """
-    return import_data(db_map, scenario_alternatives=data, make_cache=make_cache)
-
-
-def _get_scenario_alternatives_for_import(data, make_cache):
-    cache = make_cache({"scenario_alternative"}, include_ancestors=True)
-    scenario_alternative_id_lists = {x.id: x.alternative_id_list for x in cache.get("scenario", {}).values()}
-    scenario_alternative_ids = {
-        (x.scenario_id, x.alternative_id): x.id for x in cache.get("scenario_alternative", {}).values()
-    }
-    scenario_ids = {scenario.name: scenario.id for scenario in cache.get("scenario", {}).values()}
-    alternative_ids = {alternative.name: alternative.id for alternative in cache.get("alternative", {}).values()}
-    to_add = []
-    to_update = []
-    error_log = []
-    successors_ids_per_scenario = defaultdict(dict)
-    for scenario_name, alternative_name, *optionals in data:
-        try:
-            predecessor_id = alternative_ids[alternative_name]
-        except KeyError:
-            error_log.append(
-                ImportErrorLogItem(msg=f"Alternative '{alternative_name}' not found.", db_type="scenario alternative")
-            )
-            continue
-        successor = optionals[0] if optionals else None
-        if successor is not None:
-            try:
-                successor_id = alternative_ids[successor]
-            except KeyError:
-                error_log.append(
-                    ImportErrorLogItem(msg=f"Alternative '{successor}' not found.", db_type="scenario alternative")
-                )
-                continue
-        else:
-            successor_id = None
-        successors_ids_per_scenario[scenario_name][predecessor_id] = successor_id
-    predecessor_ids_per_scenario = {
-        scenario_name: {successor: predecessor for predecessor, successor in successor_ids.items()}
-        for scenario_name, successor_ids in successors_ids_per_scenario.items()
-    }
-    for scenario_name, predecessor_ids in predecessor_ids_per_scenario.items():
-        try:
-            scenario_id = scenario_ids[scenario_name]
-        except KeyError:
-            error_log.append(
-                ImportErrorLogItem(msg=f"Scenario '{scenario_name}' not found.", db_type="scenario alternative")
-            )
-            continue
-        orig_alt_id_list = scenario_alternative_id_lists.get(scenario_id, [])
-        successor_ids = successors_ids_per_scenario[scenario_name]
-        new_alt_id_list = [id_ for id_ in orig_alt_id_list if id_ not in successor_ids]
-        last_alternative_ids = []
-        for predecessor_id, successor_id in list(successor_ids.items()):
-            if successor_id is None:
-                if predecessor_id not in orig_alt_id_list:
-                    last_alternative_ids.append(predecessor_id)
-                del successor_ids[predecessor_id]
-        with suppress(KeyError):
-            del predecessor_ids[None]
-        new_alt_id_list += last_alternative_ids
-        for predecessor_id in predecessor_ids:
-            if predecessor_id not in successor_ids and predecessor_id not in new_alt_id_list:
-                new_alt_id_list.insert(0, predecessor_id)
-                break
-        while predecessor_ids:
-            for i, alternative_id in enumerate(new_alt_id_list):
-                if (predecessor := predecessor_ids.pop(alternative_id, None)) is not None:
-                    new_alt_id_list.insert(i, predecessor)
-                    break
-            else:
-                alternative_names = {name: id_ for id_, name in alternative_ids.items()}
-                for successor, predecessor in predecessor_ids.items():
-                    if predecessor not in new_alt_id_list:
-                        error_log.append(
-                            ImportErrorLogItem(
-                                msg=f"Before alternative '{alternative_names[successor]}' not found for '{alternative_names[predecessor]}'"
-                            )
-                        )
-                        break
-                else:
-                    raise RuntimeError("This should be unreachable.")
-                break
-        scenario_alternative_id_lists[scenario_id] = new_alt_id_list
-    for scenario_id, new_alt_id_list in scenario_alternative_id_lists.items():
-        for k, alt_id in enumerate(new_alt_id_list):
-            id_ = scenario_alternative_ids.get((scenario_id, alt_id))
-            item = {"scenario_id": scenario_id, "alternative_id": alt_id, "rank": k + 1}
-            if id_ is not None:
-                item["id"] = id_
-                to_update.append(item)
-            else:
-                to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_object_classes(db_map, data, make_cache=None):
-    """Imports object classes.
-
-    Example::
-
-            data = ['new_object_class', ('another_object_class', 'description', 123456)]
-            import_object_classes(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (Iterable): list/set/iterable of string object class names, or of lists/tuples with object class names,
-            and optionally description and integer display icon reference
-
-    Returns:
-        tuple of int and list: Number of successfully inserted object classes, list of errors
-    """
-    return import_data(db_map, object_classes=data, make_cache=make_cache)
-
-
-def _get_object_classes_for_import(db_map, data, make_cache):
-    cache = make_cache({"object_class"}, include_ancestors=True)
-    object_class_ids = {oc.name: oc.id for oc in cache.get("object_class", {}).values()}
-    checked = set()
-    to_add = []
-    to_update = []
-    error_log = []
-    for object_class in data:
-        if isinstance(object_class, str):
-            object_class = (object_class,)
-        name, *optionals = object_class
-        if name in checked:
-            continue
-        oc_id = object_class_ids.pop(name, None)
-        item = (
-            cache["object_class"][oc_id]._asdict()
-            if oc_id is not None
-            else {"name": name, "description": None, "display_icon": None}
-        )
-        item["type_id"] = db_map.object_class_type
-        item.update(dict(zip(("description", "display_icon"), optionals)))
-        try:
-            check_object_class(item, object_class_ids, db_map.object_class_type)
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(msg=f"Could not import object class '{name}': {e.msg}", db_type="object class")
-            )
-            continue
-        finally:
-            if oc_id is not None:
-                object_class_ids[name] = oc_id
-        checked.add(name)
-        if oc_id is not None:
-            item["id"] = oc_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_relationship_classes(db_map, data, make_cache=None):
-    """Imports relationship classes.
-
-    Example::
-
-            data = [
-                ('new_rel_class', ['object_class_1', 'object_class_2']),
-                ('another_rel_class', ['object_class_3', 'object_class_4'], 'description'),
-            ]
-            import_relationship_classes(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of lists/tuples with relationship class names,
-            list of object class names, and optionally description
-
-    Returns:
-        (Int, List) Number of successful inserted objects, list of errors
-    """
-    return import_data(db_map, relationship_classes=data, make_cache=make_cache)
-
-
-def _get_relationship_classes_for_import(db_map, data, make_cache):
-    cache = make_cache({"relationship_class"}, include_ancestors=True)
-    object_class_ids = {oc.name: oc.id for oc in cache.get("object_class", {}).values()}
-    relationship_class_ids = {x.name: x.id for x in cache.get("relationship_class", {}).values()}
-    checked = set()
-    error_log = []
-    to_add = []
-    to_update = []
-    for name, oc_names, *optionals in data:
-        if name in checked:
-            continue
-        rc_id = relationship_class_ids.pop(name, None)
-        item = (
-            cache["relationship_class"][rc_id]._asdict()
-            if rc_id is not None
-            else {
-                "name": name,
-                "object_class_id_list": [object_class_ids.get(oc, None) for oc in oc_names],
-                "description": None,
-                "display_icon": None,
-            }
-        )
-        item["type_id"] = db_map.relationship_class_type
-        item.update(dict(zip(("description", "display_icon"), optionals)))
-        try:
-            check_wide_relationship_class(
-                item, relationship_class_ids, set(object_class_ids.values()), db_map.relationship_class_type
-            )
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(
-                    f"Could not import relationship class '{name}' with object classes {tuple(oc_names)}: {e.msg}",
-                    db_type="relationship class",
-                )
-            )
-            continue
-        finally:
-            if rc_id is not None:
-                relationship_class_ids[name] = rc_id
-        checked.add(name)
-        if rc_id is not None:
-            item["id"] = rc_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_objects(db_map, data, make_cache=None):
-    """Imports list of object by name with associated object class name into given database mapping:
-    Ignores duplicate names and existing names.
-
-    Example::
-
-            data = [
-                ('object_class_name', 'new_object'),
-                ('object_class_name', 'other_object', 'description')
-            ]
-            import_objects(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of lists/tuples with object name and object class name
-
-    Returns:
-        (Int, List) Number of successful inserted objects, list of errors
-    """
-    return import_data(db_map, objects=data, make_cache=make_cache)
-
-
-def _get_objects_for_import(db_map, data, make_cache):
-    cache = make_cache({"object"}, include_ancestors=True)
-    object_class_ids = {oc.name: oc.id for oc in cache.get("object_class", {}).values()}
-    object_ids = {(o.class_id, o.name): o.id for o in cache.get("object", {}).values()}
-    checked = set()
-    error_log = []
-    to_add = []
-    to_update = []
-    for oc_name, name, *optionals in data:
-        oc_id = object_class_ids.get(oc_name, None)
-        if (oc_id, name) in checked:
-            continue
-        o_id = object_ids.pop((oc_id, name), None)
-        item = (
-            cache["object"][o_id]._asdict()
-            if o_id is not None
-            else {"name": name, "class_id": oc_id, "description": None}
-        )
-        item["type_id"] = db_map.object_entity_type
-        item.update(dict(zip(("description",), optionals)))
-        try:
-            check_object(item, object_ids, set(object_class_ids.values()), db_map.object_entity_type)
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(
-                    msg=f"Could not import object '{name}' with class '{oc_name}': {e.msg}", db_type="object"
-                )
-            )
-            continue
-        finally:
-            if o_id is not None:
-                object_ids[oc_id, name] = o_id
-        checked.add((oc_id, name))
-        if o_id is not None:
-            item["id"] = o_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_object_groups(db_map, data, make_cache=None):
-    """Imports list of object groups by name with associated object class name into given database mapping:
-    Ignores duplicate and existing (group, member) tuples.
-
-    Example::
-
-            data = [
-                ('object_class_name', 'object_group_name', 'member_name'),
-                ('object_class_name', 'object_group_name', 'another_member_name')
-            ]
-            import_object_groups(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of lists/tuples with object class name, group name,
-            and member name
-
-    Returns:
-        (Int, List) Number of successful inserted objects, list of errors
-    """
-    return import_data(db_map, object_groups=data, make_cache=make_cache)
-
-
-def _get_object_groups_for_import(db_map, data, make_cache):
-    cache = make_cache({"entity_group"}, include_ancestors=True)
-    object_class_ids = {oc.name: oc.id for oc in cache.get("object_class", {}).values()}
-    object_ids = {(o.class_id, o.name): o.id for o in cache.get("object", {}).values()}
-    objects = {}
-    for obj in cache.get("object", {}).values():
-        objects.setdefault(obj.class_id, dict())[obj.id] = obj._asdict()
-    entity_group_ids = {(x.group_id, x.member_id): x.id for x in cache.get("entity_group", {}).values()}
-    error_log = []
-    to_add = []
-    seen = set()
-    for class_name, group_name, member_name in data:
-        oc_id = object_class_ids.get(class_name)
-        g_id = object_ids.get((oc_id, group_name))
-        m_id = object_ids.get((oc_id, member_name))
-        if (g_id, m_id) in seen | entity_group_ids.keys():
-            continue
-        item = {"entity_class_id": oc_id, "entity_id": g_id, "member_id": m_id}
-        try:
-            check_entity_group(item, entity_group_ids, objects)
-            to_add.append(item)
-            seen.add((g_id, m_id))
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(
-                    msg=f"Could not import object '{member_name}' into group '{group_name}': {e.msg}",
-                    db_type="entity group",
-                )
-            )
-    return to_add, [], error_log
-
-
-def import_relationships(db_map, data, make_cache=None):
-    """Imports relationships.
-
-    Example::
-
-            data = [('relationship_class_name', ('object_name1', 'object_name2'))]
-            import_relationships(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of lists/tuples with relationship class name
-            and list/tuple of object names
-
-    Returns:
-        (Int, List) Number of successful inserted objects, list of errors
-    """
-    return import_data(db_map, relationships=data, make_cache=make_cache)
-
-
-def _make_unique_relationship_name(class_id, class_name, object_names, class_id_name_tuples):
-    base_name = class_name + "_" + "__".join([obj if obj is not None else "None" for obj in object_names])
-    name = base_name
-    while (class_id, name) in class_id_name_tuples:
-        name = base_name + uuid.uuid4().hex
-    return name
-
-
-def _get_relationships_for_import(db_map, data, make_cache):
-    cache = make_cache({"relationship"}, include_ancestors=True)
-    relationships = {x.name: x for x in cache.get("relationship", {}).values()}
-    relationship_ids_per_name = {(x.class_id, x.name): x.id for x in relationships.values()}
-    relationship_ids_per_obj_lst = {(x.class_id, x.object_id_list): x.id for x in relationships.values()}
-    relationship_classes = {
-        x.id: {"object_class_id_list": x.object_class_id_list, "name": x.name}
-        for x in cache.get("relationship_class", {}).values()
-    }
-    objects = {x.id: {"class_id": x.class_id, "name": x.name} for x in cache.get("object", {}).values()}
-    object_ids = {(o["name"], o["class_id"]): o_id for o_id, o in objects.items()}
-    relationship_class_ids = {rc["name"]: rc_id for rc_id, rc in relationship_classes.items()}
-    object_class_id_lists = {rc_id: rc["object_class_id_list"] for rc_id, rc in relationship_classes.items()}
-    error_log = []
-    to_add = []
-    to_update = []
-    checked = set()
-    for class_name, object_names, *optionals in data:
-        rc_id = relationship_class_ids.get(class_name, None)
-        oc_ids = object_class_id_lists.get(rc_id, [])
-        o_ids = tuple(object_ids.get((name, oc_id), None) for name, oc_id in zip(object_names, oc_ids))
-        if (rc_id, o_ids) in checked:
-            continue
-        r_id = relationship_ids_per_obj_lst.pop((rc_id, o_ids), None)
-        if r_id is not None:
-            r_name = cache["relationship"][r_id].name
-            relationship_ids_per_name.pop((rc_id, r_name))
-        item = (
-            cache["relationship"][r_id]._asdict()
-            if r_id is not None
-            else {
-                "name": _make_unique_relationship_name(rc_id, class_name, object_names, relationship_ids_per_name),
-                "class_id": rc_id,
-                "object_id_list": list(o_ids),
-                "object_class_id_list": oc_ids,
-                "type_id": db_map.relationship_entity_type,
-            }
-        )
-        item.update(dict(zip(("description",), optionals)))
-        try:
-            check_wide_relationship(
-                item,
-                relationship_ids_per_name,
-                relationship_ids_per_obj_lst,
-                relationship_classes,
-                objects,
-                db_map.relationship_entity_type,
-            )
-        except SpineIntegrityError as e:
-            msg = f"Could not import relationship with objects {tuple(object_names)} into '{class_name}': {e.msg}"
-            error_log.append(ImportErrorLogItem(msg=msg, db_type="relationship"))
-            continue
-        finally:
-            if r_id is not None:
-                relationship_ids_per_obj_lst[rc_id, o_ids] = r_id
-                relationship_ids_per_name[rc_id, r_name] = r_id
-        checked.add((rc_id, o_ids))
-        if r_id is not None:
-            item["id"] = r_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_object_parameters(db_map, data, make_cache=None, unparse_value=to_database):
-    """Imports list of object class parameters:
-
-    Example::
-
-            data = [
-                ('object_class_1', 'new_parameter'),
-                ('object_class_2', 'other_parameter', 'default_value', 'value_list_name', 'description')
-            ]
-            import_object_parameters(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of lists/tuples with object class name, parameter name,
-            and optionally default value, value list name, and description
-
-    Returns:
-        (Int, List) Number of successful inserted objects, list of errors
-    """
-    return import_data(db_map, object_parameters=data, make_cache=make_cache, unparse_value=unparse_value)
-
-
-def _get_object_parameters_for_import(db_map, data, make_cache, unparse_value):
-    cache = make_cache({"parameter_definition"}, include_ancestors=True)
-    parameter_ids = {
-        (x.entity_class_id, x.parameter_name): x.id for x in cache.get("parameter_definition", {}).values()
-    }
-    object_class_names = {x.id: x.name for x in cache.get("object_class", {}).values()}
-    object_class_ids = {oc_name: oc_id for oc_id, oc_name in object_class_names.items()}
-    parameter_value_lists = {}
-    parameter_value_list_ids = {}
-    for x in cache.get("parameter_value_list", {}).values():
-        parameter_value_lists[x.id] = x.value_id_list
-        parameter_value_list_ids[x.name] = x.id
-    list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
-    checked = set()
-    error_log = []
-    to_add = []
-    to_update = []
-    functions = [unparse_value, lambda x: (parameter_value_list_ids.get(x),), lambda x: (x,)]
-    for class_name, parameter_name, *optionals in data:
-        oc_id = object_class_ids.get(class_name, None)
-        checked_key = (oc_id, parameter_name)
-        if checked_key in checked:
-            continue
-        p_id = parameter_ids.pop((oc_id, parameter_name), None)
-        item = (
-            cache["parameter_definition"][p_id]._asdict()
-            if p_id is not None
-            else {
-                "name": parameter_name,
-                "entity_class_id": oc_id,
-                "object_class_id": oc_id,
-                "default_value": None,
-                "default_type": None,
-                "parameter_value_list_id": None,
-                "description": None,
-            }
-        )
-        optionals = [y for f, x in zip(functions, optionals) for y in f(x)]
-        item.update(dict(zip(("default_value", "default_type", "parameter_value_list_id", "description"), optionals)))
-        try:
-            check_parameter_definition(
-                item, parameter_ids, object_class_names.keys(), parameter_value_lists, list_values
-            )
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(
-                    f"Could not import parameter '{parameter_name}' with class '{class_name}': {e.msg}",
-                    db_type="parameter definition",
-                )
-            )
-            continue
-        finally:
-            if p_id is not None:
-                parameter_ids[oc_id, parameter_name] = p_id
-        checked.add(checked_key)
-        if p_id is not None:
-            item["id"] = p_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_relationship_parameters(db_map, data, make_cache=None, unparse_value=to_database):
-    """Imports list of relationship class parameters:
-
-    Example::
-
-            data = [
-                ('relationship_class_1', 'new_parameter'),
-                ('relationship_class_2', 'other_parameter', 'default_value', 'value_list_name', 'description')
-            ]
-            import_relationship_parameters(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of lists/tuples with relationship class name, parameter name,
-            and optionally default value, value list name, and description
-
-    Returns:
-        (Int, List) Number of successful inserted objects, list of errors
-    """
-    return import_data(db_map, relationship_parameters=data, make_cache=make_cache, unparse_value=unparse_value)
-
-
-def _get_relationship_parameters_for_import(db_map, data, make_cache, unparse_value):
-    cache = make_cache({"parameter_definition"}, include_ancestors=True)
-    parameter_ids = {
-        (x.entity_class_id, x.parameter_name): x.id for x in cache.get("parameter_definition", {}).values()
-    }
-    relationship_class_names = {x.id: x.name for x in cache.get("relationship_class", {}).values()}
-    relationship_class_ids = {rc_name: rc_id for rc_id, rc_name in relationship_class_names.items()}
-    parameter_value_lists = {}
-    parameter_value_list_ids = {}
-    for x in cache.get("parameter_value_list", {}).values():
-        parameter_value_lists[x.id] = x.value_id_list
-        parameter_value_list_ids[x.name] = x.id
-    list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
-    error_log = []
-    to_add = []
-    to_update = []
-    checked = set()
-    functions = [unparse_value, lambda x: (parameter_value_list_ids.get(x),), lambda x: (x,)]
-    for class_name, parameter_name, *optionals in data:
-        rc_id = relationship_class_ids.get(class_name, None)
-        checked_key = (rc_id, parameter_name)
-        if checked_key in checked:
-            continue
-        p_id = parameter_ids.pop((rc_id, parameter_name), None)
-        item = (
-            cache["parameter_definition"][p_id]._asdict()
-            if p_id is not None
-            else {
-                "name": parameter_name,
-                "entity_class_id": rc_id,
-                "relationship_class_id": rc_id,
-                "default_value": None,
-                "default_type": None,
-                "parameter_value_list_id": None,
-                "description": None,
-            }
-        )
-        optionals = [y for f, x in zip(functions, optionals) for y in f(x)]
-        item.update(dict(zip(("default_value", "default_type", "parameter_value_list_id", "description"), optionals)))
-        try:
-            check_parameter_definition(
-                item, parameter_ids, relationship_class_names.keys(), parameter_value_lists, list_values
-            )
-        except SpineIntegrityError as e:
-            # Relationship class doesn't exists
-            error_log.append(
-                ImportErrorLogItem(
-                    msg=f"Could not import parameter '{parameter_name}' with class '{class_name}': {e.msg}",
-                    db_type="parameter definition",
-                )
-            )
-            continue
-        finally:
-            if p_id is not None:
-                parameter_ids[rc_id, parameter_name] = p_id
-        checked.add(checked_key)
-        if p_id is not None:
-            item["id"] = p_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_object_parameter_values(db_map, data, make_cache=None, unparse_value=to_database, on_conflict="merge"):
-    """Imports object parameter values:
-
-    Example::
-
-            data = [('object_class_name', 'object_name', 'parameter_name', 123.4),
-                    ('object_class_name', 'object_name', 'parameter_name2', <TimeSeries>),
-                    ('object_class_name', 'object_name', 'parameter_name', <TimeSeries>, 'alternative')]
-            import_object_parameter_values(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of lists/tuples with
-            object_class_name, object name, parameter name, (deserialized) parameter value,
-            optional name of an alternative
-
-    Returns:
-        (Int, List) Number of successful inserted objects, list of errors
-    """
-    return import_data(
-        db_map,
-        object_parameter_values=data,
-        make_cache=make_cache,
-        unparse_value=unparse_value,
-        on_conflict=on_conflict,
-    )
-
-
-def _get_object_parameter_values_for_import(db_map, data, make_cache, unparse_value, on_conflict):
-    cache = make_cache({"parameter_value"}, include_ancestors=True)
-    object_class_ids = {x.name: x.id for x in cache.get("object_class", {}).values()}
-    parameter_value_ids = {
-        (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
-    }
-    parameters = {
-        x.id: {
-            "name": x.parameter_name,
-            "entity_class_id": x.entity_class_id,
-            "parameter_value_list_id": x.value_list_id,
-        }
-        for x in cache.get("parameter_definition", {}).values()
-    }
-    objects = {x.id: {"class_id": x.class_id, "name": x.name} for x in cache.get("object", {}).values()}
-    parameter_value_lists = {x.id: x.value_id_list for x in cache.get("parameter_value_list", {}).values()}
-    list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
-    object_ids = {(o["name"], o["class_id"]): o_id for o_id, o in objects.items()}
-    parameter_ids = {(p["name"], p["entity_class_id"]): p_id for p_id, p in parameters.items()}
-    alternatives = {a.name: a.id for a in cache.get("alternative", {}).values()}
-    alternative_ids = set(alternatives.values())
-    error_log = []
-    to_add = []
-    to_update = []
-    checked = set()
-    for class_name, object_name, parameter_name, value, *optionals in data:
-        oc_id = object_class_ids.get(class_name, None)
-        o_id = object_ids.get((object_name, oc_id), None)
-        p_id = parameter_ids.get((parameter_name, oc_id), None)
-        if optionals:
-            alternative_name = optionals[0]
-            alt_id = alternatives.get(alternative_name)
-            if not alt_id:
-                error_log.append(
-                    ImportErrorLogItem(
-                        msg=(
-                            "Could not import parameter value for "
-                            f"'{object_name}', class '{class_name}', parameter '{parameter_name}': "
-                            f"alternative '{alternative_name}' does not exist."
-                        ),
-                        db_type="parameter value",
-                    )
-                )
-                continue
-        else:
-            alt_id, alternative_name = db_map.get_import_alternative(cache=cache)
-            alternative_ids.add(alt_id)
-        checked_key = (o_id, p_id, alt_id)
-        if checked_key in checked:
-            msg = (
-                f"Could not import parameter value for '{object_name}', class '{class_name}', "
-                f"parameter '{parameter_name}', alternative {alternative_name}: "
-                "Duplicate parameter value, only first value will be considered."
-            )
-            error_log.append(ImportErrorLogItem(msg=msg, db_type="parameter value"))
-            continue
-        pv_id = parameter_value_ids.pop((o_id, p_id, alt_id), None)
-        value, type_ = unparse_value(value)
-        if pv_id is not None:
-            current_pv = cache["parameter_value"][pv_id]
-            value, type_ = fix_conflict((value, type_), (current_pv.value, current_pv.type), on_conflict)
-        item = {
-            "parameter_definition_id": p_id,
-            "entity_class_id": oc_id,
-            "entity_id": o_id,
-            "object_class_id": oc_id,
-            "object_id": o_id,
-            "value": value,
-            "type": type_,
-            "alternative_id": alt_id,
-        }
-        try:
-            check_parameter_value(
-                item, parameter_value_ids, parameters, objects, parameter_value_lists, list_values, alternative_ids
-            )
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(
-                    msg="Could not import parameter value for '{0}', class '{1}', parameter '{2}': {3}".format(
-                        object_name, class_name, parameter_name, e.msg
-                    ),
-                    db_type="parameter value",
-                )
-            )
-            continue
-        finally:
-            if pv_id is not None:
-                parameter_value_ids[o_id, p_id, alt_id] = pv_id
-        checked.add(checked_key)
-        if pv_id is not None:
-            item["id"] = pv_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_relationship_parameter_values(db_map, data, make_cache=None, unparse_value=to_database, on_conflict="merge"):
-    """Imports relationship parameter values:
-
-    Example::
-
-            data = [['example_rel_class',
-                ['example_object', 'other_object'], 'rel_parameter', 2.718],
-                ['example_object', 'other_object'], 'rel_parameter', 5.5, 'alternative']]
-            import_relationship_parameter_values(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of lists/tuples with
-            relationship class name, list of object names, parameter name, (deserialized) parameter value,
-            optional name of an alternative
-
-    Returns:
-        (Int, List) Number of successful inserted objects, list of errors
-    """
-    return import_data(
-        db_map,
-        relationship_parameter_values=data,
-        make_cache=make_cache,
-        unparse_value=unparse_value,
-        on_conflict=on_conflict,
-    )
-
-
-def _get_relationship_parameter_values_for_import(db_map, data, make_cache, unparse_value, on_conflict):
-    cache = make_cache({"parameter_value"}, include_ancestors=True)
-    object_class_id_lists = {x.id: x.object_class_id_list for x in cache.get("relationship_class", {}).values()}
-    parameter_value_ids = {
-        (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
-    }
-    parameters = {
-        x.id: {
-            "name": x.parameter_name,
-            "entity_class_id": x.entity_class_id,
-            "parameter_value_list_id": x.value_list_id,
-        }
-        for x in cache.get("parameter_definition", {}).values()
-    }
-    relationships = {
-        x.id: {"class_id": x.class_id, "name": x.name, "object_id_list": x.object_id_list}
-        for x in cache.get("relationship", {}).values()
-    }
-    parameter_value_lists = {x.id: x.value_id_list for x in cache.get("parameter_value_list", {}).values()}
-    list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
-    parameter_ids = {(p["entity_class_id"], p["name"]): p_id for p_id, p in parameters.items()}
-    relationship_ids = {(r["class_id"], tuple(r["object_id_list"])): r_id for r_id, r in relationships.items()}
-    object_ids = {(o.name, o.class_id): o.id for o in cache.get("object", {}).values()}
-    relationship_class_ids = {oc.name: oc.id for oc in cache.get("relationship_class", {}).values()}
-    alternatives = {a.name: a.id for a in cache.get("alternative", {}).values()}
-    alternative_ids = set(alternatives.values())
-    error_log = []
-    to_add = []
-    to_update = []
-    checked = set()
-    for class_name, object_names, parameter_name, value, *optionals in data:
-        rc_id = relationship_class_ids.get(class_name, None)
-        oc_ids = object_class_id_lists.get(rc_id, [])
-        if len(object_names) == len(oc_ids):
-            o_ids = tuple(object_ids.get((name, oc_id), None) for name, oc_id in zip(object_names, oc_ids))
-        else:
-            o_ids = tuple(None for _ in object_names)
-        r_id = relationship_ids.get((rc_id, o_ids), None)
-        p_id = parameter_ids.get((rc_id, parameter_name), None)
-        if optionals:
-            alternative_name = optionals[0]
-            alt_id = alternatives.get(alternative_name)
-            if not alt_id:
-                error_log.append(
-                    ImportErrorLogItem(
-                        msg=(
-                            "Could not import parameter value for "
-                            f"'{object_names}', class '{class_name}', parameter '{parameter_name}': "
-                            f"alternative {alternative_name} does not exist."
-                        ),
-                        db_type="parameter value",
-                    )
-                )
-                continue
-        else:
-            alt_id, alternative_name = db_map.get_import_alternative(cache=cache)
-            alternative_ids.add(alt_id)
-        checked_key = (r_id, p_id, alt_id)
-        if checked_key in checked:
-            msg = (
-                f"Could not import parameter value for '{object_names}', class '{class_name}', "
-                f"parameter '{parameter_name}', alternative {alternative_name}: "
-                "Duplicate parameter value, only first value will be considered."
-            )
-            error_log.append(ImportErrorLogItem(msg=msg, db_type="parameter value"))
-            continue
-        pv_id = parameter_value_ids.pop((r_id, p_id, alt_id), None)
-        value, type_ = unparse_value(value)
-        if pv_id is not None:
-            current_pv = cache["parameter_value"][pv_id]
-            value, type_ = fix_conflict((value, type_), (current_pv.value, current_pv.type), on_conflict)
-        item = {
-            "parameter_definition_id": p_id,
-            "entity_class_id": rc_id,
-            "entity_id": r_id,
-            "relationship_class_id": rc_id,
-            "relationship_id": r_id,
-            "value": value,
-            "type": type_,
-            "alternative_id": alt_id,
-        }
-        try:
-            check_parameter_value(
-                item,
-                parameter_value_ids,
-                parameters,
-                relationships,
-                parameter_value_lists,
-                list_values,
-                alternative_ids,
-            )
-        except SpineIntegrityError as e:
-            error_log.append(
-                ImportErrorLogItem(
-                    msg="Could not import parameter value for '{0}', class '{1}', parameter '{2}': {3}".format(
-                        object_names, class_name, parameter_name, e.msg
-                    ),
-                    db_type="parameter value",
-                )
-            )
-            continue
-        finally:
-            if pv_id is not None:
-                parameter_value_ids[r_id, p_id, alt_id] = pv_id
-        checked.add(checked_key)
-        if pv_id is not None:
-            item["id"] = pv_id
-            to_update.append(item)
-        else:
-            to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_parameter_value_lists(db_map, data, make_cache=None, unparse_value=to_database):
-    """Imports list of parameter value lists:
-
-    Example::
-
-            data = [
-                ['value_list_name', value1], ['value_list_name', value2],
-                ['another_value_list_name', value3],
-            ]
-            import_parameter_value_lists(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of lists/tuples with
-                                 value list name, list of values
-
-    Returns:
-        (Int, List) Number of successful inserted objects, list of errors
-    """
-    return import_data(db_map, parameter_value_lists=data, make_cache=make_cache, unparse_value=unparse_value)
-
-
-def _get_parameter_value_lists_for_import(db_map, data, make_cache):
-    cache = make_cache({"parameter_value_list"}, include_ancestors=True)
-    parameter_value_list_ids = {x.name: x.id for x in cache.get("parameter_value_list", {}).values()}
-    error_log = []
-    to_add = []
-    for name in list({x[0]: None for x in data}):
-        item = {"name": name}
-        try:
-            check_parameter_value_list(item, parameter_value_list_ids)
-        except SpineIntegrityError:
-            continue
-        to_add.append(item)
-    return to_add, [], error_log
-
-
-def _get_list_values_for_import(db_map, data, make_cache, unparse_value):
-    cache = make_cache({"list_value"}, include_ancestors=True)
-    value_lists_by_name = {x.name: (x.id, x.value_index_list) for x in cache.get("parameter_value_list", {}).values()}
-    list_value_ids_by_index = {(x.parameter_value_list_id, x.index): x.id for x in cache.get("list_value", {}).values()}
-    list_value_ids_by_value = {
-        (x.parameter_value_list_id, x.type, x.value): x.id for x in cache.get("list_value", {}).values()
-    }
-    list_names_by_id = {x.id: x.name for x in cache.get("parameter_value_list", {}).values()}
-    error_log = []
-    to_add = []
-    to_update = []
-    seen_values = set()
-    max_indexes = dict()
-    for list_name, value in data:
-        try:
-            list_id, value_index_list = value_lists_by_name.get(list_name)
-        except TypeError:
-            # cannot unpack non-iterable NoneType object
-            error_log.append(
-                ImportErrorLogItem(
-                    msg=f"Could not import value for list '{list_name}': list not found", db_type="list value"
-                )
-            )
-            continue
-        val, type_ = unparse_value(value)
-        if (list_id, type_, val) in seen_values:
-            error_log.append(
-                ImportErrorLogItem(
-                    msg=f"Could not import value for list '{list_name}': "
-                    "Duplicate value, only first will be considered",
-                    db_type="list value",
-                )
-            )
-            continue
-        max_index = max_indexes.get(list_id)
-        if max_index is not None:
-            index = max_index + 1
-        elif not value_index_list:
-            index = 0
-        else:
-            index = max(value_index_list) + 1
-        item = {"parameter_value_list_id": list_id, "value": val, "type": type_, "index": index}
-        try:
-            check_list_value(item, list_names_by_id, list_value_ids_by_index, list_value_ids_by_value)
-        except SpineIntegrityError as e:
-            if e.id is None:
-                error_log.append(
-                    ImportErrorLogItem(
-                        msg=f"Could not import value '{value}' for list '{list_name}': {e.msg}", db_type="list value"
-                    )
-                )
-            continue
-        max_indexes[list_id] = index
-        seen_values.add((list_id, type_, val))
-        to_add.append(item)
-    return to_add, to_update, error_log
-
-
-def import_metadata(db_map, data, make_cache=None):
-    """Imports metadata. Ignores duplicates.
-
-    Example::
-
-            data = ['{"name1": "value1"}', '{"name2": "value2"}']
-            import_metadata(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of string metadata entries in JSON format
-
-    Returns:
-        (Int, List) Number of successful inserted objects, list of errors
-    """
-    return import_data(db_map, metadata=data, make_cache=make_cache)
-
-
-def _get_metadata_for_import(db_map, data, make_cache):
-    cache = make_cache({"metadata"}, include_ancestors=True)
-    seen = {(x.name, x.value) for x in cache.get("metadata", {}).values()}
-    to_add = []
-    for metadata in data:
-        for name, value in _parse_metadata(metadata):
-            if (name, value) in seen:
-                continue
-            item = {"name": name, "value": value}
-            seen.add((name, value))
-            to_add.append(item)
-    return to_add, [], []
-
-
-def import_object_metadata(db_map, data, make_cache=None):
-    """Imports object metadata. Ignores duplicates.
-
-    Example::
-
-            data = [("classA", "object1", '{"name1": "value1"}'), ("classA", "object1", '{"name2": "value2"}')]
-            import_object_metadata(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of tuples with class name, object name,
-            and string metadata entries in JSON format
-
-    Returns:
-        (Int, List) Number of successful inserted items, list of errors
-    """
-    return import_data(db_map, object_metadata=data, make_cache=make_cache)
-
-
-def _get_object_metadata_for_import(db_map, data, make_cache):
-    cache = make_cache({"object", "entity_metadata"}, include_ancestors=True)
-    object_class_ids = {x.name: x.id for x in cache.get("object_class", {}).values()}
-    metadata_ids = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
-    object_ids = {(x.name, x.class_id): x.id for x in cache.get("object", {}).values()}
-    seen = {(x.entity_id, x.metadata_id) for x in cache.get("entity_metadata", {}).values()}
-    error_log = []
-    to_add = []
-    for class_name, object_name, metadata in data:
-        oc_id = object_class_ids.get(class_name, None)
-        o_id = object_ids.get((object_name, oc_id), None)
-        if o_id is None:
-            error_log.append(
-                ImportErrorLogItem(
-                    msg=f"Could not import object metadata: unknown object '{object_name}' of class '{class_name}'",
-                    db_type="object metadata",
-                )
-            )
-            continue
-        for name, value in _parse_metadata(metadata):
-            m_id = metadata_ids.get((name, value), None)
-            if m_id is None:
-                error_log.append(
-                    ImportErrorLogItem(
-                        msg=f"Could not import object metadata: unknown metadata '{name}': '{value}'",
-                        db_type="object metadata",
-                    )
-                )
-                continue
-            unique_key = (o_id, m_id)
-            if unique_key in seen:
-                continue
-            item = {"entity_id": o_id, "metadata_id": m_id}
-            seen.add(unique_key)
-            to_add.append(item)
-    return to_add, [], error_log
-
-
-def import_relationship_metadata(db_map, data, make_cache=None):
-    """Imports relationship metadata. Ignores duplicates.
-
-    Example::
-
-            data = [
-                ("classA", ("object1", "object2"), '{"name1": "value1"}'),
-                ("classA", ("object3", "object4"), '{"name2": "value2"}')
-            ]
-            import_relationship_metadata(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of tuples with class name, tuple of object names,
-            and string metadata entries in JSON format
-
-    Returns:
-        (Int, List) Number of successful inserted items, list of errors
-    """
-    return import_data(db_map, relationship_metadata=data, make_cache=make_cache)
-
-
-def _get_relationship_metadata_for_import(db_map, data, make_cache):
-    cache = make_cache({"relationship", "entity_metadata"}, include_ancestors=True)
-    relationship_class_ids = {oc.name: oc.id for oc in cache.get("relationship_class", {}).values()}
-    object_class_id_lists = {x.id: x.object_class_id_list for x in cache.get("relationship_class", {}).values()}
-    metadata_ids = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
-    object_ids = {(x.name, x.class_id): x.id for x in cache.get("object", {}).values()}
-    relationship_ids = {(x.class_id, x.object_id_list): x.id for x in cache.get("relationship", {}).values()}
-    seen = {(x.entity_id, x.metadata_id) for x in cache.get("entity_metadata", {}).values()}
-    error_log = []
-    to_add = []
-    for class_name, object_names, metadata in data:
-        rc_id = relationship_class_ids.get(class_name, None)
-        oc_ids = object_class_id_lists.get(rc_id, [])
-        o_ids = tuple(object_ids.get((name, oc_id), None) for name, oc_id in zip(object_names, oc_ids))
-        r_id = relationship_ids.get((rc_id, o_ids), None)
-        if r_id is None:
-            error_log.append(
-                ImportErrorLogItem(
-                    msg="Could not import relationship metadata: unknown relationship '{0}' of class '{1}'".format(
-                        object_names, class_name
-                    ),
-                    db_type="relationship metadata",
-                )
-            )
-            continue
-        for name, value in _parse_metadata(metadata):
-            m_id = metadata_ids.get((name, value), None)
-            if m_id is None:
-                error_log.append(
-                    ImportErrorLogItem(
-                        msg=f"Could not import relationship metadata: unknown metadata '{name}': '{value}'",
-                        db_type="relationship metadata",
-                    )
-                )
-                continue
-            unique_key = (r_id, m_id)
-            if unique_key in seen:
-                continue
-            item = {"entity_id": r_id, "metadata_id": m_id}
-            seen.add(unique_key)
-            to_add.append(item)
-    return to_add, [], error_log
-
-
-def import_object_parameter_value_metadata(db_map, data, make_cache=None):
-    """Imports object parameter value metadata. Ignores duplicates.
-
-    Example::
-
-            data = [
-                ("classA", "object1", "parameterX", '{"name1": "value1"}'),
-                ("classA", "object1", "parameterY", '{"name2": "value2"}', "alternativeA")
-            ]
-            import_object_parameter_value_metadata(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of tuples with class name, object name,
-            parameter name, string metadata entries in JSON format, and optionally alternative name
-
-    Returns:
-        (Int, List) Number of successful inserted items, list of errors
-    """
-    return import_data(db_map, object_parameter_value_metadata=data, make_cache=make_cache)
-
-
-def _get_object_parameter_value_metadata_for_import(db_map, data, make_cache):
-    cache = make_cache({"parameter_value", "parameter_value_metadata"}, include_ancestors=True)
-    object_class_ids = {x.name: x.id for x in cache.get("object_class", {}).values()}
-    object_ids = {(x.name, x.class_id): x.id for x in cache.get("object", {}).values()}
-    parameter_ids = {
-        (x.parameter_name, x.entity_class_id): x.id for x in cache.get("parameter_definition", {}).values()
-    }
-    alternative_ids = {a.name: a.id for a in cache.get("alternative", {}).values()}
-    parameter_value_ids = {
-        (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
-    }
-    metadata_ids = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
-    seen = {(x.parameter_value_id, x.metadata_id) for x in cache.get("parameter_value_metadata", {}).values()}
-    error_log = []
-    to_add = []
-    for class_name, object_name, parameter_name, metadata, *optionals in data:
-        oc_id = object_class_ids.get(class_name, None)
-        o_id = object_ids.get((object_name, oc_id), None)
-        p_id = parameter_ids.get((parameter_name, oc_id), None)
-        if optionals:
-            alternative_name = optionals[0]
-            alt_id = alternative_ids.get(alternative_name, None)
-        else:
-            alt_id, alternative_name = db_map.get_import_alternative(cache=cache)
-        pv_id = parameter_value_ids.get((o_id, p_id, alt_id), None)
-        if pv_id is None:
-            msg = (
-                "Could not import object parameter value metadata: "
-                "parameter {0} doesn't have a value for object {1}, alternative {2}".format(
-                    parameter_name, object_name, alternative_name
-                )
-            )
-            error_log.append(ImportErrorLogItem(msg=msg, db_type="object parameter value metadata"))
-            continue
-        for name, value in _parse_metadata(metadata):
-            m_id = metadata_ids.get((name, value), None)
-            if m_id is None:
-                error_log.append(
-                    ImportErrorLogItem(
-                        msg=f"Could not import object parameter value metadata: unknown metadata '{name}': '{value}'",
-                        db_type="object parameter value metadata",
-                    )
-                )
-                continue
-            unique_key = (pv_id, m_id)
-            if unique_key in seen:
-                continue
-            item = {"parameter_value_id": pv_id, "metadata_id": m_id}
-            seen.add(unique_key)
-            to_add.append(item)
-    return to_add, [], error_log
-
-
-def import_relationship_parameter_value_metadata(db_map, data, make_cache=None):
-    """Imports relationship parameter value metadata. Ignores duplicates.
-
-    Example::
-
-            data = [
-                ("classA", ("object1", "object2"), "parameterX", '{"name1": "value1"}'),
-                ("classA", ("object3", "object4"), "parameterY", '{"name2": "value2"}', "alternativeA")
-            ]
-            import_object_parameter_value_metadata(db_map, data)
-
-    Args:
-        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
-        data (List[List/Tuple]): list/set/iterable of tuples with class name, tuple of object names,
-            parameter name, string metadata entries in JSON format, and optionally alternative name
-
-    Returns:
-        (Int, List) Number of successful inserted items, list of errors
-    """
-    return import_data(db_map, relationship_parameter_value_metadata=data, make_cache=make_cache)
-
-
-def _get_relationship_parameter_value_metadata_for_import(db_map, data, make_cache):
-    cache = make_cache({"parameter_value", "parameter_value_metadata"}, include_ancestors=True)
-    relationship_class_ids = {oc.name: oc.id for oc in cache.get("relationship_class", {}).values()}
-    object_class_id_lists = {x.id: x.object_class_id_list for x in cache.get("relationship_class", {}).values()}
-    object_ids = {(x.name, x.class_id): x.id for x in cache.get("object", {}).values()}
-    relationship_ids = {(x.object_id_list, x.class_id): x.id for x in cache.get("relationship", {}).values()}
-    parameter_ids = {
-        (x.parameter_name, x.entity_class_id): x.id for x in cache.get("parameter_definition", {}).values()
-    }
-    alternative_ids = {a.name: a.id for a in cache.get("alternative", {}).values()}
-    parameter_value_ids = {
-        (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
-    }
-    metadata_ids = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
-    seen = {(x.parameter_value_id, x.metadata_id) for x in cache.get("parameter_value_metadata", {}).values()}
-    error_log = []
-    to_add = []
-    for class_name, object_names, parameter_name, metadata, *optionals in data:
-        rc_id = relationship_class_ids.get(class_name, None)
-        oc_ids = object_class_id_lists.get(rc_id, [])
-        o_ids = tuple(object_ids.get((name, oc_id), None) for name, oc_id in zip(object_names, oc_ids))
-        r_id = relationship_ids.get((o_ids, rc_id), None)
-        p_id = parameter_ids.get((parameter_name, rc_id), None)
-        if optionals:
-            alternative_name = optionals[0]
-            alt_id = alternative_ids.get(alternative_name, None)
-        else:
-            alt_id, alternative_name = db_map.get_import_alternative(cache=cache)
-        pv_id = parameter_value_ids.get((r_id, p_id, alt_id), None)
-        if pv_id is None:
-            msg = (
-                "Could not import relationship parameter value metadata: "
-                "parameter '{0}' doesn't have a value for relationship '{1}', alternative '{2}'".format(
-                    parameter_name, object_names, alternative_name
-                )
-            )
-            error_log.append(ImportErrorLogItem(msg=msg, db_type="relationship parameter value metadata"))
-            continue
-        for name, value in _parse_metadata(metadata):
-            m_id = metadata_ids.get((name, value), None)
-            if m_id is None:
-                msg = f"Could not import relationship parameter value metadata: unknown metadata '{name}': '{value}'"
-                error_log.append(ImportErrorLogItem(msg=msg, db_type="relationship parameter value metadata"))
-                continue
-            unique_key = (pv_id, m_id)
-            if unique_key in seen:
-                continue
-            item = {"parameter_value_id": pv_id, "metadata_id": m_id}
-            seen.add(unique_key)
-            to_add.append(item)
-    return to_add, [], error_log
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Functions for importing data into a Spine database using entity names as references.
+
+"""
+
+import uuid
+from collections import defaultdict
+from contextlib import suppress
+
+from .exception import SpineIntegrityError, SpineDBAPIError
+from .check_functions import (
+    check_tool,
+    check_feature,
+    check_tool_feature,
+    check_tool_feature_method,
+    check_alternative,
+    check_object_class,
+    check_object,
+    check_wide_relationship_class,
+    check_wide_relationship,
+    check_entity_group,
+    check_parameter_definition,
+    check_parameter_value,
+    check_scenario,
+    check_parameter_value_list,
+    check_list_value,
+)
+from .parameter_value import to_database, from_database, fix_conflict
+from .helpers import _parse_metadata
+
+# TODO: update docstrings
+
+
+class ImportErrorLogItem:
+    """Class to hold log data for import errors"""
+
+    def __init__(self, msg="", db_type="", imported_from="", other=""):
+        self.msg = msg
+        self.db_type = db_type
+        self.imported_from = imported_from
+        self.other = other
+
+    def __repr__(self):
+        return self.msg
+
+
+def import_data(db_map, make_cache=None, unparse_value=to_database, on_conflict="merge", **kwargs):
+    """Imports data into a Spine database using name references (rather than id references).
+
+    Example::
+
+            object_c = ['example_class', 'other_class']
+            obj_parameters = [['example_class', 'example_parameter']]
+            relationship_c = [['example_rel_class', ['example_class', 'other_class']]]
+            rel_parameters = [['example_rel_class', 'rel_parameter']]
+            objects = [['example_class', 'example_object'],
+                       ['other_class', 'other_object']]
+            object_p_values = [['example_object_class', 'example_object', 'example_parameter', 3.14]]
+            relationships = [['example_rel_class', ['example_object', 'other_object']]]
+            rel_p_values = [['example_rel_class', ['example_object', 'other_object'], 'rel_parameter', 2.718]]
+            object_groups = [['object_class_name', 'object_group_name', ['member_name', 'another_member_name']]]
+            alternatives = [['example_alternative', 'An example']]
+            scenarios = [['example_scenario', 'An example']]
+            scenario_alternatives = [('scenario', 'alternative1'), ('scenario', 'alternative0', 'alternative1')]
+            tools = [('tool1', 'Tool one description'), ('tool2', 'Tool two description']]
+
+            import_data(db_map,
+                        object_classes=object_c,
+                        relationship_classes=relationship_c,
+                        object_parameters=obj_parameters,
+                        relationship_parameters=rel_parameters,
+                        objects=objects,
+                        relationships=relationships,
+                        object_groups=object_groups,
+                        object_parameter_values=object_p_values,
+                        relationship_parameter_values=rel_p_values,
+                        alternatives=alternatives,
+                        scenarios=scenarios,
+                        scenario_alternatives=scenario_alternatives
+                        tools=tools)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        on_conflict (str): Conflict resolution strategy for ``parameter_value.fix_conflict``
+        object_classes (List[str]): List of object class names
+        relationship_classes (List[List[str, List(str)]):
+            List of lists with relationship class names and list of object class names
+        object_parameters (List[List[str, str]]):
+            list of lists with object class name and parameter name
+        relationship_parameters (List[List[str, str]]):
+            list of lists with relationship class name and parameter name
+        objects (List[List[str, str]]):
+            list of lists with object class name and object name
+        relationships: (List[List[str,List(String)]]):
+            list of lists with relationship class name and list of object names
+        object_groups (List[List/Tuple]): list/set/iterable of lists/tuples with object class name, group name,
+            and member name
+        object_parameter_values (List[List[str, str, str|numeric]]):
+            list of lists with object name, parameter name, parameter value
+        relationship_parameter_values (List[List[str, List(str), str, str|numeric]]):
+            list of lists with relationship class name, list of object names, parameter name, parameter value
+        alternatives (Iterable): alternative names or lists of two elements: alternative name and description
+        scenarios (Iterable): scenario names or lists of two elements: scenario name and description
+        scenario_alternatives (Iterable): lists of two elements: scenario name and a list of names of alternatives
+
+    Returns:
+        tuple: number of inserted/changed entities and list of ImportErrorLogItem with
+            any import errors
+    """
+    add_items_by_tablename = {
+        "alternative": db_map._add_alternatives,
+        "scenario": db_map._add_scenarios,
+        "scenario_alternative": db_map._add_scenario_alternatives,
+        "object_class": db_map._add_object_classes,
+        "relationship_class": db_map._add_wide_relationship_classes,
+        "parameter_value_list": db_map._add_parameter_value_lists,
+        "list_value": db_map._add_list_values,
+        "parameter_definition": db_map._add_parameter_definitions,
+        "feature": db_map._add_features,
+        "tool": db_map._add_tools,
+        "tool_feature": db_map._add_tool_features,
+        "tool_feature_method": db_map._add_tool_feature_methods,
+        "object": db_map._add_objects,
+        "relationship": db_map._add_wide_relationships,
+        "entity_group": db_map._add_entity_groups,
+        "parameter_value": db_map._add_parameter_values,
+        "metadata": db_map._add_metadata,
+        "entity_metadata": db_map._add_entity_metadata,
+        "parameter_value_metadata": db_map._add_parameter_value_metadata,
+    }
+    update_items_by_tablename = {
+        "alternative": db_map._update_alternatives,
+        "scenario": db_map._update_scenarios,
+        "scenario_alternative": db_map._update_scenario_alternatives,
+        "object_class": db_map._update_object_classes,
+        "relationship_class": db_map._update_wide_relationship_classes,
+        "parameter_value_list": db_map._update_parameter_value_lists,
+        "list_value": db_map._update_list_values,
+        "parameter_definition": db_map._update_parameter_definitions,
+        "feature": db_map._update_features,
+        "tool": db_map._update_tools,
+        "tool_feature": db_map._update_tool_features,
+        "object": db_map._update_objects,
+        "parameter_value": db_map._update_parameter_values,
+    }
+    error_log = []
+    num_imports = 0
+    for tablename, (to_add, to_update, errors) in get_data_for_import(
+        db_map, make_cache=make_cache, unparse_value=unparse_value, on_conflict=on_conflict, **kwargs
+    ):
+        update_items = update_items_by_tablename.get(tablename, lambda *args, **kwargs: ())
+        try:
+            updated = update_items(*to_update)
+        except SpineDBAPIError as error:
+            updated = []
+            error_log.append(ImportErrorLogItem(msg=str(error), db_type=tablename))
+        add_items = add_items_by_tablename[tablename]
+        try:
+            added = add_items(*to_add)
+        except SpineDBAPIError as error:
+            added = []
+            error_log.append(ImportErrorLogItem(msg=str(error), db_type=tablename))
+        num_imports += len(added) + len(updated)
+        error_log.extend(errors)
+    return num_imports, error_log
+
+
+def get_data_for_import(
+    db_map,
+    make_cache=None,
+    unparse_value=to_database,
+    on_conflict="merge",
+    object_classes=(),
+    relationship_classes=(),
+    parameter_value_lists=(),
+    object_parameters=(),
+    relationship_parameters=(),
+    objects=(),
+    relationships=(),
+    object_groups=(),
+    object_parameter_values=(),
+    relationship_parameter_values=(),
+    alternatives=(),
+    scenarios=(),
+    scenario_alternatives=(),
+    features=(),
+    tools=(),
+    tool_features=(),
+    tool_feature_methods=(),
+    metadata=(),
+    object_metadata=(),
+    relationship_metadata=(),
+    object_parameter_value_metadata=(),
+    relationship_parameter_value_metadata=(),
+):
+    """Returns an iterator of data for import, that the user can call instead of `import_data`
+    if they want to add and update the data by themselves.
+    Especially intended to be used with the toolbox undo/redo functionality.
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        on_conflict (str): Conflict resolution strategy for ``parameter_value.fix_conflict``
+        object_classes (List[str]): List of object class names
+        relationship_classes (List[List[str, List(str)]):
+            List of lists with relationship class names and list of object class names
+        object_parameters (List[List[str, str]]):
+            list of lists with object class name and parameter name
+        relationship_parameters (List[List[str, str]]):
+            list of lists with relationship class name and parameter name
+        objects (List[List[str, str]]):
+            list of lists with object class name and object name
+        relationships: (List[List[str,List(String)]]):
+            list of lists with relationship class name and list of object names
+        object_groups (List[List/Tuple]): list/set/iterable of lists/tuples with object class name, group name,
+            and member name
+        object_parameter_values (List[List[str, str, str|numeric]]):
+            list of lists with object name, parameter name, parameter value
+        relationship_parameter_values (List[List[str, List(str), str, str|numeric]]):
+            list of lists with relationship class name, list of object names, parameter name,
+            parameter value
+
+    Returns:
+        dict(str, list)
+    """
+    if make_cache is None:
+        make_cache = db_map.make_cache
+    # NOTE: The order is important, because of references. E.g., we want to import alternatives before parameter_values
+    if alternatives:
+        yield ("alternative", _get_alternatives_for_import(alternatives, make_cache))
+    if scenarios:
+        yield ("scenario", _get_scenarios_for_import(scenarios, make_cache))
+    if scenario_alternatives:
+        if not scenarios:
+            scenarios = (item[0] for item in scenario_alternatives)
+            yield ("scenario", _get_scenarios_for_import(scenarios, make_cache))
+        if not alternatives:
+            alternatives = (item[1] for item in scenario_alternatives)
+            yield ("alternative", _get_alternatives_for_import(alternatives, make_cache))
+        yield ("scenario_alternative", _get_scenario_alternatives_for_import(scenario_alternatives, make_cache))
+    if object_classes:
+        yield ("object_class", _get_object_classes_for_import(db_map, object_classes, make_cache))
+    if relationship_classes:
+        yield ("relationship_class", _get_relationship_classes_for_import(db_map, relationship_classes, make_cache))
+    if parameter_value_lists:
+        yield ("parameter_value_list", _get_parameter_value_lists_for_import(db_map, parameter_value_lists, make_cache))
+        yield ("list_value", _get_list_values_for_import(db_map, parameter_value_lists, make_cache, unparse_value))
+    if object_parameters:
+        yield (
+            "parameter_definition",
+            _get_object_parameters_for_import(db_map, object_parameters, make_cache, unparse_value),
+        )
+    if relationship_parameters:
+        yield (
+            "parameter_definition",
+            _get_relationship_parameters_for_import(db_map, relationship_parameters, make_cache, unparse_value),
+        )
+    if features:
+        yield ("feature", _get_features_for_import(db_map, features, make_cache))
+    if tools:
+        yield ("tool", _get_tools_for_import(db_map, tools, make_cache))
+    if tool_features:
+        yield ("tool_feature", _get_tool_features_for_import(db_map, tool_features, make_cache))
+    if tool_feature_methods:
+        yield (
+            "tool_feature_method",
+            _get_tool_feature_methods_for_import(db_map, tool_feature_methods, make_cache, unparse_value),
+        )
+    if objects:
+        yield ("object", _get_objects_for_import(db_map, objects, make_cache))
+    if relationships:
+        yield ("relationship", _get_relationships_for_import(db_map, relationships, make_cache))
+    if object_groups:
+        yield ("entity_group", _get_object_groups_for_import(db_map, object_groups, make_cache))
+    if object_parameter_values:
+        yield (
+            "parameter_value",
+            _get_object_parameter_values_for_import(
+                db_map, object_parameter_values, make_cache, unparse_value, on_conflict
+            ),
+        )
+    if relationship_parameter_values:
+        yield (
+            "parameter_value",
+            _get_relationship_parameter_values_for_import(
+                db_map, relationship_parameter_values, make_cache, unparse_value, on_conflict
+            ),
+        )
+    if metadata:
+        yield ("metadata", _get_metadata_for_import(db_map, metadata, make_cache))
+    if object_metadata:
+        yield ("entity_metadata", _get_object_metadata_for_import(db_map, object_metadata, make_cache))
+    if relationship_metadata:
+        yield ("entity_metadata", _get_relationship_metadata_for_import(db_map, relationship_metadata, make_cache))
+    if object_parameter_value_metadata:
+        yield (
+            "parameter_value_metadata",
+            _get_object_parameter_value_metadata_for_import(db_map, object_parameter_value_metadata, make_cache),
+        )
+    if relationship_parameter_value_metadata:
+        yield (
+            "parameter_value_metadata",
+            _get_relationship_parameter_value_metadata_for_import(
+                db_map, relationship_parameter_value_metadata, make_cache
+            ),
+        )
+
+
+def import_features(db_map, data, make_cache=None):
+    """
+    Imports features.
+
+    Example:
+
+        data = [('class', 'parameter'), ('another_class', 'another_parameter', 'description')]
+        import_features(db_map, data)
+
+    Args:
+        db_map (DiffDatabaseMapping): mapping for database to insert into
+        data (Iterable): an iterable of lists/tuples with class name, parameter name, and optionally description
+
+    Returns:
+        tuple of int and list: Number of successfully inserted features, list of errors
+    """
+    return import_data(db_map, features=data, make_cache=make_cache)
+
+
+def _get_features_for_import(db_map, data, make_cache):
+    cache = make_cache({"feature"}, include_ancestors=True)
+    feature_ids = {x.parameter_definition_id: x.id for x in cache.get("feature", {}).values()}
+    parameter_ids = {
+        (x.entity_class_name, x.parameter_name): (x.id, x.value_list_id)
+        for x in cache.get("parameter_definition", {}).values()
+    }
+    parameter_definitions = {
+        x.id: {
+            "name": x.parameter_name,
+            "entity_class_id": x.entity_class_id,
+            "parameter_value_list_id": x.value_list_id,
+        }
+        for x in cache.get("parameter_definition", {}).values()
+    }
+    checked = set()
+    to_add = []
+    to_update = []
+    error_log = []
+    for class_name, parameter_name, *optionals in data:
+        parameter_definition_id, parameter_value_list_id = parameter_ids.get((class_name, parameter_name), (None, None))
+        if parameter_definition_id in checked:
+            continue
+        feature_id = feature_ids.pop(parameter_definition_id, None)
+        item = (
+            cache["feature"][feature_id]._asdict()
+            if feature_id is not None
+            else {
+                "parameter_definition_id": parameter_definition_id,
+                "parameter_value_list_id": parameter_value_list_id,
+                "description": None,
+            }
+        )
+        item.update(dict(zip(("description",), optionals)))
+        try:
+            check_feature(item, feature_ids, parameter_definitions)
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(
+                    msg=f"Could not import feature '{class_name, parameter_name}': {e.msg}", db_type="feature"
+                )
+            )
+            continue
+        finally:
+            if feature_id is not None:
+                feature_ids[parameter_definition_id] = feature_id
+        checked.add(parameter_definition_id)
+        if feature_id is not None:
+            item["id"] = feature_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_tools(db_map, data, make_cache=None):
+    """
+    Imports tools.
+
+    Example:
+
+        data = ['tool', ('another_tool', 'description')]
+        import_tools(db_map, data)
+
+    Args:
+        db_map (DiffDatabaseMapping): mapping for database to insert into
+        data (Iterable): an iterable of tool names,
+            or of lists/tuples with tool names and optional descriptions
+
+    Returns:
+        tuple of int and list: Number of successfully inserted tools, list of errors
+    """
+    return import_data(db_map, tools=data, make_cache=make_cache)
+
+
+def _get_tools_for_import(db_map, data, make_cache):
+    cache = make_cache({"tool"}, include_ancestors=True)
+    tool_ids = {tool.name: tool.id for tool in cache.get("tool", {}).values()}
+    checked = set()
+    to_add = []
+    to_update = []
+    error_log = []
+    for tool in data:
+        if isinstance(tool, str):
+            tool = (tool,)
+        name, *optionals = tool
+        if name in checked:
+            continue
+        tool_id = tool_ids.pop(name, None)
+        item = cache["tool"][tool_id]._asdict() if tool_id is not None else {"name": name, "description": None}
+        item.update(dict(zip(("description",), optionals)))
+        try:
+            check_tool(item, tool_ids)
+        except SpineIntegrityError as e:
+            error_log.append(ImportErrorLogItem(msg=f"Could not import tool '{name}': {e.msg}", db_type="tool"))
+            continue
+        finally:
+            if tool_id is not None:
+                tool_ids[name] = tool_id
+        checked.add(name)
+        if tool_id is not None:
+            item["id"] = tool_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_tool_features(db_map, data, make_cache=None):
+    """
+    Imports tool features.
+
+    Example:
+
+        data = [('tool', 'class', 'parameter'), ('another_tool', 'another_class', 'another_parameter', 'required')]
+        import_tool_features(db_map, data)
+
+    Args:
+        db_map (DiffDatabaseMapping): mapping for database to insert into
+        data (Iterable): an iterable of lists/tuples with tool name, class name, parameter name,
+            and optionally description
+
+    Returns:
+        tuple of int and list: Number of successfully inserted tool features, list of errors
+    """
+    return import_data(db_map, tool_features=data, make_cache=make_cache)
+
+
+def _get_tool_features_for_import(db_map, data, make_cache):
+    cache = make_cache({"tool_feature"}, include_ancestors=True)
+    tool_feature_ids = {(x.tool_id, x.feature_id): x.id for x in cache.get("tool_feature", {}).values()}
+    tool_ids = {x.name: x.id for x in cache.get("tool", {}).values()}
+    feature_ids = {
+        (x.entity_class_name, x.parameter_definition_name): (x.id, x.parameter_value_list_id)
+        for x in cache.get("feature", {}).values()
+    }
+    tools = {x.id: x._asdict() for x in cache.get("tool", {}).values()}
+    features = {
+        x.id: {
+            "name": x.entity_class_name + "/" + x.parameter_definition_name,
+            "parameter_value_list_id": x.parameter_value_list_id,
+        }
+        for x in cache.get("feature", {}).values()
+    }
+    checked = set()
+    to_add = []
+    to_update = []
+    error_log = []
+    for tool_name, class_name, parameter_name, *optionals in data:
+        tool_id = tool_ids.get(tool_name)
+        feature_id, parameter_value_list_id = feature_ids.get((class_name, parameter_name), (None, None))
+        if (tool_id, feature_id) in checked:
+            continue
+        tool_feature_id = tool_feature_ids.pop((tool_id, feature_id), None)
+        item = (
+            cache["tool_feature"][tool_feature_id]._asdict()
+            if tool_feature_id is not None
+            else {
+                "tool_id": tool_id,
+                "feature_id": feature_id,
+                "parameter_value_list_id": parameter_value_list_id,
+                "required": False,
+            }
+        )
+        item.update(dict(zip(("required",), optionals)))
+        try:
+            check_tool_feature(item, tool_feature_ids, tools, features)
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(
+                    msg=f"Could not import tool feature '{tool_name, class_name, parameter_name}': {e.msg}",
+                    db_type="tool_feature",
+                )
+            )
+            continue
+        finally:
+            if tool_feature_id is not None:
+                tool_feature_ids[tool_id, feature_id] = tool_feature_id
+        checked.add((tool_id, feature_id))
+        if tool_feature_id is not None:
+            item["id"] = tool_feature_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_tool_feature_methods(db_map, data, make_cache=None, unparse_value=to_database):
+    """
+    Imports tool feature methods.
+
+    Example:
+
+        data = [('tool', 'class', 'parameter', 'method'), ('another_tool', 'another_class', 'another_parameter', 'another_method')]
+        import_tool_features(db_map, data)
+
+    Args:
+        db_map (DiffDatabaseMapping): mapping for database to insert into
+        data (Iterable): an iterable of lists/tuples with tool name, class name, parameter name, and method
+
+    Returns:
+        tuple of int and list: Number of successfully inserted tool features, list of errors
+    """
+    return import_data(db_map, tool_feature_methods=data, make_cache=make_cache, unparse_value=unparse_value)
+
+
+def _get_tool_feature_methods_for_import(db_map, data, make_cache, unparse_value):
+    cache = make_cache({"tool_feature_method"}, include_ancestors=True)
+    tool_feature_method_ids = {
+        (x.tool_feature_id, x.method_index): x.id for x in cache.get("tool_feature_method", {}).values()
+    }
+    tool_feature_ids = {
+        (x.tool_name, x.entity_class_name, x.parameter_definition_name): (x.id, x.parameter_value_list_id)
+        for x in cache.get("tool_feature", {}).values()
+    }
+    tool_features = {x.id: x._asdict() for x in cache.get("tool_feature", {}).values()}
+    parameter_value_lists = {
+        x.id: {"name": x.name, "value_index_list": x.value_index_list}
+        for x in cache.get("parameter_value_list", {}).values()
+    }
+    list_values = {
+        (x.parameter_value_list_id, x.index): from_database(x.value, x.type)
+        for x in cache.get("list_value", {}).values()
+    }
+    seen = set()
+    to_add = []
+    error_log = []
+    for tool_name, class_name, parameter_name, method in data:
+        tool_feature_id, parameter_value_list_id = tool_feature_ids.get(
+            (tool_name, class_name, parameter_name), (None, None)
+        )
+        parameter_value_list = parameter_value_lists.get(parameter_value_list_id, {})
+        value_index_list = parameter_value_list.get("value_index_list", [])
+        method = from_database(*unparse_value(method))
+        method_index = next(
+            iter(index for index in value_index_list if list_values.get((parameter_value_list_id, index)) == method),
+            None,
+        )
+        if (tool_feature_id, method_index) in seen | tool_feature_method_ids.keys():
+            continue
+        item = {
+            "tool_feature_id": tool_feature_id,
+            "parameter_value_list_id": parameter_value_list_id,
+            "method_index": method_index,
+        }
+        try:
+            check_tool_feature_method(item, tool_feature_method_ids, tool_features, parameter_value_lists)
+            to_add.append(item)
+            seen.add((tool_feature_id, method_index))
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(
+                    msg=(
+                        f"Could not import tool feature method '{tool_name, class_name, parameter_name, method}':"
+                        f" {e.msg}"
+                    ),
+                    db_type="tool_feature_method",
+                )
+            )
+    return to_add, [], error_log
+
+
+def import_alternatives(db_map, data, make_cache=None):
+    """
+    Imports alternatives.
+
+    Example:
+
+        data = ['new_alternative', ('another_alternative', 'description')]
+        import_alternatives(db_map, data)
+
+    Args:
+        db_map (DiffDatabaseMapping): mapping for database to insert into
+        data (Iterable): an iterable of alternative names,
+            or of lists/tuples with alternative names and optional descriptions
+
+    Returns:
+        tuple of int and list: Number of successfully inserted alternatives, list of errors
+    """
+    return import_data(db_map, alternatives=data, make_cache=make_cache)
+
+
+def _get_alternatives_for_import(data, make_cache):
+    cache = make_cache({"alternative"}, include_ancestors=True)
+    alternative_ids = {alternative.name: alternative.id for alternative in cache.get("alternative", {}).values()}
+    checked = set()
+    to_add = []
+    to_update = []
+    error_log = []
+    for alternative in data:
+        if isinstance(alternative, str):
+            alternative = (alternative,)
+        name, *optionals = alternative
+        if name in checked:
+            continue
+        alternative_id = alternative_ids.pop(name, None)
+        item = (
+            cache["alternative"][alternative_id]._asdict()
+            if alternative_id is not None
+            else {"name": name, "description": None}
+        )
+        item.update(dict(zip(("description",), optionals)))
+        try:
+            check_alternative(item, alternative_ids)
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(msg=f"Could not import alternative '{name}': {e.msg}", db_type="alternative")
+            )
+            continue
+        finally:
+            if alternative_id is not None:
+                alternative_ids[name] = alternative_id
+        checked.add(name)
+        if alternative_id is not None:
+            item["id"] = alternative_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_scenarios(db_map, data, make_cache=None):
+    """
+    Imports scenarios.
+
+    Example:
+
+        second_active = True
+        third_active = False
+        data = ['scenario', ('second_scenario', second_active), ('third_scenario', third_active, 'description')]
+        import_scenarios(db_map, data)
+
+    Args:
+        db_map (DiffDatabaseMapping): mapping for database to insert into
+        data (Iterable): an iterable of scenario names,
+            or of lists/tuples with scenario names and optional descriptions
+
+    Returns:
+        tuple of int and list: Number of successfully inserted scenarios, list of errors
+    """
+    return import_data(db_map, scenarios=data, make_cache=make_cache)
+
+
+def _get_scenarios_for_import(data, make_cache):
+    cache = make_cache({"scenario"}, include_ancestors=True)
+    scenario_ids = {scenario.name: scenario.id for scenario in cache.get("scenario", {}).values()}
+    checked = set()
+    to_add = []
+    to_update = []
+    error_log = []
+    for scenario in data:
+        if isinstance(scenario, str):
+            scenario = (scenario,)
+        name, *optionals = scenario
+        if name in checked:
+            continue
+        scenario_id = scenario_ids.pop(name, None)
+        item = (
+            cache["scenario"][scenario_id]._asdict()
+            if scenario_id is not None
+            else {"name": name, "active": False, "description": None}
+        )
+        item.update(dict(zip(("active", "description"), optionals)))
+        try:
+            check_scenario(item, scenario_ids)
+        except SpineIntegrityError as e:
+            error_log.append(ImportErrorLogItem(msg=f"Could not import scenario '{name}': {e.msg}", db_type="scenario"))
+            continue
+        finally:
+            if scenario_id is not None:
+                scenario_ids[name] = scenario_id
+        checked.add(name)
+        if scenario_id is not None:
+            item["id"] = scenario_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_scenario_alternatives(db_map, data, make_cache=None):
+    """
+    Imports scenario alternatives.
+
+    Example:
+
+        data = [('scenario', 'bottom_alternative'), ('another_scenario', 'top_alternative', 'bottom_alternative')]
+        import_scenario_alternatives(db_map, data)
+
+    Args:
+        db_map (DiffDatabaseMapping): mapping for database to insert into
+        data (Iterable): an iterable of (scenario name, alternative name,
+            and optionally, 'before' alternative name).
+            Alternatives are inserted before the 'before' alternative,
+            or at the end if not given.
+
+    Returns:
+        tuple of int and list: Number of successfully inserted scenario alternatives, list of errors
+    """
+    return import_data(db_map, scenario_alternatives=data, make_cache=make_cache)
+
+
+def _get_scenario_alternatives_for_import(data, make_cache):
+    cache = make_cache({"scenario_alternative"}, include_ancestors=True)
+    scenario_alternative_id_lists = {x.id: x.alternative_id_list for x in cache.get("scenario", {}).values()}
+    scenario_alternative_ids = {
+        (x.scenario_id, x.alternative_id): x.id for x in cache.get("scenario_alternative", {}).values()
+    }
+    scenario_ids = {scenario.name: scenario.id for scenario in cache.get("scenario", {}).values()}
+    alternative_ids = {alternative.name: alternative.id for alternative in cache.get("alternative", {}).values()}
+    to_add = []
+    to_update = []
+    error_log = []
+    successors_ids_per_scenario = defaultdict(dict)
+    for scenario_name, alternative_name, *optionals in data:
+        try:
+            predecessor_id = alternative_ids[alternative_name]
+        except KeyError:
+            error_log.append(
+                ImportErrorLogItem(msg=f"Alternative '{alternative_name}' not found.", db_type="scenario alternative")
+            )
+            continue
+        successor = optionals[0] if optionals else None
+        if successor is not None:
+            try:
+                successor_id = alternative_ids[successor]
+            except KeyError:
+                error_log.append(
+                    ImportErrorLogItem(msg=f"Alternative '{successor}' not found.", db_type="scenario alternative")
+                )
+                continue
+        else:
+            successor_id = None
+        successors_ids_per_scenario[scenario_name][predecessor_id] = successor_id
+    predecessor_ids_per_scenario = {
+        scenario_name: {successor: predecessor for predecessor, successor in successor_ids.items()}
+        for scenario_name, successor_ids in successors_ids_per_scenario.items()
+    }
+    for scenario_name, predecessor_ids in predecessor_ids_per_scenario.items():
+        try:
+            scenario_id = scenario_ids[scenario_name]
+        except KeyError:
+            error_log.append(
+                ImportErrorLogItem(msg=f"Scenario '{scenario_name}' not found.", db_type="scenario alternative")
+            )
+            continue
+        orig_alt_id_list = scenario_alternative_id_lists.get(scenario_id, [])
+        successor_ids = successors_ids_per_scenario[scenario_name]
+        new_alt_id_list = [id_ for id_ in orig_alt_id_list if id_ not in successor_ids]
+        last_alternative_ids = []
+        for predecessor_id, successor_id in list(successor_ids.items()):
+            if successor_id is None:
+                if predecessor_id not in orig_alt_id_list:
+                    last_alternative_ids.append(predecessor_id)
+                del successor_ids[predecessor_id]
+        with suppress(KeyError):
+            del predecessor_ids[None]
+        new_alt_id_list += last_alternative_ids
+        for predecessor_id in predecessor_ids:
+            if predecessor_id not in successor_ids and predecessor_id not in new_alt_id_list:
+                new_alt_id_list.insert(0, predecessor_id)
+                break
+        while predecessor_ids:
+            for i, alternative_id in enumerate(new_alt_id_list):
+                if (predecessor := predecessor_ids.pop(alternative_id, None)) is not None:
+                    new_alt_id_list.insert(i, predecessor)
+                    break
+            else:
+                alternative_names = {name: id_ for id_, name in alternative_ids.items()}
+                for successor, predecessor in predecessor_ids.items():
+                    if predecessor not in new_alt_id_list:
+                        error_log.append(
+                            ImportErrorLogItem(
+                                msg=f"Before alternative '{alternative_names[successor]}' not found for '{alternative_names[predecessor]}'"
+                            )
+                        )
+                        break
+                else:
+                    raise RuntimeError("This should be unreachable.")
+                break
+        scenario_alternative_id_lists[scenario_id] = new_alt_id_list
+    for scenario_id, new_alt_id_list in scenario_alternative_id_lists.items():
+        for k, alt_id in enumerate(new_alt_id_list):
+            id_ = scenario_alternative_ids.get((scenario_id, alt_id))
+            item = {"scenario_id": scenario_id, "alternative_id": alt_id, "rank": k + 1}
+            if id_ is not None:
+                item["id"] = id_
+                to_update.append(item)
+            else:
+                to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_object_classes(db_map, data, make_cache=None):
+    """Imports object classes.
+
+    Example::
+
+            data = ['new_object_class', ('another_object_class', 'description', 123456)]
+            import_object_classes(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (Iterable): list/set/iterable of string object class names, or of lists/tuples with object class names,
+            and optionally description and integer display icon reference
+
+    Returns:
+        tuple of int and list: Number of successfully inserted object classes, list of errors
+    """
+    return import_data(db_map, object_classes=data, make_cache=make_cache)
+
+
+def _get_object_classes_for_import(db_map, data, make_cache):
+    cache = make_cache({"object_class"}, include_ancestors=True)
+    object_class_ids = {oc.name: oc.id for oc in cache.get("object_class", {}).values()}
+    checked = set()
+    to_add = []
+    to_update = []
+    error_log = []
+    for object_class in data:
+        if isinstance(object_class, str):
+            object_class = (object_class,)
+        name, *optionals = object_class
+        if name in checked:
+            continue
+        oc_id = object_class_ids.pop(name, None)
+        item = (
+            cache["object_class"][oc_id]._asdict()
+            if oc_id is not None
+            else {"name": name, "description": None, "display_icon": None}
+        )
+        item["type_id"] = db_map.object_class_type
+        item.update(dict(zip(("description", "display_icon"), optionals)))
+        try:
+            check_object_class(item, object_class_ids, db_map.object_class_type)
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(msg=f"Could not import object class '{name}': {e.msg}", db_type="object class")
+            )
+            continue
+        finally:
+            if oc_id is not None:
+                object_class_ids[name] = oc_id
+        checked.add(name)
+        if oc_id is not None:
+            item["id"] = oc_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_relationship_classes(db_map, data, make_cache=None):
+    """Imports relationship classes.
+
+    Example::
+
+            data = [
+                ('new_rel_class', ['object_class_1', 'object_class_2']),
+                ('another_rel_class', ['object_class_3', 'object_class_4'], 'description'),
+            ]
+            import_relationship_classes(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of lists/tuples with relationship class names,
+            list of object class names, and optionally description
+
+    Returns:
+        (Int, List) Number of successful inserted objects, list of errors
+    """
+    return import_data(db_map, relationship_classes=data, make_cache=make_cache)
+
+
+def _get_relationship_classes_for_import(db_map, data, make_cache):
+    cache = make_cache({"relationship_class"}, include_ancestors=True)
+    object_class_ids = {oc.name: oc.id for oc in cache.get("object_class", {}).values()}
+    relationship_class_ids = {x.name: x.id for x in cache.get("relationship_class", {}).values()}
+    checked = set()
+    error_log = []
+    to_add = []
+    to_update = []
+    for name, oc_names, *optionals in data:
+        if name in checked:
+            continue
+        rc_id = relationship_class_ids.pop(name, None)
+        item = (
+            cache["relationship_class"][rc_id]._asdict()
+            if rc_id is not None
+            else {
+                "name": name,
+                "object_class_id_list": [object_class_ids.get(oc, None) for oc in oc_names],
+                "description": None,
+                "display_icon": None,
+            }
+        )
+        item["type_id"] = db_map.relationship_class_type
+        item.update(dict(zip(("description", "display_icon"), optionals)))
+        try:
+            check_wide_relationship_class(
+                item, relationship_class_ids, set(object_class_ids.values()), db_map.relationship_class_type
+            )
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(
+                    f"Could not import relationship class '{name}' with object classes {tuple(oc_names)}: {e.msg}",
+                    db_type="relationship class",
+                )
+            )
+            continue
+        finally:
+            if rc_id is not None:
+                relationship_class_ids[name] = rc_id
+        checked.add(name)
+        if rc_id is not None:
+            item["id"] = rc_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_objects(db_map, data, make_cache=None):
+    """Imports list of object by name with associated object class name into given database mapping:
+    Ignores duplicate names and existing names.
+
+    Example::
+
+            data = [
+                ('object_class_name', 'new_object'),
+                ('object_class_name', 'other_object', 'description')
+            ]
+            import_objects(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of lists/tuples with object name and object class name
+
+    Returns:
+        (Int, List) Number of successful inserted objects, list of errors
+    """
+    return import_data(db_map, objects=data, make_cache=make_cache)
+
+
+def _get_objects_for_import(db_map, data, make_cache):
+    cache = make_cache({"object"}, include_ancestors=True)
+    object_class_ids = {oc.name: oc.id for oc in cache.get("object_class", {}).values()}
+    object_ids = {(o.class_id, o.name): o.id for o in cache.get("object", {}).values()}
+    checked = set()
+    error_log = []
+    to_add = []
+    to_update = []
+    for oc_name, name, *optionals in data:
+        oc_id = object_class_ids.get(oc_name, None)
+        if (oc_id, name) in checked:
+            continue
+        o_id = object_ids.pop((oc_id, name), None)
+        item = (
+            cache["object"][o_id]._asdict()
+            if o_id is not None
+            else {"name": name, "class_id": oc_id, "description": None}
+        )
+        item["type_id"] = db_map.object_entity_type
+        item.update(dict(zip(("description",), optionals)))
+        try:
+            check_object(item, object_ids, set(object_class_ids.values()), db_map.object_entity_type)
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(
+                    msg=f"Could not import object '{name}' with class '{oc_name}': {e.msg}", db_type="object"
+                )
+            )
+            continue
+        finally:
+            if o_id is not None:
+                object_ids[oc_id, name] = o_id
+        checked.add((oc_id, name))
+        if o_id is not None:
+            item["id"] = o_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_object_groups(db_map, data, make_cache=None):
+    """Imports list of object groups by name with associated object class name into given database mapping:
+    Ignores duplicate and existing (group, member) tuples.
+
+    Example::
+
+            data = [
+                ('object_class_name', 'object_group_name', 'member_name'),
+                ('object_class_name', 'object_group_name', 'another_member_name')
+            ]
+            import_object_groups(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of lists/tuples with object class name, group name,
+            and member name
+
+    Returns:
+        (Int, List) Number of successful inserted objects, list of errors
+    """
+    return import_data(db_map, object_groups=data, make_cache=make_cache)
+
+
+def _get_object_groups_for_import(db_map, data, make_cache):
+    cache = make_cache({"entity_group"}, include_ancestors=True)
+    object_class_ids = {oc.name: oc.id for oc in cache.get("object_class", {}).values()}
+    object_ids = {(o.class_id, o.name): o.id for o in cache.get("object", {}).values()}
+    objects = {}
+    for obj in cache.get("object", {}).values():
+        objects.setdefault(obj.class_id, dict())[obj.id] = obj._asdict()
+    entity_group_ids = {(x.group_id, x.member_id): x.id for x in cache.get("entity_group", {}).values()}
+    error_log = []
+    to_add = []
+    seen = set()
+    for class_name, group_name, member_name in data:
+        oc_id = object_class_ids.get(class_name)
+        g_id = object_ids.get((oc_id, group_name))
+        m_id = object_ids.get((oc_id, member_name))
+        if (g_id, m_id) in seen | entity_group_ids.keys():
+            continue
+        item = {"entity_class_id": oc_id, "entity_id": g_id, "member_id": m_id}
+        try:
+            check_entity_group(item, entity_group_ids, objects)
+            to_add.append(item)
+            seen.add((g_id, m_id))
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(
+                    msg=f"Could not import object '{member_name}' into group '{group_name}': {e.msg}",
+                    db_type="entity group",
+                )
+            )
+    return to_add, [], error_log
+
+
+def import_relationships(db_map, data, make_cache=None):
+    """Imports relationships.
+
+    Example::
+
+            data = [('relationship_class_name', ('object_name1', 'object_name2'))]
+            import_relationships(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of lists/tuples with relationship class name
+            and list/tuple of object names
+
+    Returns:
+        (Int, List) Number of successful inserted objects, list of errors
+    """
+    return import_data(db_map, relationships=data, make_cache=make_cache)
+
+
+def _make_unique_relationship_name(class_id, class_name, object_names, class_id_name_tuples):
+    base_name = class_name + "_" + "__".join([obj if obj is not None else "None" for obj in object_names])
+    name = base_name
+    while (class_id, name) in class_id_name_tuples:
+        name = base_name + uuid.uuid4().hex
+    return name
+
+
+def _get_relationships_for_import(db_map, data, make_cache):
+    cache = make_cache({"relationship"}, include_ancestors=True)
+    relationships = {x.name: x for x in cache.get("relationship", {}).values()}
+    relationship_ids_per_name = {(x.class_id, x.name): x.id for x in relationships.values()}
+    relationship_ids_per_obj_lst = {(x.class_id, x.object_id_list): x.id for x in relationships.values()}
+    relationship_classes = {
+        x.id: {"object_class_id_list": x.object_class_id_list, "name": x.name}
+        for x in cache.get("relationship_class", {}).values()
+    }
+    objects = {x.id: {"class_id": x.class_id, "name": x.name} for x in cache.get("object", {}).values()}
+    object_ids = {(o["name"], o["class_id"]): o_id for o_id, o in objects.items()}
+    relationship_class_ids = {rc["name"]: rc_id for rc_id, rc in relationship_classes.items()}
+    object_class_id_lists = {rc_id: rc["object_class_id_list"] for rc_id, rc in relationship_classes.items()}
+    error_log = []
+    to_add = []
+    to_update = []
+    checked = set()
+    for class_name, object_names, *optionals in data:
+        rc_id = relationship_class_ids.get(class_name, None)
+        oc_ids = object_class_id_lists.get(rc_id, [])
+        o_ids = tuple(object_ids.get((name, oc_id), None) for name, oc_id in zip(object_names, oc_ids))
+        if (rc_id, o_ids) in checked:
+            continue
+        r_id = relationship_ids_per_obj_lst.pop((rc_id, o_ids), None)
+        if r_id is not None:
+            r_name = cache["relationship"][r_id].name
+            relationship_ids_per_name.pop((rc_id, r_name))
+        item = (
+            cache["relationship"][r_id]._asdict()
+            if r_id is not None
+            else {
+                "name": _make_unique_relationship_name(rc_id, class_name, object_names, relationship_ids_per_name),
+                "class_id": rc_id,
+                "object_id_list": list(o_ids),
+                "object_class_id_list": oc_ids,
+                "type_id": db_map.relationship_entity_type,
+            }
+        )
+        item.update(dict(zip(("description",), optionals)))
+        try:
+            check_wide_relationship(
+                item,
+                relationship_ids_per_name,
+                relationship_ids_per_obj_lst,
+                relationship_classes,
+                objects,
+                db_map.relationship_entity_type,
+            )
+        except SpineIntegrityError as e:
+            msg = f"Could not import relationship with objects {tuple(object_names)} into '{class_name}': {e.msg}"
+            error_log.append(ImportErrorLogItem(msg=msg, db_type="relationship"))
+            continue
+        finally:
+            if r_id is not None:
+                relationship_ids_per_obj_lst[rc_id, o_ids] = r_id
+                relationship_ids_per_name[rc_id, r_name] = r_id
+        checked.add((rc_id, o_ids))
+        if r_id is not None:
+            item["id"] = r_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_object_parameters(db_map, data, make_cache=None, unparse_value=to_database):
+    """Imports list of object class parameters:
+
+    Example::
+
+            data = [
+                ('object_class_1', 'new_parameter'),
+                ('object_class_2', 'other_parameter', 'default_value', 'value_list_name', 'description')
+            ]
+            import_object_parameters(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of lists/tuples with object class name, parameter name,
+            and optionally default value, value list name, and description
+
+    Returns:
+        (Int, List) Number of successful inserted objects, list of errors
+    """
+    return import_data(db_map, object_parameters=data, make_cache=make_cache, unparse_value=unparse_value)
+
+
+def _get_object_parameters_for_import(db_map, data, make_cache, unparse_value):
+    cache = make_cache({"parameter_definition"}, include_ancestors=True)
+    parameter_ids = {
+        (x.entity_class_id, x.parameter_name): x.id for x in cache.get("parameter_definition", {}).values()
+    }
+    object_class_names = {x.id: x.name for x in cache.get("object_class", {}).values()}
+    object_class_ids = {oc_name: oc_id for oc_id, oc_name in object_class_names.items()}
+    parameter_value_lists = {}
+    parameter_value_list_ids = {}
+    for x in cache.get("parameter_value_list", {}).values():
+        parameter_value_lists[x.id] = x.value_id_list
+        parameter_value_list_ids[x.name] = x.id
+    list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
+    checked = set()
+    error_log = []
+    to_add = []
+    to_update = []
+    functions = [unparse_value, lambda x: (parameter_value_list_ids.get(x),), lambda x: (x,)]
+    for class_name, parameter_name, *optionals in data:
+        oc_id = object_class_ids.get(class_name, None)
+        checked_key = (oc_id, parameter_name)
+        if checked_key in checked:
+            continue
+        p_id = parameter_ids.pop((oc_id, parameter_name), None)
+        item = (
+            cache["parameter_definition"][p_id]._asdict()
+            if p_id is not None
+            else {
+                "name": parameter_name,
+                "entity_class_id": oc_id,
+                "object_class_id": oc_id,
+                "default_value": None,
+                "default_type": None,
+                "parameter_value_list_id": None,
+                "description": None,
+            }
+        )
+        optionals = [y for f, x in zip(functions, optionals) for y in f(x)]
+        item.update(dict(zip(("default_value", "default_type", "parameter_value_list_id", "description"), optionals)))
+        try:
+            check_parameter_definition(
+                item, parameter_ids, object_class_names.keys(), parameter_value_lists, list_values
+            )
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(
+                    f"Could not import parameter '{parameter_name}' with class '{class_name}': {e.msg}",
+                    db_type="parameter definition",
+                )
+            )
+            continue
+        finally:
+            if p_id is not None:
+                parameter_ids[oc_id, parameter_name] = p_id
+        checked.add(checked_key)
+        if p_id is not None:
+            item["id"] = p_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_relationship_parameters(db_map, data, make_cache=None, unparse_value=to_database):
+    """Imports list of relationship class parameters:
+
+    Example::
+
+            data = [
+                ('relationship_class_1', 'new_parameter'),
+                ('relationship_class_2', 'other_parameter', 'default_value', 'value_list_name', 'description')
+            ]
+            import_relationship_parameters(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of lists/tuples with relationship class name, parameter name,
+            and optionally default value, value list name, and description
+
+    Returns:
+        (Int, List) Number of successful inserted objects, list of errors
+    """
+    return import_data(db_map, relationship_parameters=data, make_cache=make_cache, unparse_value=unparse_value)
+
+
+def _get_relationship_parameters_for_import(db_map, data, make_cache, unparse_value):
+    cache = make_cache({"parameter_definition"}, include_ancestors=True)
+    parameter_ids = {
+        (x.entity_class_id, x.parameter_name): x.id for x in cache.get("parameter_definition", {}).values()
+    }
+    relationship_class_names = {x.id: x.name for x in cache.get("relationship_class", {}).values()}
+    relationship_class_ids = {rc_name: rc_id for rc_id, rc_name in relationship_class_names.items()}
+    parameter_value_lists = {}
+    parameter_value_list_ids = {}
+    for x in cache.get("parameter_value_list", {}).values():
+        parameter_value_lists[x.id] = x.value_id_list
+        parameter_value_list_ids[x.name] = x.id
+    list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
+    error_log = []
+    to_add = []
+    to_update = []
+    checked = set()
+    functions = [unparse_value, lambda x: (parameter_value_list_ids.get(x),), lambda x: (x,)]
+    for class_name, parameter_name, *optionals in data:
+        rc_id = relationship_class_ids.get(class_name, None)
+        checked_key = (rc_id, parameter_name)
+        if checked_key in checked:
+            continue
+        p_id = parameter_ids.pop((rc_id, parameter_name), None)
+        item = (
+            cache["parameter_definition"][p_id]._asdict()
+            if p_id is not None
+            else {
+                "name": parameter_name,
+                "entity_class_id": rc_id,
+                "relationship_class_id": rc_id,
+                "default_value": None,
+                "default_type": None,
+                "parameter_value_list_id": None,
+                "description": None,
+            }
+        )
+        optionals = [y for f, x in zip(functions, optionals) for y in f(x)]
+        item.update(dict(zip(("default_value", "default_type", "parameter_value_list_id", "description"), optionals)))
+        try:
+            check_parameter_definition(
+                item, parameter_ids, relationship_class_names.keys(), parameter_value_lists, list_values
+            )
+        except SpineIntegrityError as e:
+            # Relationship class doesn't exists
+            error_log.append(
+                ImportErrorLogItem(
+                    msg=f"Could not import parameter '{parameter_name}' with class '{class_name}': {e.msg}",
+                    db_type="parameter definition",
+                )
+            )
+            continue
+        finally:
+            if p_id is not None:
+                parameter_ids[rc_id, parameter_name] = p_id
+        checked.add(checked_key)
+        if p_id is not None:
+            item["id"] = p_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_object_parameter_values(db_map, data, make_cache=None, unparse_value=to_database, on_conflict="merge"):
+    """Imports object parameter values:
+
+    Example::
+
+            data = [('object_class_name', 'object_name', 'parameter_name', 123.4),
+                    ('object_class_name', 'object_name', 'parameter_name2', <TimeSeries>),
+                    ('object_class_name', 'object_name', 'parameter_name', <TimeSeries>, 'alternative')]
+            import_object_parameter_values(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of lists/tuples with
+            object_class_name, object name, parameter name, (deserialized) parameter value,
+            optional name of an alternative
+
+    Returns:
+        (Int, List) Number of successful inserted objects, list of errors
+    """
+    return import_data(
+        db_map,
+        object_parameter_values=data,
+        make_cache=make_cache,
+        unparse_value=unparse_value,
+        on_conflict=on_conflict,
+    )
+
+
+def _get_object_parameter_values_for_import(db_map, data, make_cache, unparse_value, on_conflict):
+    cache = make_cache({"parameter_value"}, include_ancestors=True)
+    object_class_ids = {x.name: x.id for x in cache.get("object_class", {}).values()}
+    parameter_value_ids = {
+        (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
+    }
+    parameters = {
+        x.id: {
+            "name": x.parameter_name,
+            "entity_class_id": x.entity_class_id,
+            "parameter_value_list_id": x.value_list_id,
+        }
+        for x in cache.get("parameter_definition", {}).values()
+    }
+    objects = {x.id: {"class_id": x.class_id, "name": x.name} for x in cache.get("object", {}).values()}
+    parameter_value_lists = {x.id: x.value_id_list for x in cache.get("parameter_value_list", {}).values()}
+    list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
+    object_ids = {(o["name"], o["class_id"]): o_id for o_id, o in objects.items()}
+    parameter_ids = {(p["name"], p["entity_class_id"]): p_id for p_id, p in parameters.items()}
+    alternatives = {a.name: a.id for a in cache.get("alternative", {}).values()}
+    alternative_ids = set(alternatives.values())
+    error_log = []
+    to_add = []
+    to_update = []
+    checked = set()
+    for class_name, object_name, parameter_name, value, *optionals in data:
+        oc_id = object_class_ids.get(class_name, None)
+        o_id = object_ids.get((object_name, oc_id), None)
+        p_id = parameter_ids.get((parameter_name, oc_id), None)
+        if optionals:
+            alternative_name = optionals[0]
+            alt_id = alternatives.get(alternative_name)
+            if not alt_id:
+                error_log.append(
+                    ImportErrorLogItem(
+                        msg=(
+                            "Could not import parameter value for "
+                            f"'{object_name}', class '{class_name}', parameter '{parameter_name}': "
+                            f"alternative '{alternative_name}' does not exist."
+                        ),
+                        db_type="parameter value",
+                    )
+                )
+                continue
+        else:
+            alt_id, alternative_name = db_map.get_import_alternative(cache=cache)
+            alternative_ids.add(alt_id)
+        checked_key = (o_id, p_id, alt_id)
+        if checked_key in checked:
+            msg = (
+                f"Could not import parameter value for '{object_name}', class '{class_name}', "
+                f"parameter '{parameter_name}', alternative {alternative_name}: "
+                "Duplicate parameter value, only first value will be considered."
+            )
+            error_log.append(ImportErrorLogItem(msg=msg, db_type="parameter value"))
+            continue
+        pv_id = parameter_value_ids.pop((o_id, p_id, alt_id), None)
+        value, type_ = unparse_value(value)
+        if pv_id is not None:
+            current_pv = cache["parameter_value"][pv_id]
+            value, type_ = fix_conflict((value, type_), (current_pv.value, current_pv.type), on_conflict)
+        item = {
+            "parameter_definition_id": p_id,
+            "entity_class_id": oc_id,
+            "entity_id": o_id,
+            "object_class_id": oc_id,
+            "object_id": o_id,
+            "value": value,
+            "type": type_,
+            "alternative_id": alt_id,
+        }
+        try:
+            check_parameter_value(
+                item, parameter_value_ids, parameters, objects, parameter_value_lists, list_values, alternative_ids
+            )
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(
+                    msg="Could not import parameter value for '{0}', class '{1}', parameter '{2}': {3}".format(
+                        object_name, class_name, parameter_name, e.msg
+                    ),
+                    db_type="parameter value",
+                )
+            )
+            continue
+        finally:
+            if pv_id is not None:
+                parameter_value_ids[o_id, p_id, alt_id] = pv_id
+        checked.add(checked_key)
+        if pv_id is not None:
+            item["id"] = pv_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_relationship_parameter_values(db_map, data, make_cache=None, unparse_value=to_database, on_conflict="merge"):
+    """Imports relationship parameter values:
+
+    Example::
+
+            data = [['example_rel_class',
+                ['example_object', 'other_object'], 'rel_parameter', 2.718],
+                ['example_object', 'other_object'], 'rel_parameter', 5.5, 'alternative']]
+            import_relationship_parameter_values(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of lists/tuples with
+            relationship class name, list of object names, parameter name, (deserialized) parameter value,
+            optional name of an alternative
+
+    Returns:
+        (Int, List) Number of successful inserted objects, list of errors
+    """
+    return import_data(
+        db_map,
+        relationship_parameter_values=data,
+        make_cache=make_cache,
+        unparse_value=unparse_value,
+        on_conflict=on_conflict,
+    )
+
+
+def _get_relationship_parameter_values_for_import(db_map, data, make_cache, unparse_value, on_conflict):
+    cache = make_cache({"parameter_value"}, include_ancestors=True)
+    object_class_id_lists = {x.id: x.object_class_id_list for x in cache.get("relationship_class", {}).values()}
+    parameter_value_ids = {
+        (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
+    }
+    parameters = {
+        x.id: {
+            "name": x.parameter_name,
+            "entity_class_id": x.entity_class_id,
+            "parameter_value_list_id": x.value_list_id,
+        }
+        for x in cache.get("parameter_definition", {}).values()
+    }
+    relationships = {
+        x.id: {"class_id": x.class_id, "name": x.name, "object_id_list": x.object_id_list}
+        for x in cache.get("relationship", {}).values()
+    }
+    parameter_value_lists = {x.id: x.value_id_list for x in cache.get("parameter_value_list", {}).values()}
+    list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
+    parameter_ids = {(p["entity_class_id"], p["name"]): p_id for p_id, p in parameters.items()}
+    relationship_ids = {(r["class_id"], tuple(r["object_id_list"])): r_id for r_id, r in relationships.items()}
+    object_ids = {(o.name, o.class_id): o.id for o in cache.get("object", {}).values()}
+    relationship_class_ids = {oc.name: oc.id for oc in cache.get("relationship_class", {}).values()}
+    alternatives = {a.name: a.id for a in cache.get("alternative", {}).values()}
+    alternative_ids = set(alternatives.values())
+    error_log = []
+    to_add = []
+    to_update = []
+    checked = set()
+    for class_name, object_names, parameter_name, value, *optionals in data:
+        rc_id = relationship_class_ids.get(class_name, None)
+        oc_ids = object_class_id_lists.get(rc_id, [])
+        if len(object_names) == len(oc_ids):
+            o_ids = tuple(object_ids.get((name, oc_id), None) for name, oc_id in zip(object_names, oc_ids))
+        else:
+            o_ids = tuple(None for _ in object_names)
+        r_id = relationship_ids.get((rc_id, o_ids), None)
+        p_id = parameter_ids.get((rc_id, parameter_name), None)
+        if optionals:
+            alternative_name = optionals[0]
+            alt_id = alternatives.get(alternative_name)
+            if not alt_id:
+                error_log.append(
+                    ImportErrorLogItem(
+                        msg=(
+                            "Could not import parameter value for "
+                            f"'{object_names}', class '{class_name}', parameter '{parameter_name}': "
+                            f"alternative {alternative_name} does not exist."
+                        ),
+                        db_type="parameter value",
+                    )
+                )
+                continue
+        else:
+            alt_id, alternative_name = db_map.get_import_alternative(cache=cache)
+            alternative_ids.add(alt_id)
+        checked_key = (r_id, p_id, alt_id)
+        if checked_key in checked:
+            msg = (
+                f"Could not import parameter value for '{object_names}', class '{class_name}', "
+                f"parameter '{parameter_name}', alternative {alternative_name}: "
+                "Duplicate parameter value, only first value will be considered."
+            )
+            error_log.append(ImportErrorLogItem(msg=msg, db_type="parameter value"))
+            continue
+        pv_id = parameter_value_ids.pop((r_id, p_id, alt_id), None)
+        value, type_ = unparse_value(value)
+        if pv_id is not None:
+            current_pv = cache["parameter_value"][pv_id]
+            value, type_ = fix_conflict((value, type_), (current_pv.value, current_pv.type), on_conflict)
+        item = {
+            "parameter_definition_id": p_id,
+            "entity_class_id": rc_id,
+            "entity_id": r_id,
+            "relationship_class_id": rc_id,
+            "relationship_id": r_id,
+            "value": value,
+            "type": type_,
+            "alternative_id": alt_id,
+        }
+        try:
+            check_parameter_value(
+                item,
+                parameter_value_ids,
+                parameters,
+                relationships,
+                parameter_value_lists,
+                list_values,
+                alternative_ids,
+            )
+        except SpineIntegrityError as e:
+            error_log.append(
+                ImportErrorLogItem(
+                    msg="Could not import parameter value for '{0}', class '{1}', parameter '{2}': {3}".format(
+                        object_names, class_name, parameter_name, e.msg
+                    ),
+                    db_type="parameter value",
+                )
+            )
+            continue
+        finally:
+            if pv_id is not None:
+                parameter_value_ids[r_id, p_id, alt_id] = pv_id
+        checked.add(checked_key)
+        if pv_id is not None:
+            item["id"] = pv_id
+            to_update.append(item)
+        else:
+            to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_parameter_value_lists(db_map, data, make_cache=None, unparse_value=to_database):
+    """Imports list of parameter value lists:
+
+    Example::
+
+            data = [
+                ['value_list_name', value1], ['value_list_name', value2],
+                ['another_value_list_name', value3],
+            ]
+            import_parameter_value_lists(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of lists/tuples with
+                                 value list name, list of values
+
+    Returns:
+        (Int, List) Number of successful inserted objects, list of errors
+    """
+    return import_data(db_map, parameter_value_lists=data, make_cache=make_cache, unparse_value=unparse_value)
+
+
+def _get_parameter_value_lists_for_import(db_map, data, make_cache):
+    cache = make_cache({"parameter_value_list"}, include_ancestors=True)
+    parameter_value_list_ids = {x.name: x.id for x in cache.get("parameter_value_list", {}).values()}
+    error_log = []
+    to_add = []
+    for name in list({x[0]: None for x in data}):
+        item = {"name": name}
+        try:
+            check_parameter_value_list(item, parameter_value_list_ids)
+        except SpineIntegrityError:
+            continue
+        to_add.append(item)
+    return to_add, [], error_log
+
+
+def _get_list_values_for_import(db_map, data, make_cache, unparse_value):
+    cache = make_cache({"list_value"}, include_ancestors=True)
+    value_lists_by_name = {x.name: (x.id, x.value_index_list) for x in cache.get("parameter_value_list", {}).values()}
+    list_value_ids_by_index = {(x.parameter_value_list_id, x.index): x.id for x in cache.get("list_value", {}).values()}
+    list_value_ids_by_value = {
+        (x.parameter_value_list_id, x.type, x.value): x.id for x in cache.get("list_value", {}).values()
+    }
+    list_names_by_id = {x.id: x.name for x in cache.get("parameter_value_list", {}).values()}
+    error_log = []
+    to_add = []
+    to_update = []
+    seen_values = set()
+    max_indexes = dict()
+    for list_name, value in data:
+        try:
+            list_id, value_index_list = value_lists_by_name.get(list_name)
+        except TypeError:
+            # cannot unpack non-iterable NoneType object
+            error_log.append(
+                ImportErrorLogItem(
+                    msg=f"Could not import value for list '{list_name}': list not found", db_type="list value"
+                )
+            )
+            continue
+        val, type_ = unparse_value(value)
+        if (list_id, type_, val) in seen_values:
+            error_log.append(
+                ImportErrorLogItem(
+                    msg=f"Could not import value for list '{list_name}': "
+                    "Duplicate value, only first will be considered",
+                    db_type="list value",
+                )
+            )
+            continue
+        max_index = max_indexes.get(list_id)
+        if max_index is not None:
+            index = max_index + 1
+        elif not value_index_list:
+            index = 0
+        else:
+            index = max(value_index_list) + 1
+        item = {"parameter_value_list_id": list_id, "value": val, "type": type_, "index": index}
+        try:
+            check_list_value(item, list_names_by_id, list_value_ids_by_index, list_value_ids_by_value)
+        except SpineIntegrityError as e:
+            if e.id is None:
+                error_log.append(
+                    ImportErrorLogItem(
+                        msg=f"Could not import value '{value}' for list '{list_name}': {e.msg}", db_type="list value"
+                    )
+                )
+            continue
+        max_indexes[list_id] = index
+        seen_values.add((list_id, type_, val))
+        to_add.append(item)
+    return to_add, to_update, error_log
+
+
+def import_metadata(db_map, data, make_cache=None):
+    """Imports metadata. Ignores duplicates.
+
+    Example::
+
+            data = ['{"name1": "value1"}', '{"name2": "value2"}']
+            import_metadata(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of string metadata entries in JSON format
+
+    Returns:
+        (Int, List) Number of successful inserted objects, list of errors
+    """
+    return import_data(db_map, metadata=data, make_cache=make_cache)
+
+
+def _get_metadata_for_import(db_map, data, make_cache):
+    cache = make_cache({"metadata"}, include_ancestors=True)
+    seen = {(x.name, x.value) for x in cache.get("metadata", {}).values()}
+    to_add = []
+    for metadata in data:
+        for name, value in _parse_metadata(metadata):
+            if (name, value) in seen:
+                continue
+            item = {"name": name, "value": value}
+            seen.add((name, value))
+            to_add.append(item)
+    return to_add, [], []
+
+
+def import_object_metadata(db_map, data, make_cache=None):
+    """Imports object metadata. Ignores duplicates.
+
+    Example::
+
+            data = [("classA", "object1", '{"name1": "value1"}'), ("classA", "object1", '{"name2": "value2"}')]
+            import_object_metadata(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of tuples with class name, object name,
+            and string metadata entries in JSON format
+
+    Returns:
+        (Int, List) Number of successful inserted items, list of errors
+    """
+    return import_data(db_map, object_metadata=data, make_cache=make_cache)
+
+
+def _get_object_metadata_for_import(db_map, data, make_cache):
+    cache = make_cache({"object", "entity_metadata"}, include_ancestors=True)
+    object_class_ids = {x.name: x.id for x in cache.get("object_class", {}).values()}
+    metadata_ids = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
+    object_ids = {(x.name, x.class_id): x.id for x in cache.get("object", {}).values()}
+    seen = {(x.entity_id, x.metadata_id) for x in cache.get("entity_metadata", {}).values()}
+    error_log = []
+    to_add = []
+    for class_name, object_name, metadata in data:
+        oc_id = object_class_ids.get(class_name, None)
+        o_id = object_ids.get((object_name, oc_id), None)
+        if o_id is None:
+            error_log.append(
+                ImportErrorLogItem(
+                    msg=f"Could not import object metadata: unknown object '{object_name}' of class '{class_name}'",
+                    db_type="object metadata",
+                )
+            )
+            continue
+        for name, value in _parse_metadata(metadata):
+            m_id = metadata_ids.get((name, value), None)
+            if m_id is None:
+                error_log.append(
+                    ImportErrorLogItem(
+                        msg=f"Could not import object metadata: unknown metadata '{name}': '{value}'",
+                        db_type="object metadata",
+                    )
+                )
+                continue
+            unique_key = (o_id, m_id)
+            if unique_key in seen:
+                continue
+            item = {"entity_id": o_id, "metadata_id": m_id}
+            seen.add(unique_key)
+            to_add.append(item)
+    return to_add, [], error_log
+
+
+def import_relationship_metadata(db_map, data, make_cache=None):
+    """Imports relationship metadata. Ignores duplicates.
+
+    Example::
+
+            data = [
+                ("classA", ("object1", "object2"), '{"name1": "value1"}'),
+                ("classA", ("object3", "object4"), '{"name2": "value2"}')
+            ]
+            import_relationship_metadata(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of tuples with class name, tuple of object names,
+            and string metadata entries in JSON format
+
+    Returns:
+        (Int, List) Number of successful inserted items, list of errors
+    """
+    return import_data(db_map, relationship_metadata=data, make_cache=make_cache)
+
+
+def _get_relationship_metadata_for_import(db_map, data, make_cache):
+    cache = make_cache({"relationship", "entity_metadata"}, include_ancestors=True)
+    relationship_class_ids = {oc.name: oc.id for oc in cache.get("relationship_class", {}).values()}
+    object_class_id_lists = {x.id: x.object_class_id_list for x in cache.get("relationship_class", {}).values()}
+    metadata_ids = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
+    object_ids = {(x.name, x.class_id): x.id for x in cache.get("object", {}).values()}
+    relationship_ids = {(x.class_id, x.object_id_list): x.id for x in cache.get("relationship", {}).values()}
+    seen = {(x.entity_id, x.metadata_id) for x in cache.get("entity_metadata", {}).values()}
+    error_log = []
+    to_add = []
+    for class_name, object_names, metadata in data:
+        rc_id = relationship_class_ids.get(class_name, None)
+        oc_ids = object_class_id_lists.get(rc_id, [])
+        o_ids = tuple(object_ids.get((name, oc_id), None) for name, oc_id in zip(object_names, oc_ids))
+        r_id = relationship_ids.get((rc_id, o_ids), None)
+        if r_id is None:
+            error_log.append(
+                ImportErrorLogItem(
+                    msg="Could not import relationship metadata: unknown relationship '{0}' of class '{1}'".format(
+                        object_names, class_name
+                    ),
+                    db_type="relationship metadata",
+                )
+            )
+            continue
+        for name, value in _parse_metadata(metadata):
+            m_id = metadata_ids.get((name, value), None)
+            if m_id is None:
+                error_log.append(
+                    ImportErrorLogItem(
+                        msg=f"Could not import relationship metadata: unknown metadata '{name}': '{value}'",
+                        db_type="relationship metadata",
+                    )
+                )
+                continue
+            unique_key = (r_id, m_id)
+            if unique_key in seen:
+                continue
+            item = {"entity_id": r_id, "metadata_id": m_id}
+            seen.add(unique_key)
+            to_add.append(item)
+    return to_add, [], error_log
+
+
+def import_object_parameter_value_metadata(db_map, data, make_cache=None):
+    """Imports object parameter value metadata. Ignores duplicates.
+
+    Example::
+
+            data = [
+                ("classA", "object1", "parameterX", '{"name1": "value1"}'),
+                ("classA", "object1", "parameterY", '{"name2": "value2"}', "alternativeA")
+            ]
+            import_object_parameter_value_metadata(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of tuples with class name, object name,
+            parameter name, string metadata entries in JSON format, and optionally alternative name
+
+    Returns:
+        (Int, List) Number of successful inserted items, list of errors
+    """
+    return import_data(db_map, object_parameter_value_metadata=data, make_cache=make_cache)
+
+
+def _get_object_parameter_value_metadata_for_import(db_map, data, make_cache):
+    cache = make_cache({"parameter_value", "parameter_value_metadata"}, include_ancestors=True)
+    object_class_ids = {x.name: x.id for x in cache.get("object_class", {}).values()}
+    object_ids = {(x.name, x.class_id): x.id for x in cache.get("object", {}).values()}
+    parameter_ids = {
+        (x.parameter_name, x.entity_class_id): x.id for x in cache.get("parameter_definition", {}).values()
+    }
+    alternative_ids = {a.name: a.id for a in cache.get("alternative", {}).values()}
+    parameter_value_ids = {
+        (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
+    }
+    metadata_ids = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
+    seen = {(x.parameter_value_id, x.metadata_id) for x in cache.get("parameter_value_metadata", {}).values()}
+    error_log = []
+    to_add = []
+    for class_name, object_name, parameter_name, metadata, *optionals in data:
+        oc_id = object_class_ids.get(class_name, None)
+        o_id = object_ids.get((object_name, oc_id), None)
+        p_id = parameter_ids.get((parameter_name, oc_id), None)
+        if optionals:
+            alternative_name = optionals[0]
+            alt_id = alternative_ids.get(alternative_name, None)
+        else:
+            alt_id, alternative_name = db_map.get_import_alternative(cache=cache)
+        pv_id = parameter_value_ids.get((o_id, p_id, alt_id), None)
+        if pv_id is None:
+            msg = (
+                "Could not import object parameter value metadata: "
+                "parameter {0} doesn't have a value for object {1}, alternative {2}".format(
+                    parameter_name, object_name, alternative_name
+                )
+            )
+            error_log.append(ImportErrorLogItem(msg=msg, db_type="object parameter value metadata"))
+            continue
+        for name, value in _parse_metadata(metadata):
+            m_id = metadata_ids.get((name, value), None)
+            if m_id is None:
+                error_log.append(
+                    ImportErrorLogItem(
+                        msg=f"Could not import object parameter value metadata: unknown metadata '{name}': '{value}'",
+                        db_type="object parameter value metadata",
+                    )
+                )
+                continue
+            unique_key = (pv_id, m_id)
+            if unique_key in seen:
+                continue
+            item = {"parameter_value_id": pv_id, "metadata_id": m_id}
+            seen.add(unique_key)
+            to_add.append(item)
+    return to_add, [], error_log
+
+
+def import_relationship_parameter_value_metadata(db_map, data, make_cache=None):
+    """Imports relationship parameter value metadata. Ignores duplicates.
+
+    Example::
+
+            data = [
+                ("classA", ("object1", "object2"), "parameterX", '{"name1": "value1"}'),
+                ("classA", ("object3", "object4"), "parameterY", '{"name2": "value2"}', "alternativeA")
+            ]
+            import_object_parameter_value_metadata(db_map, data)
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): mapping for database to insert into
+        data (List[List/Tuple]): list/set/iterable of tuples with class name, tuple of object names,
+            parameter name, string metadata entries in JSON format, and optionally alternative name
+
+    Returns:
+        (Int, List) Number of successful inserted items, list of errors
+    """
+    return import_data(db_map, relationship_parameter_value_metadata=data, make_cache=make_cache)
+
+
+def _get_relationship_parameter_value_metadata_for_import(db_map, data, make_cache):
+    cache = make_cache({"parameter_value", "parameter_value_metadata"}, include_ancestors=True)
+    relationship_class_ids = {oc.name: oc.id for oc in cache.get("relationship_class", {}).values()}
+    object_class_id_lists = {x.id: x.object_class_id_list for x in cache.get("relationship_class", {}).values()}
+    object_ids = {(x.name, x.class_id): x.id for x in cache.get("object", {}).values()}
+    relationship_ids = {(x.object_id_list, x.class_id): x.id for x in cache.get("relationship", {}).values()}
+    parameter_ids = {
+        (x.parameter_name, x.entity_class_id): x.id for x in cache.get("parameter_definition", {}).values()
+    }
+    alternative_ids = {a.name: a.id for a in cache.get("alternative", {}).values()}
+    parameter_value_ids = {
+        (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
+    }
+    metadata_ids = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
+    seen = {(x.parameter_value_id, x.metadata_id) for x in cache.get("parameter_value_metadata", {}).values()}
+    error_log = []
+    to_add = []
+    for class_name, object_names, parameter_name, metadata, *optionals in data:
+        rc_id = relationship_class_ids.get(class_name, None)
+        oc_ids = object_class_id_lists.get(rc_id, [])
+        o_ids = tuple(object_ids.get((name, oc_id), None) for name, oc_id in zip(object_names, oc_ids))
+        r_id = relationship_ids.get((o_ids, rc_id), None)
+        p_id = parameter_ids.get((parameter_name, rc_id), None)
+        if optionals:
+            alternative_name = optionals[0]
+            alt_id = alternative_ids.get(alternative_name, None)
+        else:
+            alt_id, alternative_name = db_map.get_import_alternative(cache=cache)
+        pv_id = parameter_value_ids.get((r_id, p_id, alt_id), None)
+        if pv_id is None:
+            msg = (
+                "Could not import relationship parameter value metadata: "
+                "parameter '{0}' doesn't have a value for relationship '{1}', alternative '{2}'".format(
+                    parameter_name, object_names, alternative_name
+                )
+            )
+            error_log.append(ImportErrorLogItem(msg=msg, db_type="relationship parameter value metadata"))
+            continue
+        for name, value in _parse_metadata(metadata):
+            m_id = metadata_ids.get((name, value), None)
+            if m_id is None:
+                msg = f"Could not import relationship parameter value metadata: unknown metadata '{name}': '{value}'"
+                error_log.append(ImportErrorLogItem(msg=msg, db_type="relationship parameter value metadata"))
+                continue
+            unique_key = (pv_id, m_id)
+            if unique_key in seen:
+                continue
+            item = {"parameter_value_id": pv_id, "metadata_id": m_id}
+            seen.add(unique_key)
+            to_add.append(item)
+    return to_add, [], error_log
```

### Comparing `spinedb_api-0.30.3/spinedb_api/import_mapping/__init__.py` & `spinedb_api-0.30.4/tests/spine_io/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Init file for tests.spine_io package. Intentionally empty.
+
+"""
```

### Comparing `spinedb_api-0.30.3/spinedb_api/import_mapping/generator.py` & `spinedb_api-0.30.4/spinedb_api/import_mapping/generator.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,392 +1,392 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains `get_mapped_data()` that converts rows of tabular data into a dictionary for import to a Spine DB,
-using ``import_functions.import_data()``
-
-"""
-
-from copy import deepcopy
-from .import_mapping_compat import import_mapping_from_dict
-from .import_mapping import ImportMapping, check_validity
-from ..mapping import Position
-from ..parameter_value import (
-    convert_leaf_maps_to_specialized_containers,
-    Map,
-    TimeSeriesVariableResolution,
-    TimePattern,
-    Array,
-    from_database,
-    split_value_and_type,
-)
-from ..exception import ParameterValueFormatError
-
-
-_NO_VALUE = object()
-
-
-def identity(x):
-    """Returns argument unchanged.
-
-    Args:
-        x (Any): value to return
-
-    Returns:
-        Any: x
-    """
-    return x
-
-
-def get_mapped_data(
-    data_source,
-    mappings,
-    data_header=None,
-    table_name="",
-    column_convert_fns=None,
-    default_column_convert_fn=None,
-    row_convert_fns=None,
-    unparse_value=identity,
-):
-    """
-    Args:
-        data_source (Iterable): Yields rows (lists)
-        mappings (list(ImportMapping)): Mappings from data rows into mapped data for ``import_data()``
-        data_header (list, optional): table header
-        table_name (str, optional): table name
-        column_convert_fns (dict(int,function), optional): mapping from column number to convert function
-        default_column_convert_fn (Callable, optional): default convert function for surplus columns
-        row_convert_fns (dict(int,function), optional): mapping from row number to convert function
-        unparse_value (Callable): a callable that converts values to database format
-
-    Returns:
-        dict: Mapped data, ready for ``import_data()``
-        list: Conversion errors
-    """
-    # Sanitize mappings
-    for k, mapping in enumerate(mappings):
-        if isinstance(mapping, (list, dict)):
-            mappings[k] = import_mapping_from_dict(mapping)
-        elif not isinstance(mapping, ImportMapping):
-            raise TypeError(f"mapping must be a dict or ImportMapping subclass, instead got: {type(mapping).__name__}")
-    mapped_data = {}
-    errors = []
-    rows = list(data_source)
-    if not rows:
-        return mapped_data, errors
-    if column_convert_fns is None:
-        column_convert_fns = {}
-    if row_convert_fns is None:
-        row_convert_fns = {}
-    if default_column_convert_fn is None:
-        default_column_convert_fn = column_convert_fns[max(column_convert_fns)] if column_convert_fns else identity
-    for mapping in mappings:
-        read_state = {}
-        mapping = deepcopy(mapping)
-        mapping.polish(table_name, data_header)
-        mapping_errors = check_validity(mapping)
-        if mapping_errors:
-            errors += mapping_errors
-            continue
-        # Find pivoted and unpivoted mappings
-        pivoted, non_pivoted, pivoted_from_header, last = _split_mapping(mapping)
-        # If there are no pivoted mappings, we can just feed the rows to our mapping directly
-        if not (pivoted or pivoted_from_header):
-            start_pos = mapping.read_start_row
-            for k, row in enumerate(rows[mapping.read_start_row :]):
-                if not _is_valid_row(row):
-                    continue
-                row = _convert_row(row, column_convert_fns, start_pos + k, errors)
-                mapping.import_row(row, read_state, mapped_data)
-            continue
-        # There are pivoted mappings. We unpivot the table
-        unpivoted_rows, pivoted_pos, non_pivoted_pos, unpivoted_column_pos = _unpivot_rows(
-            rows, data_header, pivoted, non_pivoted, pivoted_from_header, mapping.skip_columns
-        )
-        if not unpivoted_column_pos:
-            continue
-        # Reposition row convert functions
-        row_convert_fns = {k: row_convert_fns[pos] for k, pos in enumerate(pivoted_pos) if pos in row_convert_fns}
-        # If there are only pivoted mappings, we can just feed the unpivoted rows
-        if not non_pivoted:
-            # Reposition pivoted mappings:
-            last.position = -1
-            for k, m in enumerate(pivoted):
-                m.position = k
-            for k, row in enumerate(unpivoted_rows):
-                if not _is_valid_row(row):
-                    continue
-                row = _convert_row(row, row_convert_fns, k, errors)
-                mapping.import_row(row, read_state, mapped_data)
-            continue
-        # There are both pivoted and unpivoted mappings
-        # Reposition mappings:
-        # - The last mapping (typically, parameter value) will read from the last position in the row
-        # - The pivoted mappings will read from positions to the left of that
-        k = None
-        last.position = -1
-        for k, m in enumerate(reversed(pivoted)):
-            m.position = -(k + 2)
-        # Feed rows: To each regular row, we append each unpivoted row, plus the item at the intersection,
-        # and feed that to the mapping
-        last_pivoted_row_pos = max(pivoted_pos, default=0) + 1
-        last_non_pivoted_column_pos = max(non_pivoted_pos, default=0) + 1
-        start_pos = max(mapping.read_start_row, last_pivoted_row_pos)
-        min_row_length = max(unpivoted_column_pos)
-        for i, row in enumerate(rows[start_pos:]):
-            if len(row) < min_row_length + 1:
-                error = f"Could not process incomplete row {i + 1}"
-                errors.append(error)
-                continue
-            if not _is_valid_row(row[:last_non_pivoted_column_pos]):
-                continue
-            row = _convert_row(row, column_convert_fns, start_pos + i, errors, default_column_convert_fn)
-            non_pivoted_row = row[:last_non_pivoted_column_pos]
-            for column_pos, unpivoted_row in zip(unpivoted_column_pos, unpivoted_rows):
-                if not _is_valid_row(unpivoted_row):
-                    continue
-                unpivoted_row = _convert_row(unpivoted_row, row_convert_fns, k, errors)
-                full_row = non_pivoted_row + unpivoted_row
-                full_row.append(row[column_pos])
-                mapping.import_row(full_row, read_state, mapped_data)
-    _make_relationship_classes(mapped_data)
-    _make_parameter_values(mapped_data, unparse_value)
-    return mapped_data, errors
-
-
-def _is_valid_row(row):
-    return row is not None and not all(i is None for i in row)
-
-
-def _convert_row(row, convert_fns, row_number, errors, default_convert_fn=lambda x: x):
-    new_row = []
-    for j, item in enumerate(row):
-        if item is None:
-            new_row.append(item)
-            continue
-        convert_fn = convert_fns.get(j, default_convert_fn)
-        try:
-            item = convert_fn(item)
-        except (ValueError, ParameterValueFormatError):
-            error = f"Could not convert '{item}' to type '{convert_fn.DISPLAY_NAME}' (near row {row_number})"
-            errors.append(error)
-        new_row.append(item)
-    return new_row
-
-
-def _split_mapping(mapping):
-    """Splits the given mapping into pivot components.
-
-    Args:
-        mapping (ImportMapping)
-
-    Returns:
-        list(ImportMapping): Pivoted mappings (reading from rows)
-        list(ImportMapping): Non-pivoted mappings ('regular', reading from columns)
-        list(ImportMapping): Pivoted from header mappings
-        ImportMapping: last mapping (typically representing the parameter value)
-    """
-    flattened = mapping.flatten()
-    pivoted = []
-    non_pivoted = []
-    pivoted_from_header = []
-    for m in flattened:
-        if (pivoted or pivoted_from_header) and m is flattened[-1]:
-            # If any other mapping is pivoted, ignore last mapping's position
-            break
-        if m.position == Position.header and m.value is None:
-            pivoted_from_header.append(m)
-            continue
-        if not isinstance(m.position, int):
-            continue
-        if m.position < 0:
-            pivoted.append(m)
-        else:
-            non_pivoted.append(m)
-    return pivoted, non_pivoted, pivoted_from_header, flattened[-1]
-
-
-def _unpivot_rows(rows, data_header, pivoted, non_pivoted, pivoted_from_header, skip_columns):
-    """Unpivots rows.
-
-    Args:
-        rows (list of list): Source table rows
-        data_header (list): Source table header
-        pivoted (list of ImportMapping): Pivoted mappings (reading from rows)
-        non_pivoted (list of ImportMapping): Non-pivoted mappings ('regular', reading from columns)
-        pivoted_from_header (list of ImportMapping): Mappings pivoted from header
-
-    Returns:
-        list of list: Unpivoted rows
-        int: Position of last pivoted row
-        int: Position of last non-pivoted row
-        list of int: Columns positions corresponding to unpivoted rows
-    """
-    # First we collect pivoted and unpivoted positions
-    pivoted_pos = [-(m.position + 1) for m in pivoted]  # (-1) -> (0), (-2) -> (1), (-3) -> (2), etc.
-    non_pivoted_pos = [m.position for m in non_pivoted]
-    # Collect pivoted rows
-    pivoted_rows = [rows[pos] for pos in pivoted_pos] if non_pivoted_pos else rows
-    # Prepend as many headers as needed
-    for m in pivoted_from_header:
-        pivoted.insert(0, m)
-        pivoted_rows.insert(0, data_header)
-    if pivoted_from_header:
-        pivoted_pos.append(-1)  # This is so ``last_pivoted_row_pos`` below gets the right value
-    # Collect non pivoted and skipped positions
-    skip_pos = set(skip_columns) | set(non_pivoted_pos)
-    # Remove items in those positions from pivoted rows
-    if skip_pos:
-        pivoted_rows = [[item for k, item in enumerate(row) if k not in skip_pos] for row in pivoted_rows]
-    # Unpivot
-    unpivoted_rows = [list(row) for row in zip(*pivoted_rows)]
-    if not non_pivoted_pos:
-        last_pivoted_position = max(pivoted_pos)
-        if pivoted_from_header:
-            last_pivoted_position += 1
-        expanded_pivoted_rows = []
-        for row in unpivoted_rows:
-            head = row[: last_pivoted_position + 1]
-            for data in row[last_pivoted_position + 1 :]:
-                expanded_pivoted_rows.append(head + [data])
-        unpivoted_rows = expanded_pivoted_rows
-    unpivoted_column_pos = [k for k in range(len(rows[0])) if k not in skip_pos] if rows else []
-    return unpivoted_rows, pivoted_pos, non_pivoted_pos, unpivoted_column_pos
-
-
-def _make_relationship_classes(mapped_data):
-    rows = mapped_data.get("relationship_classes")
-    if rows is None:
-        return
-    full_rows = []
-    for class_name, object_classes in rows.items():
-        full_rows.append((class_name, object_classes))
-    mapped_data["relationship_classes"] = full_rows
-
-
-def _make_parameter_values(mapped_data, unparse_value):
-    value_pos = 3
-    for key in ("object_parameter_values", "relationship_parameter_values"):
-        rows = mapped_data.get(key)
-        if rows is None:
-            continue
-        valued_rows = []
-        for row in rows:
-            raw_value = _make_value(row, value_pos)
-            if raw_value is _NO_VALUE:
-                continue
-            value = unparse_value(raw_value)
-            if value is not None:
-                row[value_pos] = value
-                valued_rows.append(row)
-        mapped_data[key] = valued_rows
-    value_pos = 0
-    for key in ("object_parameters", "relationship_parameters"):
-        rows = mapped_data.get(key)
-        if rows is None:
-            continue
-        full_rows = []
-        for entity_definition, extras in rows.items():
-            if extras:
-                value = unparse_value(_make_value(extras, value_pos))
-                if value is not None:
-                    extras[value_pos] = value
-                    full_rows.append(entity_definition + tuple(extras))
-            else:
-                full_rows.append(entity_definition)
-        mapped_data[key] = full_rows
-
-
-def _make_value(row, value_pos):
-    try:
-        value = row[value_pos]
-    except IndexError:
-        return None
-    if isinstance(value, dict):
-        if "data" not in value:
-            return _NO_VALUE
-        return _parameter_value_from_dict(value)
-    if isinstance(value, str):
-        try:
-            return from_database(*split_value_and_type(value))
-        except ParameterValueFormatError:
-            pass
-    return value
-
-
-def _parameter_value_from_dict(d):
-    mapped_index_names = d.get("index_names", {0: ""})
-    index_names = (max(mapped_index_names) + 1) * [""]
-    for i, name in mapped_index_names.items():
-        index_names[i] = name
-    if d["type"] == "map":
-        map_ = _table_to_map(d["data"], compress=d.get("compress", False))
-        if index_names != [""]:
-            _apply_index_names(map_, index_names)
-        return map_
-    if d["type"] == "time_pattern":
-        return TimePattern(*zip(*d["data"]), index_name=index_names[0])
-    if d["type"] == "time_series":
-        options = d.get("options", {})
-        ignore_year = options.get("ignore_year", False)
-        repeat = options.get("repeat", False)
-        return TimeSeriesVariableResolution(*zip(*d["data"]), ignore_year, repeat, index_name=index_names[0])
-    if d["type"] == "array":
-        return Array(d["data"], index_name=index_names[0])
-
-
-def _table_to_map(table, compress=False):
-    d = _table_to_dict(table)
-    m = _dict_to_map_recursive(d)
-    if compress:
-        return convert_leaf_maps_to_specialized_containers(m)
-    return m
-
-
-def _table_to_dict(table):
-    map_dict = dict()
-    for row in table:
-        row = [item for item in row if item not in (None, "")]
-        if len(row) < 2:
-            continue
-        d = map_dict
-        for item in row[:-2]:
-            d = d.setdefault(item, dict())
-        d[row[-2]] = row[-1]
-    return map_dict
-
-
-def _dict_to_map_recursive(d):
-    indexes = list()
-    values = list()
-    for key, value in d.items():
-        if isinstance(value, dict):
-            value = _dict_to_map_recursive(value)
-        indexes.append(key)
-        values.append(value)
-    return Map(indexes, values)
-
-
-def _apply_index_names(map_, index_names):
-    """Applies index names to Map.
-
-    Args:
-        map_ (Map): target Map.
-        index_names (Sequence of str): index names, one for each Map depth
-    """
-    name = index_names[0]
-    if name:
-        map_.index_name = index_names[0]
-    if len(index_names) == 1:
-        return
-    for v in map_.values:
-        if isinstance(v, Map):
-            _apply_index_names(v, index_names[1:])
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains `get_mapped_data()` that converts rows of tabular data into a dictionary for import to a Spine DB,
+using ``import_functions.import_data()``
+
+"""
+
+from copy import deepcopy
+from .import_mapping_compat import import_mapping_from_dict
+from .import_mapping import ImportMapping, check_validity
+from ..mapping import Position
+from ..parameter_value import (
+    convert_leaf_maps_to_specialized_containers,
+    Map,
+    TimeSeriesVariableResolution,
+    TimePattern,
+    Array,
+    from_database,
+    split_value_and_type,
+)
+from ..exception import ParameterValueFormatError
+
+
+_NO_VALUE = object()
+
+
+def identity(x):
+    """Returns argument unchanged.
+
+    Args:
+        x (Any): value to return
+
+    Returns:
+        Any: x
+    """
+    return x
+
+
+def get_mapped_data(
+    data_source,
+    mappings,
+    data_header=None,
+    table_name="",
+    column_convert_fns=None,
+    default_column_convert_fn=None,
+    row_convert_fns=None,
+    unparse_value=identity,
+):
+    """
+    Args:
+        data_source (Iterable): Yields rows (lists)
+        mappings (list(ImportMapping)): Mappings from data rows into mapped data for ``import_data()``
+        data_header (list, optional): table header
+        table_name (str, optional): table name
+        column_convert_fns (dict(int,function), optional): mapping from column number to convert function
+        default_column_convert_fn (Callable, optional): default convert function for surplus columns
+        row_convert_fns (dict(int,function), optional): mapping from row number to convert function
+        unparse_value (Callable): a callable that converts values to database format
+
+    Returns:
+        dict: Mapped data, ready for ``import_data()``
+        list: Conversion errors
+    """
+    # Sanitize mappings
+    for k, mapping in enumerate(mappings):
+        if isinstance(mapping, (list, dict)):
+            mappings[k] = import_mapping_from_dict(mapping)
+        elif not isinstance(mapping, ImportMapping):
+            raise TypeError(f"mapping must be a dict or ImportMapping subclass, instead got: {type(mapping).__name__}")
+    mapped_data = {}
+    errors = []
+    rows = list(data_source)
+    if not rows:
+        return mapped_data, errors
+    if column_convert_fns is None:
+        column_convert_fns = {}
+    if row_convert_fns is None:
+        row_convert_fns = {}
+    if default_column_convert_fn is None:
+        default_column_convert_fn = column_convert_fns[max(column_convert_fns)] if column_convert_fns else identity
+    for mapping in mappings:
+        read_state = {}
+        mapping = deepcopy(mapping)
+        mapping.polish(table_name, data_header)
+        mapping_errors = check_validity(mapping)
+        if mapping_errors:
+            errors += mapping_errors
+            continue
+        # Find pivoted and unpivoted mappings
+        pivoted, non_pivoted, pivoted_from_header, last = _split_mapping(mapping)
+        # If there are no pivoted mappings, we can just feed the rows to our mapping directly
+        if not (pivoted or pivoted_from_header):
+            start_pos = mapping.read_start_row
+            for k, row in enumerate(rows[mapping.read_start_row :]):
+                if not _is_valid_row(row):
+                    continue
+                row = _convert_row(row, column_convert_fns, start_pos + k, errors)
+                mapping.import_row(row, read_state, mapped_data)
+            continue
+        # There are pivoted mappings. We unpivot the table
+        unpivoted_rows, pivoted_pos, non_pivoted_pos, unpivoted_column_pos = _unpivot_rows(
+            rows, data_header, pivoted, non_pivoted, pivoted_from_header, mapping.skip_columns
+        )
+        if not unpivoted_column_pos:
+            continue
+        # Reposition row convert functions
+        row_convert_fns = {k: row_convert_fns[pos] for k, pos in enumerate(pivoted_pos) if pos in row_convert_fns}
+        # If there are only pivoted mappings, we can just feed the unpivoted rows
+        if not non_pivoted:
+            # Reposition pivoted mappings:
+            last.position = -1
+            for k, m in enumerate(pivoted):
+                m.position = k
+            for k, row in enumerate(unpivoted_rows):
+                if not _is_valid_row(row):
+                    continue
+                row = _convert_row(row, row_convert_fns, k, errors)
+                mapping.import_row(row, read_state, mapped_data)
+            continue
+        # There are both pivoted and unpivoted mappings
+        # Reposition mappings:
+        # - The last mapping (typically, parameter value) will read from the last position in the row
+        # - The pivoted mappings will read from positions to the left of that
+        k = None
+        last.position = -1
+        for k, m in enumerate(reversed(pivoted)):
+            m.position = -(k + 2)
+        # Feed rows: To each regular row, we append each unpivoted row, plus the item at the intersection,
+        # and feed that to the mapping
+        last_pivoted_row_pos = max(pivoted_pos, default=0) + 1
+        last_non_pivoted_column_pos = max(non_pivoted_pos, default=0) + 1
+        start_pos = max(mapping.read_start_row, last_pivoted_row_pos)
+        min_row_length = max(unpivoted_column_pos)
+        for i, row in enumerate(rows[start_pos:]):
+            if len(row) < min_row_length + 1:
+                error = f"Could not process incomplete row {i + 1}"
+                errors.append(error)
+                continue
+            if not _is_valid_row(row[:last_non_pivoted_column_pos]):
+                continue
+            row = _convert_row(row, column_convert_fns, start_pos + i, errors, default_column_convert_fn)
+            non_pivoted_row = row[:last_non_pivoted_column_pos]
+            for column_pos, unpivoted_row in zip(unpivoted_column_pos, unpivoted_rows):
+                if not _is_valid_row(unpivoted_row):
+                    continue
+                unpivoted_row = _convert_row(unpivoted_row, row_convert_fns, k, errors)
+                full_row = non_pivoted_row + unpivoted_row
+                full_row.append(row[column_pos])
+                mapping.import_row(full_row, read_state, mapped_data)
+    _make_relationship_classes(mapped_data)
+    _make_parameter_values(mapped_data, unparse_value)
+    return mapped_data, errors
+
+
+def _is_valid_row(row):
+    return row is not None and not all(i is None for i in row)
+
+
+def _convert_row(row, convert_fns, row_number, errors, default_convert_fn=lambda x: x):
+    new_row = []
+    for j, item in enumerate(row):
+        if item is None:
+            new_row.append(item)
+            continue
+        convert_fn = convert_fns.get(j, default_convert_fn)
+        try:
+            item = convert_fn(item)
+        except (ValueError, ParameterValueFormatError):
+            error = f"Could not convert '{item}' to type '{convert_fn.DISPLAY_NAME}' (near row {row_number})"
+            errors.append(error)
+        new_row.append(item)
+    return new_row
+
+
+def _split_mapping(mapping):
+    """Splits the given mapping into pivot components.
+
+    Args:
+        mapping (ImportMapping)
+
+    Returns:
+        list(ImportMapping): Pivoted mappings (reading from rows)
+        list(ImportMapping): Non-pivoted mappings ('regular', reading from columns)
+        list(ImportMapping): Pivoted from header mappings
+        ImportMapping: last mapping (typically representing the parameter value)
+    """
+    flattened = mapping.flatten()
+    pivoted = []
+    non_pivoted = []
+    pivoted_from_header = []
+    for m in flattened:
+        if (pivoted or pivoted_from_header) and m is flattened[-1]:
+            # If any other mapping is pivoted, ignore last mapping's position
+            break
+        if m.position == Position.header and m.value is None:
+            pivoted_from_header.append(m)
+            continue
+        if not isinstance(m.position, int):
+            continue
+        if m.position < 0:
+            pivoted.append(m)
+        else:
+            non_pivoted.append(m)
+    return pivoted, non_pivoted, pivoted_from_header, flattened[-1]
+
+
+def _unpivot_rows(rows, data_header, pivoted, non_pivoted, pivoted_from_header, skip_columns):
+    """Unpivots rows.
+
+    Args:
+        rows (list of list): Source table rows
+        data_header (list): Source table header
+        pivoted (list of ImportMapping): Pivoted mappings (reading from rows)
+        non_pivoted (list of ImportMapping): Non-pivoted mappings ('regular', reading from columns)
+        pivoted_from_header (list of ImportMapping): Mappings pivoted from header
+
+    Returns:
+        list of list: Unpivoted rows
+        int: Position of last pivoted row
+        int: Position of last non-pivoted row
+        list of int: Columns positions corresponding to unpivoted rows
+    """
+    # First we collect pivoted and unpivoted positions
+    pivoted_pos = [-(m.position + 1) for m in pivoted]  # (-1) -> (0), (-2) -> (1), (-3) -> (2), etc.
+    non_pivoted_pos = [m.position for m in non_pivoted]
+    # Collect pivoted rows
+    pivoted_rows = [rows[pos] for pos in pivoted_pos] if non_pivoted_pos else rows
+    # Prepend as many headers as needed
+    for m in pivoted_from_header:
+        pivoted.insert(0, m)
+        pivoted_rows.insert(0, data_header)
+    if pivoted_from_header:
+        pivoted_pos.append(-1)  # This is so ``last_pivoted_row_pos`` below gets the right value
+    # Collect non pivoted and skipped positions
+    skip_pos = set(skip_columns) | set(non_pivoted_pos)
+    # Remove items in those positions from pivoted rows
+    if skip_pos:
+        pivoted_rows = [[item for k, item in enumerate(row) if k not in skip_pos] for row in pivoted_rows]
+    # Unpivot
+    unpivoted_rows = [list(row) for row in zip(*pivoted_rows)]
+    if not non_pivoted_pos:
+        last_pivoted_position = max(pivoted_pos)
+        if pivoted_from_header:
+            last_pivoted_position += 1
+        expanded_pivoted_rows = []
+        for row in unpivoted_rows:
+            head = row[: last_pivoted_position + 1]
+            for data in row[last_pivoted_position + 1 :]:
+                expanded_pivoted_rows.append(head + [data])
+        unpivoted_rows = expanded_pivoted_rows
+    unpivoted_column_pos = [k for k in range(len(rows[0])) if k not in skip_pos] if rows else []
+    return unpivoted_rows, pivoted_pos, non_pivoted_pos, unpivoted_column_pos
+
+
+def _make_relationship_classes(mapped_data):
+    rows = mapped_data.get("relationship_classes")
+    if rows is None:
+        return
+    full_rows = []
+    for class_name, object_classes in rows.items():
+        full_rows.append((class_name, object_classes))
+    mapped_data["relationship_classes"] = full_rows
+
+
+def _make_parameter_values(mapped_data, unparse_value):
+    value_pos = 3
+    for key in ("object_parameter_values", "relationship_parameter_values"):
+        rows = mapped_data.get(key)
+        if rows is None:
+            continue
+        valued_rows = []
+        for row in rows:
+            raw_value = _make_value(row, value_pos)
+            if raw_value is _NO_VALUE:
+                continue
+            value = unparse_value(raw_value)
+            if value is not None:
+                row[value_pos] = value
+                valued_rows.append(row)
+        mapped_data[key] = valued_rows
+    value_pos = 0
+    for key in ("object_parameters", "relationship_parameters"):
+        rows = mapped_data.get(key)
+        if rows is None:
+            continue
+        full_rows = []
+        for entity_definition, extras in rows.items():
+            if extras:
+                value = unparse_value(_make_value(extras, value_pos))
+                if value is not None:
+                    extras[value_pos] = value
+                    full_rows.append(entity_definition + tuple(extras))
+            else:
+                full_rows.append(entity_definition)
+        mapped_data[key] = full_rows
+
+
+def _make_value(row, value_pos):
+    try:
+        value = row[value_pos]
+    except IndexError:
+        return None
+    if isinstance(value, dict):
+        if "data" not in value:
+            return _NO_VALUE
+        return _parameter_value_from_dict(value)
+    if isinstance(value, str):
+        try:
+            return from_database(*split_value_and_type(value))
+        except ParameterValueFormatError:
+            pass
+    return value
+
+
+def _parameter_value_from_dict(d):
+    mapped_index_names = d.get("index_names", {0: ""})
+    index_names = (max(mapped_index_names) + 1) * [""]
+    for i, name in mapped_index_names.items():
+        index_names[i] = name
+    if d["type"] == "map":
+        map_ = _table_to_map(d["data"], compress=d.get("compress", False))
+        if index_names != [""]:
+            _apply_index_names(map_, index_names)
+        return map_
+    if d["type"] == "time_pattern":
+        return TimePattern(*zip(*d["data"]), index_name=index_names[0])
+    if d["type"] == "time_series":
+        options = d.get("options", {})
+        ignore_year = options.get("ignore_year", False)
+        repeat = options.get("repeat", False)
+        return TimeSeriesVariableResolution(*zip(*d["data"]), ignore_year, repeat, index_name=index_names[0])
+    if d["type"] == "array":
+        return Array(d["data"], index_name=index_names[0])
+
+
+def _table_to_map(table, compress=False):
+    d = _table_to_dict(table)
+    m = _dict_to_map_recursive(d)
+    if compress:
+        return convert_leaf_maps_to_specialized_containers(m)
+    return m
+
+
+def _table_to_dict(table):
+    map_dict = dict()
+    for row in table:
+        row = [item for item in row if item not in (None, "")]
+        if len(row) < 2:
+            continue
+        d = map_dict
+        for item in row[:-2]:
+            d = d.setdefault(item, dict())
+        d[row[-2]] = row[-1]
+    return map_dict
+
+
+def _dict_to_map_recursive(d):
+    indexes = list()
+    values = list()
+    for key, value in d.items():
+        if isinstance(value, dict):
+            value = _dict_to_map_recursive(value)
+        indexes.append(key)
+        values.append(value)
+    return Map(indexes, values)
+
+
+def _apply_index_names(map_, index_names):
+    """Applies index names to Map.
+
+    Args:
+        map_ (Map): target Map.
+        index_names (Sequence of str): index names, one for each Map depth
+    """
+    name = index_names[0]
+    if name:
+        map_.index_name = index_names[0]
+    if len(index_names) == 1:
+        return
+    for v in map_.values:
+        if isinstance(v, Map):
+            _apply_index_names(v, index_names[1:])
```

### Comparing `spinedb_api-0.30.3/spinedb_api/import_mapping/import_mapping.py` & `spinedb_api-0.30.4/spinedb_api/import_mapping/import_mapping.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,1157 +1,1157 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Contains import mappings for database items such as entities, entity classes and parameter values.
-
-"""
-
-from distutils.util import strtobool
-from enum import auto, Enum, unique
-from spinedb_api.mapping import Mapping, Position, unflatten, is_pivoted
-from spinedb_api.exception import InvalidMappingComponent
-
-
-@unique
-class ImportKey(Enum):
-    CLASS_NAME = auto()
-    RELATIONSHIP_DIMENSION_COUNT = auto()
-    OBJECT_CLASS_NAME = auto()
-    OBJECT_NAME = auto()
-    GROUP_NAME = auto()
-    MEMBER_NAME = auto()
-    PARAMETER_NAME = auto()
-    PARAMETER_DEFINITION = auto()
-    PARAMETER_DEFINITION_EXTRAS = auto()
-    PARAMETER_DEFAULT_VALUES = auto()
-    PARAMETER_DEFAULT_VALUE_INDEXES = auto()
-    PARAMETER_VALUES = auto()
-    PARAMETER_VALUE_INDEXES = auto()
-    RELATIONSHIP_CLASS_NAME = auto()
-    OBJECT_CLASS_NAMES = auto()
-    OBJECT_NAMES = auto()
-    ALTERNATIVE_NAME = auto()
-    SCENARIO_NAME = auto()
-    SCENARIO_ALTERNATIVE = auto()
-    FEATURE = auto()
-    TOOL_NAME = auto()
-    TOOL_FEATURE = auto()
-    TOOL_FEATURE_METHOD = auto()
-    PARAMETER_VALUE_LIST_NAME = auto()
-
-    def __str__(self):
-        name = {
-            self.ALTERNATIVE_NAME.value: "Alternative names",
-            self.CLASS_NAME.value: "Class names",
-            self.OBJECT_CLASS_NAME.value: "Object class names",
-            self.OBJECT_NAME.value: "Object names",
-            self.GROUP_NAME.value: "Group names",
-            self.MEMBER_NAME.value: "Member names",
-            self.PARAMETER_NAME.value: "Parameter names",
-            self.PARAMETER_DEFINITION.value: "Parameter names",
-            self.PARAMETER_DEFAULT_VALUE_INDEXES.value: "Parameter indexes",
-            self.PARAMETER_VALUE_INDEXES.value: "Parameter indexes",
-            self.RELATIONSHIP_CLASS_NAME.value: "Relationship class names",
-            self.OBJECT_CLASS_NAMES.value: "Object class names",
-            self.OBJECT_NAMES.value: "Object names",
-            self.PARAMETER_VALUE_LIST_NAME.value: "Parameter value lists",
-            self.SCENARIO_NAME.value: "Scenario names",
-            self.SCENARIO_ALTERNATIVE.value: "Alternative names",
-            self.TOOL_NAME.value: "Tool names",
-            self.FEATURE.value: "Entity class names",
-            self.TOOL_FEATURE.value: "Entity class names",
-            self.TOOL_FEATURE_METHOD.value: "Entity class names",
-        }.get(self.value)
-        if name is not None:
-            return name
-        return super().__str__()
-
-
-class KeyFix(Exception):
-    """Opposite of KeyError"""
-
-
-def check_validity(root_mapping):
-    class _DummySourceRow:
-        def __getitem__(self, key):
-            return "true"
-
-    errors = []
-    source_row = _DummySourceRow()
-    root_mapping.import_row(source_row, {}, {}, errors)
-    return errors
-
-
-class ImportMapping(Mapping):
-    """Base class for import mappings."""
-
-    def __init__(self, position, value=None, skip_columns=None, read_start_row=0, filter_re=""):
-        """
-        Args:
-            position (int or Position): what to map in the source table
-            value (Any, optional): fixed value
-            skip_columns (Iterable of int, optional): index of columns that should be skipped;
-                useful when source is pivoted
-            read_start_row (int): at which source row importing should start
-            filter_re (str): regular expression for filtering
-        """
-        super().__init__(position, value, filter_re)
-        self._skip_columns = None
-        self._read_start_row = None
-        self.skip_columns = skip_columns
-        self.read_start_row = read_start_row
-        self._has_filter_cached = None
-        self._index = None
-
-    @property
-    def skip_columns(self):
-        return self._skip_columns
-
-    @skip_columns.setter
-    def skip_columns(self, skip_columns=None):
-        if skip_columns is None:
-            self._skip_columns = []
-            return
-        if isinstance(skip_columns, (str, int)):
-            self._skip_columns = [skip_columns]
-            return
-        if isinstance(skip_columns, list):
-            bad_types = [
-                f"{type(column).__name__} at index {i}"
-                for i, column in enumerate(skip_columns)
-                if not isinstance(column, (str, int))
-            ]
-            if bad_types:
-                bad_types = ", ".join(bad_types)
-                raise TypeError(f"skip_columns must be str, int or list of str, int, instead got list with {bad_types}")
-            self._skip_columns = skip_columns
-            return
-        raise TypeError(f"skip_columns must be str, int or list of str, int, instead got {type(skip_columns).__name__}")
-
-    @property
-    def read_start_row(self):
-        return self._read_start_row
-
-    @read_start_row.setter
-    def read_start_row(self, row):
-        if not isinstance(row, int):
-            raise TypeError(f"row must be int, instead got {type(row).__name__}")
-        if row < 0:
-            raise ValueError(f"row must be >= 0 ({row})")
-        self._read_start_row = row
-
-    def polish(self, table_name, source_header, for_preview=False):
-        """Polishes the mapping before an import operation.
-        'Expands' transient ``position`` and ``value`` attributes into their final value.
-
-        Args:
-            table_name (str)
-            source_header (list(str))
-        """
-        self._polish_for_import(table_name, source_header)
-        if for_preview:
-            self._polish_for_preview(source_header)
-
-    def _polish_for_import(self, table_name, source_header):
-        # FIXME: Polish skip columns
-        if self.child is not None:
-            self.child._polish_for_import(table_name, source_header)
-        if isinstance(self.position, str):
-            # Column mapping with string position, we need to find the index in the header
-            try:
-                self.position = source_header.index(self.position)
-                return
-            except ValueError:
-                msg = f"'{self.position}' is not in '{source_header}'"
-                raise InvalidMappingComponent(msg)
-        if self.position == Position.table_name:
-            # Table name mapping, we set the fixed value to the table name
-            self.value = table_name
-            return
-        if self.position == Position.header:
-            if self.value is None:
-                # Row mapping from header, we handle this one separately
-                return
-            # Column header mapping, the value indicates which field
-            if isinstance(self.value, str):
-                # If the value is indeed in the header, we're good
-                if self.value in source_header:
-                    return
-                try:
-                    # Not in the header, maybe it's a stringified index?
-                    self.value = int(self.value)
-                except ValueError:
-                    msg = f"'{self.value}' is not in header '{source_header}'"
-                    raise InvalidMappingComponent(msg)
-            # Integer value, we try and get the actual value from that index in the header
-            try:
-                self._index = self.value
-                self.value = source_header[self.value]
-            except IndexError:
-                msg = f"'{self.value}' is not a valid index in header '{source_header}'"
-                raise InvalidMappingComponent(msg)
-
-    def _polish_for_preview(self, source_header):
-        if self.position == Position.header and self.value is not None:
-            self.value = self._index
-        if self.child is not None:
-            self.child._polish_for_preview(source_header)
-
-    @property
-    def rank(self):
-        if self.parent is None:
-            return 0
-        return self.parent.rank + 1
-
-    def _filter_accepts_row(self, source_row):
-        """Whether or not the row passes the filter for this mapping."""
-        if self.position == Position.hidden and self.value is None:
-            return True
-        if self._filter_re is None:
-            return True
-        source_data = self._data(source_row)
-        return self._filter_re.search(str(source_data)) is not None
-
-    def has_filter(self):
-        """Whether mapping or one of its children has filter configured.
-
-        Returns:
-            bool: True if mapping or one of its children has filter configured , False otherwise
-        """
-        if self._has_filter_cached is None:
-            child_has_filter = self._child.has_filter() if self._child is not None else False
-            has_filter = (self.position != Position.hidden or self.value is not None) and self._filter_re is not None
-            self._has_filter_cached = child_has_filter or has_filter
-        return self._has_filter_cached
-
-    def filter_accepts_row(self, source_row):
-        """Whether or not the row passes the filter for all mappings in the hierarchy."""
-        return self._filter_accepts_row(source_row) and (
-            self.child is None or self.child.filter_accepts_row(source_row)
-        )
-
-    def import_row(self, source_row, state, mapped_data, errors=None):
-        if self.has_filter() and not self.filter_accepts_row(source_row):
-            return
-        if errors is None:
-            errors = []
-        if not (self.position == Position.hidden and self.value is None):
-            source_data = self._data(source_row)
-            if source_data is None:
-                self._skip_row(state)
-            else:
-                try:
-                    self._import_row(source_data, state, mapped_data)
-                except KeyError as err:
-                    for key in err.args:
-                        msg = f"Required key '{key}' is invalid"
-                        error = InvalidMappingComponent(msg, self.rank, key)
-                        errors.append(error)
-                except KeyFix as fix:
-                    indexes = set()
-                    for key in fix.args:
-                        indexes |= {k for k, err in enumerate(errors) if err.key == key}
-                    for k in sorted(indexes, reverse=True):
-                        errors.pop(k)
-        if self.child is not None:
-            self.child.import_row(source_row, state, mapped_data, errors=errors)
-
-    def _data(self, source_row):  # pylint: disable=arguments-differ
-        if source_row is None:
-            return None
-        return source_row[self.position]
-
-    def _import_row(self, source_data, state, mapped_data):
-        raise NotImplementedError()
-
-    def _skip_row(self, state):
-        """Called when the source data is None. Do necessary clean ups on state."""
-
-    def is_constant(self):
-        return self.position == Position.hidden and self.value is not None
-
-    def is_pivoted(self):
-        if is_pivoted(self.position):
-            return True
-        if self.position == Position.header and self.value is None and self.child is not None:
-            return True
-        if self.child is None:
-            return False
-        return self.child.is_pivoted()
-
-    def to_dict(self):
-        d = super().to_dict()
-        if self.skip_columns:
-            d["skip_columns"] = self.skip_columns
-        if self.read_start_row:
-            d["read_start_row"] = self.read_start_row
-        return d
-
-    @classmethod
-    def reconstruct(cls, position, value, skip_columns, read_start_row, filter_re, mapping_dict):
-        """
-        Reconstructs mapping.
-
-        Args:
-            position (int or Position, optional): mapping's position
-            value (Any): fixed value
-            skip_columns (Iterable of Int, optional): skipped columns
-            read_start_row (int): first source row to read
-            filter_re (str): filter regular expression
-            mapping_dict (dict): serialized mapping
-
-        Returns:
-            Mapping: reconstructed mapping
-        """
-        mapping = cls(position, value, skip_columns, read_start_row, filter_re)
-        return mapping
-
-
-class ImportObjectsMixin:
-    def __init__(self, position, value=None, skip_columns=None, read_start_row=0, filter_re="", import_objects=False):
-        super().__init__(position, value, skip_columns, read_start_row, filter_re)
-        self.import_objects = import_objects
-
-    def to_dict(self):
-        d = super().to_dict()
-        if self.import_objects:
-            d["import_objects"] = True
-        return d
-
-    @classmethod
-    def reconstruct(cls, position, value, skip_columns, read_start_row, filter_re, mapping_dict):
-        import_objects = mapping_dict.get("import_objects", False)
-        mapping = cls(position, value, skip_columns, read_start_row, filter_re, import_objects)
-        return mapping
-
-
-class IndexedValueMixin:
-    def __init__(
-        self, position, value=None, skip_columns=None, read_start_row=0, filter_re="", compress=False, options=None
-    ):
-        super().__init__(position, value, skip_columns, read_start_row, filter_re)
-        if options is None:
-            options = {}
-        self.compress = compress
-        self.options = options
-
-    def to_dict(self):
-        d = super().to_dict()
-        if self.compress:
-            d["compress"] = True
-        if self.options:
-            d["options"] = self.options
-        return d
-
-    @classmethod
-    def reconstruct(cls, position, value, skip_columns, read_start_row, filter_re, mapping_dict):
-        compress = mapping_dict.get("compress", False)
-        options = mapping_dict.get("options")
-        mapping = cls(position, value, skip_columns, read_start_row, filter_re, compress, options)
-        return mapping
-
-
-class ObjectClassMapping(ImportMapping):
-    """Maps object classes.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "ObjectClass"
-
-    def _import_row(self, source_data, state, mapped_data):
-        object_class_name = state[ImportKey.OBJECT_CLASS_NAME] = str(source_data)
-        object_classes = mapped_data.setdefault("object_classes", set())
-        object_classes.add(object_class_name)
-
-
-class ObjectMapping(ImportMapping):
-    """Maps objects.
-
-    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectClassMapping`.
-    """
-
-    MAP_TYPE = "Object"
-
-    def _import_row(self, source_data, state, mapped_data):
-        object_class_name = state[ImportKey.OBJECT_CLASS_NAME]
-        object_name = state[ImportKey.OBJECT_NAME] = str(source_data)
-        if isinstance(self.child, ObjectGroupMapping):
-            raise KeyError(ImportKey.MEMBER_NAME)
-        mapped_data.setdefault("objects", set()).add((object_class_name, object_name))
-
-
-class ObjectMetadataMapping(ImportMapping):
-    """Maps object metadata.
-
-    Cannot be used as the topmost mapping; must have :class:`ObjectClassMapping` and :class:`ObjectMapping` as parents.
-    """
-
-    MAP_TYPE = "ObjectMetadata"
-
-    def _import_row(self, source_data, state, mapped_data):
-        pass
-
-
-class ObjectGroupMapping(ImportObjectsMixin, ImportMapping):
-    """Maps object groups.
-
-    Cannot be used as the topmost mapping; must have :class:`ObjectClassMapping` and :class:`ObjectMapping` as parents.
-    """
-
-    MAP_TYPE = "ObjectGroup"
-
-    def _import_row(self, source_data, state, mapped_data):
-        object_class_name = state[ImportKey.OBJECT_CLASS_NAME]
-        group_name = state.get(ImportKey.OBJECT_NAME)
-        if group_name is None:
-            raise KeyError(ImportKey.GROUP_NAME)
-        member_name = str(source_data)
-        mapped_data.setdefault("object_groups", set()).add((object_class_name, group_name, member_name))
-        if self.import_objects:
-            objects = (object_class_name, group_name), (object_class_name, member_name)
-            mapped_data.setdefault("objects", set()).update(objects)
-        raise KeyFix(ImportKey.MEMBER_NAME)
-
-
-class RelationshipClassMapping(ImportMapping):
-    """Maps relationships classes.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "RelationshipClass"
-
-    def _import_row(self, source_data, state, mapped_data):
-        dim_count = len([m for m in self.flatten() if isinstance(m, RelationshipClassObjectClassMapping)])
-        state[ImportKey.RELATIONSHIP_DIMENSION_COUNT] = dim_count
-        relationship_class_name = state[ImportKey.RELATIONSHIP_CLASS_NAME] = str(source_data)
-        object_class_names = state[ImportKey.OBJECT_CLASS_NAMES] = []
-        relationship_classes = mapped_data.setdefault("relationship_classes", dict())
-        relationship_classes[relationship_class_name] = object_class_names
-        raise KeyError(ImportKey.OBJECT_CLASS_NAMES)
-
-
-class RelationshipClassObjectClassMapping(ImportMapping):
-    """Maps relationship class object classes.
-
-    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
-    """
-
-    MAP_TYPE = "RelationshipClassObjectClass"
-
-    def _import_row(self, source_data, state, mapped_data):
-        _ = state[ImportKey.RELATIONSHIP_CLASS_NAME]
-        object_class_names = state[ImportKey.OBJECT_CLASS_NAMES]
-        object_class_name = str(source_data)
-        object_class_names.append(object_class_name)
-        if len(object_class_names) == state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
-            raise KeyFix(ImportKey.OBJECT_CLASS_NAMES)
-
-
-class RelationshipMapping(ImportMapping):
-    """Maps relationships.
-
-    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
-    """
-
-    MAP_TYPE = "Relationship"
-
-    def _import_row(self, source_data, state, mapped_data):
-        # Don't access state[ImportKey.RELATIONSHIP_CLASS_NAME], we don't want to catch errors here
-        # because this one's invisible.
-        state[ImportKey.OBJECT_NAMES] = []
-
-
-class RelationshipObjectMapping(ImportObjectsMixin, ImportMapping):
-    """Maps relationship's objects.
-
-    Cannot be used as the topmost mapping; must have :class:`RelationshipClassMapping` and :class:`RelationshipMapping`
-    as parents.
-    """
-
-    MAP_TYPE = "RelationshipObject"
-
-    def _import_row(self, source_data, state, mapped_data):
-        relationship_class_name = state[ImportKey.RELATIONSHIP_CLASS_NAME]
-        object_class_names = state[ImportKey.OBJECT_CLASS_NAMES]
-        if len(object_class_names) != state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
-            raise KeyError(ImportKey.OBJECT_CLASS_NAMES)
-        object_names = state[ImportKey.OBJECT_NAMES]
-        object_name = str(source_data)
-        object_names.append(object_name)
-        if self.import_objects:
-            k = len(object_names) - 1
-            object_class_name = object_class_names[k]
-            mapped_data.setdefault("object_classes", set()).add(object_class_name)
-            mapped_data.setdefault("objects", set()).add((object_class_name, object_name))
-        if len(object_names) == state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
-            relationships = mapped_data.setdefault("relationships", set())
-            relationships.add((relationship_class_name, tuple(object_names)))
-            raise KeyFix(ImportKey.OBJECT_NAMES)
-        raise KeyError(ImportKey.OBJECT_NAMES)
-
-
-class RelationshipMetadataMapping(ImportMapping):
-    """Maps relationship metadata.
-
-    Cannot be used as the topmost mapping; must have :class:`RelationshipClassMapping`, a :class:`RelationshipMapping`
-    and one or more :class:`RelationshipObjectMapping` as parents.
-    """
-
-    MAP_TYPE = "RelationshipMetadata"
-
-    def _import_row(self, source_data, state, mapped_data):
-        pass
-
-
-class ParameterDefinitionMapping(ImportMapping):
-    """Maps parameter definitions.
-
-    Cannot be used as the topmost mapping; must have an entity class mapping as one of parents.
-    """
-
-    MAP_TYPE = "ParameterDefinition"
-
-    def _import_row(self, source_data, state, mapped_data):
-        object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
-        if object_class_name is not None:
-            class_name = object_class_name
-            map_key = "object_parameters"
-        else:
-            relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
-            if relationship_class_name is not None:
-                class_name = relationship_class_name
-                map_key = "relationship_parameters"
-            else:
-                raise KeyError(ImportKey.CLASS_NAME)
-        parameter_name = state[ImportKey.PARAMETER_NAME] = str(source_data)
-        definition_extras = state[ImportKey.PARAMETER_DEFINITION_EXTRAS] = []
-        parameter_definition_key = state[ImportKey.PARAMETER_DEFINITION] = class_name, parameter_name
-        default_values = state.get(ImportKey.PARAMETER_DEFAULT_VALUES)
-        if default_values is None or parameter_definition_key not in default_values:
-            mapped_data.setdefault(map_key, dict())[parameter_definition_key] = definition_extras
-
-
-class ParameterDefaultValueMapping(ImportMapping):
-    """Maps scalar (non-indexed) default values
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
-    """
-
-    MAP_TYPE = "ParameterDefaultValue"
-
-    def _import_row(self, source_data, state, mapped_data):
-        default_value = source_data
-        if default_value == "":
-            return
-        parameter_definition_extras = state[ImportKey.PARAMETER_DEFINITION_EXTRAS]
-        parameter_definition_extras.append(default_value)
-        value_list_name = state.get(ImportKey.PARAMETER_VALUE_LIST_NAME)
-        if value_list_name is not None:
-            parameter_definition_extras.append(value_list_name)
-
-
-class ParameterDefaultValueTypeMapping(IndexedValueMixin, ImportMapping):
-    MAP_TYPE = "ParameterDefaultValueType"
-
-    def _import_row(self, source_data, state, mapped_data):
-        parameter_definition = state.get(ImportKey.PARAMETER_DEFINITION)
-        if parameter_definition is None:
-            # Don't catch errors here, this one's invisible
-            return
-        default_values = state.setdefault(ImportKey.PARAMETER_DEFAULT_VALUES, {})
-        if parameter_definition in default_values:
-            return
-        value_type = str(source_data)
-        default_value = default_values[parameter_definition] = {"type": value_type}
-        if self.compress and value_type == "map":
-            default_value["compress"] = self.compress
-        if self.options and value_type == "time_series":
-            default_value["options"] = self.options
-        parameter_definition_extras = state[ImportKey.PARAMETER_DEFINITION_EXTRAS]
-        parameter_definition_extras.append(default_value)
-        value_list_name = state.get(ImportKey.PARAMETER_VALUE_LIST_NAME)
-        if value_list_name is not None:
-            parameter_definition_extras.append(value_list_name)
-
-
-class IndexNameMappingBase(ImportMapping):
-    """Base class for index name mappings."""
-
-    _STATE_KEY = NotImplemented
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._id = None
-
-    def _value_key(self, state):
-        raise NotImplementedError()
-
-    def _import_row(self, source_data, state, mapped_data):
-        values = state[self._STATE_KEY]
-        value = values[self._value_key(state)]
-        if self._id is None:
-            self._id = 0
-            current = self
-            while True:
-                if current.parent is None:
-                    break
-                current = current.parent
-                if isinstance(current, type(self)):
-                    self._id += 1
-        value.setdefault("index_names", {})[self._id] = source_data
-
-
-class DefaultValueIndexNameMapping(IndexNameMappingBase):
-    """Maps default value index names.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefaultValueTypeMapping` as parent.
-    """
-
-    MAP_TYPE = "DefaultValueIndexName"
-    _STATE_KEY = ImportKey.PARAMETER_DEFAULT_VALUES
-
-    def _value_key(self, state):
-        return _default_value_key(state)
-
-
-class ParameterDefaultValueIndexMapping(ImportMapping):
-    """Maps default value indexes.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
-    """
-
-    MAP_TYPE = "ParameterDefaultValueIndex"
-
-    def _import_row(self, source_data, state, mapped_data):
-        _ = state[ImportKey.PARAMETER_NAME]
-        index = source_data
-        state.setdefault(ImportKey.PARAMETER_DEFAULT_VALUE_INDEXES, []).append(index)
-
-
-class ExpandedParameterDefaultValueMapping(ImportMapping):
-    """Maps indexed default values.
-
-    Whenever this mapping is a child of :class:`ParameterDefaultValueIndexMapping`, it maps individual values of
-    indexed parameters.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
-    """
-
-    MAP_TYPE = "ExpandedDefaultValue"
-
-    def _import_row(self, source_data, state, mapped_data):
-        values = state.setdefault(ImportKey.PARAMETER_DEFAULT_VALUES, {})
-        value = values[_default_value_key(state)]
-        val = source_data
-        data = value.setdefault("data", [])
-        if value["type"] == "array":
-            data.append(val)
-            return
-        indexes = state.pop(ImportKey.PARAMETER_DEFAULT_VALUE_INDEXES)
-        data.append(indexes + [val])
-
-    def _skip_row(self, state):
-        state.pop(ImportKey.PARAMETER_DEFAULT_VALUE_INDEXES, None)
-
-
-class ParameterValueMapping(ImportMapping):
-    """Maps scalar (non-indexed) parameter values.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
-    an :class:`AlternativeMapping` as parents.
-    """
-
-    MAP_TYPE = "ParameterValue"
-
-    def _import_row(self, source_data, state, mapped_data):
-        value = source_data
-        if value == "":
-            return
-        object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
-        relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
-        if object_class_name is not None:
-            class_name = object_class_name
-            entity_name = state[ImportKey.OBJECT_NAME]
-            map_key = "object_parameter_values"
-        elif relationship_class_name is not None:
-            object_names = state[ImportKey.OBJECT_NAMES]
-            if len(object_names) != state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
-                raise KeyError(ImportKey.OBJECT_NAMES)
-            class_name = relationship_class_name
-            entity_name = object_names
-            map_key = "relationship_parameter_values"
-        else:
-            raise KeyError(ImportKey.CLASS_NAME)
-        parameter_name = state[ImportKey.PARAMETER_NAME]
-        parameter_value = [class_name, entity_name, parameter_name, value]
-        alternative_name = state.get(ImportKey.ALTERNATIVE_NAME)
-        if alternative_name is not None:
-            parameter_value.append(alternative_name)
-        mapped_data.setdefault(map_key, list()).append(parameter_value)
-
-
-class ParameterValueTypeMapping(IndexedValueMixin, ImportMapping):
-    MAP_TYPE = "ParameterValueType"
-
-    def _import_row(self, source_data, state, mapped_data):
-        parameter_name = state.get(ImportKey.PARAMETER_NAME)
-        if parameter_name is None:
-            # Don't catch errors here, this one's invisible
-            return
-        object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
-        values = state.setdefault(ImportKey.PARAMETER_VALUES, {})
-        if object_class_name is not None:
-            class_name = object_class_name
-            entity_name = state[ImportKey.OBJECT_NAME]
-            map_key = "object_parameter_values"
-        else:
-            relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
-            if relationship_class_name is not None:
-                class_name = relationship_class_name
-                entity_name = tuple(state[ImportKey.OBJECT_NAMES])
-                map_key = "relationship_parameter_values"
-            else:
-                raise KeyError(ImportKey.CLASS_NAME)
-        alternative_name = state.get(ImportKey.ALTERNATIVE_NAME)
-        key = (class_name, entity_name, parameter_name, alternative_name)
-        if key in values:
-            return
-        value_type = str(source_data)
-        value = values[key] = {"type": value_type}  # See import_mapping.generator._parameter_value_from_dict()
-        if self.compress and value_type == "map":
-            value["compress"] = self.compress
-        if self.options and value_type == "time_series":
-            value["options"] = self.options
-        parameter_value = [class_name, entity_name, parameter_name, value]
-        if alternative_name is not None:
-            parameter_value.append(alternative_name)
-        mapped_data.setdefault(map_key, list()).append(parameter_value)
-
-
-class ParameterValueMetadataMapping(ImportMapping):
-    """Maps relationship metadata.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterValueMapping` or
-    a :class:`ParameterValueTypeMapping` as parent.
-    """
-
-    MAP_TYPE = "ParameterValueMetadata"
-
-    def _import_row(self, source_data, state, mapped_data):
-        pass
-
-
-class ParameterValueIndexMapping(ImportMapping):
-    """Maps parameter value indexes.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
-    an :class:`ParameterValueTypeMapping` as parents.
-    """
-
-    MAP_TYPE = "ParameterValueIndex"
-
-    def _import_row(self, source_data, state, mapped_data):
-        _ = state[ImportKey.PARAMETER_NAME]
-        index = source_data
-        state.setdefault(ImportKey.PARAMETER_VALUE_INDEXES, []).append(index)
-
-
-class IndexNameMapping(IndexNameMappingBase):
-    """Maps index names for indexed parameter values.
-
-    Cannot be used as the topmost mapping; must have an :class:`ParameterValueTypeMapping` as a parent.
-    """
-
-    MAP_TYPE = "IndexName"
-    _STATE_KEY = ImportKey.PARAMETER_VALUES
-
-    def _value_key(self, state):
-        return _parameter_value_key(state)
-
-
-class ExpandedParameterValueMapping(ImportMapping):
-    """Maps parameter values.
-
-    Whenever this mapping is a child of :class:`ParameterValueIndexMapping`, it maps individual values of indexed
-    parameters.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
-    an :class:`ParameterValueTypeMapping` as parents.
-    """
-
-    MAP_TYPE = "ExpandedValue"
-
-    def _import_row(self, source_data, state, mapped_data):
-        values = state.setdefault(ImportKey.PARAMETER_VALUES, {})
-        value = values[_parameter_value_key(state)]
-        data = value.setdefault("data", [])
-        if value["type"] == "array":
-            data.append(source_data)
-            return
-        indexes = state.pop(ImportKey.PARAMETER_VALUE_INDEXES)
-        data.append(indexes + [source_data])
-
-    def _skip_row(self, state):
-        state.pop(ImportKey.PARAMETER_VALUE_INDEXES, None)
-
-
-class ParameterValueListMapping(ImportMapping):
-    """Maps parameter value list names.
-
-    Can be used as the topmost mapping; in case the mapping has a :class:`ParameterDefinitionMapping` as parent,
-    yields value list name for that parameter definition.
-    """
-
-    MAP_TYPE = "ParameterValueList"
-
-    def _import_row(self, source_data, state, mapped_data):
-        if self.parent is not None:
-            # Trigger a KeyError in case there's no parameter definition, so check_validity() registers the issue
-            _ = state[ImportKey.PARAMETER_DEFINITION]
-        state[ImportKey.PARAMETER_VALUE_LIST_NAME] = str(source_data)
-
-
-class ParameterValueListValueMapping(ImportMapping):
-    """Maps parameter value list values.
-
-    Cannot be used as the topmost mapping; must have a :class:`ParameterValueListMapping` as parent.
-
-    """
-
-    MAP_TYPE = "ParameterValueListValue"
-
-    def _import_row(self, source_data, state, mapped_data):
-        list_value = source_data
-        if list_value == "":
-            return
-        value_list_name = state[ImportKey.PARAMETER_VALUE_LIST_NAME]
-        mapped_data.setdefault("parameter_value_lists", list()).append([value_list_name, list_value])
-
-
-class AlternativeMapping(ImportMapping):
-    """Maps alternatives.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "Alternative"
-
-    def _import_row(self, source_data, state, mapped_data):
-        alternative = state[ImportKey.ALTERNATIVE_NAME] = str(source_data)
-        mapped_data.setdefault("alternatives", set()).add(alternative)
-
-
-class ScenarioMapping(ImportMapping):
-    """Maps scenarios.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "Scenario"
-
-    def _import_row(self, source_data, state, mapped_data):
-        state[ImportKey.SCENARIO_NAME] = str(source_data)
-
-
-class ScenarioActiveFlagMapping(ImportMapping):
-    """Maps scenario active flags.
-
-    Cannot be used as the topmost mapping; must have a :class:`ScenarioMapping` as parent.
-    """
-
-    MAP_TYPE = "ScenarioActiveFlag"
-
-    def _import_row(self, source_data, state, mapped_data):
-        scenario = state[ImportKey.SCENARIO_NAME]
-        active = bool(strtobool(str(source_data)))
-        mapped_data.setdefault("scenarios", set()).add((scenario, active))
-
-
-class ScenarioAlternativeMapping(ImportMapping):
-    """Maps scenario alternatives.
-
-    Cannot be used as the topmost mapping; must have a :class:`ScenarioMapping` as parent.
-    """
-
-    MAP_TYPE = "ScenarioAlternative"
-
-    def _import_row(self, source_data, state, mapped_data):
-        alternative = str(source_data)
-        if not alternative:
-            return
-        scenario = state[ImportKey.SCENARIO_NAME]
-        scen_alt = state[ImportKey.SCENARIO_ALTERNATIVE] = [scenario, alternative]
-        mapped_data.setdefault("scenario_alternatives", list()).append(scen_alt)
-
-
-class ScenarioBeforeAlternativeMapping(ImportMapping):
-    """Maps scenario 'before' alternatives.
-
-    Cannot be used as the topmost mapping; must have a :class:`ScenarioAlternativeMapping` as parent.
-    """
-
-    MAP_TYPE = "ScenarioBeforeAlternative"
-
-    def _import_row(self, source_data, state, mapped_data):
-        scen_alt = state[ImportKey.SCENARIO_ALTERNATIVE]
-        alternative = str(source_data)
-        scen_alt.append(alternative)
-
-
-class ToolMapping(ImportMapping):
-    """Maps tools.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "Tool"
-
-    def _import_row(self, source_data, state, mapped_data):
-        tool = state[ImportKey.TOOL_NAME] = str(source_data)
-        if self.child is None:
-            mapped_data.setdefault("tools", set()).add(tool)
-
-
-class FeatureEntityClassMapping(ImportMapping):
-    """Maps feature entity classes.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "FeatureEntityClass"
-
-    def _import_row(self, source_data, state, mapped_data):
-        entity_class = str(source_data)
-        state[ImportKey.FEATURE] = [entity_class]
-
-
-class FeatureParameterDefinitionMapping(ImportMapping):
-    """Maps feature parameter definitions.
-
-    Cannot be used as the topmost mapping; must have a :class:`FeatureEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "FeatureParameterDefinition"
-
-    def _import_row(self, source_data, state, mapped_data):
-        feature = state[ImportKey.FEATURE]
-        parameter = str(source_data)
-        feature.append(parameter)
-        mapped_data.setdefault("features", set()).add(tuple(feature))
-
-
-class ToolFeatureEntityClassMapping(ImportMapping):
-    """Maps tool feature entity classes.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureEntityClass"
-
-    def _import_row(self, source_data, state, mapped_data):
-        tool = state[ImportKey.TOOL_NAME]
-        entity_class = str(source_data)
-        tool_feature = [tool, entity_class]
-        state[ImportKey.TOOL_FEATURE] = tool_feature
-        mapped_data.setdefault("tool_features", list()).append(tool_feature)
-
-
-class ToolFeatureParameterDefinitionMapping(ImportMapping):
-    """Maps tool feature parameter definitions.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureParameterDefinition"
-
-    def _import_row(self, source_data, state, mapped_data):
-        tool_feature = state[ImportKey.TOOL_FEATURE]
-        parameter = str(source_data)
-        tool_feature.append(parameter)
-
-
-class ToolFeatureRequiredFlagMapping(ImportMapping):
-    """Maps tool feature required flags.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureRequiredFlag"
-
-    def _import_row(self, source_data, state, mapped_data):
-        required = bool(strtobool(str(source_data)))
-        tool_feature = state[ImportKey.TOOL_FEATURE]
-        tool_feature.append(required)
-
-
-class ToolFeatureMethodEntityClassMapping(ImportMapping):
-    """Maps tool feature method entity classes.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureMethodEntityClass"
-
-    def _import_row(self, source_data, state, mapped_data):
-        tool_name = state[ImportKey.TOOL_NAME]
-        entity_class = str(source_data)
-        tool_feature_method = [tool_name, entity_class]
-        state[ImportKey.TOOL_FEATURE_METHOD] = tool_feature_method
-        mapped_data.setdefault("tool_feature_methods", list()).append(tool_feature_method)
-
-
-class ToolFeatureMethodParameterDefinitionMapping(ImportMapping):
-    """Maps tool feature method parameter definitions.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureMethodParameterDefinition"
-
-    def _import_row(self, source_data, state, mapped_data):
-        tool_feature_method = state[ImportKey.TOOL_FEATURE_METHOD]
-        parameter = str(source_data)
-        tool_feature_method.append(parameter)
-
-
-class ToolFeatureMethodMethodMapping(ImportMapping):
-    """Maps tool feature method methods.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureMethodMethod"
-
-    def _import_row(self, source_data, state, mapped_data):
-        method = source_data
-        if method == "":
-            return
-        tool_feature_method = state[ImportKey.TOOL_FEATURE_METHOD]
-        tool_feature_method.append(method)
-
-
-def from_dict(serialized):
-    """
-    Deserializes mappings.
-
-    Args:
-        serialized (list): serialize mappings
-
-    Returns:
-        Mapping: root mapping
-    """
-    mappings = {
-        klass.MAP_TYPE: klass
-        for klass in (
-            ObjectClassMapping,
-            ObjectMapping,
-            ObjectMetadataMapping,
-            ObjectGroupMapping,
-            RelationshipClassMapping,
-            RelationshipClassObjectClassMapping,
-            RelationshipMapping,
-            RelationshipObjectMapping,
-            RelationshipMetadataMapping,
-            ParameterDefinitionMapping,
-            ParameterDefaultValueMapping,
-            ParameterDefaultValueTypeMapping,
-            ParameterDefaultValueIndexMapping,
-            ExpandedParameterDefaultValueMapping,
-            ParameterValueMapping,
-            ParameterValueTypeMapping,
-            ParameterValueMetadataMapping,
-            IndexNameMapping,
-            ParameterValueIndexMapping,
-            ExpandedParameterValueMapping,
-            ParameterValueListMapping,
-            ParameterValueListValueMapping,
-            AlternativeMapping,
-            ScenarioMapping,
-            ScenarioActiveFlagMapping,
-            ScenarioAlternativeMapping,
-            ScenarioBeforeAlternativeMapping,
-            ToolMapping,
-            FeatureEntityClassMapping,
-            FeatureParameterDefinitionMapping,
-            ToolFeatureEntityClassMapping,
-            ToolFeatureParameterDefinitionMapping,
-            ToolFeatureRequiredFlagMapping,
-            ToolFeatureMethodEntityClassMapping,
-            ToolFeatureMethodParameterDefinitionMapping,
-            ToolFeatureMethodMethodMapping,
-        )
-    }
-    # Legacy
-    mappings["ParameterIndex"] = ParameterValueIndexMapping
-    flattened = list()
-    for mapping_dict in serialized:
-        position = mapping_dict["position"]
-        value = mapping_dict.get("value")
-        skip_columns = mapping_dict.get("skip_columns")
-        read_start_row = mapping_dict.get("read_start_row", 0)
-        filter_re = mapping_dict.get("filter_re", "")
-        if isinstance(position, str):
-            position = Position(position)
-        flattened.append(
-            mappings[mapping_dict["map_type"]].reconstruct(
-                position, value, skip_columns, read_start_row, filter_re, mapping_dict
-            )
-        )
-        if mapping_dict["map_type"] == "ObjectGroup":
-            # Legacy: dropping parameter mappings from object groups
-            break
-    return unflatten(flattened)
-
-
-def _parameter_value_key(state):
-    """Creates parameter value's key from current state.
-
-    Args:
-        state (dict): import state
-
-    Returns:
-        tuple of str: class name, entity name and parameter name
-    """
-    object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
-    if object_class_name is not None:
-        class_name = object_class_name
-        entity_name = state[ImportKey.OBJECT_NAME]
-    else:
-        relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
-        if relationship_class_name is None:
-            raise KeyError(ImportKey.CLASS_NAME)
-        object_names = state[ImportKey.OBJECT_NAMES]
-        if len(object_names) != state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
-            raise KeyError(ImportKey.OBJECT_NAMES)
-        class_name = relationship_class_name
-        entity_name = tuple(object_names)
-    parameter_name = state[ImportKey.PARAMETER_NAME]
-    alternative_name = state.get(ImportKey.ALTERNATIVE_NAME)
-    return class_name, entity_name, parameter_name, alternative_name
-
-
-def _default_value_key(state):
-    """Creates parameter default value's key from current state.
-
-    Args:
-        state (dict): import state
-
-    Returns:
-        tuple of str: class name and parameter name
-    """
-    class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
-    if class_name is None:
-        class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
-        if class_name is None:
-            raise KeyError(ImportKey.CLASS_NAME)
-    parameter_name = state[ImportKey.PARAMETER_NAME]
-    return class_name, parameter_name
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Contains import mappings for database items such as entities, entity classes and parameter values.
+
+"""
+
+from distutils.util import strtobool
+from enum import auto, Enum, unique
+from spinedb_api.mapping import Mapping, Position, unflatten, is_pivoted
+from spinedb_api.exception import InvalidMappingComponent
+
+
+@unique
+class ImportKey(Enum):
+    CLASS_NAME = auto()
+    RELATIONSHIP_DIMENSION_COUNT = auto()
+    OBJECT_CLASS_NAME = auto()
+    OBJECT_NAME = auto()
+    GROUP_NAME = auto()
+    MEMBER_NAME = auto()
+    PARAMETER_NAME = auto()
+    PARAMETER_DEFINITION = auto()
+    PARAMETER_DEFINITION_EXTRAS = auto()
+    PARAMETER_DEFAULT_VALUES = auto()
+    PARAMETER_DEFAULT_VALUE_INDEXES = auto()
+    PARAMETER_VALUES = auto()
+    PARAMETER_VALUE_INDEXES = auto()
+    RELATIONSHIP_CLASS_NAME = auto()
+    OBJECT_CLASS_NAMES = auto()
+    OBJECT_NAMES = auto()
+    ALTERNATIVE_NAME = auto()
+    SCENARIO_NAME = auto()
+    SCENARIO_ALTERNATIVE = auto()
+    FEATURE = auto()
+    TOOL_NAME = auto()
+    TOOL_FEATURE = auto()
+    TOOL_FEATURE_METHOD = auto()
+    PARAMETER_VALUE_LIST_NAME = auto()
+
+    def __str__(self):
+        name = {
+            self.ALTERNATIVE_NAME.value: "Alternative names",
+            self.CLASS_NAME.value: "Class names",
+            self.OBJECT_CLASS_NAME.value: "Object class names",
+            self.OBJECT_NAME.value: "Object names",
+            self.GROUP_NAME.value: "Group names",
+            self.MEMBER_NAME.value: "Member names",
+            self.PARAMETER_NAME.value: "Parameter names",
+            self.PARAMETER_DEFINITION.value: "Parameter names",
+            self.PARAMETER_DEFAULT_VALUE_INDEXES.value: "Parameter indexes",
+            self.PARAMETER_VALUE_INDEXES.value: "Parameter indexes",
+            self.RELATIONSHIP_CLASS_NAME.value: "Relationship class names",
+            self.OBJECT_CLASS_NAMES.value: "Object class names",
+            self.OBJECT_NAMES.value: "Object names",
+            self.PARAMETER_VALUE_LIST_NAME.value: "Parameter value lists",
+            self.SCENARIO_NAME.value: "Scenario names",
+            self.SCENARIO_ALTERNATIVE.value: "Alternative names",
+            self.TOOL_NAME.value: "Tool names",
+            self.FEATURE.value: "Entity class names",
+            self.TOOL_FEATURE.value: "Entity class names",
+            self.TOOL_FEATURE_METHOD.value: "Entity class names",
+        }.get(self.value)
+        if name is not None:
+            return name
+        return super().__str__()
+
+
+class KeyFix(Exception):
+    """Opposite of KeyError"""
+
+
+def check_validity(root_mapping):
+    class _DummySourceRow:
+        def __getitem__(self, key):
+            return "true"
+
+    errors = []
+    source_row = _DummySourceRow()
+    root_mapping.import_row(source_row, {}, {}, errors)
+    return errors
+
+
+class ImportMapping(Mapping):
+    """Base class for import mappings."""
+
+    def __init__(self, position, value=None, skip_columns=None, read_start_row=0, filter_re=""):
+        """
+        Args:
+            position (int or Position): what to map in the source table
+            value (Any, optional): fixed value
+            skip_columns (Iterable of int, optional): index of columns that should be skipped;
+                useful when source is pivoted
+            read_start_row (int): at which source row importing should start
+            filter_re (str): regular expression for filtering
+        """
+        super().__init__(position, value, filter_re)
+        self._skip_columns = None
+        self._read_start_row = None
+        self.skip_columns = skip_columns
+        self.read_start_row = read_start_row
+        self._has_filter_cached = None
+        self._index = None
+
+    @property
+    def skip_columns(self):
+        return self._skip_columns
+
+    @skip_columns.setter
+    def skip_columns(self, skip_columns=None):
+        if skip_columns is None:
+            self._skip_columns = []
+            return
+        if isinstance(skip_columns, (str, int)):
+            self._skip_columns = [skip_columns]
+            return
+        if isinstance(skip_columns, list):
+            bad_types = [
+                f"{type(column).__name__} at index {i}"
+                for i, column in enumerate(skip_columns)
+                if not isinstance(column, (str, int))
+            ]
+            if bad_types:
+                bad_types = ", ".join(bad_types)
+                raise TypeError(f"skip_columns must be str, int or list of str, int, instead got list with {bad_types}")
+            self._skip_columns = skip_columns
+            return
+        raise TypeError(f"skip_columns must be str, int or list of str, int, instead got {type(skip_columns).__name__}")
+
+    @property
+    def read_start_row(self):
+        return self._read_start_row
+
+    @read_start_row.setter
+    def read_start_row(self, row):
+        if not isinstance(row, int):
+            raise TypeError(f"row must be int, instead got {type(row).__name__}")
+        if row < 0:
+            raise ValueError(f"row must be >= 0 ({row})")
+        self._read_start_row = row
+
+    def polish(self, table_name, source_header, for_preview=False):
+        """Polishes the mapping before an import operation.
+        'Expands' transient ``position`` and ``value`` attributes into their final value.
+
+        Args:
+            table_name (str)
+            source_header (list(str))
+        """
+        self._polish_for_import(table_name, source_header)
+        if for_preview:
+            self._polish_for_preview(source_header)
+
+    def _polish_for_import(self, table_name, source_header):
+        # FIXME: Polish skip columns
+        if self.child is not None:
+            self.child._polish_for_import(table_name, source_header)
+        if isinstance(self.position, str):
+            # Column mapping with string position, we need to find the index in the header
+            try:
+                self.position = source_header.index(self.position)
+                return
+            except ValueError:
+                msg = f"'{self.position}' is not in '{source_header}'"
+                raise InvalidMappingComponent(msg)
+        if self.position == Position.table_name:
+            # Table name mapping, we set the fixed value to the table name
+            self.value = table_name
+            return
+        if self.position == Position.header:
+            if self.value is None:
+                # Row mapping from header, we handle this one separately
+                return
+            # Column header mapping, the value indicates which field
+            if isinstance(self.value, str):
+                # If the value is indeed in the header, we're good
+                if self.value in source_header:
+                    return
+                try:
+                    # Not in the header, maybe it's a stringified index?
+                    self.value = int(self.value)
+                except ValueError:
+                    msg = f"'{self.value}' is not in header '{source_header}'"
+                    raise InvalidMappingComponent(msg)
+            # Integer value, we try and get the actual value from that index in the header
+            try:
+                self._index = self.value
+                self.value = source_header[self.value]
+            except IndexError:
+                msg = f"'{self.value}' is not a valid index in header '{source_header}'"
+                raise InvalidMappingComponent(msg)
+
+    def _polish_for_preview(self, source_header):
+        if self.position == Position.header and self.value is not None:
+            self.value = self._index
+        if self.child is not None:
+            self.child._polish_for_preview(source_header)
+
+    @property
+    def rank(self):
+        if self.parent is None:
+            return 0
+        return self.parent.rank + 1
+
+    def _filter_accepts_row(self, source_row):
+        """Whether or not the row passes the filter for this mapping."""
+        if self.position == Position.hidden and self.value is None:
+            return True
+        if self._filter_re is None:
+            return True
+        source_data = self._data(source_row)
+        return self._filter_re.search(str(source_data)) is not None
+
+    def has_filter(self):
+        """Whether mapping or one of its children has filter configured.
+
+        Returns:
+            bool: True if mapping or one of its children has filter configured , False otherwise
+        """
+        if self._has_filter_cached is None:
+            child_has_filter = self._child.has_filter() if self._child is not None else False
+            has_filter = (self.position != Position.hidden or self.value is not None) and self._filter_re is not None
+            self._has_filter_cached = child_has_filter or has_filter
+        return self._has_filter_cached
+
+    def filter_accepts_row(self, source_row):
+        """Whether or not the row passes the filter for all mappings in the hierarchy."""
+        return self._filter_accepts_row(source_row) and (
+            self.child is None or self.child.filter_accepts_row(source_row)
+        )
+
+    def import_row(self, source_row, state, mapped_data, errors=None):
+        if self.has_filter() and not self.filter_accepts_row(source_row):
+            return
+        if errors is None:
+            errors = []
+        if not (self.position == Position.hidden and self.value is None):
+            source_data = self._data(source_row)
+            if source_data is None:
+                self._skip_row(state)
+            else:
+                try:
+                    self._import_row(source_data, state, mapped_data)
+                except KeyError as err:
+                    for key in err.args:
+                        msg = f"Required key '{key}' is invalid"
+                        error = InvalidMappingComponent(msg, self.rank, key)
+                        errors.append(error)
+                except KeyFix as fix:
+                    indexes = set()
+                    for key in fix.args:
+                        indexes |= {k for k, err in enumerate(errors) if err.key == key}
+                    for k in sorted(indexes, reverse=True):
+                        errors.pop(k)
+        if self.child is not None:
+            self.child.import_row(source_row, state, mapped_data, errors=errors)
+
+    def _data(self, source_row):  # pylint: disable=arguments-differ
+        if source_row is None:
+            return None
+        return source_row[self.position]
+
+    def _import_row(self, source_data, state, mapped_data):
+        raise NotImplementedError()
+
+    def _skip_row(self, state):
+        """Called when the source data is None. Do necessary clean ups on state."""
+
+    def is_constant(self):
+        return self.position == Position.hidden and self.value is not None
+
+    def is_pivoted(self):
+        if is_pivoted(self.position):
+            return True
+        if self.position == Position.header and self.value is None and self.child is not None:
+            return True
+        if self.child is None:
+            return False
+        return self.child.is_pivoted()
+
+    def to_dict(self):
+        d = super().to_dict()
+        if self.skip_columns:
+            d["skip_columns"] = self.skip_columns
+        if self.read_start_row:
+            d["read_start_row"] = self.read_start_row
+        return d
+
+    @classmethod
+    def reconstruct(cls, position, value, skip_columns, read_start_row, filter_re, mapping_dict):
+        """
+        Reconstructs mapping.
+
+        Args:
+            position (int or Position, optional): mapping's position
+            value (Any): fixed value
+            skip_columns (Iterable of Int, optional): skipped columns
+            read_start_row (int): first source row to read
+            filter_re (str): filter regular expression
+            mapping_dict (dict): serialized mapping
+
+        Returns:
+            Mapping: reconstructed mapping
+        """
+        mapping = cls(position, value, skip_columns, read_start_row, filter_re)
+        return mapping
+
+
+class ImportObjectsMixin:
+    def __init__(self, position, value=None, skip_columns=None, read_start_row=0, filter_re="", import_objects=False):
+        super().__init__(position, value, skip_columns, read_start_row, filter_re)
+        self.import_objects = import_objects
+
+    def to_dict(self):
+        d = super().to_dict()
+        if self.import_objects:
+            d["import_objects"] = True
+        return d
+
+    @classmethod
+    def reconstruct(cls, position, value, skip_columns, read_start_row, filter_re, mapping_dict):
+        import_objects = mapping_dict.get("import_objects", False)
+        mapping = cls(position, value, skip_columns, read_start_row, filter_re, import_objects)
+        return mapping
+
+
+class IndexedValueMixin:
+    def __init__(
+        self, position, value=None, skip_columns=None, read_start_row=0, filter_re="", compress=False, options=None
+    ):
+        super().__init__(position, value, skip_columns, read_start_row, filter_re)
+        if options is None:
+            options = {}
+        self.compress = compress
+        self.options = options
+
+    def to_dict(self):
+        d = super().to_dict()
+        if self.compress:
+            d["compress"] = True
+        if self.options:
+            d["options"] = self.options
+        return d
+
+    @classmethod
+    def reconstruct(cls, position, value, skip_columns, read_start_row, filter_re, mapping_dict):
+        compress = mapping_dict.get("compress", False)
+        options = mapping_dict.get("options")
+        mapping = cls(position, value, skip_columns, read_start_row, filter_re, compress, options)
+        return mapping
+
+
+class ObjectClassMapping(ImportMapping):
+    """Maps object classes.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "ObjectClass"
+
+    def _import_row(self, source_data, state, mapped_data):
+        object_class_name = state[ImportKey.OBJECT_CLASS_NAME] = str(source_data)
+        object_classes = mapped_data.setdefault("object_classes", set())
+        object_classes.add(object_class_name)
+
+
+class ObjectMapping(ImportMapping):
+    """Maps objects.
+
+    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectClassMapping`.
+    """
+
+    MAP_TYPE = "Object"
+
+    def _import_row(self, source_data, state, mapped_data):
+        object_class_name = state[ImportKey.OBJECT_CLASS_NAME]
+        object_name = state[ImportKey.OBJECT_NAME] = str(source_data)
+        if isinstance(self.child, ObjectGroupMapping):
+            raise KeyError(ImportKey.MEMBER_NAME)
+        mapped_data.setdefault("objects", set()).add((object_class_name, object_name))
+
+
+class ObjectMetadataMapping(ImportMapping):
+    """Maps object metadata.
+
+    Cannot be used as the topmost mapping; must have :class:`ObjectClassMapping` and :class:`ObjectMapping` as parents.
+    """
+
+    MAP_TYPE = "ObjectMetadata"
+
+    def _import_row(self, source_data, state, mapped_data):
+        pass
+
+
+class ObjectGroupMapping(ImportObjectsMixin, ImportMapping):
+    """Maps object groups.
+
+    Cannot be used as the topmost mapping; must have :class:`ObjectClassMapping` and :class:`ObjectMapping` as parents.
+    """
+
+    MAP_TYPE = "ObjectGroup"
+
+    def _import_row(self, source_data, state, mapped_data):
+        object_class_name = state[ImportKey.OBJECT_CLASS_NAME]
+        group_name = state.get(ImportKey.OBJECT_NAME)
+        if group_name is None:
+            raise KeyError(ImportKey.GROUP_NAME)
+        member_name = str(source_data)
+        mapped_data.setdefault("object_groups", set()).add((object_class_name, group_name, member_name))
+        if self.import_objects:
+            objects = (object_class_name, group_name), (object_class_name, member_name)
+            mapped_data.setdefault("objects", set()).update(objects)
+        raise KeyFix(ImportKey.MEMBER_NAME)
+
+
+class RelationshipClassMapping(ImportMapping):
+    """Maps relationships classes.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "RelationshipClass"
+
+    def _import_row(self, source_data, state, mapped_data):
+        dim_count = len([m for m in self.flatten() if isinstance(m, RelationshipClassObjectClassMapping)])
+        state[ImportKey.RELATIONSHIP_DIMENSION_COUNT] = dim_count
+        relationship_class_name = state[ImportKey.RELATIONSHIP_CLASS_NAME] = str(source_data)
+        object_class_names = state[ImportKey.OBJECT_CLASS_NAMES] = []
+        relationship_classes = mapped_data.setdefault("relationship_classes", dict())
+        relationship_classes[relationship_class_name] = object_class_names
+        raise KeyError(ImportKey.OBJECT_CLASS_NAMES)
+
+
+class RelationshipClassObjectClassMapping(ImportMapping):
+    """Maps relationship class object classes.
+
+    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
+    """
+
+    MAP_TYPE = "RelationshipClassObjectClass"
+
+    def _import_row(self, source_data, state, mapped_data):
+        _ = state[ImportKey.RELATIONSHIP_CLASS_NAME]
+        object_class_names = state[ImportKey.OBJECT_CLASS_NAMES]
+        object_class_name = str(source_data)
+        object_class_names.append(object_class_name)
+        if len(object_class_names) == state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
+            raise KeyFix(ImportKey.OBJECT_CLASS_NAMES)
+
+
+class RelationshipMapping(ImportMapping):
+    """Maps relationships.
+
+    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
+    """
+
+    MAP_TYPE = "Relationship"
+
+    def _import_row(self, source_data, state, mapped_data):
+        # Don't access state[ImportKey.RELATIONSHIP_CLASS_NAME], we don't want to catch errors here
+        # because this one's invisible.
+        state[ImportKey.OBJECT_NAMES] = []
+
+
+class RelationshipObjectMapping(ImportObjectsMixin, ImportMapping):
+    """Maps relationship's objects.
+
+    Cannot be used as the topmost mapping; must have :class:`RelationshipClassMapping` and :class:`RelationshipMapping`
+    as parents.
+    """
+
+    MAP_TYPE = "RelationshipObject"
+
+    def _import_row(self, source_data, state, mapped_data):
+        relationship_class_name = state[ImportKey.RELATIONSHIP_CLASS_NAME]
+        object_class_names = state[ImportKey.OBJECT_CLASS_NAMES]
+        if len(object_class_names) != state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
+            raise KeyError(ImportKey.OBJECT_CLASS_NAMES)
+        object_names = state[ImportKey.OBJECT_NAMES]
+        object_name = str(source_data)
+        object_names.append(object_name)
+        if self.import_objects:
+            k = len(object_names) - 1
+            object_class_name = object_class_names[k]
+            mapped_data.setdefault("object_classes", set()).add(object_class_name)
+            mapped_data.setdefault("objects", set()).add((object_class_name, object_name))
+        if len(object_names) == state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
+            relationships = mapped_data.setdefault("relationships", set())
+            relationships.add((relationship_class_name, tuple(object_names)))
+            raise KeyFix(ImportKey.OBJECT_NAMES)
+        raise KeyError(ImportKey.OBJECT_NAMES)
+
+
+class RelationshipMetadataMapping(ImportMapping):
+    """Maps relationship metadata.
+
+    Cannot be used as the topmost mapping; must have :class:`RelationshipClassMapping`, a :class:`RelationshipMapping`
+    and one or more :class:`RelationshipObjectMapping` as parents.
+    """
+
+    MAP_TYPE = "RelationshipMetadata"
+
+    def _import_row(self, source_data, state, mapped_data):
+        pass
+
+
+class ParameterDefinitionMapping(ImportMapping):
+    """Maps parameter definitions.
+
+    Cannot be used as the topmost mapping; must have an entity class mapping as one of parents.
+    """
+
+    MAP_TYPE = "ParameterDefinition"
+
+    def _import_row(self, source_data, state, mapped_data):
+        object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
+        if object_class_name is not None:
+            class_name = object_class_name
+            map_key = "object_parameters"
+        else:
+            relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
+            if relationship_class_name is not None:
+                class_name = relationship_class_name
+                map_key = "relationship_parameters"
+            else:
+                raise KeyError(ImportKey.CLASS_NAME)
+        parameter_name = state[ImportKey.PARAMETER_NAME] = str(source_data)
+        definition_extras = state[ImportKey.PARAMETER_DEFINITION_EXTRAS] = []
+        parameter_definition_key = state[ImportKey.PARAMETER_DEFINITION] = class_name, parameter_name
+        default_values = state.get(ImportKey.PARAMETER_DEFAULT_VALUES)
+        if default_values is None or parameter_definition_key not in default_values:
+            mapped_data.setdefault(map_key, dict())[parameter_definition_key] = definition_extras
+
+
+class ParameterDefaultValueMapping(ImportMapping):
+    """Maps scalar (non-indexed) default values
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
+    """
+
+    MAP_TYPE = "ParameterDefaultValue"
+
+    def _import_row(self, source_data, state, mapped_data):
+        default_value = source_data
+        if default_value == "":
+            return
+        parameter_definition_extras = state[ImportKey.PARAMETER_DEFINITION_EXTRAS]
+        parameter_definition_extras.append(default_value)
+        value_list_name = state.get(ImportKey.PARAMETER_VALUE_LIST_NAME)
+        if value_list_name is not None:
+            parameter_definition_extras.append(value_list_name)
+
+
+class ParameterDefaultValueTypeMapping(IndexedValueMixin, ImportMapping):
+    MAP_TYPE = "ParameterDefaultValueType"
+
+    def _import_row(self, source_data, state, mapped_data):
+        parameter_definition = state.get(ImportKey.PARAMETER_DEFINITION)
+        if parameter_definition is None:
+            # Don't catch errors here, this one's invisible
+            return
+        default_values = state.setdefault(ImportKey.PARAMETER_DEFAULT_VALUES, {})
+        if parameter_definition in default_values:
+            return
+        value_type = str(source_data)
+        default_value = default_values[parameter_definition] = {"type": value_type}
+        if self.compress and value_type == "map":
+            default_value["compress"] = self.compress
+        if self.options and value_type == "time_series":
+            default_value["options"] = self.options
+        parameter_definition_extras = state[ImportKey.PARAMETER_DEFINITION_EXTRAS]
+        parameter_definition_extras.append(default_value)
+        value_list_name = state.get(ImportKey.PARAMETER_VALUE_LIST_NAME)
+        if value_list_name is not None:
+            parameter_definition_extras.append(value_list_name)
+
+
+class IndexNameMappingBase(ImportMapping):
+    """Base class for index name mappings."""
+
+    _STATE_KEY = NotImplemented
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._id = None
+
+    def _value_key(self, state):
+        raise NotImplementedError()
+
+    def _import_row(self, source_data, state, mapped_data):
+        values = state[self._STATE_KEY]
+        value = values[self._value_key(state)]
+        if self._id is None:
+            self._id = 0
+            current = self
+            while True:
+                if current.parent is None:
+                    break
+                current = current.parent
+                if isinstance(current, type(self)):
+                    self._id += 1
+        value.setdefault("index_names", {})[self._id] = source_data
+
+
+class DefaultValueIndexNameMapping(IndexNameMappingBase):
+    """Maps default value index names.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefaultValueTypeMapping` as parent.
+    """
+
+    MAP_TYPE = "DefaultValueIndexName"
+    _STATE_KEY = ImportKey.PARAMETER_DEFAULT_VALUES
+
+    def _value_key(self, state):
+        return _default_value_key(state)
+
+
+class ParameterDefaultValueIndexMapping(ImportMapping):
+    """Maps default value indexes.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
+    """
+
+    MAP_TYPE = "ParameterDefaultValueIndex"
+
+    def _import_row(self, source_data, state, mapped_data):
+        _ = state[ImportKey.PARAMETER_NAME]
+        index = source_data
+        state.setdefault(ImportKey.PARAMETER_DEFAULT_VALUE_INDEXES, []).append(index)
+
+
+class ExpandedParameterDefaultValueMapping(ImportMapping):
+    """Maps indexed default values.
+
+    Whenever this mapping is a child of :class:`ParameterDefaultValueIndexMapping`, it maps individual values of
+    indexed parameters.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
+    """
+
+    MAP_TYPE = "ExpandedDefaultValue"
+
+    def _import_row(self, source_data, state, mapped_data):
+        values = state.setdefault(ImportKey.PARAMETER_DEFAULT_VALUES, {})
+        value = values[_default_value_key(state)]
+        val = source_data
+        data = value.setdefault("data", [])
+        if value["type"] == "array":
+            data.append(val)
+            return
+        indexes = state.pop(ImportKey.PARAMETER_DEFAULT_VALUE_INDEXES)
+        data.append(indexes + [val])
+
+    def _skip_row(self, state):
+        state.pop(ImportKey.PARAMETER_DEFAULT_VALUE_INDEXES, None)
+
+
+class ParameterValueMapping(ImportMapping):
+    """Maps scalar (non-indexed) parameter values.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
+    an :class:`AlternativeMapping` as parents.
+    """
+
+    MAP_TYPE = "ParameterValue"
+
+    def _import_row(self, source_data, state, mapped_data):
+        value = source_data
+        if value == "":
+            return
+        object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
+        relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
+        if object_class_name is not None:
+            class_name = object_class_name
+            entity_name = state[ImportKey.OBJECT_NAME]
+            map_key = "object_parameter_values"
+        elif relationship_class_name is not None:
+            object_names = state[ImportKey.OBJECT_NAMES]
+            if len(object_names) != state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
+                raise KeyError(ImportKey.OBJECT_NAMES)
+            class_name = relationship_class_name
+            entity_name = object_names
+            map_key = "relationship_parameter_values"
+        else:
+            raise KeyError(ImportKey.CLASS_NAME)
+        parameter_name = state[ImportKey.PARAMETER_NAME]
+        parameter_value = [class_name, entity_name, parameter_name, value]
+        alternative_name = state.get(ImportKey.ALTERNATIVE_NAME)
+        if alternative_name is not None:
+            parameter_value.append(alternative_name)
+        mapped_data.setdefault(map_key, list()).append(parameter_value)
+
+
+class ParameterValueTypeMapping(IndexedValueMixin, ImportMapping):
+    MAP_TYPE = "ParameterValueType"
+
+    def _import_row(self, source_data, state, mapped_data):
+        parameter_name = state.get(ImportKey.PARAMETER_NAME)
+        if parameter_name is None:
+            # Don't catch errors here, this one's invisible
+            return
+        object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
+        values = state.setdefault(ImportKey.PARAMETER_VALUES, {})
+        if object_class_name is not None:
+            class_name = object_class_name
+            entity_name = state[ImportKey.OBJECT_NAME]
+            map_key = "object_parameter_values"
+        else:
+            relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
+            if relationship_class_name is not None:
+                class_name = relationship_class_name
+                entity_name = tuple(state[ImportKey.OBJECT_NAMES])
+                map_key = "relationship_parameter_values"
+            else:
+                raise KeyError(ImportKey.CLASS_NAME)
+        alternative_name = state.get(ImportKey.ALTERNATIVE_NAME)
+        key = (class_name, entity_name, parameter_name, alternative_name)
+        if key in values:
+            return
+        value_type = str(source_data)
+        value = values[key] = {"type": value_type}  # See import_mapping.generator._parameter_value_from_dict()
+        if self.compress and value_type == "map":
+            value["compress"] = self.compress
+        if self.options and value_type == "time_series":
+            value["options"] = self.options
+        parameter_value = [class_name, entity_name, parameter_name, value]
+        if alternative_name is not None:
+            parameter_value.append(alternative_name)
+        mapped_data.setdefault(map_key, list()).append(parameter_value)
+
+
+class ParameterValueMetadataMapping(ImportMapping):
+    """Maps relationship metadata.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterValueMapping` or
+    a :class:`ParameterValueTypeMapping` as parent.
+    """
+
+    MAP_TYPE = "ParameterValueMetadata"
+
+    def _import_row(self, source_data, state, mapped_data):
+        pass
+
+
+class ParameterValueIndexMapping(ImportMapping):
+    """Maps parameter value indexes.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
+    an :class:`ParameterValueTypeMapping` as parents.
+    """
+
+    MAP_TYPE = "ParameterValueIndex"
+
+    def _import_row(self, source_data, state, mapped_data):
+        _ = state[ImportKey.PARAMETER_NAME]
+        index = source_data
+        state.setdefault(ImportKey.PARAMETER_VALUE_INDEXES, []).append(index)
+
+
+class IndexNameMapping(IndexNameMappingBase):
+    """Maps index names for indexed parameter values.
+
+    Cannot be used as the topmost mapping; must have an :class:`ParameterValueTypeMapping` as a parent.
+    """
+
+    MAP_TYPE = "IndexName"
+    _STATE_KEY = ImportKey.PARAMETER_VALUES
+
+    def _value_key(self, state):
+        return _parameter_value_key(state)
+
+
+class ExpandedParameterValueMapping(ImportMapping):
+    """Maps parameter values.
+
+    Whenever this mapping is a child of :class:`ParameterValueIndexMapping`, it maps individual values of indexed
+    parameters.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
+    an :class:`ParameterValueTypeMapping` as parents.
+    """
+
+    MAP_TYPE = "ExpandedValue"
+
+    def _import_row(self, source_data, state, mapped_data):
+        values = state.setdefault(ImportKey.PARAMETER_VALUES, {})
+        value = values[_parameter_value_key(state)]
+        data = value.setdefault("data", [])
+        if value["type"] == "array":
+            data.append(source_data)
+            return
+        indexes = state.pop(ImportKey.PARAMETER_VALUE_INDEXES)
+        data.append(indexes + [source_data])
+
+    def _skip_row(self, state):
+        state.pop(ImportKey.PARAMETER_VALUE_INDEXES, None)
+
+
+class ParameterValueListMapping(ImportMapping):
+    """Maps parameter value list names.
+
+    Can be used as the topmost mapping; in case the mapping has a :class:`ParameterDefinitionMapping` as parent,
+    yields value list name for that parameter definition.
+    """
+
+    MAP_TYPE = "ParameterValueList"
+
+    def _import_row(self, source_data, state, mapped_data):
+        if self.parent is not None:
+            # Trigger a KeyError in case there's no parameter definition, so check_validity() registers the issue
+            _ = state[ImportKey.PARAMETER_DEFINITION]
+        state[ImportKey.PARAMETER_VALUE_LIST_NAME] = str(source_data)
+
+
+class ParameterValueListValueMapping(ImportMapping):
+    """Maps parameter value list values.
+
+    Cannot be used as the topmost mapping; must have a :class:`ParameterValueListMapping` as parent.
+
+    """
+
+    MAP_TYPE = "ParameterValueListValue"
+
+    def _import_row(self, source_data, state, mapped_data):
+        list_value = source_data
+        if list_value == "":
+            return
+        value_list_name = state[ImportKey.PARAMETER_VALUE_LIST_NAME]
+        mapped_data.setdefault("parameter_value_lists", list()).append([value_list_name, list_value])
+
+
+class AlternativeMapping(ImportMapping):
+    """Maps alternatives.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "Alternative"
+
+    def _import_row(self, source_data, state, mapped_data):
+        alternative = state[ImportKey.ALTERNATIVE_NAME] = str(source_data)
+        mapped_data.setdefault("alternatives", set()).add(alternative)
+
+
+class ScenarioMapping(ImportMapping):
+    """Maps scenarios.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "Scenario"
+
+    def _import_row(self, source_data, state, mapped_data):
+        state[ImportKey.SCENARIO_NAME] = str(source_data)
+
+
+class ScenarioActiveFlagMapping(ImportMapping):
+    """Maps scenario active flags.
+
+    Cannot be used as the topmost mapping; must have a :class:`ScenarioMapping` as parent.
+    """
+
+    MAP_TYPE = "ScenarioActiveFlag"
+
+    def _import_row(self, source_data, state, mapped_data):
+        scenario = state[ImportKey.SCENARIO_NAME]
+        active = bool(strtobool(str(source_data)))
+        mapped_data.setdefault("scenarios", set()).add((scenario, active))
+
+
+class ScenarioAlternativeMapping(ImportMapping):
+    """Maps scenario alternatives.
+
+    Cannot be used as the topmost mapping; must have a :class:`ScenarioMapping` as parent.
+    """
+
+    MAP_TYPE = "ScenarioAlternative"
+
+    def _import_row(self, source_data, state, mapped_data):
+        alternative = str(source_data)
+        if not alternative:
+            return
+        scenario = state[ImportKey.SCENARIO_NAME]
+        scen_alt = state[ImportKey.SCENARIO_ALTERNATIVE] = [scenario, alternative]
+        mapped_data.setdefault("scenario_alternatives", list()).append(scen_alt)
+
+
+class ScenarioBeforeAlternativeMapping(ImportMapping):
+    """Maps scenario 'before' alternatives.
+
+    Cannot be used as the topmost mapping; must have a :class:`ScenarioAlternativeMapping` as parent.
+    """
+
+    MAP_TYPE = "ScenarioBeforeAlternative"
+
+    def _import_row(self, source_data, state, mapped_data):
+        scen_alt = state[ImportKey.SCENARIO_ALTERNATIVE]
+        alternative = str(source_data)
+        scen_alt.append(alternative)
+
+
+class ToolMapping(ImportMapping):
+    """Maps tools.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "Tool"
+
+    def _import_row(self, source_data, state, mapped_data):
+        tool = state[ImportKey.TOOL_NAME] = str(source_data)
+        if self.child is None:
+            mapped_data.setdefault("tools", set()).add(tool)
+
+
+class FeatureEntityClassMapping(ImportMapping):
+    """Maps feature entity classes.
+
+    Can be used as the topmost mapping.
+    """
+
+    MAP_TYPE = "FeatureEntityClass"
+
+    def _import_row(self, source_data, state, mapped_data):
+        entity_class = str(source_data)
+        state[ImportKey.FEATURE] = [entity_class]
+
+
+class FeatureParameterDefinitionMapping(ImportMapping):
+    """Maps feature parameter definitions.
+
+    Cannot be used as the topmost mapping; must have a :class:`FeatureEntityClassMapping` as parent.
+    """
+
+    MAP_TYPE = "FeatureParameterDefinition"
+
+    def _import_row(self, source_data, state, mapped_data):
+        feature = state[ImportKey.FEATURE]
+        parameter = str(source_data)
+        feature.append(parameter)
+        mapped_data.setdefault("features", set()).add(tuple(feature))
+
+
+class ToolFeatureEntityClassMapping(ImportMapping):
+    """Maps tool feature entity classes.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureEntityClass"
+
+    def _import_row(self, source_data, state, mapped_data):
+        tool = state[ImportKey.TOOL_NAME]
+        entity_class = str(source_data)
+        tool_feature = [tool, entity_class]
+        state[ImportKey.TOOL_FEATURE] = tool_feature
+        mapped_data.setdefault("tool_features", list()).append(tool_feature)
+
+
+class ToolFeatureParameterDefinitionMapping(ImportMapping):
+    """Maps tool feature parameter definitions.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureParameterDefinition"
+
+    def _import_row(self, source_data, state, mapped_data):
+        tool_feature = state[ImportKey.TOOL_FEATURE]
+        parameter = str(source_data)
+        tool_feature.append(parameter)
+
+
+class ToolFeatureRequiredFlagMapping(ImportMapping):
+    """Maps tool feature required flags.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureRequiredFlag"
+
+    def _import_row(self, source_data, state, mapped_data):
+        required = bool(strtobool(str(source_data)))
+        tool_feature = state[ImportKey.TOOL_FEATURE]
+        tool_feature.append(required)
+
+
+class ToolFeatureMethodEntityClassMapping(ImportMapping):
+    """Maps tool feature method entity classes.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureMethodEntityClass"
+
+    def _import_row(self, source_data, state, mapped_data):
+        tool_name = state[ImportKey.TOOL_NAME]
+        entity_class = str(source_data)
+        tool_feature_method = [tool_name, entity_class]
+        state[ImportKey.TOOL_FEATURE_METHOD] = tool_feature_method
+        mapped_data.setdefault("tool_feature_methods", list()).append(tool_feature_method)
+
+
+class ToolFeatureMethodParameterDefinitionMapping(ImportMapping):
+    """Maps tool feature method parameter definitions.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureMethodParameterDefinition"
+
+    def _import_row(self, source_data, state, mapped_data):
+        tool_feature_method = state[ImportKey.TOOL_FEATURE_METHOD]
+        parameter = str(source_data)
+        tool_feature_method.append(parameter)
+
+
+class ToolFeatureMethodMethodMapping(ImportMapping):
+    """Maps tool feature method methods.
+
+    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
+    """
+
+    MAP_TYPE = "ToolFeatureMethodMethod"
+
+    def _import_row(self, source_data, state, mapped_data):
+        method = source_data
+        if method == "":
+            return
+        tool_feature_method = state[ImportKey.TOOL_FEATURE_METHOD]
+        tool_feature_method.append(method)
+
+
+def from_dict(serialized):
+    """
+    Deserializes mappings.
+
+    Args:
+        serialized (list): serialize mappings
+
+    Returns:
+        Mapping: root mapping
+    """
+    mappings = {
+        klass.MAP_TYPE: klass
+        for klass in (
+            ObjectClassMapping,
+            ObjectMapping,
+            ObjectMetadataMapping,
+            ObjectGroupMapping,
+            RelationshipClassMapping,
+            RelationshipClassObjectClassMapping,
+            RelationshipMapping,
+            RelationshipObjectMapping,
+            RelationshipMetadataMapping,
+            ParameterDefinitionMapping,
+            ParameterDefaultValueMapping,
+            ParameterDefaultValueTypeMapping,
+            ParameterDefaultValueIndexMapping,
+            ExpandedParameterDefaultValueMapping,
+            ParameterValueMapping,
+            ParameterValueTypeMapping,
+            ParameterValueMetadataMapping,
+            IndexNameMapping,
+            ParameterValueIndexMapping,
+            ExpandedParameterValueMapping,
+            ParameterValueListMapping,
+            ParameterValueListValueMapping,
+            AlternativeMapping,
+            ScenarioMapping,
+            ScenarioActiveFlagMapping,
+            ScenarioAlternativeMapping,
+            ScenarioBeforeAlternativeMapping,
+            ToolMapping,
+            FeatureEntityClassMapping,
+            FeatureParameterDefinitionMapping,
+            ToolFeatureEntityClassMapping,
+            ToolFeatureParameterDefinitionMapping,
+            ToolFeatureRequiredFlagMapping,
+            ToolFeatureMethodEntityClassMapping,
+            ToolFeatureMethodParameterDefinitionMapping,
+            ToolFeatureMethodMethodMapping,
+        )
+    }
+    # Legacy
+    mappings["ParameterIndex"] = ParameterValueIndexMapping
+    flattened = list()
+    for mapping_dict in serialized:
+        position = mapping_dict["position"]
+        value = mapping_dict.get("value")
+        skip_columns = mapping_dict.get("skip_columns")
+        read_start_row = mapping_dict.get("read_start_row", 0)
+        filter_re = mapping_dict.get("filter_re", "")
+        if isinstance(position, str):
+            position = Position(position)
+        flattened.append(
+            mappings[mapping_dict["map_type"]].reconstruct(
+                position, value, skip_columns, read_start_row, filter_re, mapping_dict
+            )
+        )
+        if mapping_dict["map_type"] == "ObjectGroup":
+            # Legacy: dropping parameter mappings from object groups
+            break
+    return unflatten(flattened)
+
+
+def _parameter_value_key(state):
+    """Creates parameter value's key from current state.
+
+    Args:
+        state (dict): import state
+
+    Returns:
+        tuple of str: class name, entity name and parameter name
+    """
+    object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
+    if object_class_name is not None:
+        class_name = object_class_name
+        entity_name = state[ImportKey.OBJECT_NAME]
+    else:
+        relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
+        if relationship_class_name is None:
+            raise KeyError(ImportKey.CLASS_NAME)
+        object_names = state[ImportKey.OBJECT_NAMES]
+        if len(object_names) != state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
+            raise KeyError(ImportKey.OBJECT_NAMES)
+        class_name = relationship_class_name
+        entity_name = tuple(object_names)
+    parameter_name = state[ImportKey.PARAMETER_NAME]
+    alternative_name = state.get(ImportKey.ALTERNATIVE_NAME)
+    return class_name, entity_name, parameter_name, alternative_name
+
+
+def _default_value_key(state):
+    """Creates parameter default value's key from current state.
+
+    Args:
+        state (dict): import state
+
+    Returns:
+        tuple of str: class name and parameter name
+    """
+    class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
+    if class_name is None:
+        class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
+        if class_name is None:
+            raise KeyError(ImportKey.CLASS_NAME)
+    parameter_name = state[ImportKey.PARAMETER_NAME]
+    return class_name, parameter_name
```

### Comparing `spinedb_api-0.30.3/spinedb_api/import_mapping/import_mapping_compat.py` & `spinedb_api-0.30.4/spinedb_api/import_mapping/import_mapping_compat.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,395 +1,395 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Functions for creating import mappings from dicts.
-
-"""
-from .import_mapping import (
-    Position,
-    ObjectClassMapping,
-    RelationshipClassMapping,
-    RelationshipClassObjectClassMapping,
-    ObjectMapping,
-    ObjectMetadataMapping,
-    RelationshipMapping,
-    RelationshipObjectMapping,
-    RelationshipMetadataMapping,
-    ParameterDefinitionMapping,
-    ParameterDefaultValueMapping,
-    ParameterDefaultValueTypeMapping,
-    ParameterDefaultValueIndexMapping,
-    ExpandedParameterDefaultValueMapping,
-    ParameterValueMapping,
-    ParameterValueTypeMapping,
-    ParameterValueMetadataMapping,
-    ParameterValueIndexMapping,
-    ExpandedParameterValueMapping,
-    AlternativeMapping,
-    ScenarioMapping,
-    ScenarioActiveFlagMapping,
-    ScenarioAlternativeMapping,
-    ScenarioBeforeAlternativeMapping,
-    ToolMapping,
-    FeatureEntityClassMapping,
-    FeatureParameterDefinitionMapping,
-    ToolFeatureEntityClassMapping,
-    ToolFeatureParameterDefinitionMapping,
-    ToolFeatureRequiredFlagMapping,
-    ToolFeatureMethodEntityClassMapping,
-    ToolFeatureMethodParameterDefinitionMapping,
-    ToolFeatureMethodMethodMapping,
-    ObjectGroupMapping,
-    ParameterValueListMapping,
-    ParameterValueListValueMapping,
-    from_dict as mapping_from_dict,
-    IndexNameMapping,
-    DefaultValueIndexNameMapping,
-)
-from ..mapping import to_dict as import_mapping_to_dict
-
-
-def parse_named_mapping_spec(named_mapping_spec):
-    if len(named_mapping_spec) == 1:
-        name, mapping_spec = next(iter(named_mapping_spec.items()))
-        mapping = mapping_spec["mapping"]
-    else:
-        # Legacy
-        name = named_mapping_spec.get("mapping_name", "")
-        mapping = named_mapping_spec
-    return name, import_mapping_from_dict(mapping)
-
-
-def unparse_named_mapping_spec(name, root_mapping):
-    return {name: {"mapping": import_mapping_to_dict(root_mapping)}}
-
-
-def import_mapping_from_dict(map_dict):
-    """Creates Mapping object from a dict"""
-    if isinstance(map_dict, list):
-        # New system, flattened mapping as list
-        return mapping_from_dict(map_dict)
-    # Compatibility system, plain dict
-    if not isinstance(map_dict, dict):
-        raise TypeError(f"map_dict must be a dict, instead it was: {type(map_dict)}")
-    map_type = map_dict.get("map_type")
-    legacy_mapping_from_dict = {
-        "ObjectClass": _object_class_mapping_from_dict,
-        "RelationshipClass": _relationship_class_mapping_from_dict,
-        "Alternative": _alternative_mapping_from_dict,
-        "Scenario": _scenario_mapping_from_dict,
-        "ScenarioAlternative": _scenario_alternative_mapping_from_dict,
-        "Tool": _tool_mapping_from_dict,
-        "Feature": _feature_mapping_from_dict,
-        "ToolFeature": _tool_feature_mapping_from_dict,
-        "ToolFeatureMethod": _tool_feature_method_mapping_from_dict,
-        "ObjectGroup": _object_group_mapping_from_dict,
-        "ParameterValueList": _parameter_value_list_mapping_from_dict,
-    }
-    from_dict = legacy_mapping_from_dict.get(map_type)
-    if from_dict is not None:
-        return from_dict(map_dict)
-    raise ValueError(f'invalid "map_type" value, expected any of {", ".join(legacy_mapping_from_dict)}, got {map_type}')
-
-
-def _parameter_value_list_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    value = map_dict.get("value")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ParameterValueListMapping(
-        *_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row
-    )
-    root_mapping.child = ParameterValueListValueMapping(*_pos_and_val(value))
-    return root_mapping
-
-
-def _alternative_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = AlternativeMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    return root_mapping
-
-
-def _scenario_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    active = map_dict.get("active", "false")
-    root_mapping = ScenarioMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    root_mapping.child = ScenarioActiveFlagMapping(*_pos_and_val(active))
-    return root_mapping
-
-
-def _scenario_alternative_mapping_from_dict(map_dict):
-    scenario_name = map_dict.get("scenario_name")
-    alternative_name = map_dict.get("alternative_name")
-    before_alternative_name = map_dict.get("before_alternative_name")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ScenarioMapping(
-        *_pos_and_val(scenario_name), skip_columns=skip_columns, read_start_row=read_start_row
-    )
-    scen_alt_mapping = root_mapping.child = ScenarioAlternativeMapping(*_pos_and_val(alternative_name))
-    scen_alt_mapping.child = ScenarioBeforeAlternativeMapping(*_pos_and_val(before_alternative_name))
-    return root_mapping
-
-
-def _tool_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ToolMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    return root_mapping
-
-
-def _feature_mapping_from_dict(map_dict):
-    entity_class_name = map_dict.get("entity_class_name")
-    parameter_definition_name = map_dict.get("parameter_definition_name")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = FeatureEntityClassMapping(
-        *_pos_and_val(entity_class_name), skip_columns=skip_columns, read_start_row=read_start_row
-    )
-    root_mapping.child = FeatureParameterDefinitionMapping(*_pos_and_val(parameter_definition_name))
-    return root_mapping
-
-
-def _tool_feature_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    entity_class_name = map_dict.get("entity_class_name")
-    parameter_definition_name = map_dict.get("parameter_definition_name")
-    required = map_dict.get("required", "false")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ToolMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    root_mapping.child = ent_class_mapping = ToolFeatureEntityClassMapping(*_pos_and_val(entity_class_name))
-    ent_class_mapping.child = param_def_mapping = ToolFeatureParameterDefinitionMapping(
-        *_pos_and_val(parameter_definition_name)
-    )
-    param_def_mapping.child = ToolFeatureRequiredFlagMapping(*_pos_and_val(required))
-    return root_mapping
-
-
-def _tool_feature_method_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    entity_class_name = map_dict.get("entity_class_name")
-    parameter_definition_name = map_dict.get("parameter_definition_name")
-    method = map_dict.get("method")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ToolMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    root_mapping.child = ent_class_mapping = ToolFeatureMethodEntityClassMapping(*_pos_and_val(entity_class_name))
-    ent_class_mapping.child = param_def_mapping = ToolFeatureMethodParameterDefinitionMapping(
-        *_pos_and_val(parameter_definition_name)
-    )
-    param_def_mapping.child = ToolFeatureMethodMethodMapping(*_pos_and_val(method))
-    return root_mapping
-
-
-def _object_class_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    objects = map_dict.get("objects", map_dict.get("object"))
-    object_metadata = map_dict.get("object_metadata", None)
-    parameters = map_dict.get("parameters")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ObjectClassMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    object_mapping = root_mapping.child = ObjectMapping(*_pos_and_val(objects))
-    object_metadata_mapping = object_mapping.child = ObjectMetadataMapping(*_pos_and_val(object_metadata))
-    object_metadata_mapping.child = parameter_mapping_from_dict(parameters)
-    return root_mapping
-
-
-def _object_group_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    groups = map_dict.get("groups")
-    members = map_dict.get("members")
-    import_objects = map_dict.get("import_objects", False)
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ObjectClassMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    object_mapping = root_mapping.child = ObjectMapping(*_pos_and_val(groups))
-    object_mapping.child = ObjectGroupMapping(*_pos_and_val(members), import_objects=import_objects)
-    return root_mapping
-
-
-def _relationship_class_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    objects = map_dict.get("objects")
-    if objects is None:
-        objects = [None]
-    object_classes = map_dict.get("object_classes")
-    if object_classes is None:
-        object_classes = [None]
-    relationship_metadata = map_dict.get("relationship_metadata")
-    parameters = map_dict.get("parameters")
-    import_objects = map_dict.get("import_objects", False)
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = RelationshipClassMapping(
-        *_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row
-    )
-    parent_mapping = root_mapping
-    for klass in object_classes:
-        class_mapping = RelationshipClassObjectClassMapping(*_pos_and_val(klass))
-        parent_mapping.child = class_mapping
-        parent_mapping = class_mapping
-    relationship_mapping = parent_mapping.child = RelationshipMapping(Position.hidden, value="relationship")
-    parent_mapping = relationship_mapping
-    for obj in objects:
-        object_mapping = RelationshipObjectMapping(*_pos_and_val(obj), import_objects=import_objects)
-        parent_mapping.child = object_mapping
-        parent_mapping = object_mapping
-    relationship_metadata_mapping = parent_mapping.child = RelationshipMetadataMapping(
-        *_pos_and_val(relationship_metadata)
-    )
-    relationship_metadata_mapping.child = parameter_mapping_from_dict(parameters)
-    return root_mapping
-
-
-def parameter_mapping_from_dict(map_dict):
-    if map_dict is None:
-        return None
-    map_type = map_dict.get("map_type")
-    if map_type == "parameter" or "parameter_type" in map_dict:
-        _fix_parameter_mapping_dict(map_dict)
-    map_type = map_dict.get("map_type")
-    if map_type == "None":
-        return None
-    param_def_mapping = ParameterDefinitionMapping(*_pos_and_val(map_dict.get("name")))
-    if map_type == "ParameterDefinition":
-        default_value_dict = map_dict.get("default_value")
-        value_list_name = map_dict.get("parameter_value_list_name")
-        param_def_mapping.child = value_list_mapping = ParameterValueListMapping(*_pos_and_val(value_list_name))
-        value_list_mapping.child = parameter_default_value_mapping_from_dict(default_value_dict)
-        return param_def_mapping
-    alternative_name = map_dict.get("alternative_name")
-    parameter_value_metadata = map_dict.get("parameter_value_metadata")
-    param_def_mapping.child = alt_mapping = AlternativeMapping(*_pos_and_val(alternative_name))
-    alt_mapping.child = param_val_metadata_mapping = ParameterValueMetadataMapping(
-        *_pos_and_val(parameter_value_metadata)
-    )
-    param_val_metadata_mapping.child = parameter_value_mapping_from_dict(map_dict.get("value"))
-    return param_def_mapping
-
-
-def parameter_default_value_mapping_from_dict(default_value_dict):
-    if default_value_dict is None:
-        return ParameterDefaultValueMapping(*_pos_and_val(None))
-    value_type = default_value_dict["value_type"].replace(" ", "_")
-    main_value = default_value_dict.get("main_value")
-    if value_type == "single_value":
-        return ParameterDefaultValueMapping(*_pos_and_val(main_value))
-    extra_dimensions = default_value_dict.get("extra_dimensions", [None])
-    compress = default_value_dict.get("compress", False)
-    options = default_value_dict.get("options", {})
-    root_mapping = ParameterDefaultValueTypeMapping(Position.hidden, value_type, compress=compress, options=options)
-    parent_mapping = root_mapping
-    for ed in extra_dimensions:
-        name_mapping = DefaultValueIndexNameMapping(Position.hidden, value=None)
-        parent_mapping.child = name_mapping
-        index_mapping = ParameterDefaultValueIndexMapping(*_pos_and_val(ed))
-        name_mapping.child = index_mapping
-        parent_mapping = index_mapping
-    parent_mapping.child = ExpandedParameterDefaultValueMapping(*_pos_and_val(main_value))
-    return root_mapping
-
-
-def parameter_value_mapping_from_dict(value_dict):
-    if value_dict is None:
-        return ParameterValueMapping(*_pos_and_val(None))
-    value_type = value_dict["value_type"].replace(" ", "_")
-    main_value = value_dict.get("main_value")
-    if value_type == "single_value":
-        return ParameterValueMapping(*_pos_and_val(main_value))
-    extra_dimensions = value_dict.get("extra_dimensions", [None])
-    compress = value_dict.get("compress", False)
-    options = value_dict.get("options", {})
-    root_mapping = ParameterValueTypeMapping(Position.hidden, value_type, compress=compress, options=options)
-    parent_mapping = root_mapping
-    for ed in extra_dimensions:
-        name_mapping = IndexNameMapping(Position.hidden, value=None)
-        parent_mapping.child = name_mapping
-        index_mapping = ParameterValueIndexMapping(*_pos_and_val(ed))
-        name_mapping.child = index_mapping
-        parent_mapping = index_mapping
-    parent_mapping.child = ExpandedParameterValueMapping(*_pos_and_val(main_value))
-    return root_mapping
-
-
-def _fix_parameter_mapping_dict(map_dict):
-    # Even deeper legacy
-    parameter_type = map_dict.pop("parameter_type", None)
-    if parameter_type == "definition":
-        map_dict["map_type"] = "ParameterDefinition"
-    else:
-        value_dict = map_dict.copy()
-        value_dict.pop("name", None)
-        value_dict["value_type"] = parameter_type if parameter_type else "single value"
-        value_dict["main_value"] = value_dict.pop("value", None)
-        map_dict["map_type"] = "ParameterValue"
-        map_dict["value"] = value_dict
-
-
-def _pos_and_val(x):
-    if not isinstance(x, dict):
-        map_type = "constant" if isinstance(x, str) else "column"
-        map_dict = {"map_type": map_type, "reference": x}
-    else:
-        map_dict = x
-    map_type = map_dict.get("map_type")
-    ref = map_dict.get("reference", map_dict.get("value_reference"))
-    if isinstance(ref, str) and not ref:
-        ref = None
-    # None, or invalid reference
-    if map_type == "None" or ref is None:
-        return Position.hidden, None  # This combination disables the mapping
-    # Constant
-    if map_type == "constant":
-        if not isinstance(ref, str):
-            raise TypeError(f"Constant reference must be str, instead got: {type(ref).__name__}")
-        return Position.hidden, ref
-    # Table name
-    if map_type == "table_name":
-        return Position.table_name, None
-    # Row or column reference, including header
-    if not isinstance(ref, (str, int)):
-        raise TypeError(f"Row or column reference must be str or int, instead got: {type(ref).__name__}")
-    # 1. Column header
-    if map_type in ("column_name", "column_header"):
-        if isinstance(ref, int) and ref < 0:
-            ref = 0
-        return Position.header, ref
-    # 2. Data row or column
-    try:
-        ref = int(ref)
-    except ValueError:
-        pass
-    # 2a. Column
-    if map_type == "column":
-        if isinstance(ref, int) and ref < 0:
-            ref = 0
-        return ref, None
-    # 2b. Row
-    if map_type == "row":
-        if isinstance(ref, int):
-            if ref == -1:
-                return Position.header, None
-            if ref < -1:
-                ref = 0
-            return -(ref + 1), None  # pylint: disable=invalid-unary-operand-type
-        if ref.lower() == "header":
-            return Position.header, None
-        raise ValueError(f"If row reference is str, it must be 'header'. Instead got '{ref}'")
-    # Fallback to invalid
-    return Position.hidden, None
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Functions for creating import mappings from dicts.
+
+"""
+from .import_mapping import (
+    Position,
+    ObjectClassMapping,
+    RelationshipClassMapping,
+    RelationshipClassObjectClassMapping,
+    ObjectMapping,
+    ObjectMetadataMapping,
+    RelationshipMapping,
+    RelationshipObjectMapping,
+    RelationshipMetadataMapping,
+    ParameterDefinitionMapping,
+    ParameterDefaultValueMapping,
+    ParameterDefaultValueTypeMapping,
+    ParameterDefaultValueIndexMapping,
+    ExpandedParameterDefaultValueMapping,
+    ParameterValueMapping,
+    ParameterValueTypeMapping,
+    ParameterValueMetadataMapping,
+    ParameterValueIndexMapping,
+    ExpandedParameterValueMapping,
+    AlternativeMapping,
+    ScenarioMapping,
+    ScenarioActiveFlagMapping,
+    ScenarioAlternativeMapping,
+    ScenarioBeforeAlternativeMapping,
+    ToolMapping,
+    FeatureEntityClassMapping,
+    FeatureParameterDefinitionMapping,
+    ToolFeatureEntityClassMapping,
+    ToolFeatureParameterDefinitionMapping,
+    ToolFeatureRequiredFlagMapping,
+    ToolFeatureMethodEntityClassMapping,
+    ToolFeatureMethodParameterDefinitionMapping,
+    ToolFeatureMethodMethodMapping,
+    ObjectGroupMapping,
+    ParameterValueListMapping,
+    ParameterValueListValueMapping,
+    from_dict as mapping_from_dict,
+    IndexNameMapping,
+    DefaultValueIndexNameMapping,
+)
+from ..mapping import to_dict as import_mapping_to_dict
+
+
+def parse_named_mapping_spec(named_mapping_spec):
+    if len(named_mapping_spec) == 1:
+        name, mapping_spec = next(iter(named_mapping_spec.items()))
+        mapping = mapping_spec["mapping"]
+    else:
+        # Legacy
+        name = named_mapping_spec.get("mapping_name", "")
+        mapping = named_mapping_spec
+    return name, import_mapping_from_dict(mapping)
+
+
+def unparse_named_mapping_spec(name, root_mapping):
+    return {name: {"mapping": import_mapping_to_dict(root_mapping)}}
+
+
+def import_mapping_from_dict(map_dict):
+    """Creates Mapping object from a dict"""
+    if isinstance(map_dict, list):
+        # New system, flattened mapping as list
+        return mapping_from_dict(map_dict)
+    # Compatibility system, plain dict
+    if not isinstance(map_dict, dict):
+        raise TypeError(f"map_dict must be a dict, instead it was: {type(map_dict)}")
+    map_type = map_dict.get("map_type")
+    legacy_mapping_from_dict = {
+        "ObjectClass": _object_class_mapping_from_dict,
+        "RelationshipClass": _relationship_class_mapping_from_dict,
+        "Alternative": _alternative_mapping_from_dict,
+        "Scenario": _scenario_mapping_from_dict,
+        "ScenarioAlternative": _scenario_alternative_mapping_from_dict,
+        "Tool": _tool_mapping_from_dict,
+        "Feature": _feature_mapping_from_dict,
+        "ToolFeature": _tool_feature_mapping_from_dict,
+        "ToolFeatureMethod": _tool_feature_method_mapping_from_dict,
+        "ObjectGroup": _object_group_mapping_from_dict,
+        "ParameterValueList": _parameter_value_list_mapping_from_dict,
+    }
+    from_dict = legacy_mapping_from_dict.get(map_type)
+    if from_dict is not None:
+        return from_dict(map_dict)
+    raise ValueError(f'invalid "map_type" value, expected any of {", ".join(legacy_mapping_from_dict)}, got {map_type}')
+
+
+def _parameter_value_list_mapping_from_dict(map_dict):
+    name = map_dict.get("name")
+    value = map_dict.get("value")
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    root_mapping = ParameterValueListMapping(
+        *_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row
+    )
+    root_mapping.child = ParameterValueListValueMapping(*_pos_and_val(value))
+    return root_mapping
+
+
+def _alternative_mapping_from_dict(map_dict):
+    name = map_dict.get("name")
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    root_mapping = AlternativeMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
+    return root_mapping
+
+
+def _scenario_mapping_from_dict(map_dict):
+    name = map_dict.get("name")
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    active = map_dict.get("active", "false")
+    root_mapping = ScenarioMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
+    root_mapping.child = ScenarioActiveFlagMapping(*_pos_and_val(active))
+    return root_mapping
+
+
+def _scenario_alternative_mapping_from_dict(map_dict):
+    scenario_name = map_dict.get("scenario_name")
+    alternative_name = map_dict.get("alternative_name")
+    before_alternative_name = map_dict.get("before_alternative_name")
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    root_mapping = ScenarioMapping(
+        *_pos_and_val(scenario_name), skip_columns=skip_columns, read_start_row=read_start_row
+    )
+    scen_alt_mapping = root_mapping.child = ScenarioAlternativeMapping(*_pos_and_val(alternative_name))
+    scen_alt_mapping.child = ScenarioBeforeAlternativeMapping(*_pos_and_val(before_alternative_name))
+    return root_mapping
+
+
+def _tool_mapping_from_dict(map_dict):
+    name = map_dict.get("name")
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    root_mapping = ToolMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
+    return root_mapping
+
+
+def _feature_mapping_from_dict(map_dict):
+    entity_class_name = map_dict.get("entity_class_name")
+    parameter_definition_name = map_dict.get("parameter_definition_name")
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    root_mapping = FeatureEntityClassMapping(
+        *_pos_and_val(entity_class_name), skip_columns=skip_columns, read_start_row=read_start_row
+    )
+    root_mapping.child = FeatureParameterDefinitionMapping(*_pos_and_val(parameter_definition_name))
+    return root_mapping
+
+
+def _tool_feature_mapping_from_dict(map_dict):
+    name = map_dict.get("name")
+    entity_class_name = map_dict.get("entity_class_name")
+    parameter_definition_name = map_dict.get("parameter_definition_name")
+    required = map_dict.get("required", "false")
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    root_mapping = ToolMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
+    root_mapping.child = ent_class_mapping = ToolFeatureEntityClassMapping(*_pos_and_val(entity_class_name))
+    ent_class_mapping.child = param_def_mapping = ToolFeatureParameterDefinitionMapping(
+        *_pos_and_val(parameter_definition_name)
+    )
+    param_def_mapping.child = ToolFeatureRequiredFlagMapping(*_pos_and_val(required))
+    return root_mapping
+
+
+def _tool_feature_method_mapping_from_dict(map_dict):
+    name = map_dict.get("name")
+    entity_class_name = map_dict.get("entity_class_name")
+    parameter_definition_name = map_dict.get("parameter_definition_name")
+    method = map_dict.get("method")
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    root_mapping = ToolMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
+    root_mapping.child = ent_class_mapping = ToolFeatureMethodEntityClassMapping(*_pos_and_val(entity_class_name))
+    ent_class_mapping.child = param_def_mapping = ToolFeatureMethodParameterDefinitionMapping(
+        *_pos_and_val(parameter_definition_name)
+    )
+    param_def_mapping.child = ToolFeatureMethodMethodMapping(*_pos_and_val(method))
+    return root_mapping
+
+
+def _object_class_mapping_from_dict(map_dict):
+    name = map_dict.get("name")
+    objects = map_dict.get("objects", map_dict.get("object"))
+    object_metadata = map_dict.get("object_metadata", None)
+    parameters = map_dict.get("parameters")
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    root_mapping = ObjectClassMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
+    object_mapping = root_mapping.child = ObjectMapping(*_pos_and_val(objects))
+    object_metadata_mapping = object_mapping.child = ObjectMetadataMapping(*_pos_and_val(object_metadata))
+    object_metadata_mapping.child = parameter_mapping_from_dict(parameters)
+    return root_mapping
+
+
+def _object_group_mapping_from_dict(map_dict):
+    name = map_dict.get("name")
+    groups = map_dict.get("groups")
+    members = map_dict.get("members")
+    import_objects = map_dict.get("import_objects", False)
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    root_mapping = ObjectClassMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
+    object_mapping = root_mapping.child = ObjectMapping(*_pos_and_val(groups))
+    object_mapping.child = ObjectGroupMapping(*_pos_and_val(members), import_objects=import_objects)
+    return root_mapping
+
+
+def _relationship_class_mapping_from_dict(map_dict):
+    name = map_dict.get("name")
+    objects = map_dict.get("objects")
+    if objects is None:
+        objects = [None]
+    object_classes = map_dict.get("object_classes")
+    if object_classes is None:
+        object_classes = [None]
+    relationship_metadata = map_dict.get("relationship_metadata")
+    parameters = map_dict.get("parameters")
+    import_objects = map_dict.get("import_objects", False)
+    skip_columns = map_dict.get("skip_columns", [])
+    read_start_row = map_dict.get("read_start_row", 0)
+    root_mapping = RelationshipClassMapping(
+        *_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row
+    )
+    parent_mapping = root_mapping
+    for klass in object_classes:
+        class_mapping = RelationshipClassObjectClassMapping(*_pos_and_val(klass))
+        parent_mapping.child = class_mapping
+        parent_mapping = class_mapping
+    relationship_mapping = parent_mapping.child = RelationshipMapping(Position.hidden, value="relationship")
+    parent_mapping = relationship_mapping
+    for obj in objects:
+        object_mapping = RelationshipObjectMapping(*_pos_and_val(obj), import_objects=import_objects)
+        parent_mapping.child = object_mapping
+        parent_mapping = object_mapping
+    relationship_metadata_mapping = parent_mapping.child = RelationshipMetadataMapping(
+        *_pos_and_val(relationship_metadata)
+    )
+    relationship_metadata_mapping.child = parameter_mapping_from_dict(parameters)
+    return root_mapping
+
+
+def parameter_mapping_from_dict(map_dict):
+    if map_dict is None:
+        return None
+    map_type = map_dict.get("map_type")
+    if map_type == "parameter" or "parameter_type" in map_dict:
+        _fix_parameter_mapping_dict(map_dict)
+    map_type = map_dict.get("map_type")
+    if map_type == "None":
+        return None
+    param_def_mapping = ParameterDefinitionMapping(*_pos_and_val(map_dict.get("name")))
+    if map_type == "ParameterDefinition":
+        default_value_dict = map_dict.get("default_value")
+        value_list_name = map_dict.get("parameter_value_list_name")
+        param_def_mapping.child = value_list_mapping = ParameterValueListMapping(*_pos_and_val(value_list_name))
+        value_list_mapping.child = parameter_default_value_mapping_from_dict(default_value_dict)
+        return param_def_mapping
+    alternative_name = map_dict.get("alternative_name")
+    parameter_value_metadata = map_dict.get("parameter_value_metadata")
+    param_def_mapping.child = alt_mapping = AlternativeMapping(*_pos_and_val(alternative_name))
+    alt_mapping.child = param_val_metadata_mapping = ParameterValueMetadataMapping(
+        *_pos_and_val(parameter_value_metadata)
+    )
+    param_val_metadata_mapping.child = parameter_value_mapping_from_dict(map_dict.get("value"))
+    return param_def_mapping
+
+
+def parameter_default_value_mapping_from_dict(default_value_dict):
+    if default_value_dict is None:
+        return ParameterDefaultValueMapping(*_pos_and_val(None))
+    value_type = default_value_dict["value_type"].replace(" ", "_")
+    main_value = default_value_dict.get("main_value")
+    if value_type == "single_value":
+        return ParameterDefaultValueMapping(*_pos_and_val(main_value))
+    extra_dimensions = default_value_dict.get("extra_dimensions", [None])
+    compress = default_value_dict.get("compress", False)
+    options = default_value_dict.get("options", {})
+    root_mapping = ParameterDefaultValueTypeMapping(Position.hidden, value_type, compress=compress, options=options)
+    parent_mapping = root_mapping
+    for ed in extra_dimensions:
+        name_mapping = DefaultValueIndexNameMapping(Position.hidden, value=None)
+        parent_mapping.child = name_mapping
+        index_mapping = ParameterDefaultValueIndexMapping(*_pos_and_val(ed))
+        name_mapping.child = index_mapping
+        parent_mapping = index_mapping
+    parent_mapping.child = ExpandedParameterDefaultValueMapping(*_pos_and_val(main_value))
+    return root_mapping
+
+
+def parameter_value_mapping_from_dict(value_dict):
+    if value_dict is None:
+        return ParameterValueMapping(*_pos_and_val(None))
+    value_type = value_dict["value_type"].replace(" ", "_")
+    main_value = value_dict.get("main_value")
+    if value_type == "single_value":
+        return ParameterValueMapping(*_pos_and_val(main_value))
+    extra_dimensions = value_dict.get("extra_dimensions", [None])
+    compress = value_dict.get("compress", False)
+    options = value_dict.get("options", {})
+    root_mapping = ParameterValueTypeMapping(Position.hidden, value_type, compress=compress, options=options)
+    parent_mapping = root_mapping
+    for ed in extra_dimensions:
+        name_mapping = IndexNameMapping(Position.hidden, value=None)
+        parent_mapping.child = name_mapping
+        index_mapping = ParameterValueIndexMapping(*_pos_and_val(ed))
+        name_mapping.child = index_mapping
+        parent_mapping = index_mapping
+    parent_mapping.child = ExpandedParameterValueMapping(*_pos_and_val(main_value))
+    return root_mapping
+
+
+def _fix_parameter_mapping_dict(map_dict):
+    # Even deeper legacy
+    parameter_type = map_dict.pop("parameter_type", None)
+    if parameter_type == "definition":
+        map_dict["map_type"] = "ParameterDefinition"
+    else:
+        value_dict = map_dict.copy()
+        value_dict.pop("name", None)
+        value_dict["value_type"] = parameter_type if parameter_type else "single value"
+        value_dict["main_value"] = value_dict.pop("value", None)
+        map_dict["map_type"] = "ParameterValue"
+        map_dict["value"] = value_dict
+
+
+def _pos_and_val(x):
+    if not isinstance(x, dict):
+        map_type = "constant" if isinstance(x, str) else "column"
+        map_dict = {"map_type": map_type, "reference": x}
+    else:
+        map_dict = x
+    map_type = map_dict.get("map_type")
+    ref = map_dict.get("reference", map_dict.get("value_reference"))
+    if isinstance(ref, str) and not ref:
+        ref = None
+    # None, or invalid reference
+    if map_type == "None" or ref is None:
+        return Position.hidden, None  # This combination disables the mapping
+    # Constant
+    if map_type == "constant":
+        if not isinstance(ref, str):
+            raise TypeError(f"Constant reference must be str, instead got: {type(ref).__name__}")
+        return Position.hidden, ref
+    # Table name
+    if map_type == "table_name":
+        return Position.table_name, None
+    # Row or column reference, including header
+    if not isinstance(ref, (str, int)):
+        raise TypeError(f"Row or column reference must be str or int, instead got: {type(ref).__name__}")
+    # 1. Column header
+    if map_type in ("column_name", "column_header"):
+        if isinstance(ref, int) and ref < 0:
+            ref = 0
+        return Position.header, ref
+    # 2. Data row or column
+    try:
+        ref = int(ref)
+    except ValueError:
+        pass
+    # 2a. Column
+    if map_type == "column":
+        if isinstance(ref, int) and ref < 0:
+            ref = 0
+        return ref, None
+    # 2b. Row
+    if map_type == "row":
+        if isinstance(ref, int):
+            if ref == -1:
+                return Position.header, None
+            if ref < -1:
+                ref = 0
+            return -(ref + 1), None  # pylint: disable=invalid-unary-operand-type
+        if ref.lower() == "header":
+            return Position.header, None
+        raise ValueError(f"If row reference is str, it must be 'header'. Instead got '{ref}'")
+    # Fallback to invalid
+    return Position.hidden, None
```

### Comparing `spinedb_api-0.30.3/spinedb_api/import_mapping/type_conversion.py` & `spinedb_api-0.30.4/spinedb_api/import_mapping/type_conversion.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,118 +1,118 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Type conversion functions.
-
-"""
-
-import re
-from distutils.util import strtobool
-from spinedb_api.parameter_value import DateTime, Duration, ParameterValueFormatError
-
-
-def value_to_convert_spec(value):
-    if isinstance(value, ConvertSpec):
-        return value
-    if isinstance(value, str):
-        spec = {
-            "datetime": DateTimeConvertSpec,
-            "duration": DurationConvertSpec,
-            "float": FloatConvertSpec,
-            "string": StringConvertSpec,
-            "boolean": BooleanConvertSpec,
-        }.get(value)
-        return spec()
-    if isinstance(value, dict):
-        start_datetime = DateTime(value.get("start_datetime"))
-        duration = Duration(value.get("duration"))
-        start_int = value.get("start_int")
-        return IntegerSequenceDateTimeConvertSpec(start_datetime, start_int, duration)
-    raise TypeError(f"value must be str or dict instead got {type(value).__name__}")
-
-
-class ConvertSpec:
-    DISPLAY_NAME = ""
-    RETURN_TYPE = str
-
-    def __call__(self, value):
-        try:
-            return self.RETURN_TYPE(value)
-        except ValueError as error:
-            if not value:
-                return None
-            raise error
-
-    def to_json_value(self):
-        return self.DISPLAY_NAME
-
-
-class DateTimeConvertSpec(ConvertSpec):
-    DISPLAY_NAME = "datetime"
-    RETURN_TYPE = DateTime
-
-
-class DurationConvertSpec(ConvertSpec):
-    DISPLAY_NAME = "duration"
-    RETURN_TYPE = Duration
-
-
-class FloatConvertSpec(ConvertSpec):
-    DISPLAY_NAME = "float"
-    RETURN_TYPE = float
-
-
-class StringConvertSpec(ConvertSpec):
-    DISPLAY_NAME = "string"
-    RETURN_TYPE = str
-
-
-class BooleanConvertSpec(ConvertSpec):
-    DISPLAY_NAME = "boolean"
-    RETURN_TYPE = bool
-
-    def __call__(self, value):
-        return self.RETURN_TYPE(strtobool(str(value)))
-
-
-class IntegerSequenceDateTimeConvertSpec(ConvertSpec):
-    DISPLAY_NAME = "integer sequence datetime"
-    RETURN_TYPE = DateTime
-
-    def __init__(self, start_datetime, start_int, duration):
-        if not isinstance(start_datetime, DateTime):
-            start_datetime = DateTime(start_datetime)
-        if not isinstance(duration, Duration):
-            duration = Duration(duration)
-        self.start_datetime = start_datetime
-        self.start_int = start_int
-        self.duration = duration
-        self.pattern = re.compile(r"[0-9]+|$")
-
-    def __call__(self, value):
-        start_datetime = self.start_datetime.value
-        duration = self.duration.value
-        start_int = self.start_int
-        pattern = self.pattern
-        try:
-            int_str = pattern.search(str(value)).group()
-            int_value = int(int_str) - start_int
-            return DateTime(start_datetime + int_value * duration)
-        except (ValueError, ParameterValueFormatError):
-            raise ValueError(f"Could not convert '{value}' to a DateTime")
-
-    def to_json_value(self):
-        return {
-            "name": self.DISPLAY_NAME,
-            "start_datetime": self.start_datetime.value.isoformat(),
-            "duration": str(self.duration),
-            "start_int": self.start_int,
-        }
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Type conversion functions.
+
+"""
+
+import re
+from distutils.util import strtobool
+from spinedb_api.parameter_value import DateTime, Duration, ParameterValueFormatError
+
+
+def value_to_convert_spec(value):
+    if isinstance(value, ConvertSpec):
+        return value
+    if isinstance(value, str):
+        spec = {
+            "datetime": DateTimeConvertSpec,
+            "duration": DurationConvertSpec,
+            "float": FloatConvertSpec,
+            "string": StringConvertSpec,
+            "boolean": BooleanConvertSpec,
+        }.get(value)
+        return spec()
+    if isinstance(value, dict):
+        start_datetime = DateTime(value.get("start_datetime"))
+        duration = Duration(value.get("duration"))
+        start_int = value.get("start_int")
+        return IntegerSequenceDateTimeConvertSpec(start_datetime, start_int, duration)
+    raise TypeError(f"value must be str or dict instead got {type(value).__name__}")
+
+
+class ConvertSpec:
+    DISPLAY_NAME = ""
+    RETURN_TYPE = str
+
+    def __call__(self, value):
+        try:
+            return self.RETURN_TYPE(value)
+        except ValueError as error:
+            if not value:
+                return None
+            raise error
+
+    def to_json_value(self):
+        return self.DISPLAY_NAME
+
+
+class DateTimeConvertSpec(ConvertSpec):
+    DISPLAY_NAME = "datetime"
+    RETURN_TYPE = DateTime
+
+
+class DurationConvertSpec(ConvertSpec):
+    DISPLAY_NAME = "duration"
+    RETURN_TYPE = Duration
+
+
+class FloatConvertSpec(ConvertSpec):
+    DISPLAY_NAME = "float"
+    RETURN_TYPE = float
+
+
+class StringConvertSpec(ConvertSpec):
+    DISPLAY_NAME = "string"
+    RETURN_TYPE = str
+
+
+class BooleanConvertSpec(ConvertSpec):
+    DISPLAY_NAME = "boolean"
+    RETURN_TYPE = bool
+
+    def __call__(self, value):
+        return self.RETURN_TYPE(strtobool(str(value)))
+
+
+class IntegerSequenceDateTimeConvertSpec(ConvertSpec):
+    DISPLAY_NAME = "integer sequence datetime"
+    RETURN_TYPE = DateTime
+
+    def __init__(self, start_datetime, start_int, duration):
+        if not isinstance(start_datetime, DateTime):
+            start_datetime = DateTime(start_datetime)
+        if not isinstance(duration, Duration):
+            duration = Duration(duration)
+        self.start_datetime = start_datetime
+        self.start_int = start_int
+        self.duration = duration
+        self.pattern = re.compile(r"[0-9]+|$")
+
+    def __call__(self, value):
+        start_datetime = self.start_datetime.value
+        duration = self.duration.value
+        start_int = self.start_int
+        pattern = self.pattern
+        try:
+            int_str = pattern.search(str(value)).group()
+            int_value = int(int_str) - start_int
+            return DateTime(start_datetime + int_value * duration)
+        except (ValueError, ParameterValueFormatError):
+            raise ValueError(f"Could not convert '{value}' to a DateTime")
+
+    def to_json_value(self):
+        return {
+            "name": self.DISPLAY_NAME,
+            "start_datetime": self.start_datetime.value.isoformat(),
+            "duration": str(self.duration),
+            "start_int": self.start_int,
+        }
```

### Comparing `spinedb_api-0.30.3/spinedb_api/mapping.py` & `spinedb_api-0.30.4/spinedb_api/mapping.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,293 +1,293 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Contains export mappings for database items such as entities, entity classes and parameter values.
-
-"""
-
-from enum import Enum, unique
-from itertools import takewhile
-import re
-
-
-@unique
-class Position(Enum):
-    """Export item positions when they are not in columns."""
-
-    hidden = "hidden"
-    table_name = "table_name"
-    header = "header"
-
-
-def is_pivoted(position):
-    """Checks if position is pivoted.
-
-    Args:
-        position (Position or int): position
-
-    Returns:
-        bool: True if position is pivoted, False otherwise
-    """
-    return isinstance(position, int) and position < 0
-
-
-def is_regular(position):
-    """Checks if position is column index.
-
-    Args:
-        position (Position or int): position
-
-    Returns:
-        bool: True if position is a column index, False otherwise
-    """
-    return isinstance(position, int) and position >= 0
-
-
-class Mapping:
-    """Base class for import/export item mappings.
-
-    Attributes:
-        position (int or Position): defines where the data is written/read in the output table.
-            Nonnegative numbers are columns, negative numbers are pivot rows, and then there are some special cases
-            in the Position enum.
-        parent (Mapping or None): Another mapping that's the 'parent' of this one.
-            Used to determine if a mapping is root, in which case it needs to yield the header.
-    """
-
-    MAP_TYPE = None
-    """Mapping type identifier for serialization."""
-
-    def __init__(self, position, value=None, filter_re=""):
-        """
-        Args:
-            position (int or Position): column index or Position
-            value (Any): fixed value
-            filter_re (str): regular expression for filtering
-        """
-        self._child = None
-        self._value = None
-        self._unfixed_value_data = self._data
-        self._filter_re = None
-        self.parent = None
-        self.position = position
-        self.value = value
-        self.filter_re = filter_re
-
-    @property
-    def child(self):
-        return self._child
-
-    @child.setter
-    def child(self, child):
-        self._child = child
-        if isinstance(child, Mapping):
-            child.parent = self
-
-    @property
-    def value(self):
-        """Fixed value."""
-        return self._value
-
-    @value.setter
-    def value(self, value):
-        self._value = value
-        self._set_fixed_value_data()
-
-    @property
-    def filter_re(self):
-        return self._filter_re.pattern if self._filter_re is not None else ""
-
-    @filter_re.setter
-    def filter_re(self, filter_re):
-        self._filter_re = re.compile(filter_re) if filter_re else None
-
-    def _data(self, row):
-        raise NotImplementedError()
-
-    def _fixed_value_data(self, _row):
-        return self._value
-
-    def _set_fixed_value_data(self):
-        if self._value is None:
-            self._data = self._unfixed_value_data
-            return
-        self._data = self._fixed_value_data
-
-    def __eq__(self, other):
-        if not isinstance(other, Mapping):
-            return NotImplemented
-        return (
-            self.MAP_TYPE == other.MAP_TYPE
-            and self.position == other.position
-            and self.child == other.child
-            and self._filter_re == other._filter_re
-        )
-
-    def tail_mapping(self):
-        """Returns the last mapping in the chain.
-
-        Returns:
-            Mapping: last child mapping
-        """
-        if self._child is None:
-            return self
-        return self._child.tail_mapping()
-
-    def count_mappings(self):
-        """
-        Counts this and child mappings.
-
-        Returns:
-            int: number of mappings
-        """
-        return 1 + (self.child.count_mappings() if self.child is not None else 0)
-
-    def flatten(self):
-        """
-        Flattens the mapping tree.
-
-        Returns:
-            list of Mapping: mappings in parent-child-grand child-etc order
-        """
-        return [self] + (self.child.flatten() if self.child is not None else [])
-
-    def is_pivoted(self):
-        """
-        Queries recursively if export items are pivoted.
-
-        Returns:
-            bool: True if any of the items is pivoted, False otherwise
-        """
-        if is_pivoted(self.position):
-            return True
-        if self.child is None:
-            return False
-        return self.child.is_pivoted()
-
-    def non_pivoted_width(self, parent_is_pivoted=False):
-        """
-        Calculates columnar width of non-pivoted data.
-
-        Args:
-            parent_is_pivoted (bool): True if a parent item is pivoted, False otherwise
-
-        Returns:
-            int: non-pivoted data width
-        """
-        if self.child is None:
-            if is_regular(self.position) and not parent_is_pivoted:
-                return self.position + 1
-            return 0
-        width = self.position + 1 if is_regular(self.position) else 0
-        return max(width, self.child.non_pivoted_width(parent_is_pivoted or is_pivoted(self.position)))
-
-    def non_pivoted_columns(self, parent_is_pivoted=False):
-        """Gathers non-pivoted columns from mappings.
-
-        Args:
-            parent_is_pivoted (bool): True if a parent item is pivoted, False otherwise
-
-        Returns:
-            list of int: indexes of non-pivoted columns
-        """
-        if self._child is None:
-            if is_regular(self.position) and not parent_is_pivoted:
-                return [self.position]
-            return []
-        pivoted = is_pivoted(self.position)
-        return ([self.position] if is_regular(self.position) else []) + self._child.non_pivoted_columns(
-            parent_is_pivoted or pivoted
-        )
-
-    def last_pivot_row(self):
-        return max(
-            [-(m.position + 1) for m in self.flatten() if isinstance(m.position, int) and m.position < 0], default=-1
-        )
-
-    def query_parents(self, what):
-        """Queries parent mapping for specific information.
-
-        Args:
-            what (str): query identifier
-
-        Returns:
-            Any: query result or None if no parent recognized the identifier
-        """
-        if self.parent is None:
-            return None
-        return self.parent.query_parents(what)
-
-    def to_dict(self):
-        """
-        Serializes mapping into dict.
-
-        Returns:
-            dict: serialized mapping
-        """
-        position = self.position.value if isinstance(self.position, Position) else self.position
-        mapping_dict = {"map_type": self.MAP_TYPE, "position": position}
-        if self.value is not None:
-            mapping_dict["value"] = self.value
-        if self._filter_re is not None:
-            mapping_dict["filter_re"] = self._filter_re.pattern
-        return mapping_dict
-
-
-def unflatten(mappings):
-    """
-    Builds a mapping hierarchy from flattened mappings.
-
-    Args:
-        mappings (Iterable of Mapping): flattened mappings
-
-    Returns:
-        Mapping: root mapping
-    """
-    root = None
-    current = None
-    for mapping in mappings:
-        if root is None:
-            root = mapping
-        else:
-            current.child = mapping
-        current = mapping
-    current.child = None
-    return root
-
-
-def value_index(flattened_mappings):
-    """
-    Returns index of last non-hidden mapping in flattened mapping list.
-
-    Args:
-        flattened_mappings (list of Mapping): flattened mappings
-
-    Returns:
-        int: value mapping index
-    """
-    return (
-        len(flattened_mappings)
-        - 1
-        - len(list(takewhile(lambda m: m.position == Position.hidden, reversed(flattened_mappings))))
-    )
-
-
-def to_dict(root_mapping):
-    """
-    Serializes mappings into JSON compatible data structure.
-
-    Args:
-        root_mapping (Mapping): root mapping
-
-    Returns:
-        list: serialized mappings
-    """
-    return list(mapping.to_dict() for mapping in root_mapping.flatten())
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Contains export mappings for database items such as entities, entity classes and parameter values.
+
+"""
+
+from enum import Enum, unique
+from itertools import takewhile
+import re
+
+
+@unique
+class Position(Enum):
+    """Export item positions when they are not in columns."""
+
+    hidden = "hidden"
+    table_name = "table_name"
+    header = "header"
+
+
+def is_pivoted(position):
+    """Checks if position is pivoted.
+
+    Args:
+        position (Position or int): position
+
+    Returns:
+        bool: True if position is pivoted, False otherwise
+    """
+    return isinstance(position, int) and position < 0
+
+
+def is_regular(position):
+    """Checks if position is column index.
+
+    Args:
+        position (Position or int): position
+
+    Returns:
+        bool: True if position is a column index, False otherwise
+    """
+    return isinstance(position, int) and position >= 0
+
+
+class Mapping:
+    """Base class for import/export item mappings.
+
+    Attributes:
+        position (int or Position): defines where the data is written/read in the output table.
+            Nonnegative numbers are columns, negative numbers are pivot rows, and then there are some special cases
+            in the Position enum.
+        parent (Mapping or None): Another mapping that's the 'parent' of this one.
+            Used to determine if a mapping is root, in which case it needs to yield the header.
+    """
+
+    MAP_TYPE = None
+    """Mapping type identifier for serialization."""
+
+    def __init__(self, position, value=None, filter_re=""):
+        """
+        Args:
+            position (int or Position): column index or Position
+            value (Any): fixed value
+            filter_re (str): regular expression for filtering
+        """
+        self._child = None
+        self._value = None
+        self._unfixed_value_data = self._data
+        self._filter_re = None
+        self.parent = None
+        self.position = position
+        self.value = value
+        self.filter_re = filter_re
+
+    @property
+    def child(self):
+        return self._child
+
+    @child.setter
+    def child(self, child):
+        self._child = child
+        if isinstance(child, Mapping):
+            child.parent = self
+
+    @property
+    def value(self):
+        """Fixed value."""
+        return self._value
+
+    @value.setter
+    def value(self, value):
+        self._value = value
+        self._set_fixed_value_data()
+
+    @property
+    def filter_re(self):
+        return self._filter_re.pattern if self._filter_re is not None else ""
+
+    @filter_re.setter
+    def filter_re(self, filter_re):
+        self._filter_re = re.compile(filter_re) if filter_re else None
+
+    def _data(self, row):
+        raise NotImplementedError()
+
+    def _fixed_value_data(self, _row):
+        return self._value
+
+    def _set_fixed_value_data(self):
+        if self._value is None:
+            self._data = self._unfixed_value_data
+            return
+        self._data = self._fixed_value_data
+
+    def __eq__(self, other):
+        if not isinstance(other, Mapping):
+            return NotImplemented
+        return (
+            self.MAP_TYPE == other.MAP_TYPE
+            and self.position == other.position
+            and self.child == other.child
+            and self._filter_re == other._filter_re
+        )
+
+    def tail_mapping(self):
+        """Returns the last mapping in the chain.
+
+        Returns:
+            Mapping: last child mapping
+        """
+        if self._child is None:
+            return self
+        return self._child.tail_mapping()
+
+    def count_mappings(self):
+        """
+        Counts this and child mappings.
+
+        Returns:
+            int: number of mappings
+        """
+        return 1 + (self.child.count_mappings() if self.child is not None else 0)
+
+    def flatten(self):
+        """
+        Flattens the mapping tree.
+
+        Returns:
+            list of Mapping: mappings in parent-child-grand child-etc order
+        """
+        return [self] + (self.child.flatten() if self.child is not None else [])
+
+    def is_pivoted(self):
+        """
+        Queries recursively if export items are pivoted.
+
+        Returns:
+            bool: True if any of the items is pivoted, False otherwise
+        """
+        if is_pivoted(self.position):
+            return True
+        if self.child is None:
+            return False
+        return self.child.is_pivoted()
+
+    def non_pivoted_width(self, parent_is_pivoted=False):
+        """
+        Calculates columnar width of non-pivoted data.
+
+        Args:
+            parent_is_pivoted (bool): True if a parent item is pivoted, False otherwise
+
+        Returns:
+            int: non-pivoted data width
+        """
+        if self.child is None:
+            if is_regular(self.position) and not parent_is_pivoted:
+                return self.position + 1
+            return 0
+        width = self.position + 1 if is_regular(self.position) else 0
+        return max(width, self.child.non_pivoted_width(parent_is_pivoted or is_pivoted(self.position)))
+
+    def non_pivoted_columns(self, parent_is_pivoted=False):
+        """Gathers non-pivoted columns from mappings.
+
+        Args:
+            parent_is_pivoted (bool): True if a parent item is pivoted, False otherwise
+
+        Returns:
+            list of int: indexes of non-pivoted columns
+        """
+        if self._child is None:
+            if is_regular(self.position) and not parent_is_pivoted:
+                return [self.position]
+            return []
+        pivoted = is_pivoted(self.position)
+        return ([self.position] if is_regular(self.position) else []) + self._child.non_pivoted_columns(
+            parent_is_pivoted or pivoted
+        )
+
+    def last_pivot_row(self):
+        return max(
+            [-(m.position + 1) for m in self.flatten() if isinstance(m.position, int) and m.position < 0], default=-1
+        )
+
+    def query_parents(self, what):
+        """Queries parent mapping for specific information.
+
+        Args:
+            what (str): query identifier
+
+        Returns:
+            Any: query result or None if no parent recognized the identifier
+        """
+        if self.parent is None:
+            return None
+        return self.parent.query_parents(what)
+
+    def to_dict(self):
+        """
+        Serializes mapping into dict.
+
+        Returns:
+            dict: serialized mapping
+        """
+        position = self.position.value if isinstance(self.position, Position) else self.position
+        mapping_dict = {"map_type": self.MAP_TYPE, "position": position}
+        if self.value is not None:
+            mapping_dict["value"] = self.value
+        if self._filter_re is not None:
+            mapping_dict["filter_re"] = self._filter_re.pattern
+        return mapping_dict
+
+
+def unflatten(mappings):
+    """
+    Builds a mapping hierarchy from flattened mappings.
+
+    Args:
+        mappings (Iterable of Mapping): flattened mappings
+
+    Returns:
+        Mapping: root mapping
+    """
+    root = None
+    current = None
+    for mapping in mappings:
+        if root is None:
+            root = mapping
+        else:
+            current.child = mapping
+        current = mapping
+    current.child = None
+    return root
+
+
+def value_index(flattened_mappings):
+    """
+    Returns index of last non-hidden mapping in flattened mapping list.
+
+    Args:
+        flattened_mappings (list of Mapping): flattened mappings
+
+    Returns:
+        int: value mapping index
+    """
+    return (
+        len(flattened_mappings)
+        - 1
+        - len(list(takewhile(lambda m: m.position == Position.hidden, reversed(flattened_mappings))))
+    )
+
+
+def to_dict(root_mapping):
+    """
+    Serializes mappings into JSON compatible data structure.
+
+    Args:
+        root_mapping (Mapping): root mapping
+
+    Returns:
+        list: serialized mappings
+    """
+    return list(mapping.to_dict() for mapping in root_mapping.flatten())
```

### Comparing `spinedb_api-0.30.3/spinedb_api/parameter_value.py` & `spinedb_api-0.30.4/spinedb_api/parameter_value.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,1591 +1,1591 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Support utilities and classes to deal with Spine data (relationship)
-parameter values.
-
-The `from_database` function reads the database's value format returning
-a float, Datatime, Duration, TimePattern, TimeSeriesFixedResolution
-TimeSeriesVariableResolution or Map objects.
-
-The above objects can be converted back to the database format by the `to_database` free function
-or by their `to_database` member functions.
-
-Individual datetimes are represented as datetime objects from the standard Python library.
-Individual time steps are represented as relativedelta objects from the dateutil package.
-Datetime indexes (as returned by TimeSeries.indexes()) are represented as
-numpy.ndarray arrays holding numpy.datetime64 objects.
-
-"""
-
-from collections.abc import Sequence
-from copy import copy
-from datetime import datetime
-import json
-from json.decoder import JSONDecodeError
-from numbers import Number
-import re
-import dateutil.parser
-from dateutil.relativedelta import relativedelta
-import numpy as np
-from .exception import ParameterValueFormatError
-
-# Defaulting to seconds precision in numpy.
-_NUMPY_DATETIME_DTYPE = "datetime64[s]"
-NUMPY_DATETIME64_UNIT = "s"
-# Default start time guess, actual value not currently given in the JSON specification.
-_TIME_SERIES_DEFAULT_START = "0001-01-01T00:00:00"
-# Default resolution if it is omitted from the index entry.
-_TIME_SERIES_DEFAULT_RESOLUTION = "1h"
-# Default unit if resolution is given as a number instead of a string.
-_TIME_SERIES_PLAIN_INDEX_UNIT = "m"
-
-
-def duration_to_relativedelta(duration):
-    """
-    Converts a duration to a relativedelta object.
-
-    Args:
-        duration (str): a duration specification
-
-    Returns:
-        a relativedelta object corresponding to the given duration
-    """
-    try:
-        count, abbreviation, full_unit = re.split("\\s|([a-z]|[A-Z])", duration, maxsplit=1)
-        count = int(count)
-    except ValueError:
-        raise ParameterValueFormatError(f'Could not parse duration "{duration}"')
-    unit = abbreviation if abbreviation is not None else full_unit
-    if unit in ["s", "second", "seconds"]:
-        return relativedelta(seconds=count)
-    if unit in ["m", "minute", "minutes"]:
-        return relativedelta(minutes=count)
-    if unit in ["h", "hour", "hours"]:
-        return relativedelta(hours=count)
-    if unit in ["D", "day", "days"]:
-        return relativedelta(days=count)
-    if unit in ["M", "month", "months"]:
-        return relativedelta(months=count)
-    if unit in ["Y", "year", "years"]:
-        return relativedelta(years=count)
-    raise ParameterValueFormatError(f'Could not parse duration "{duration}"')
-
-
-def relativedelta_to_duration(delta):
-    """
-    Converts a relativedelta to duration.
-
-    Args:
-        delta (relativedelta): the relativedelta to convert
-
-    Returns:
-        a duration string
-    """
-    if delta.seconds > 0:
-        seconds = delta.seconds
-        seconds += 60 * delta.minutes
-        seconds += 60 * 60 * delta.hours
-        seconds += 60 * 60 * 24 * delta.days
-        # Skipping months and years since dateutil does not use them here
-        # and they wouldn't make much sense anyway.
-        return f"{seconds}s"
-    if delta.minutes > 0:
-        minutes = delta.minutes
-        minutes += 60 * delta.hours
-        minutes += 60 * 24 * delta.days
-        return f"{minutes}m"
-    if delta.hours > 0:
-        hours = delta.hours
-        hours += 24 * delta.days
-        return f"{hours}h"
-    if delta.days > 0:
-        return f"{delta.days}D"
-    if delta.months > 0:
-        months = delta.months
-        months += 12 * delta.years
-        return f"{months}M"
-    if delta.years > 0:
-        return f"{delta.years}Y"
-    return "0h"
-
-
-def load_db_value(db_value, value_type=None):
-    """
-    Loads a database parameter value into a Python object using JSON.
-    Adds the "type" property to dicts representing complex types.
-
-    Args:
-        db_value (bytes, optional): a value in the database
-        value_type (str, optional): the type in case of complex ones
-
-    Returns:
-        Any: the parsed parameter value
-    """
-    if db_value is None:
-        return None
-    try:
-        parsed = json.loads(db_value)
-    except JSONDecodeError as err:
-        raise ParameterValueFormatError(f"Could not decode the value: {err}") from err
-    if isinstance(parsed, dict):
-        return {"type": value_type, **parsed}
-    return parsed
-
-
-def dump_db_value(parsed_value):
-    """
-    Dumps a Python object into a database parameter value using JSON.
-    Extracts the "type" property from dicts representing complex types.
-
-    Args:
-        parsed_value (Any): the Python object
-
-    Returns:
-        str: the database parameter value
-        str: the type
-    """
-    value_type = parsed_value.pop("type") if isinstance(parsed_value, dict) else None
-    db_value = json.dumps(parsed_value).encode("UTF8")
-    if isinstance(parsed_value, dict) and value_type is not None:
-        parsed_value["type"] = value_type
-    return db_value, value_type
-
-
-def from_database(database_value, value_type=None):
-    """
-    Converts a parameter value from its database representation into an encoded Python object.
-
-    Args:
-        database_value (bytes, optional): a value in the database
-        value_type (str, optional): the type in case of complex ones
-
-    Returns:
-        Any: the encoded parameter value
-    """
-    parsed = load_db_value(database_value, value_type)
-    if isinstance(parsed, dict):
-        return from_dict(parsed)
-    if isinstance(parsed, bool):
-        return parsed
-    if isinstance(parsed, Number):
-        return float(parsed)
-    return parsed
-
-
-def from_database_to_single_value(database_value, value_type):
-    """
-    Converts a value from its database representation into a single value.
-
-    Indexed values get converted to their type string.
-
-    Args:
-        database_value (bytes): a value in the database
-        value_type (str, optional): value's type
-
-    Returns:
-        Any: single-value representation
-    """
-    if value_type is None or value_type not in ("map", "time_series", "time_pattern", "array"):
-        return from_database(database_value, value_type)
-    return value_type
-
-
-def from_database_to_dimension_count(database_value, value_type):
-    """
-    Counts dimensions of value's database representation
-
-    Args:
-        database_value (bytes): a value in the database
-        value_type (str, optional): value's type
-
-    Returns:
-        int: number of dimensions
-    """
-
-    if value_type in {"time_series", "time_pattern", "array"}:
-        return 1
-    if value_type == "map":
-        map_value = from_database(database_value, value_type)
-        return map_dimensions(map_value)
-    return 0
-
-
-def to_database(parsed_value):
-    """
-    Converts an encoded Python object into its database representation.
-
-    Args:
-        value: the value to convert. It can be the result of either ``load_db_value`` or ``from_database```.
-
-    Returns:
-        bytes: value's database representation as bytes
-        str: the value type
-    """
-    if hasattr(parsed_value, "to_database"):
-        return parsed_value.to_database()
-    db_value = json.dumps(parsed_value).encode("UTF8")
-    return db_value, None
-
-
-def from_dict(value_dict):
-    """
-    Converts a complex (relationship) parameter value from its dictionary representation to a Python object.
-
-    Args:
-        value_dict (dict): value's dictionary; a parsed JSON object with the "type" key
-
-    Returns:
-        the encoded (relationship) parameter value
-    """
-    value_type = value_dict["type"]
-    try:
-        if value_type == "date_time":
-            return _datetime_from_database(value_dict["data"])
-        if value_type == "duration":
-            return _duration_from_database(value_dict["data"])
-        if value_type == "map":
-            return _map_from_database(value_dict)
-        if value_type == "time_pattern":
-            return _time_pattern_from_database(value_dict)
-        if value_type == "time_series":
-            return _time_series_from_database(value_dict)
-        if value_type == "array":
-            return _array_from_database(value_dict)
-        raise ParameterValueFormatError(f'Unknown parameter value type "{value_type}"')
-    except KeyError as error:
-        raise ParameterValueFormatError(f'"{error.args[0]}" is missing in the parameter value description')
-
-
-def fix_conflict(new, old, on_conflict="merge"):
-    """Resolves conflicts between parameter values:
-
-    Args:
-        new (any): new parameter value to write
-        old (any): existing parameter value in the db
-        on_conflict (str): conflict resolution strategy:
-            - 'merge': Merge indexes if possible, otherwise replace
-            - 'replace': Replace old with new
-            - 'keep': keep old
-
-    Returns:
-        any: a parameter value with conflicts resolved
-    """
-    funcs = {"keep": lambda new, old: old, "replace": lambda new, old: new, "merge": merge}
-    func = funcs.get(on_conflict)
-    if func is None:
-        raise RuntimeError(
-            f"Invalid conflict resolution strategy {on_conflict}, valid strategies are {', '.join(funcs)}"
-        )
-    return func(new, old)
-
-
-def merge(value, other):
-    """Merges other into value, returns the result.
-    Args:
-        value (tuple): recipient value and type
-        other (tuple): other value and type
-
-    Returns:
-        tuple: value and type of merged value
-    """
-    parsed_value = from_database(*value)
-    if not hasattr(parsed_value, "merge"):
-        return value
-    parsed_other = from_database(*other)
-    return to_database(parsed_value.merge(parsed_other))
-
-
-def merge_parsed(parsed_value, parsed_other):
-    if not hasattr(parsed_value, "merge"):
-        return parsed_value
-    return parsed_value.merge(parsed_other)
-
-
-def _break_dictionary(data):
-    """Converts {"index": value} style dictionary into (list(indexes), numpy.ndarray(values)) tuple."""
-    if not isinstance(data, dict):
-        raise ParameterValueFormatError(
-            f"expected data to be in dictionary format, instead got '{type(data).__name__}'"
-        )
-    indexes, values = zip(*data.items())
-    return list(indexes), np.array(values)
-
-
-def _datetime_from_database(value):
-    """Converts a datetime database value into a DateTime object."""
-    try:
-        stamp = dateutil.parser.parse(value)
-    except ValueError:
-        raise ParameterValueFormatError(f'Could not parse datetime from "{value}"')
-    return DateTime(stamp)
-
-
-def _duration_from_database(value):
-    """
-    Converts a duration database value into a Duration object.
-
-    The deprecated 'variable durations' will be converted to Arrays.
-    """
-    if isinstance(value, (str, int)):
-        # Set default unit to minutes if value is a plain number.
-        if not isinstance(value, str):
-            value = f"{value}m"
-    elif isinstance(value, Sequence):
-        # This type of 'variable duration' is deprecated. We make an Array instead.
-        # Set default unit to minutes for plain numbers in value.
-        value = [v if isinstance(v, str) else f"{v}m" for v in value]
-        return Array([Duration(v) for v in value])
-    else:
-        raise ParameterValueFormatError("Duration value is of unsupported type")
-    return Duration(value)
-
-
-def _time_series_from_database(value_dict):
-    """Converts a time series database value into a time series object.
-
-    Args:
-        value_dict (dict): time series dictionary
-
-    Returns:
-        TimeSeries: restored time series
-    """
-    data = value_dict["data"]
-    if isinstance(data, dict):
-        return _time_series_from_dictionary(value_dict)
-    if isinstance(data, list):
-        if isinstance(data[0], Sequence):
-            return _time_series_from_two_columns(value_dict)
-        return _time_series_from_single_column(value_dict)
-    raise ParameterValueFormatError("Unrecognized time series format")
-
-
-def _variable_resolution_time_series_info_from_index(value):
-    """Returns ignore_year and repeat from index if present or their default values."""
-    if "index" in value:
-        data_index = value["index"]
-        try:
-            ignore_year = bool(data_index.get("ignore_year", False))
-        except ValueError:
-            raise ParameterValueFormatError(f'Could not decode ignore_year from "{data_index["ignore_year"]}"')
-        try:
-            repeat = bool(data_index.get("repeat", False))
-        except ValueError:
-            raise ParameterValueFormatError(f'Could not decode repeat from "{data_index["repeat"]}"')
-    else:
-        ignore_year = False
-        repeat = False
-    return ignore_year, repeat
-
-
-def _time_series_from_dictionary(value_dict):
-    """Converts a dictionary style time series into a TimeSeriesVariableResolution object.
-
-    Args:
-        value_dict (dict): time series dictionary
-
-    Returns:
-        TimeSeriesVariableResolution: restored time series
-    """
-    data = value_dict["data"]
-    stamps = list()
-    values = np.empty(len(data))
-    for index, (stamp, series_value) in enumerate(data.items()):
-        try:
-            stamp = np.datetime64(stamp, NUMPY_DATETIME64_UNIT)
-        except ValueError:
-            raise ParameterValueFormatError(f'Could not decode time stamp "{stamp}"')
-        stamps.append(stamp)
-        values[index] = series_value
-    stamps = np.array(stamps)
-    ignore_year, repeat = _variable_resolution_time_series_info_from_index(value_dict)
-    return TimeSeriesVariableResolution(stamps, values, ignore_year, repeat, value_dict.get("index_name", ""))
-
-
-def _time_series_from_single_column(value_dict):
-    """Converts a time series dictionary into a TimeSeriesFixedResolution object.
-
-    Args:
-        value_dict (dict): time series dictionary
-
-    Returns:
-        TimeSeriesFixedResolution: restored time series
-    """
-    if "index" in value_dict:
-        value_index = value_dict["index"]
-        start = value_index["start"] if "start" in value_index else _TIME_SERIES_DEFAULT_START
-        resolution = value_index["resolution"] if "resolution" in value_index else _TIME_SERIES_DEFAULT_RESOLUTION
-        if "ignore_year" in value_index:
-            try:
-                ignore_year = bool(value_index["ignore_year"])
-            except ValueError:
-                raise ParameterValueFormatError(f'Could not decode ignore_year value "{value_index["ignore_year"]}"')
-        else:
-            ignore_year = "start" not in value_index
-        if "repeat" in value_index:
-            try:
-                repeat = bool(value_index["repeat"])
-            except ValueError:
-                raise ParameterValueFormatError(f'Could not decode repeat value "{value_index["ignore_year"]}"')
-        else:
-            repeat = "start" not in value_index
-    else:
-        start = _TIME_SERIES_DEFAULT_START
-        resolution = _TIME_SERIES_DEFAULT_RESOLUTION
-        ignore_year = True
-        repeat = True
-    if isinstance(resolution, str) or not isinstance(resolution, Sequence):
-        # Always work with lists to simplify the code.
-        resolution = [resolution]
-    relativedeltas = list()
-    for duration in resolution:
-        if not isinstance(duration, str):
-            duration = str(duration) + _TIME_SERIES_PLAIN_INDEX_UNIT
-        relativedeltas.append(duration_to_relativedelta(duration))
-    try:
-        start = dateutil.parser.parse(start)
-    except ValueError:
-        raise ParameterValueFormatError(f'Could not decode start value "{start}"')
-    values = np.array(value_dict["data"])
-    return TimeSeriesFixedResolution(
-        start, relativedeltas, values, ignore_year, repeat, value_dict.get("index_name", "")
-    )
-
-
-def _time_series_from_two_columns(value_dict):
-    """Converts a two column style time series into a TimeSeriesVariableResolution object.
-
-    Args:
-        value_dict (dict): time series dictionary
-
-    Returns:
-        TimeSeriesVariableResolution: restored time series
-    """
-    data = value_dict["data"]
-    stamps = list()
-    values = np.empty(len(data))
-    for index, element in enumerate(data):
-        if not isinstance(element, Sequence) or len(element) != 2:
-            raise ParameterValueFormatError("Invalid value in time series array")
-        try:
-            stamp = np.datetime64(element[0], NUMPY_DATETIME64_UNIT)
-        except ValueError:
-            raise ParameterValueFormatError(f'Could not decode time stamp "{element[0]}"')
-        stamps.append(stamp)
-        values[index] = element[1]
-    stamps = np.array(stamps)
-    ignore_year, repeat = _variable_resolution_time_series_info_from_index(value_dict)
-    return TimeSeriesVariableResolution(stamps, values, ignore_year, repeat, value_dict.get("index_name", ""))
-
-
-def _time_pattern_from_database(value_dict):
-    """Converts a time pattern database value into a TimePattern object.
-
-    Args:
-        value_dict (dict): time pattern dictionary
-
-    Returns:
-        TimePattern: restored time pattern
-    """
-    patterns, values = _break_dictionary(value_dict["data"])
-    return TimePattern(patterns, values, value_dict.get("index_name", "p"))
-
-
-def _map_from_database(value_dict):
-    """Converts a map from its database representation to a Map object.
-
-    Args:
-        value_dict (dict): Map dictionary
-
-    Returns:
-        Map: restored Map
-    """
-    index_type = _map_index_type_from_database(value_dict["index_type"])
-    index_name = value_dict.get("index_name", "x")
-    data = value_dict["data"]
-    if isinstance(data, dict):
-        indexes = _map_indexes_from_database(data.keys(), index_type)
-        values = _map_values_from_database(data.values())
-    elif isinstance(data, Sequence):
-        if not data:
-            indexes = list()
-            values = list()
-        else:
-            indexes_in_db = list()
-            values_in_db = list()
-            for row in data:
-                if not isinstance(row, Sequence) or len(row) != 2:
-                    raise ParameterValueFormatError('"data" is not a nested two column array.')
-                indexes_in_db.append(row[0])
-                values_in_db.append(row[1])
-            indexes = _map_indexes_from_database(indexes_in_db, index_type)
-            values = _map_values_from_database(values_in_db)
-    else:
-        raise ParameterValueFormatError('"data" attribute is not a dict or array.')
-    return Map(indexes, values, index_type, index_name)
-
-
-def _map_index_type_from_database(index_type_in_db):
-    """Returns the type corresponding to index_type string."""
-    index_type = {"str": str, "date_time": DateTime, "duration": Duration, "float": float}.get(index_type_in_db, None)
-    if index_type is None:
-        raise ParameterValueFormatError(f'Unknown index_type "{index_type_in_db}".')
-    return index_type
-
-
-def _map_index_type_to_database(index_type):
-    """Returns the string corresponding to given index type."""
-    if issubclass(index_type, str):
-        return "str"
-    if issubclass(index_type, float):
-        return "float"
-    if index_type == DateTime:
-        return "date_time"
-    if index_type == Duration:
-        return "duration"
-    raise ParameterValueFormatError(f'Unknown index type "{index_type.__name__}".')
-
-
-def _map_indexes_from_database(indexes_in_db, index_type):
-    """Converts map's indexes from their database format."""
-    try:
-        indexes = [index_type(index) for index in indexes_in_db]
-    except ValueError as error:
-        raise ParameterValueFormatError(
-            f'Failed to read index of type "{_map_index_type_to_database(index_type)}": {error}'
-        )
-    else:
-        return indexes
-
-
-def _map_index_to_database(index):
-    """Converts a single map index to database format."""
-    if hasattr(index, "value_to_database_data"):
-        return index.value_to_database_data()
-    return index
-
-
-def _map_value_to_database(value):
-    """Converts a single map value to database format."""
-    if hasattr(value, "to_dict"):
-        return dict(type=value.type_(), **value.to_dict())
-    return value
-
-
-def _map_values_from_database(values_in_db):
-    """Converts map's values from their database format."""
-    if not values_in_db:
-        return list()
-    values = list()
-    for value_in_db in values_in_db:
-        value = from_dict(value_in_db) if isinstance(value_in_db, dict) else value_in_db
-        if isinstance(value, int):
-            value = float(value)
-        elif value is not None and not isinstance(value, (float, bool, Duration, IndexedValue, str, DateTime)):
-            raise ParameterValueFormatError(f'Unsupported value type for Map: "{type(value).__name__}".')
-        values.append(value)
-    return values
-
-
-def _array_from_database(value_dict):
-    """Converts a value dictionary to Array.
-
-    Args:
-          value_dict (dict): array dictionary
-
-    Returns:
-          Array: Array value
-    """
-    value_type_id = value_dict.get("value_type", "float")
-    value_type = {"float": float, "str": str, "date_time": DateTime, "duration": Duration, "time_period": str}.get(
-        value_type_id, None
-    )
-    if value_type is None:
-        raise ParameterValueFormatError(f'Unsupported value type for Array: "{value_type_id}".')
-    try:
-        data = [value_type(x) for x in value_dict["data"]]
-    except (TypeError, ParameterValueFormatError) as error:
-        raise ParameterValueFormatError(f'Failed to read values for Array: {error}')
-    else:
-        index_name = value_dict.get("index_name", "i")
-        return Array(data, value_type, index_name)
-
-
-class ListValueRef:
-    def __init__(self, list_value_id):
-        self._list_value_id = list_value_id
-
-    @staticmethod
-    def type_():
-        return "list_value_ref"
-
-    def to_database(self):
-        """Returns the database representation of this object as JSON."""
-        return json.dumps(self._list_value_id).encode("UTF8"), self.type_()
-
-
-class DateTime:
-    """A single datetime value."""
-
-    VALUE_TYPE = "single value"
-
-    def __init__(self, value=None):
-        """
-        Args:
-            value (DataTime or str or datetime.datetime): a timestamp
-        """
-        if value is None:
-            value = datetime(year=2000, month=1, day=1)
-        elif isinstance(value, str):
-            try:
-                value = dateutil.parser.parse(value)
-            except ValueError:
-                raise ParameterValueFormatError(f'Could not parse datetime from "{value}"')
-        elif isinstance(value, DateTime):
-            value = copy(value._value)
-        elif not isinstance(value, datetime):
-            raise ParameterValueFormatError(f'"{type(value).__name__}" cannot be converted to DateTime.')
-        self._value = value
-
-    def __eq__(self, other):
-        """Returns True if other is equal to this object."""
-        if not isinstance(other, DateTime):
-            return NotImplemented
-        return self._value == other._value
-
-    def __lt__(self, other):
-        if not isinstance(other, DateTime):
-            return NotImplemented
-        return self._value < other._value
-
-    def __hash__(self):
-        return hash(self._value)
-
-    def __str__(self):
-        return self._value.isoformat()
-
-    def value_to_database_data(self):
-        """Returns the database representation of the datetime."""
-        return self._value.isoformat()
-
-    def to_dict(self):
-        """Returns the database representation of this object."""
-        return {"data": self.value_to_database_data()}
-
-    @staticmethod
-    def type_():
-        return "date_time"
-
-    def to_database(self):
-        """Returns the database representation of this object as JSON."""
-        return json.dumps(self.to_dict()).encode("UTF8"), self.type_()
-
-    @property
-    def value(self):
-        """Returns the value as a datetime object."""
-        return self._value
-
-
-class Duration:
-    """
-    This class represents a duration in time.
-
-    Durations are always handled as relativedeltas.
-    """
-
-    VALUE_TYPE = "single value"
-
-    def __init__(self, value=None):
-        """
-        Args:
-            value (str or relativedelta): the time step
-        """
-        if value is None:
-            value = relativedelta(hours=1)
-        elif isinstance(value, str):
-            value = duration_to_relativedelta(value)
-        elif isinstance(value, Duration):
-            value = copy(value._value)
-        if not isinstance(value, relativedelta):
-            raise ParameterValueFormatError(f'Could not parse duration from "{value}"')
-        self._value = value
-
-    def __eq__(self, other):
-        """Returns True if other is equal to this object."""
-        if not isinstance(other, Duration):
-            return NotImplemented
-        return self._value == other._value
-
-    def __hash__(self):
-        return hash(self._value)
-
-    def __str__(self):
-        return str(relativedelta_to_duration(self._value))
-
-    def value_to_database_data(self):
-        """Returns the 'data' attribute part of Duration's database representation."""
-        return relativedelta_to_duration(self._value)
-
-    def to_dict(self):
-        """Returns the database representation of the duration."""
-        return {"data": self.value_to_database_data()}
-
-    @staticmethod
-    def type_():
-        return "duration"
-
-    def to_database(self):
-        """Returns the database representation of the duration as JSON."""
-        return json.dumps(self.to_dict()).encode("UTF8"), self.type_()
-
-    @property
-    def value(self):
-        """Returns the duration as a :class:`relativedelta`."""
-        return self._value
-
-
-class _Indexes(np.ndarray):
-    """
-    A subclass of numpy.ndarray that keeps a lookup dictionary from elements to positions.
-    Used by methods get_value and set_value of IndexedValue, to avoid something like
-
-        position = indexes.index(element)
-
-    which might be too slow compared to dictionary lookup.
-    """
-
-    def __new__(cls, other, dtype=None):
-        obj = np.asarray(other, dtype=dtype).view(cls)
-        obj.position_lookup = {index: k for k, index in enumerate(other)}
-        return obj
-
-    def __array_finalize__(self, obj):
-        if obj is None:
-            return
-        # pylint: disable=attribute-defined-outside-init
-        self.position_lookup = getattr(obj, 'position_lookup', {})
-
-    def __setitem__(self, position, index):
-        old_index = self.__getitem__(position)
-        self.position_lookup[index] = self.position_lookup.pop(old_index, '')
-        super().__setitem__(position, index)
-
-    def __eq__(self, other):
-        return np.all(super().__eq__(other))
-
-    def __bool__(self):
-        return np.size(self) != 0
-
-
-class IndexedValue:
-    """
-    An abstract base class for indexed values.
-
-    Attributes:
-        index_name (str): index name
-    """
-
-    VALUE_TYPE = NotImplemented
-
-    def __init__(self, index_name):
-        """
-        Args:
-            index_name (str): index name
-        """
-        self._indexes = None
-        self._values = None
-        self.index_name = index_name
-
-    def __bool__(self):
-        # NOTE: Use self.indexes rather than self._indexes, otherwise TimeSeriesFixedResolution gives wrong result
-        return bool(self.indexes)
-
-    def __len__(self):
-        """Returns the number of values."""
-        return len(self.indexes)
-
-    @staticmethod
-    def type_():
-        """Returns a type identifier string.
-
-        Returns:
-            str: type identifier
-        """
-        raise NotImplementedError()
-
-    @property
-    def indexes(self):
-        """Returns the indexes."""
-        return self._indexes
-
-    @indexes.setter
-    def indexes(self, indexes):
-        """Sets the indexes."""
-        self._indexes = _Indexes(indexes)
-
-    def to_database(self):
-        """Return the database representation of the value."""
-        return json.dumps(self.to_dict()).encode("UTF8"), self.type_()
-
-    @property
-    def values(self):
-        """Returns the data values."""
-        return self._values
-
-    @values.setter
-    def values(self, values):
-        """Sets the values."""
-        self._values = values
-
-    def get_nearest(self, index):
-        pos = np.searchsorted(self.indexes, index)
-        return self.values[pos]
-
-    def get_value(self, index):
-        """Returns the value at the given index."""
-        pos = self.indexes.position_lookup.get(index)
-        if pos is None:
-            return None
-        return self.values[pos]
-
-    def set_value(self, index, value):
-        """Sets the value at the given index."""
-        pos = self.indexes.position_lookup.get(index)
-        if pos is not None:
-            self.values[pos] = value
-
-    def to_dict(self):
-        """Converts the value to a Python dictionary.
-
-        Returns:
-            dict(): mapping from indexes to values
-        """
-        raise NotImplementedError()
-
-    def merge(self, other):
-        if not isinstance(other, type(self)):
-            return self
-        new_indexes = np.unique(np.concatenate((self.indexes, other.indexes)))
-        new_indexes.sort(kind='mergesort')
-        _merge = lambda value, other: other if value is None else merge_parsed(value, other)
-        new_values = [_merge(self.get_value(index), other.get_value(index)) for index in new_indexes]
-        self.indexes = new_indexes
-        self.values = new_values
-        return self
-
-
-class Array(IndexedValue):
-    """A one dimensional array with zero based indexing."""
-
-    VALUE_TYPE = "array"
-    DEFAULT_INDEX_NAME = "i"
-
-    def __init__(self, values, value_type=None, index_name=""):
-        """
-        Args:
-            values (Sequence): array's values
-            value_type (Type, optional): array element type; will be deduced from the array if not given
-                and defaults to float if ``values`` is empty
-            index_name (str): index name
-        """
-        super().__init__(index_name if index_name else Array.DEFAULT_INDEX_NAME)
-        if value_type is None:
-            value_type = type(values[0]) if values else float
-            if value_type == int:
-                try:
-                    values = [float(x) for x in values]
-                except ValueError:
-                    raise ParameterValueFormatError("Cannot convert array's values to float.")
-                value_type = float
-        if any(not isinstance(x, value_type) for x in values):
-            try:
-                values = [value_type(x) for x in values]
-            except ValueError:
-                raise ParameterValueFormatError("Not all array's values are of the same type.")
-        self.indexes = range(len(values))
-        self.values = list(values)
-        self._value_type = value_type
-
-    def __eq__(self, other):
-        if not isinstance(other, Array):
-            return NotImplemented
-        return np.array_equal(self._values, other._values) and self.index_name == other.index_name
-
-    @staticmethod
-    def type_():
-        return "array"
-
-    def to_dict(self):
-        """See base class."""
-        value_type_id = {
-            float: "float",
-            str: "str",  # String could also mean time_period but we don't have any way to distinguish that, yet.
-            DateTime: "date_time",
-            Duration: "duration",
-        }.get(self._value_type)
-        if value_type_id is None:
-            raise ParameterValueFormatError(f"Cannot write unsupported array value type: {self._value_type.__name__}")
-        if value_type_id in ("float", "str"):
-            data = self._values
-        else:
-            data = [x.value_to_database_data() for x in self._values]
-        value_dict = {"value_type": value_type_id, "data": data}
-        if self.index_name != "i":
-            value_dict["index_name"] = self.index_name
-        return value_dict
-
-    @property
-    def value_type(self):
-        """Returns the type of array's elements."""
-        return self._value_type
-
-
-class IndexedNumberArray(IndexedValue):
-    """
-    An abstract base class for indexed floats.
-
-    The indexes and numbers are stored in numpy.ndarrays.
-    """
-
-    def __init__(self, index_name, values):
-        """
-        Args:
-            index_name (str): index name
-            values (Sequence): array's values; index handling should be implemented by subclasses
-        """
-        super().__init__(index_name)
-        self.values = values
-
-    @IndexedValue.values.setter
-    def values(self, values):
-        """Sets the values."""
-        if not isinstance(values, np.ndarray) or not values.dtype == np.dtype(float):
-            values = np.array(values, dtype=float)
-        self._values = values
-
-    @staticmethod
-    def type_():
-        raise NotImplementedError()
-
-    def to_dict(self):
-        """Return the database representation of the value."""
-        raise NotImplementedError()
-
-
-class TimeSeries(IndexedNumberArray):
-    """An abstract base class for time series."""
-
-    VALUE_TYPE = "time series"
-    DEFAULT_INDEX_NAME = "t"
-
-    def __init__(self, values, ignore_year, repeat, index_name=""):
-        """
-        Args:
-            values (Sequence): an array of values
-            ignore_year (bool): True if the year should be ignored in the time stamps
-            repeat (bool): True if the series should be repeated from the beginning
-            index_name (str): index name
-        """
-        if len(values) < 1:
-            raise ParameterValueFormatError("Time series too short. Must have one or more values")
-        super().__init__(index_name if index_name else TimeSeries.DEFAULT_INDEX_NAME, values)
-        self._ignore_year = ignore_year
-        self._repeat = repeat
-
-    def __len__(self):
-        """Returns the number of values."""
-        return len(self._values)
-
-    @property
-    def ignore_year(self):
-        """Returns True if the year should be ignored."""
-        return self._ignore_year
-
-    @ignore_year.setter
-    def ignore_year(self, ignore_year):
-        self._ignore_year = bool(ignore_year)
-
-    @property
-    def repeat(self):
-        """Returns True if the series should be repeated."""
-        return self._repeat
-
-    @repeat.setter
-    def repeat(self, repeat):
-        self._repeat = bool(repeat)
-
-    @staticmethod
-    def type_():
-        return "time_series"
-
-    def to_dict(self):
-        """Return the database representation of the value."""
-        raise NotImplementedError()
-
-
-def _check_time_pattern_index(union_str):
-    """
-    Checks if a time pattern index has the right format.
-
-    Args:
-        union_str (str): The time pattern index to check. Generally assumed to be a union of interval intersections.
-
-    Raises:
-        ParameterValueFormatError: If the given string doesn't comply with time pattern spec.
-    """
-    if not union_str:
-        # We accept empty strings so we can add empty rows in the parameter value editor UI
-        return
-    union_dlm = ","
-    intersection_dlm = ";"
-    range_dlm = "-"
-    regexp = r"(Y|M|D|WD|h|m|s)"
-    for intersection_str in union_str.split(union_dlm):
-        for interval_str in intersection_str.split(intersection_dlm):
-            m = re.match(regexp, interval_str)
-            if m is None:
-                raise ParameterValueFormatError(
-                    f"Invalid interval {interval_str}, it should start with either Y, M, D, WD, h, m, or s."
-                )
-            key = m.group(0)
-            lower_upper_str = interval_str[len(key) :]
-            lower_upper = lower_upper_str.split(range_dlm)
-            if len(lower_upper) != 2:
-                raise ParameterValueFormatError(
-                    f"Invalid interval bounds {lower_upper_str}, it should be two integers separated by dash (-)."
-                )
-            lower_str, upper_str = lower_upper
-            try:
-                lower = int(lower_str)
-            except:
-                raise ParameterValueFormatError(f"Invalid lower bound {lower_str}, must be an integer.")
-            try:
-                upper = int(upper_str)
-            except:
-                raise ParameterValueFormatError(f"Invalid upper bound {upper_str}, must be an integer.")
-            if lower > upper:
-                raise ParameterValueFormatError(f"Lower bound {lower} can't be higher than upper bound {upper}.")
-
-
-class _TimePatternIndexes(_Indexes):
-    """An array of *checked* time pattern indexes."""
-
-    def __array_finalize__(self, obj):
-        """Checks indexes when building the array."""
-        for x in obj:
-            _check_time_pattern_index(x)
-        super().__array_finalize__(obj)
-
-    def __eq__(self, other):
-        return list(self) == list(other)
-
-    def __setitem__(self, position, index):
-        """Checks indexes when setting and item."""
-        _check_time_pattern_index(index)
-        super().__setitem__(position, index)
-
-
-class TimePattern(IndexedNumberArray):
-    """Represents a time pattern (relationship) parameter value."""
-
-    VALUE_TYPE = "time pattern"
-    DEFAULT_INDEX_NAME = "p"
-
-    def __init__(self, indexes, values, index_name=""):
-        """
-        Args:
-            indexes (list): a list of time pattern strings
-            values (Sequence): an array of values corresponding to the time patterns
-            index_name (str): index name
-        """
-        if len(indexes) != len(values):
-            raise ParameterValueFormatError("Length of values does not match length of indexes")
-        if not indexes:
-            raise ParameterValueFormatError("Empty time pattern not allowed")
-        super().__init__(index_name if index_name else TimePattern.DEFAULT_INDEX_NAME, values)
-        self.indexes = indexes
-
-    def __eq__(self, other):
-        """Returns True if other is equal to this object."""
-        if not isinstance(other, TimePattern):
-            return NotImplemented
-        return (
-            self._indexes == other._indexes
-            and np.all(self._values == other._values)
-            and self.index_name == other.index_name
-        )
-
-    @IndexedNumberArray.indexes.setter
-    def indexes(self, indexes):
-        """Sets the indexes."""
-        self._indexes = _TimePatternIndexes(indexes, dtype=np.object_)
-
-    @staticmethod
-    def type_():
-        return "time_pattern"
-
-    def to_dict(self):
-        """Returns the database representation of this time pattern."""
-        value_dict = {"data": dict(zip(self._indexes, self._values))}
-        if self.index_name != "p":
-            value_dict["index_name"] = self.index_name
-        return value_dict
-
-
-class TimeSeriesFixedResolution(TimeSeries):
-    """
-    A time series with fixed durations between the time stamps.
-
-    When getting the indexes the durations are applied cyclically.
-
-    Currently, there is no support for the `ignore_year` and `repeat` options
-    other than having getters for their values.
-    """
-
-    _memoized_indexes = {}
-
-    def __init__(self, start, resolution, values, ignore_year, repeat, index_name=""):
-        """
-        Args:
-            start (str or datetime or datetime64): the first time stamp
-            resolution (str, relativedelta, list): duration(s) between the time stamps
-            values (Sequence): data values at each time stamp
-            ignore_year (bool): whether or not the time-series should apply to any year
-            repeat (bool): whether or not the time series should repeat cyclically
-            index_name (str): index name
-        """
-        super().__init__(values, ignore_year, repeat, index_name)
-        self._start = None
-        self._resolution = None
-        self.start = start
-        self.resolution = resolution
-
-    def __eq__(self, other):
-        """Returns True if other is equal to this object."""
-        if not isinstance(other, TimeSeriesFixedResolution):
-            return NotImplemented
-        return (
-            self._start == other._start
-            and self._resolution == other._resolution
-            and np.array_equal(self._values, other._values, equal_nan=True)
-            and self._ignore_year == other._ignore_year
-            and self._repeat == other._repeat
-            and self.index_name == other.index_name
-        )
-
-    def _get_memoized_indexes(self):
-        key = (self.start, tuple(self.resolution), len(self))
-        memoized_indexes = self._memoized_indexes.get(key)
-        if memoized_indexes is not None:
-            return memoized_indexes
-        step_index = 0
-        step_cycle_index = 0
-        full_cycle_duration = sum(self._resolution, relativedelta())
-        stamps = np.empty(len(self), dtype=_NUMPY_DATETIME_DTYPE)
-        stamps[0] = self._start
-        for stamp_index in range(1, len(self._values)):
-            if step_index >= len(self._resolution):
-                step_index = 0
-                step_cycle_index += 1
-            current_cycle_duration = sum(self._resolution[: step_index + 1], relativedelta())
-            duration_from_start = step_cycle_index * full_cycle_duration + current_cycle_duration
-            stamps[stamp_index] = self._start + duration_from_start
-            step_index += 1
-        memoized_indexes = self._memoized_indexes[key] = np.array(stamps, dtype=_NUMPY_DATETIME_DTYPE)
-        return memoized_indexes
-
-    @property
-    def indexes(self):
-        """Returns the time stamps as a numpy.ndarray of numpy.datetime64 objects."""
-        if self._indexes is None:
-            self.indexes = self._get_memoized_indexes()
-        return IndexedValue.indexes.fget(self)
-
-    @indexes.setter
-    def indexes(self, indexes):
-        """Sets the indexes."""
-        # Needed because we redefine the setter
-        self._indexes = _Indexes(indexes)
-
-    @property
-    def start(self):
-        """Returns the start index."""
-        return self._start
-
-    @start.setter
-    def start(self, start):
-        """
-        Sets the start datetime.
-
-        Args:
-            start (datetime or datetime64 or str): the start of the series
-        """
-        if isinstance(start, str):
-            try:
-                self._start = dateutil.parser.parse(start)
-            except ValueError:
-                raise ParameterValueFormatError(f'Cannot parse start time "{start}"')
-        elif isinstance(start, np.datetime64):
-            self._start = start.tolist()
-        else:
-            self._start = start
-        self._indexes = None
-
-    @property
-    def resolution(self):
-        """Returns the resolution as list of durations."""
-        return self._resolution
-
-    @resolution.setter
-    def resolution(self, resolution):
-        """
-        Sets the resolution.
-
-        Args:
-            resolution (str, relativedelta, list): resolution or a list thereof
-        """
-        if isinstance(resolution, str):
-            resolution = [duration_to_relativedelta(resolution)]
-        elif not isinstance(resolution, Sequence):
-            resolution = [resolution]
-        else:
-            for i in range(len(resolution)):
-                if isinstance(resolution[i], str):
-                    resolution[i] = duration_to_relativedelta(resolution[i])
-        if not resolution:
-            raise ParameterValueFormatError("Resolution cannot be zero.")
-        self._resolution = resolution
-        self._indexes = None
-
-    def to_dict(self):
-        """Returns the value in its database representation."""
-        if len(self._resolution) > 1:
-            resolution_as_json = [relativedelta_to_duration(step) for step in self._resolution]
-        else:
-            resolution_as_json = relativedelta_to_duration(self._resolution[0])
-        value_dict = {
-            "index": {
-                "start": str(self._start),
-                "resolution": resolution_as_json,
-                "ignore_year": self._ignore_year,
-                "repeat": self._repeat,
-            },
-            "data": self._values.tolist(),
-        }
-        if self.index_name != "t":
-            value_dict["index_name"] = self.index_name
-        return value_dict
-
-
-class TimeSeriesVariableResolution(TimeSeries):
-    """A class representing time series data with variable time steps."""
-
-    def __init__(self, indexes, values, ignore_year, repeat, index_name=""):
-        """
-        Args:
-            indexes (Sequence): time stamps as numpy.datetime64 objects
-            values (Sequence): the values corresponding to the time stamps
-            ignore_year (bool): True if the stamp year should be ignored
-            repeat (bool): True if the series should be repeated from the beginning
-            index_name (str): index name
-        """
-        super().__init__(values, ignore_year, repeat, index_name)
-        if len(indexes) != len(values):
-            raise ParameterValueFormatError("Length of values does not match length of indexes")
-        if not isinstance(indexes, np.ndarray):
-            date_times = np.empty(len(indexes), dtype=_NUMPY_DATETIME_DTYPE)
-            for i, index in enumerate(indexes):
-                if isinstance(index, DateTime):
-                    date_times[i] = np.datetime64(index.value, NUMPY_DATETIME64_UNIT)
-                else:
-                    try:
-                        date_times[i] = np.datetime64(index, NUMPY_DATETIME64_UNIT)
-                    except ValueError:
-                        raise ParameterValueFormatError(
-                            f'Cannot convert "{index}" of type {type(index).__name__} to time stamp.'
-                        )
-            indexes = date_times
-        self.indexes = indexes
-
-    def __eq__(self, other):
-        """Returns True if other is equal to this object."""
-        if not isinstance(other, TimeSeriesVariableResolution):
-            return NotImplemented
-        return (
-            np.array_equal(self._indexes, other._indexes)
-            and np.array_equal(self._values, other._values, equal_nan=True)
-            and self._ignore_year == other._ignore_year
-            and self._repeat == other._repeat
-            and self.index_name == other.index_name
-        )
-
-    def to_dict(self):
-        """Returns the value in its database representation"""
-        value_dict = dict()
-        value_dict["data"] = {str(index): float(value) for index, value in zip(self._indexes, self._values)}
-        # Add "index" entry only if its contents are not set to their default values.
-        if self._ignore_year:
-            value_dict.setdefault("index", dict())["ignore_year"] = self._ignore_year
-        if self._repeat:
-            value_dict.setdefault("index", dict())["repeat"] = self._repeat
-        if self.index_name != "t":
-            value_dict["index_name"] = self.index_name
-        return value_dict
-
-
-class Map(IndexedValue):
-    """A nested general purpose indexed value."""
-
-    VALUE_TYPE = "map"
-    DEFAULT_INDEX_NAME = "x"
-
-    def __init__(self, indexes, values, index_type=None, index_name=""):
-        """
-        Args:
-            indexes (Sequence): map's indexes
-            values (Sequence): map's values
-            index_type (type or NoneType): index type or None to deduce from indexes
-            index_name (str): index name
-        """
-        if not indexes and index_type is None:
-            raise ParameterValueFormatError("Cannot deduce index type from empty indexes list.")
-        if indexes and index_type is not None and not isinstance(indexes[0], index_type):
-            raise ParameterValueFormatError('Type of index does not match "index_type" argument.')
-        if len(indexes) != len(values):
-            raise ParameterValueFormatError("Length of values does not match length of indexes")
-        super().__init__(index_name if index_name else Map.DEFAULT_INDEX_NAME)
-        self.indexes = indexes
-        self._index_type = index_type if index_type is not None else type(indexes[0])
-        self._values = values
-
-    def __eq__(self, other):
-        if not isinstance(other, Map):
-            return NotImplemented
-        return other._indexes == self._indexes and other._values == self._values and self.index_name == other.index_name
-
-    def is_nested(self):
-        """Returns True if any of the values is also a map."""
-        return any(isinstance(value, Map) for value in self._values)
-
-    def value_to_database_data(self):
-        """Returns map's database representation's 'data' dictionary."""
-        data = list()
-        for index, value in zip(self._indexes, self._values):
-            index_in_db = _map_index_to_database(index)
-            value_in_db = _map_value_to_database(value)
-            data.append([index_in_db, value_in_db])
-        return data
-
-    @staticmethod
-    def type_():
-        return "map"
-
-    def to_dict(self):
-        """Returns map's database representation."""
-        value_dict = {
-            "index_type": _map_index_type_to_database(self._index_type),
-            "data": self.value_to_database_data(),
-        }
-        if self.index_name != "x":
-            value_dict["index_name"] = self.index_name
-        return value_dict
-
-
-def map_dimensions(map_):
-    """Counts Map's dimensions.
-
-    Args:
-        map_ (Map): a Map
-
-    Returns:
-        int: number of dimensions
-    """
-    nested = 0
-    for v in map_.values:
-        if isinstance(v, Map):
-            nested = max(nested, map_dimensions(v))
-        elif isinstance(v, IndexedValue):
-            nested = max(nested, 1)
-    return 1 + nested
-
-
-def convert_leaf_maps_to_specialized_containers(map_):
-    """
-    Converts suitable leaf maps to corresponding specialized containers.
-
-    Currently supported conversions:
-
-    - index_type: :class:`DateTime`, all values ``float`` -> :class"`TimeSeries`
-
-    Args:
-        map_ (Map): a map to process
-
-    Returns:
-        IndexedValue: a map with leaves converted or specialized container if map was convertible in itself
-    """
-    converted_container = _try_convert_to_container(map_)
-    if converted_container is not None:
-        return converted_container
-    new_values = list()
-    for _, value in zip(map_.indexes, map_.values):
-        if isinstance(value, Map):
-            converted = convert_leaf_maps_to_specialized_containers(value)
-            new_values.append(converted)
-        else:
-            new_values.append(value)
-    return Map(map_.indexes, new_values, index_name=map_.index_name)
-
-
-def convert_containers_to_maps(value):
-    """
-    Converts indexed values into maps.
-
-    if ``value`` is :class:`Map` converts leaf values into Maps recursively.
-
-    Args:
-        value (IndexedValue): a value to convert
-
-    Returns:
-        Map: converted Map
-    """
-    if isinstance(value, Map):
-        if not value:
-            return value
-        new_values = list()
-        for _, x in zip(value.indexes, value.values):
-            if isinstance(x, IndexedValue):
-                new_values.append(convert_containers_to_maps(x))
-            else:
-                new_values.append(x)
-        return Map(list(value.indexes), new_values, index_name=value.index_name)
-    if isinstance(value, IndexedValue):
-        if not value:
-            if isinstance(value, TimeSeries):
-                return Map([], [], DateTime, index_name=TimeSeries.DEFAULT_INDEX_NAME)
-            return Map([], [], str)
-        return Map(list(value.indexes), list(value.values), index_name=value.index_name)
-    return value
-
-
-def convert_map_to_table(map_, make_square=True, row_this_far=None, empty=None):
-    """
-    Converts :class:`Map` into list of rows recursively.
-
-    Args:
-        map_ (Map): map to convert
-        make_square (bool): if True, append None to shorter rows, otherwise leave the row as is
-        row_this_far (list, optional): current row; used for recursion
-        empty (Any, optional): object to fill empty cells with
-
-    Returns:
-        list of list: map's rows
-    """
-    if row_this_far is None:
-        row_this_far = list()
-    rows = list()
-    for index, value in zip(map_.indexes, map_.values):
-        if not isinstance(value, Map):
-            rows.append(row_this_far + [index, value])
-        else:
-            rows += convert_map_to_table(value, False, row_this_far + [index])
-    if make_square:
-        max_length = 0
-        for row in rows:
-            max_length = max(max_length, len(row))
-        equal_length_rows = list()
-        for row in rows:
-            equal_length_row = row + (max_length - len(row)) * [empty]
-            equal_length_rows.append(equal_length_row)
-        return equal_length_rows
-    return rows
-
-
-def convert_map_to_dict(map_):
-    """
-    Converts :class:`Map` to nested dictionaries.
-
-    Args:
-        map_ (Map): map to convert
-
-    Returns:
-        dict: Map as a dict
-    """
-    d = dict()
-    for index, x in zip(map_.indexes, map_.values):
-        if isinstance(x, Map):
-            x = convert_map_to_dict(x)
-        d[index] = x
-    return d
-
-
-def _try_convert_to_container(map_):
-    """
-    Tries to convert a map to corresponding specialized container.
-
-    Args:
-        map_ (Map): a map to convert
-
-    Returns:
-        TimeSeriesVariableResolution or None: converted Map or None if the map couldn't be converted
-    """
-    if not map_:
-        return None
-    stamps = list()
-    values = list()
-    for index, value in zip(map_.indexes, map_.values):
-        if not isinstance(index, DateTime) or not isinstance(value, float):
-            return None
-        stamps.append(index)
-        values.append(value)
-    return TimeSeriesVariableResolution(stamps, values, False, False, index_name=map_.index_name)
-
-
-# List of scalar types that are supported by the spinedb_api
-SUPPORTED_TYPES = (Duration, DateTime, float, str)
-
-
-def join_value_and_type(db_value, db_type):
-    """Joins database value and type into a string.
-    The resulting string is a JSON string.
-    In case of complex types (duration, date_time, time_series, time_pattern, array, map),
-    the type is just added as top-level key.
-
-    Args:
-        db_value (bytes): database value
-        db_type (str, optional): value type
-
-    Returns:
-        str: parameter value as JSON with an additional `type` field.
-    """
-    try:
-        parsed = load_db_value(db_value, db_type)
-    except ParameterValueFormatError:
-        parsed = None
-    return json.dumps(parsed)
-
-
-def split_value_and_type(value_and_type):
-    """Splits the given string into value and type.
-    The string must be the result of calling ``join_value_and_type`` or have the same form.
-
-    Args:
-        value_and_type (str)
-
-    Returns:
-        bytes
-        str or NoneType
-    """
-    try:
-        parsed = json.loads(value_and_type)
-    except (TypeError, json.JSONDecodeError):
-        parsed = value_and_type
-    return dump_db_value(parsed)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Support utilities and classes to deal with Spine data (relationship)
+parameter values.
+
+The `from_database` function reads the database's value format returning
+a float, Datatime, Duration, TimePattern, TimeSeriesFixedResolution
+TimeSeriesVariableResolution or Map objects.
+
+The above objects can be converted back to the database format by the `to_database` free function
+or by their `to_database` member functions.
+
+Individual datetimes are represented as datetime objects from the standard Python library.
+Individual time steps are represented as relativedelta objects from the dateutil package.
+Datetime indexes (as returned by TimeSeries.indexes()) are represented as
+numpy.ndarray arrays holding numpy.datetime64 objects.
+
+"""
+
+from collections.abc import Sequence
+from copy import copy
+from datetime import datetime
+import json
+from json.decoder import JSONDecodeError
+from numbers import Number
+import re
+import dateutil.parser
+from dateutil.relativedelta import relativedelta
+import numpy as np
+from .exception import ParameterValueFormatError
+
+# Defaulting to seconds precision in numpy.
+_NUMPY_DATETIME_DTYPE = "datetime64[s]"
+NUMPY_DATETIME64_UNIT = "s"
+# Default start time guess, actual value not currently given in the JSON specification.
+_TIME_SERIES_DEFAULT_START = "0001-01-01T00:00:00"
+# Default resolution if it is omitted from the index entry.
+_TIME_SERIES_DEFAULT_RESOLUTION = "1h"
+# Default unit if resolution is given as a number instead of a string.
+_TIME_SERIES_PLAIN_INDEX_UNIT = "m"
+
+
+def duration_to_relativedelta(duration):
+    """
+    Converts a duration to a relativedelta object.
+
+    Args:
+        duration (str): a duration specification
+
+    Returns:
+        a relativedelta object corresponding to the given duration
+    """
+    try:
+        count, abbreviation, full_unit = re.split("\\s|([a-z]|[A-Z])", duration, maxsplit=1)
+        count = int(count)
+    except ValueError:
+        raise ParameterValueFormatError(f'Could not parse duration "{duration}"')
+    unit = abbreviation if abbreviation is not None else full_unit
+    if unit in ["s", "second", "seconds"]:
+        return relativedelta(seconds=count)
+    if unit in ["m", "minute", "minutes"]:
+        return relativedelta(minutes=count)
+    if unit in ["h", "hour", "hours"]:
+        return relativedelta(hours=count)
+    if unit in ["D", "day", "days"]:
+        return relativedelta(days=count)
+    if unit in ["M", "month", "months"]:
+        return relativedelta(months=count)
+    if unit in ["Y", "year", "years"]:
+        return relativedelta(years=count)
+    raise ParameterValueFormatError(f'Could not parse duration "{duration}"')
+
+
+def relativedelta_to_duration(delta):
+    """
+    Converts a relativedelta to duration.
+
+    Args:
+        delta (relativedelta): the relativedelta to convert
+
+    Returns:
+        a duration string
+    """
+    if delta.seconds > 0:
+        seconds = delta.seconds
+        seconds += 60 * delta.minutes
+        seconds += 60 * 60 * delta.hours
+        seconds += 60 * 60 * 24 * delta.days
+        # Skipping months and years since dateutil does not use them here
+        # and they wouldn't make much sense anyway.
+        return f"{seconds}s"
+    if delta.minutes > 0:
+        minutes = delta.minutes
+        minutes += 60 * delta.hours
+        minutes += 60 * 24 * delta.days
+        return f"{minutes}m"
+    if delta.hours > 0:
+        hours = delta.hours
+        hours += 24 * delta.days
+        return f"{hours}h"
+    if delta.days > 0:
+        return f"{delta.days}D"
+    if delta.months > 0:
+        months = delta.months
+        months += 12 * delta.years
+        return f"{months}M"
+    if delta.years > 0:
+        return f"{delta.years}Y"
+    return "0h"
+
+
+def load_db_value(db_value, value_type=None):
+    """
+    Loads a database parameter value into a Python object using JSON.
+    Adds the "type" property to dicts representing complex types.
+
+    Args:
+        db_value (bytes, optional): a value in the database
+        value_type (str, optional): the type in case of complex ones
+
+    Returns:
+        Any: the parsed parameter value
+    """
+    if db_value is None:
+        return None
+    try:
+        parsed = json.loads(db_value)
+    except JSONDecodeError as err:
+        raise ParameterValueFormatError(f"Could not decode the value: {err}") from err
+    if isinstance(parsed, dict):
+        return {"type": value_type, **parsed}
+    return parsed
+
+
+def dump_db_value(parsed_value):
+    """
+    Dumps a Python object into a database parameter value using JSON.
+    Extracts the "type" property from dicts representing complex types.
+
+    Args:
+        parsed_value (Any): the Python object
+
+    Returns:
+        str: the database parameter value
+        str: the type
+    """
+    value_type = parsed_value.pop("type") if isinstance(parsed_value, dict) else None
+    db_value = json.dumps(parsed_value).encode("UTF8")
+    if isinstance(parsed_value, dict) and value_type is not None:
+        parsed_value["type"] = value_type
+    return db_value, value_type
+
+
+def from_database(database_value, value_type=None):
+    """
+    Converts a parameter value from its database representation into an encoded Python object.
+
+    Args:
+        database_value (bytes, optional): a value in the database
+        value_type (str, optional): the type in case of complex ones
+
+    Returns:
+        Any: the encoded parameter value
+    """
+    parsed = load_db_value(database_value, value_type)
+    if isinstance(parsed, dict):
+        return from_dict(parsed)
+    if isinstance(parsed, bool):
+        return parsed
+    if isinstance(parsed, Number):
+        return float(parsed)
+    return parsed
+
+
+def from_database_to_single_value(database_value, value_type):
+    """
+    Converts a value from its database representation into a single value.
+
+    Indexed values get converted to their type string.
+
+    Args:
+        database_value (bytes): a value in the database
+        value_type (str, optional): value's type
+
+    Returns:
+        Any: single-value representation
+    """
+    if value_type is None or value_type not in ("map", "time_series", "time_pattern", "array"):
+        return from_database(database_value, value_type)
+    return value_type
+
+
+def from_database_to_dimension_count(database_value, value_type):
+    """
+    Counts dimensions of value's database representation
+
+    Args:
+        database_value (bytes): a value in the database
+        value_type (str, optional): value's type
+
+    Returns:
+        int: number of dimensions
+    """
+
+    if value_type in {"time_series", "time_pattern", "array"}:
+        return 1
+    if value_type == "map":
+        map_value = from_database(database_value, value_type)
+        return map_dimensions(map_value)
+    return 0
+
+
+def to_database(parsed_value):
+    """
+    Converts an encoded Python object into its database representation.
+
+    Args:
+        value: the value to convert. It can be the result of either ``load_db_value`` or ``from_database```.
+
+    Returns:
+        bytes: value's database representation as bytes
+        str: the value type
+    """
+    if hasattr(parsed_value, "to_database"):
+        return parsed_value.to_database()
+    db_value = json.dumps(parsed_value).encode("UTF8")
+    return db_value, None
+
+
+def from_dict(value_dict):
+    """
+    Converts a complex (relationship) parameter value from its dictionary representation to a Python object.
+
+    Args:
+        value_dict (dict): value's dictionary; a parsed JSON object with the "type" key
+
+    Returns:
+        the encoded (relationship) parameter value
+    """
+    value_type = value_dict["type"]
+    try:
+        if value_type == "date_time":
+            return _datetime_from_database(value_dict["data"])
+        if value_type == "duration":
+            return _duration_from_database(value_dict["data"])
+        if value_type == "map":
+            return _map_from_database(value_dict)
+        if value_type == "time_pattern":
+            return _time_pattern_from_database(value_dict)
+        if value_type == "time_series":
+            return _time_series_from_database(value_dict)
+        if value_type == "array":
+            return _array_from_database(value_dict)
+        raise ParameterValueFormatError(f'Unknown parameter value type "{value_type}"')
+    except KeyError as error:
+        raise ParameterValueFormatError(f'"{error.args[0]}" is missing in the parameter value description')
+
+
+def fix_conflict(new, old, on_conflict="merge"):
+    """Resolves conflicts between parameter values:
+
+    Args:
+        new (any): new parameter value to write
+        old (any): existing parameter value in the db
+        on_conflict (str): conflict resolution strategy:
+            - 'merge': Merge indexes if possible, otherwise replace
+            - 'replace': Replace old with new
+            - 'keep': keep old
+
+    Returns:
+        any: a parameter value with conflicts resolved
+    """
+    funcs = {"keep": lambda new, old: old, "replace": lambda new, old: new, "merge": merge}
+    func = funcs.get(on_conflict)
+    if func is None:
+        raise RuntimeError(
+            f"Invalid conflict resolution strategy {on_conflict}, valid strategies are {', '.join(funcs)}"
+        )
+    return func(new, old)
+
+
+def merge(value, other):
+    """Merges other into value, returns the result.
+    Args:
+        value (tuple): recipient value and type
+        other (tuple): other value and type
+
+    Returns:
+        tuple: value and type of merged value
+    """
+    parsed_value = from_database(*value)
+    if not hasattr(parsed_value, "merge"):
+        return value
+    parsed_other = from_database(*other)
+    return to_database(parsed_value.merge(parsed_other))
+
+
+def merge_parsed(parsed_value, parsed_other):
+    if not hasattr(parsed_value, "merge"):
+        return parsed_value
+    return parsed_value.merge(parsed_other)
+
+
+def _break_dictionary(data):
+    """Converts {"index": value} style dictionary into (list(indexes), numpy.ndarray(values)) tuple."""
+    if not isinstance(data, dict):
+        raise ParameterValueFormatError(
+            f"expected data to be in dictionary format, instead got '{type(data).__name__}'"
+        )
+    indexes, values = zip(*data.items())
+    return list(indexes), np.array(values)
+
+
+def _datetime_from_database(value):
+    """Converts a datetime database value into a DateTime object."""
+    try:
+        stamp = dateutil.parser.parse(value)
+    except ValueError:
+        raise ParameterValueFormatError(f'Could not parse datetime from "{value}"')
+    return DateTime(stamp)
+
+
+def _duration_from_database(value):
+    """
+    Converts a duration database value into a Duration object.
+
+    The deprecated 'variable durations' will be converted to Arrays.
+    """
+    if isinstance(value, (str, int)):
+        # Set default unit to minutes if value is a plain number.
+        if not isinstance(value, str):
+            value = f"{value}m"
+    elif isinstance(value, Sequence):
+        # This type of 'variable duration' is deprecated. We make an Array instead.
+        # Set default unit to minutes for plain numbers in value.
+        value = [v if isinstance(v, str) else f"{v}m" for v in value]
+        return Array([Duration(v) for v in value])
+    else:
+        raise ParameterValueFormatError("Duration value is of unsupported type")
+    return Duration(value)
+
+
+def _time_series_from_database(value_dict):
+    """Converts a time series database value into a time series object.
+
+    Args:
+        value_dict (dict): time series dictionary
+
+    Returns:
+        TimeSeries: restored time series
+    """
+    data = value_dict["data"]
+    if isinstance(data, dict):
+        return _time_series_from_dictionary(value_dict)
+    if isinstance(data, list):
+        if isinstance(data[0], Sequence):
+            return _time_series_from_two_columns(value_dict)
+        return _time_series_from_single_column(value_dict)
+    raise ParameterValueFormatError("Unrecognized time series format")
+
+
+def _variable_resolution_time_series_info_from_index(value):
+    """Returns ignore_year and repeat from index if present or their default values."""
+    if "index" in value:
+        data_index = value["index"]
+        try:
+            ignore_year = bool(data_index.get("ignore_year", False))
+        except ValueError:
+            raise ParameterValueFormatError(f'Could not decode ignore_year from "{data_index["ignore_year"]}"')
+        try:
+            repeat = bool(data_index.get("repeat", False))
+        except ValueError:
+            raise ParameterValueFormatError(f'Could not decode repeat from "{data_index["repeat"]}"')
+    else:
+        ignore_year = False
+        repeat = False
+    return ignore_year, repeat
+
+
+def _time_series_from_dictionary(value_dict):
+    """Converts a dictionary style time series into a TimeSeriesVariableResolution object.
+
+    Args:
+        value_dict (dict): time series dictionary
+
+    Returns:
+        TimeSeriesVariableResolution: restored time series
+    """
+    data = value_dict["data"]
+    stamps = list()
+    values = np.empty(len(data))
+    for index, (stamp, series_value) in enumerate(data.items()):
+        try:
+            stamp = np.datetime64(stamp, NUMPY_DATETIME64_UNIT)
+        except ValueError:
+            raise ParameterValueFormatError(f'Could not decode time stamp "{stamp}"')
+        stamps.append(stamp)
+        values[index] = series_value
+    stamps = np.array(stamps)
+    ignore_year, repeat = _variable_resolution_time_series_info_from_index(value_dict)
+    return TimeSeriesVariableResolution(stamps, values, ignore_year, repeat, value_dict.get("index_name", ""))
+
+
+def _time_series_from_single_column(value_dict):
+    """Converts a time series dictionary into a TimeSeriesFixedResolution object.
+
+    Args:
+        value_dict (dict): time series dictionary
+
+    Returns:
+        TimeSeriesFixedResolution: restored time series
+    """
+    if "index" in value_dict:
+        value_index = value_dict["index"]
+        start = value_index["start"] if "start" in value_index else _TIME_SERIES_DEFAULT_START
+        resolution = value_index["resolution"] if "resolution" in value_index else _TIME_SERIES_DEFAULT_RESOLUTION
+        if "ignore_year" in value_index:
+            try:
+                ignore_year = bool(value_index["ignore_year"])
+            except ValueError:
+                raise ParameterValueFormatError(f'Could not decode ignore_year value "{value_index["ignore_year"]}"')
+        else:
+            ignore_year = "start" not in value_index
+        if "repeat" in value_index:
+            try:
+                repeat = bool(value_index["repeat"])
+            except ValueError:
+                raise ParameterValueFormatError(f'Could not decode repeat value "{value_index["ignore_year"]}"')
+        else:
+            repeat = "start" not in value_index
+    else:
+        start = _TIME_SERIES_DEFAULT_START
+        resolution = _TIME_SERIES_DEFAULT_RESOLUTION
+        ignore_year = True
+        repeat = True
+    if isinstance(resolution, str) or not isinstance(resolution, Sequence):
+        # Always work with lists to simplify the code.
+        resolution = [resolution]
+    relativedeltas = list()
+    for duration in resolution:
+        if not isinstance(duration, str):
+            duration = str(duration) + _TIME_SERIES_PLAIN_INDEX_UNIT
+        relativedeltas.append(duration_to_relativedelta(duration))
+    try:
+        start = dateutil.parser.parse(start)
+    except ValueError:
+        raise ParameterValueFormatError(f'Could not decode start value "{start}"')
+    values = np.array(value_dict["data"])
+    return TimeSeriesFixedResolution(
+        start, relativedeltas, values, ignore_year, repeat, value_dict.get("index_name", "")
+    )
+
+
+def _time_series_from_two_columns(value_dict):
+    """Converts a two column style time series into a TimeSeriesVariableResolution object.
+
+    Args:
+        value_dict (dict): time series dictionary
+
+    Returns:
+        TimeSeriesVariableResolution: restored time series
+    """
+    data = value_dict["data"]
+    stamps = list()
+    values = np.empty(len(data))
+    for index, element in enumerate(data):
+        if not isinstance(element, Sequence) or len(element) != 2:
+            raise ParameterValueFormatError("Invalid value in time series array")
+        try:
+            stamp = np.datetime64(element[0], NUMPY_DATETIME64_UNIT)
+        except ValueError:
+            raise ParameterValueFormatError(f'Could not decode time stamp "{element[0]}"')
+        stamps.append(stamp)
+        values[index] = element[1]
+    stamps = np.array(stamps)
+    ignore_year, repeat = _variable_resolution_time_series_info_from_index(value_dict)
+    return TimeSeriesVariableResolution(stamps, values, ignore_year, repeat, value_dict.get("index_name", ""))
+
+
+def _time_pattern_from_database(value_dict):
+    """Converts a time pattern database value into a TimePattern object.
+
+    Args:
+        value_dict (dict): time pattern dictionary
+
+    Returns:
+        TimePattern: restored time pattern
+    """
+    patterns, values = _break_dictionary(value_dict["data"])
+    return TimePattern(patterns, values, value_dict.get("index_name", "p"))
+
+
+def _map_from_database(value_dict):
+    """Converts a map from its database representation to a Map object.
+
+    Args:
+        value_dict (dict): Map dictionary
+
+    Returns:
+        Map: restored Map
+    """
+    index_type = _map_index_type_from_database(value_dict["index_type"])
+    index_name = value_dict.get("index_name", "x")
+    data = value_dict["data"]
+    if isinstance(data, dict):
+        indexes = _map_indexes_from_database(data.keys(), index_type)
+        values = _map_values_from_database(data.values())
+    elif isinstance(data, Sequence):
+        if not data:
+            indexes = list()
+            values = list()
+        else:
+            indexes_in_db = list()
+            values_in_db = list()
+            for row in data:
+                if not isinstance(row, Sequence) or len(row) != 2:
+                    raise ParameterValueFormatError('"data" is not a nested two column array.')
+                indexes_in_db.append(row[0])
+                values_in_db.append(row[1])
+            indexes = _map_indexes_from_database(indexes_in_db, index_type)
+            values = _map_values_from_database(values_in_db)
+    else:
+        raise ParameterValueFormatError('"data" attribute is not a dict or array.')
+    return Map(indexes, values, index_type, index_name)
+
+
+def _map_index_type_from_database(index_type_in_db):
+    """Returns the type corresponding to index_type string."""
+    index_type = {"str": str, "date_time": DateTime, "duration": Duration, "float": float}.get(index_type_in_db, None)
+    if index_type is None:
+        raise ParameterValueFormatError(f'Unknown index_type "{index_type_in_db}".')
+    return index_type
+
+
+def _map_index_type_to_database(index_type):
+    """Returns the string corresponding to given index type."""
+    if issubclass(index_type, str):
+        return "str"
+    if issubclass(index_type, float):
+        return "float"
+    if index_type == DateTime:
+        return "date_time"
+    if index_type == Duration:
+        return "duration"
+    raise ParameterValueFormatError(f'Unknown index type "{index_type.__name__}".')
+
+
+def _map_indexes_from_database(indexes_in_db, index_type):
+    """Converts map's indexes from their database format."""
+    try:
+        indexes = [index_type(index) for index in indexes_in_db]
+    except ValueError as error:
+        raise ParameterValueFormatError(
+            f'Failed to read index of type "{_map_index_type_to_database(index_type)}": {error}'
+        )
+    else:
+        return indexes
+
+
+def _map_index_to_database(index):
+    """Converts a single map index to database format."""
+    if hasattr(index, "value_to_database_data"):
+        return index.value_to_database_data()
+    return index
+
+
+def _map_value_to_database(value):
+    """Converts a single map value to database format."""
+    if hasattr(value, "to_dict"):
+        return dict(type=value.type_(), **value.to_dict())
+    return value
+
+
+def _map_values_from_database(values_in_db):
+    """Converts map's values from their database format."""
+    if not values_in_db:
+        return list()
+    values = list()
+    for value_in_db in values_in_db:
+        value = from_dict(value_in_db) if isinstance(value_in_db, dict) else value_in_db
+        if isinstance(value, int):
+            value = float(value)
+        elif value is not None and not isinstance(value, (float, bool, Duration, IndexedValue, str, DateTime)):
+            raise ParameterValueFormatError(f'Unsupported value type for Map: "{type(value).__name__}".')
+        values.append(value)
+    return values
+
+
+def _array_from_database(value_dict):
+    """Converts a value dictionary to Array.
+
+    Args:
+          value_dict (dict): array dictionary
+
+    Returns:
+          Array: Array value
+    """
+    value_type_id = value_dict.get("value_type", "float")
+    value_type = {"float": float, "str": str, "date_time": DateTime, "duration": Duration, "time_period": str}.get(
+        value_type_id, None
+    )
+    if value_type is None:
+        raise ParameterValueFormatError(f'Unsupported value type for Array: "{value_type_id}".')
+    try:
+        data = [value_type(x) for x in value_dict["data"]]
+    except (TypeError, ParameterValueFormatError) as error:
+        raise ParameterValueFormatError(f'Failed to read values for Array: {error}')
+    else:
+        index_name = value_dict.get("index_name", "i")
+        return Array(data, value_type, index_name)
+
+
+class ListValueRef:
+    def __init__(self, list_value_id):
+        self._list_value_id = list_value_id
+
+    @staticmethod
+    def type_():
+        return "list_value_ref"
+
+    def to_database(self):
+        """Returns the database representation of this object as JSON."""
+        return json.dumps(self._list_value_id).encode("UTF8"), self.type_()
+
+
+class DateTime:
+    """A single datetime value."""
+
+    VALUE_TYPE = "single value"
+
+    def __init__(self, value=None):
+        """
+        Args:
+            value (DataTime or str or datetime.datetime): a timestamp
+        """
+        if value is None:
+            value = datetime(year=2000, month=1, day=1)
+        elif isinstance(value, str):
+            try:
+                value = dateutil.parser.parse(value)
+            except ValueError:
+                raise ParameterValueFormatError(f'Could not parse datetime from "{value}"')
+        elif isinstance(value, DateTime):
+            value = copy(value._value)
+        elif not isinstance(value, datetime):
+            raise ParameterValueFormatError(f'"{type(value).__name__}" cannot be converted to DateTime.')
+        self._value = value
+
+    def __eq__(self, other):
+        """Returns True if other is equal to this object."""
+        if not isinstance(other, DateTime):
+            return NotImplemented
+        return self._value == other._value
+
+    def __lt__(self, other):
+        if not isinstance(other, DateTime):
+            return NotImplemented
+        return self._value < other._value
+
+    def __hash__(self):
+        return hash(self._value)
+
+    def __str__(self):
+        return self._value.isoformat()
+
+    def value_to_database_data(self):
+        """Returns the database representation of the datetime."""
+        return self._value.isoformat()
+
+    def to_dict(self):
+        """Returns the database representation of this object."""
+        return {"data": self.value_to_database_data()}
+
+    @staticmethod
+    def type_():
+        return "date_time"
+
+    def to_database(self):
+        """Returns the database representation of this object as JSON."""
+        return json.dumps(self.to_dict()).encode("UTF8"), self.type_()
+
+    @property
+    def value(self):
+        """Returns the value as a datetime object."""
+        return self._value
+
+
+class Duration:
+    """
+    This class represents a duration in time.
+
+    Durations are always handled as relativedeltas.
+    """
+
+    VALUE_TYPE = "single value"
+
+    def __init__(self, value=None):
+        """
+        Args:
+            value (str or relativedelta): the time step
+        """
+        if value is None:
+            value = relativedelta(hours=1)
+        elif isinstance(value, str):
+            value = duration_to_relativedelta(value)
+        elif isinstance(value, Duration):
+            value = copy(value._value)
+        if not isinstance(value, relativedelta):
+            raise ParameterValueFormatError(f'Could not parse duration from "{value}"')
+        self._value = value
+
+    def __eq__(self, other):
+        """Returns True if other is equal to this object."""
+        if not isinstance(other, Duration):
+            return NotImplemented
+        return self._value == other._value
+
+    def __hash__(self):
+        return hash(self._value)
+
+    def __str__(self):
+        return str(relativedelta_to_duration(self._value))
+
+    def value_to_database_data(self):
+        """Returns the 'data' attribute part of Duration's database representation."""
+        return relativedelta_to_duration(self._value)
+
+    def to_dict(self):
+        """Returns the database representation of the duration."""
+        return {"data": self.value_to_database_data()}
+
+    @staticmethod
+    def type_():
+        return "duration"
+
+    def to_database(self):
+        """Returns the database representation of the duration as JSON."""
+        return json.dumps(self.to_dict()).encode("UTF8"), self.type_()
+
+    @property
+    def value(self):
+        """Returns the duration as a :class:`relativedelta`."""
+        return self._value
+
+
+class _Indexes(np.ndarray):
+    """
+    A subclass of numpy.ndarray that keeps a lookup dictionary from elements to positions.
+    Used by methods get_value and set_value of IndexedValue, to avoid something like
+
+        position = indexes.index(element)
+
+    which might be too slow compared to dictionary lookup.
+    """
+
+    def __new__(cls, other, dtype=None):
+        obj = np.asarray(other, dtype=dtype).view(cls)
+        obj.position_lookup = {index: k for k, index in enumerate(other)}
+        return obj
+
+    def __array_finalize__(self, obj):
+        if obj is None:
+            return
+        # pylint: disable=attribute-defined-outside-init
+        self.position_lookup = getattr(obj, 'position_lookup', {})
+
+    def __setitem__(self, position, index):
+        old_index = self.__getitem__(position)
+        self.position_lookup[index] = self.position_lookup.pop(old_index, '')
+        super().__setitem__(position, index)
+
+    def __eq__(self, other):
+        return np.all(super().__eq__(other))
+
+    def __bool__(self):
+        return np.size(self) != 0
+
+
+class IndexedValue:
+    """
+    An abstract base class for indexed values.
+
+    Attributes:
+        index_name (str): index name
+    """
+
+    VALUE_TYPE = NotImplemented
+
+    def __init__(self, index_name):
+        """
+        Args:
+            index_name (str): index name
+        """
+        self._indexes = None
+        self._values = None
+        self.index_name = index_name
+
+    def __bool__(self):
+        # NOTE: Use self.indexes rather than self._indexes, otherwise TimeSeriesFixedResolution gives wrong result
+        return bool(self.indexes)
+
+    def __len__(self):
+        """Returns the number of values."""
+        return len(self.indexes)
+
+    @staticmethod
+    def type_():
+        """Returns a type identifier string.
+
+        Returns:
+            str: type identifier
+        """
+        raise NotImplementedError()
+
+    @property
+    def indexes(self):
+        """Returns the indexes."""
+        return self._indexes
+
+    @indexes.setter
+    def indexes(self, indexes):
+        """Sets the indexes."""
+        self._indexes = _Indexes(indexes)
+
+    def to_database(self):
+        """Return the database representation of the value."""
+        return json.dumps(self.to_dict()).encode("UTF8"), self.type_()
+
+    @property
+    def values(self):
+        """Returns the data values."""
+        return self._values
+
+    @values.setter
+    def values(self, values):
+        """Sets the values."""
+        self._values = values
+
+    def get_nearest(self, index):
+        pos = np.searchsorted(self.indexes, index)
+        return self.values[pos]
+
+    def get_value(self, index):
+        """Returns the value at the given index."""
+        pos = self.indexes.position_lookup.get(index)
+        if pos is None:
+            return None
+        return self.values[pos]
+
+    def set_value(self, index, value):
+        """Sets the value at the given index."""
+        pos = self.indexes.position_lookup.get(index)
+        if pos is not None:
+            self.values[pos] = value
+
+    def to_dict(self):
+        """Converts the value to a Python dictionary.
+
+        Returns:
+            dict(): mapping from indexes to values
+        """
+        raise NotImplementedError()
+
+    def merge(self, other):
+        if not isinstance(other, type(self)):
+            return self
+        new_indexes = np.unique(np.concatenate((self.indexes, other.indexes)))
+        new_indexes.sort(kind='mergesort')
+        _merge = lambda value, other: other if value is None else merge_parsed(value, other)
+        new_values = [_merge(self.get_value(index), other.get_value(index)) for index in new_indexes]
+        self.indexes = new_indexes
+        self.values = new_values
+        return self
+
+
+class Array(IndexedValue):
+    """A one dimensional array with zero based indexing."""
+
+    VALUE_TYPE = "array"
+    DEFAULT_INDEX_NAME = "i"
+
+    def __init__(self, values, value_type=None, index_name=""):
+        """
+        Args:
+            values (Sequence): array's values
+            value_type (Type, optional): array element type; will be deduced from the array if not given
+                and defaults to float if ``values`` is empty
+            index_name (str): index name
+        """
+        super().__init__(index_name if index_name else Array.DEFAULT_INDEX_NAME)
+        if value_type is None:
+            value_type = type(values[0]) if values else float
+            if value_type == int:
+                try:
+                    values = [float(x) for x in values]
+                except ValueError:
+                    raise ParameterValueFormatError("Cannot convert array's values to float.")
+                value_type = float
+        if any(not isinstance(x, value_type) for x in values):
+            try:
+                values = [value_type(x) for x in values]
+            except ValueError:
+                raise ParameterValueFormatError("Not all array's values are of the same type.")
+        self.indexes = range(len(values))
+        self.values = list(values)
+        self._value_type = value_type
+
+    def __eq__(self, other):
+        if not isinstance(other, Array):
+            return NotImplemented
+        return np.array_equal(self._values, other._values) and self.index_name == other.index_name
+
+    @staticmethod
+    def type_():
+        return "array"
+
+    def to_dict(self):
+        """See base class."""
+        value_type_id = {
+            float: "float",
+            str: "str",  # String could also mean time_period but we don't have any way to distinguish that, yet.
+            DateTime: "date_time",
+            Duration: "duration",
+        }.get(self._value_type)
+        if value_type_id is None:
+            raise ParameterValueFormatError(f"Cannot write unsupported array value type: {self._value_type.__name__}")
+        if value_type_id in ("float", "str"):
+            data = self._values
+        else:
+            data = [x.value_to_database_data() for x in self._values]
+        value_dict = {"value_type": value_type_id, "data": data}
+        if self.index_name != "i":
+            value_dict["index_name"] = self.index_name
+        return value_dict
+
+    @property
+    def value_type(self):
+        """Returns the type of array's elements."""
+        return self._value_type
+
+
+class IndexedNumberArray(IndexedValue):
+    """
+    An abstract base class for indexed floats.
+
+    The indexes and numbers are stored in numpy.ndarrays.
+    """
+
+    def __init__(self, index_name, values):
+        """
+        Args:
+            index_name (str): index name
+            values (Sequence): array's values; index handling should be implemented by subclasses
+        """
+        super().__init__(index_name)
+        self.values = values
+
+    @IndexedValue.values.setter
+    def values(self, values):
+        """Sets the values."""
+        if not isinstance(values, np.ndarray) or not values.dtype == np.dtype(float):
+            values = np.array(values, dtype=float)
+        self._values = values
+
+    @staticmethod
+    def type_():
+        raise NotImplementedError()
+
+    def to_dict(self):
+        """Return the database representation of the value."""
+        raise NotImplementedError()
+
+
+class TimeSeries(IndexedNumberArray):
+    """An abstract base class for time series."""
+
+    VALUE_TYPE = "time series"
+    DEFAULT_INDEX_NAME = "t"
+
+    def __init__(self, values, ignore_year, repeat, index_name=""):
+        """
+        Args:
+            values (Sequence): an array of values
+            ignore_year (bool): True if the year should be ignored in the time stamps
+            repeat (bool): True if the series should be repeated from the beginning
+            index_name (str): index name
+        """
+        if len(values) < 1:
+            raise ParameterValueFormatError("Time series too short. Must have one or more values")
+        super().__init__(index_name if index_name else TimeSeries.DEFAULT_INDEX_NAME, values)
+        self._ignore_year = ignore_year
+        self._repeat = repeat
+
+    def __len__(self):
+        """Returns the number of values."""
+        return len(self._values)
+
+    @property
+    def ignore_year(self):
+        """Returns True if the year should be ignored."""
+        return self._ignore_year
+
+    @ignore_year.setter
+    def ignore_year(self, ignore_year):
+        self._ignore_year = bool(ignore_year)
+
+    @property
+    def repeat(self):
+        """Returns True if the series should be repeated."""
+        return self._repeat
+
+    @repeat.setter
+    def repeat(self, repeat):
+        self._repeat = bool(repeat)
+
+    @staticmethod
+    def type_():
+        return "time_series"
+
+    def to_dict(self):
+        """Return the database representation of the value."""
+        raise NotImplementedError()
+
+
+def _check_time_pattern_index(union_str):
+    """
+    Checks if a time pattern index has the right format.
+
+    Args:
+        union_str (str): The time pattern index to check. Generally assumed to be a union of interval intersections.
+
+    Raises:
+        ParameterValueFormatError: If the given string doesn't comply with time pattern spec.
+    """
+    if not union_str:
+        # We accept empty strings so we can add empty rows in the parameter value editor UI
+        return
+    union_dlm = ","
+    intersection_dlm = ";"
+    range_dlm = "-"
+    regexp = r"(Y|M|D|WD|h|m|s)"
+    for intersection_str in union_str.split(union_dlm):
+        for interval_str in intersection_str.split(intersection_dlm):
+            m = re.match(regexp, interval_str)
+            if m is None:
+                raise ParameterValueFormatError(
+                    f"Invalid interval {interval_str}, it should start with either Y, M, D, WD, h, m, or s."
+                )
+            key = m.group(0)
+            lower_upper_str = interval_str[len(key) :]
+            lower_upper = lower_upper_str.split(range_dlm)
+            if len(lower_upper) != 2:
+                raise ParameterValueFormatError(
+                    f"Invalid interval bounds {lower_upper_str}, it should be two integers separated by dash (-)."
+                )
+            lower_str, upper_str = lower_upper
+            try:
+                lower = int(lower_str)
+            except:
+                raise ParameterValueFormatError(f"Invalid lower bound {lower_str}, must be an integer.")
+            try:
+                upper = int(upper_str)
+            except:
+                raise ParameterValueFormatError(f"Invalid upper bound {upper_str}, must be an integer.")
+            if lower > upper:
+                raise ParameterValueFormatError(f"Lower bound {lower} can't be higher than upper bound {upper}.")
+
+
+class _TimePatternIndexes(_Indexes):
+    """An array of *checked* time pattern indexes."""
+
+    def __array_finalize__(self, obj):
+        """Checks indexes when building the array."""
+        for x in obj:
+            _check_time_pattern_index(x)
+        super().__array_finalize__(obj)
+
+    def __eq__(self, other):
+        return list(self) == list(other)
+
+    def __setitem__(self, position, index):
+        """Checks indexes when setting and item."""
+        _check_time_pattern_index(index)
+        super().__setitem__(position, index)
+
+
+class TimePattern(IndexedNumberArray):
+    """Represents a time pattern (relationship) parameter value."""
+
+    VALUE_TYPE = "time pattern"
+    DEFAULT_INDEX_NAME = "p"
+
+    def __init__(self, indexes, values, index_name=""):
+        """
+        Args:
+            indexes (list): a list of time pattern strings
+            values (Sequence): an array of values corresponding to the time patterns
+            index_name (str): index name
+        """
+        if len(indexes) != len(values):
+            raise ParameterValueFormatError("Length of values does not match length of indexes")
+        if not indexes:
+            raise ParameterValueFormatError("Empty time pattern not allowed")
+        super().__init__(index_name if index_name else TimePattern.DEFAULT_INDEX_NAME, values)
+        self.indexes = indexes
+
+    def __eq__(self, other):
+        """Returns True if other is equal to this object."""
+        if not isinstance(other, TimePattern):
+            return NotImplemented
+        return (
+            self._indexes == other._indexes
+            and np.all(self._values == other._values)
+            and self.index_name == other.index_name
+        )
+
+    @IndexedNumberArray.indexes.setter
+    def indexes(self, indexes):
+        """Sets the indexes."""
+        self._indexes = _TimePatternIndexes(indexes, dtype=np.object_)
+
+    @staticmethod
+    def type_():
+        return "time_pattern"
+
+    def to_dict(self):
+        """Returns the database representation of this time pattern."""
+        value_dict = {"data": dict(zip(self._indexes, self._values))}
+        if self.index_name != "p":
+            value_dict["index_name"] = self.index_name
+        return value_dict
+
+
+class TimeSeriesFixedResolution(TimeSeries):
+    """
+    A time series with fixed durations between the time stamps.
+
+    When getting the indexes the durations are applied cyclically.
+
+    Currently, there is no support for the `ignore_year` and `repeat` options
+    other than having getters for their values.
+    """
+
+    _memoized_indexes = {}
+
+    def __init__(self, start, resolution, values, ignore_year, repeat, index_name=""):
+        """
+        Args:
+            start (str or datetime or datetime64): the first time stamp
+            resolution (str, relativedelta, list): duration(s) between the time stamps
+            values (Sequence): data values at each time stamp
+            ignore_year (bool): whether or not the time-series should apply to any year
+            repeat (bool): whether or not the time series should repeat cyclically
+            index_name (str): index name
+        """
+        super().__init__(values, ignore_year, repeat, index_name)
+        self._start = None
+        self._resolution = None
+        self.start = start
+        self.resolution = resolution
+
+    def __eq__(self, other):
+        """Returns True if other is equal to this object."""
+        if not isinstance(other, TimeSeriesFixedResolution):
+            return NotImplemented
+        return (
+            self._start == other._start
+            and self._resolution == other._resolution
+            and np.array_equal(self._values, other._values, equal_nan=True)
+            and self._ignore_year == other._ignore_year
+            and self._repeat == other._repeat
+            and self.index_name == other.index_name
+        )
+
+    def _get_memoized_indexes(self):
+        key = (self.start, tuple(self.resolution), len(self))
+        memoized_indexes = self._memoized_indexes.get(key)
+        if memoized_indexes is not None:
+            return memoized_indexes
+        step_index = 0
+        step_cycle_index = 0
+        full_cycle_duration = sum(self._resolution, relativedelta())
+        stamps = np.empty(len(self), dtype=_NUMPY_DATETIME_DTYPE)
+        stamps[0] = self._start
+        for stamp_index in range(1, len(self._values)):
+            if step_index >= len(self._resolution):
+                step_index = 0
+                step_cycle_index += 1
+            current_cycle_duration = sum(self._resolution[: step_index + 1], relativedelta())
+            duration_from_start = step_cycle_index * full_cycle_duration + current_cycle_duration
+            stamps[stamp_index] = self._start + duration_from_start
+            step_index += 1
+        memoized_indexes = self._memoized_indexes[key] = np.array(stamps, dtype=_NUMPY_DATETIME_DTYPE)
+        return memoized_indexes
+
+    @property
+    def indexes(self):
+        """Returns the time stamps as a numpy.ndarray of numpy.datetime64 objects."""
+        if self._indexes is None:
+            self.indexes = self._get_memoized_indexes()
+        return IndexedValue.indexes.fget(self)
+
+    @indexes.setter
+    def indexes(self, indexes):
+        """Sets the indexes."""
+        # Needed because we redefine the setter
+        self._indexes = _Indexes(indexes)
+
+    @property
+    def start(self):
+        """Returns the start index."""
+        return self._start
+
+    @start.setter
+    def start(self, start):
+        """
+        Sets the start datetime.
+
+        Args:
+            start (datetime or datetime64 or str): the start of the series
+        """
+        if isinstance(start, str):
+            try:
+                self._start = dateutil.parser.parse(start)
+            except ValueError:
+                raise ParameterValueFormatError(f'Cannot parse start time "{start}"')
+        elif isinstance(start, np.datetime64):
+            self._start = start.tolist()
+        else:
+            self._start = start
+        self._indexes = None
+
+    @property
+    def resolution(self):
+        """Returns the resolution as list of durations."""
+        return self._resolution
+
+    @resolution.setter
+    def resolution(self, resolution):
+        """
+        Sets the resolution.
+
+        Args:
+            resolution (str, relativedelta, list): resolution or a list thereof
+        """
+        if isinstance(resolution, str):
+            resolution = [duration_to_relativedelta(resolution)]
+        elif not isinstance(resolution, Sequence):
+            resolution = [resolution]
+        else:
+            for i in range(len(resolution)):
+                if isinstance(resolution[i], str):
+                    resolution[i] = duration_to_relativedelta(resolution[i])
+        if not resolution:
+            raise ParameterValueFormatError("Resolution cannot be zero.")
+        self._resolution = resolution
+        self._indexes = None
+
+    def to_dict(self):
+        """Returns the value in its database representation."""
+        if len(self._resolution) > 1:
+            resolution_as_json = [relativedelta_to_duration(step) for step in self._resolution]
+        else:
+            resolution_as_json = relativedelta_to_duration(self._resolution[0])
+        value_dict = {
+            "index": {
+                "start": str(self._start),
+                "resolution": resolution_as_json,
+                "ignore_year": self._ignore_year,
+                "repeat": self._repeat,
+            },
+            "data": self._values.tolist(),
+        }
+        if self.index_name != "t":
+            value_dict["index_name"] = self.index_name
+        return value_dict
+
+
+class TimeSeriesVariableResolution(TimeSeries):
+    """A class representing time series data with variable time steps."""
+
+    def __init__(self, indexes, values, ignore_year, repeat, index_name=""):
+        """
+        Args:
+            indexes (Sequence): time stamps as numpy.datetime64 objects
+            values (Sequence): the values corresponding to the time stamps
+            ignore_year (bool): True if the stamp year should be ignored
+            repeat (bool): True if the series should be repeated from the beginning
+            index_name (str): index name
+        """
+        super().__init__(values, ignore_year, repeat, index_name)
+        if len(indexes) != len(values):
+            raise ParameterValueFormatError("Length of values does not match length of indexes")
+        if not isinstance(indexes, np.ndarray):
+            date_times = np.empty(len(indexes), dtype=_NUMPY_DATETIME_DTYPE)
+            for i, index in enumerate(indexes):
+                if isinstance(index, DateTime):
+                    date_times[i] = np.datetime64(index.value, NUMPY_DATETIME64_UNIT)
+                else:
+                    try:
+                        date_times[i] = np.datetime64(index, NUMPY_DATETIME64_UNIT)
+                    except ValueError:
+                        raise ParameterValueFormatError(
+                            f'Cannot convert "{index}" of type {type(index).__name__} to time stamp.'
+                        )
+            indexes = date_times
+        self.indexes = indexes
+
+    def __eq__(self, other):
+        """Returns True if other is equal to this object."""
+        if not isinstance(other, TimeSeriesVariableResolution):
+            return NotImplemented
+        return (
+            np.array_equal(self._indexes, other._indexes)
+            and np.array_equal(self._values, other._values, equal_nan=True)
+            and self._ignore_year == other._ignore_year
+            and self._repeat == other._repeat
+            and self.index_name == other.index_name
+        )
+
+    def to_dict(self):
+        """Returns the value in its database representation"""
+        value_dict = dict()
+        value_dict["data"] = {str(index): float(value) for index, value in zip(self._indexes, self._values)}
+        # Add "index" entry only if its contents are not set to their default values.
+        if self._ignore_year:
+            value_dict.setdefault("index", dict())["ignore_year"] = self._ignore_year
+        if self._repeat:
+            value_dict.setdefault("index", dict())["repeat"] = self._repeat
+        if self.index_name != "t":
+            value_dict["index_name"] = self.index_name
+        return value_dict
+
+
+class Map(IndexedValue):
+    """A nested general purpose indexed value."""
+
+    VALUE_TYPE = "map"
+    DEFAULT_INDEX_NAME = "x"
+
+    def __init__(self, indexes, values, index_type=None, index_name=""):
+        """
+        Args:
+            indexes (Sequence): map's indexes
+            values (Sequence): map's values
+            index_type (type or NoneType): index type or None to deduce from indexes
+            index_name (str): index name
+        """
+        if not indexes and index_type is None:
+            raise ParameterValueFormatError("Cannot deduce index type from empty indexes list.")
+        if indexes and index_type is not None and not isinstance(indexes[0], index_type):
+            raise ParameterValueFormatError('Type of index does not match "index_type" argument.')
+        if len(indexes) != len(values):
+            raise ParameterValueFormatError("Length of values does not match length of indexes")
+        super().__init__(index_name if index_name else Map.DEFAULT_INDEX_NAME)
+        self.indexes = indexes
+        self._index_type = index_type if index_type is not None else type(indexes[0])
+        self._values = values
+
+    def __eq__(self, other):
+        if not isinstance(other, Map):
+            return NotImplemented
+        return other._indexes == self._indexes and other._values == self._values and self.index_name == other.index_name
+
+    def is_nested(self):
+        """Returns True if any of the values is also a map."""
+        return any(isinstance(value, Map) for value in self._values)
+
+    def value_to_database_data(self):
+        """Returns map's database representation's 'data' dictionary."""
+        data = list()
+        for index, value in zip(self._indexes, self._values):
+            index_in_db = _map_index_to_database(index)
+            value_in_db = _map_value_to_database(value)
+            data.append([index_in_db, value_in_db])
+        return data
+
+    @staticmethod
+    def type_():
+        return "map"
+
+    def to_dict(self):
+        """Returns map's database representation."""
+        value_dict = {
+            "index_type": _map_index_type_to_database(self._index_type),
+            "data": self.value_to_database_data(),
+        }
+        if self.index_name != "x":
+            value_dict["index_name"] = self.index_name
+        return value_dict
+
+
+def map_dimensions(map_):
+    """Counts Map's dimensions.
+
+    Args:
+        map_ (Map): a Map
+
+    Returns:
+        int: number of dimensions
+    """
+    nested = 0
+    for v in map_.values:
+        if isinstance(v, Map):
+            nested = max(nested, map_dimensions(v))
+        elif isinstance(v, IndexedValue):
+            nested = max(nested, 1)
+    return 1 + nested
+
+
+def convert_leaf_maps_to_specialized_containers(map_):
+    """
+    Converts suitable leaf maps to corresponding specialized containers.
+
+    Currently supported conversions:
+
+    - index_type: :class:`DateTime`, all values ``float`` -> :class"`TimeSeries`
+
+    Args:
+        map_ (Map): a map to process
+
+    Returns:
+        IndexedValue: a map with leaves converted or specialized container if map was convertible in itself
+    """
+    converted_container = _try_convert_to_container(map_)
+    if converted_container is not None:
+        return converted_container
+    new_values = list()
+    for _, value in zip(map_.indexes, map_.values):
+        if isinstance(value, Map):
+            converted = convert_leaf_maps_to_specialized_containers(value)
+            new_values.append(converted)
+        else:
+            new_values.append(value)
+    return Map(map_.indexes, new_values, index_name=map_.index_name)
+
+
+def convert_containers_to_maps(value):
+    """
+    Converts indexed values into maps.
+
+    if ``value`` is :class:`Map` converts leaf values into Maps recursively.
+
+    Args:
+        value (IndexedValue): a value to convert
+
+    Returns:
+        Map: converted Map
+    """
+    if isinstance(value, Map):
+        if not value:
+            return value
+        new_values = list()
+        for _, x in zip(value.indexes, value.values):
+            if isinstance(x, IndexedValue):
+                new_values.append(convert_containers_to_maps(x))
+            else:
+                new_values.append(x)
+        return Map(list(value.indexes), new_values, index_name=value.index_name)
+    if isinstance(value, IndexedValue):
+        if not value:
+            if isinstance(value, TimeSeries):
+                return Map([], [], DateTime, index_name=TimeSeries.DEFAULT_INDEX_NAME)
+            return Map([], [], str)
+        return Map(list(value.indexes), list(value.values), index_name=value.index_name)
+    return value
+
+
+def convert_map_to_table(map_, make_square=True, row_this_far=None, empty=None):
+    """
+    Converts :class:`Map` into list of rows recursively.
+
+    Args:
+        map_ (Map): map to convert
+        make_square (bool): if True, append None to shorter rows, otherwise leave the row as is
+        row_this_far (list, optional): current row; used for recursion
+        empty (Any, optional): object to fill empty cells with
+
+    Returns:
+        list of list: map's rows
+    """
+    if row_this_far is None:
+        row_this_far = list()
+    rows = list()
+    for index, value in zip(map_.indexes, map_.values):
+        if not isinstance(value, Map):
+            rows.append(row_this_far + [index, value])
+        else:
+            rows += convert_map_to_table(value, False, row_this_far + [index])
+    if make_square:
+        max_length = 0
+        for row in rows:
+            max_length = max(max_length, len(row))
+        equal_length_rows = list()
+        for row in rows:
+            equal_length_row = row + (max_length - len(row)) * [empty]
+            equal_length_rows.append(equal_length_row)
+        return equal_length_rows
+    return rows
+
+
+def convert_map_to_dict(map_):
+    """
+    Converts :class:`Map` to nested dictionaries.
+
+    Args:
+        map_ (Map): map to convert
+
+    Returns:
+        dict: Map as a dict
+    """
+    d = dict()
+    for index, x in zip(map_.indexes, map_.values):
+        if isinstance(x, Map):
+            x = convert_map_to_dict(x)
+        d[index] = x
+    return d
+
+
+def _try_convert_to_container(map_):
+    """
+    Tries to convert a map to corresponding specialized container.
+
+    Args:
+        map_ (Map): a map to convert
+
+    Returns:
+        TimeSeriesVariableResolution or None: converted Map or None if the map couldn't be converted
+    """
+    if not map_:
+        return None
+    stamps = list()
+    values = list()
+    for index, value in zip(map_.indexes, map_.values):
+        if not isinstance(index, DateTime) or not isinstance(value, float):
+            return None
+        stamps.append(index)
+        values.append(value)
+    return TimeSeriesVariableResolution(stamps, values, False, False, index_name=map_.index_name)
+
+
+# List of scalar types that are supported by the spinedb_api
+SUPPORTED_TYPES = (Duration, DateTime, float, str)
+
+
+def join_value_and_type(db_value, db_type):
+    """Joins database value and type into a string.
+    The resulting string is a JSON string.
+    In case of complex types (duration, date_time, time_series, time_pattern, array, map),
+    the type is just added as top-level key.
+
+    Args:
+        db_value (bytes): database value
+        db_type (str, optional): value type
+
+    Returns:
+        str: parameter value as JSON with an additional `type` field.
+    """
+    try:
+        parsed = load_db_value(db_value, db_type)
+    except ParameterValueFormatError:
+        parsed = None
+    return json.dumps(parsed)
+
+
+def split_value_and_type(value_and_type):
+    """Splits the given string into value and type.
+    The string must be the result of calling ``join_value_and_type`` or have the same form.
+
+    Args:
+        value_and_type (str)
+
+    Returns:
+        bytes
+        str or NoneType
+    """
+    try:
+        parsed = json.loads(value_and_type)
+    except (TypeError, json.JSONDecodeError):
+        parsed = value_and_type
+    return dump_db_value(parsed)
```

### Comparing `spinedb_api-0.30.3/spinedb_api/purge.py` & `spinedb_api-0.30.4/spinedb_api/purge.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,80 +1,80 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Functions to purge dbs.
-
-"""
-
-from .db_mapping import DatabaseMapping
-from .exception import SpineDBAPIError, SpineDBVersionError
-from .filters.tools import clear_filter_configs
-from .helpers import remove_credentials_from_url
-
-
-def _ids_for_item_type(db_map, item_type):
-    """Queries ids for given database item type.
-
-    Args:
-        db_map (DatabaseMapping): database map
-        item_type (str): database item type
-
-    Returns:
-        set of int: item ids
-    """
-    sq_attr = db_map.cache_sqs[item_type]
-    return {row.id for row in db_map.query(getattr(db_map, sq_attr))}
-
-
-def purge_url(url, purge_settings, logger=None):
-    try:
-        db_map = DatabaseMapping(url)
-    except (SpineDBAPIError, SpineDBVersionError) as err:
-        sanitized_url = clear_filter_configs(remove_credentials_from_url(url))
-        if logger:
-            logger.msg_warning.emit(f"Failed to purge url <b>{sanitized_url}</b>: {err}")
-        return False
-    success = purge(db_map, purge_settings, logger=logger)
-    db_map.connection.close()
-    return success
-
-
-def purge(db_map, purge_settings, logger=None):
-    """Removes items from database.
-
-    Args:
-        db_map (DatabaseMapping): target database mapping
-        purge_settings (dict, optional): mapping from item type to purge flag
-        logger (LoggerInterface): logger
-
-    Returns:
-        bool: True if operation was successful, False otherwise
-    """
-    if purge_settings is None:
-        # Bring all the pain
-        purge_settings = {item_type: True for item_type in DatabaseMapping.ITEM_TYPES}
-    removable_db_map_data = {
-        item_type: _ids_for_item_type(db_map, item_type) for item_type, checked in purge_settings.items() if checked
-    }
-    removable_db_map_data = {item_type: ids for item_type, ids in removable_db_map_data.items() if ids}
-    if removable_db_map_data:
-        try:
-            if logger:
-                logger.msg.emit("Purging database...")
-            db_map.cascade_remove_items(**removable_db_map_data)
-            db_map.commit_session("Purge database")
-            if logger:
-                logger.msg.emit("Database purged")
-        except SpineDBAPIError:
-            if logger:
-                logger.msg_error.emit("Failed to purge database.")
-            return False
-    return True
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Functions to purge dbs.
+
+"""
+
+from .db_mapping import DatabaseMapping
+from .exception import SpineDBAPIError, SpineDBVersionError
+from .filters.tools import clear_filter_configs
+from .helpers import remove_credentials_from_url
+
+
+def _ids_for_item_type(db_map, item_type):
+    """Queries ids for given database item type.
+
+    Args:
+        db_map (DatabaseMapping): database map
+        item_type (str): database item type
+
+    Returns:
+        set of int: item ids
+    """
+    sq_attr = db_map.cache_sqs[item_type]
+    return {row.id for row in db_map.query(getattr(db_map, sq_attr))}
+
+
+def purge_url(url, purge_settings, logger=None):
+    try:
+        db_map = DatabaseMapping(url)
+    except (SpineDBAPIError, SpineDBVersionError) as err:
+        sanitized_url = clear_filter_configs(remove_credentials_from_url(url))
+        if logger:
+            logger.msg_warning.emit(f"Failed to purge url <b>{sanitized_url}</b>: {err}")
+        return False
+    success = purge(db_map, purge_settings, logger=logger)
+    db_map.connection.close()
+    return success
+
+
+def purge(db_map, purge_settings, logger=None):
+    """Removes items from database.
+
+    Args:
+        db_map (DatabaseMapping): target database mapping
+        purge_settings (dict, optional): mapping from item type to purge flag
+        logger (LoggerInterface): logger
+
+    Returns:
+        bool: True if operation was successful, False otherwise
+    """
+    if purge_settings is None:
+        # Bring all the pain
+        purge_settings = {item_type: True for item_type in DatabaseMapping.ITEM_TYPES}
+    removable_db_map_data = {
+        item_type: _ids_for_item_type(db_map, item_type) for item_type, checked in purge_settings.items() if checked
+    }
+    removable_db_map_data = {item_type: ids for item_type, ids in removable_db_map_data.items() if ids}
+    if removable_db_map_data:
+        try:
+            if logger:
+                logger.msg.emit("Purging database...")
+            db_map.cascade_remove_items(**removable_db_map_data)
+            db_map.commit_session("Purge database")
+            if logger:
+                logger.msg.emit("Database purged")
+        except SpineDBAPIError:
+            if logger:
+                logger.msg_error.emit("Failed to purge database.")
+            return False
+    return True
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_db_server.py` & `spinedb_api-0.30.4/spinedb_api/spine_db_server.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,604 +1,604 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains the SpineDBServer class.
-
-"""
-
-from urllib.parse import urlunsplit
-from contextlib import contextmanager
-import socketserver
-import multiprocessing as mp
-from multiprocessing.queues import Queue as MPQueue
-import threading
-import atexit
-import traceback
-import time
-import uuid
-from queue import Queue
-from sqlalchemy.exc import DBAPIError
-from spinedb_api import __version__ as spinedb_api_version
-from .db_mapping import DatabaseMapping
-from .import_functions import import_data
-from .export_functions import export_data
-from .parameter_value import dump_db_value
-from .server_client_helpers import ReceiveAllMixing, encode, decode
-from .filters.scenario_filter import scenario_filter_config
-from .filters.tool_filter import tool_filter_config
-from .filters.alternative_filter import alternative_filter_config
-from .filters.tools import apply_filter_stack
-from .spine_db_client import SpineDBClient
-
-_required_client_version = 6
-
-
-def _parse_value(v, value_type=None):
-    return (v, value_type)
-
-
-def _unparse_value(value_and_type):
-    if isinstance(value_and_type, (tuple, list)) and len(value_and_type) == 2:
-        value, type_ = value_and_type
-        if value is None or (isinstance(value, bytes) and (isinstance(type_, str) or type_ is None)):
-            # Tuple of value and type ready to go
-            return value, type_
-    # JSON object
-    return dump_db_value(value_and_type)
-
-
-class SpineDBServer(socketserver.TCPServer):
-    def __init__(self, manager_queue, ordering, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.manager_queue = manager_queue
-        self.ordering = ordering
-
-
-class _DeepCopyableQueue(MPQueue):
-    def __init__(self, *args, **kwargs):
-        ctx = mp.get_context()
-        super().__init__(*args, **kwargs, ctx=ctx)
-
-    def __deepcopy__(self, _memo):
-        return self
-
-
-class _DBServerManager:
-    _SHUTDOWN = "shutdown"
-    _CHECKOUT_COMPLETE = "checkout_complete"
-
-    def __init__(self):
-        super().__init__()
-        self._servers = {}
-        self._checkouts = {}
-        self._waiters = {}
-        self._queue = _DeepCopyableQueue()
-        self._process = mp.Process(target=self._do_work)
-        self._process.start()
-
-    def shutdown(self):
-        self._queue.put(self._SHUTDOWN)
-        self._process.join()
-
-    @property
-    def queue(self):
-        return self._queue
-
-    @property
-    def _handlers(self):
-        return {
-            "start_server": self._start_server,
-            "shutdown_server": self._shutdown_server,
-            "shutdown_servers": self._shutdown_servers,
-            "db_checkin": self._db_checkin,
-            "db_checkout": self._db_checkout,
-            "quick_db_checkout": self._quick_db_checkout,
-            "cancel_db_checkout": self._cancel_db_checkout,
-        }
-
-    def _do_work(self):
-        while True:
-            msg = self._queue.get()
-            if msg == self._SHUTDOWN:
-                break
-            output_queue, request, args, kwargs = msg
-            handler = self._handlers[request]
-            result = handler(*args, **kwargs)
-            output_queue.put(result)
-        for server_address in list(self._servers):
-            self._shutdown_server(server_address)
-
-    def _start_server(self, db_url, upgrade, memory, ordering):
-        host = "127.0.0.1"
-        while True:
-            with socketserver.TCPServer((host, 0), None) as s:
-                port = s.server_address[1]
-            try:
-                server = SpineDBServer(self._queue, ordering, (host, port), DBRequestHandler)
-                break
-            except OSError:
-                # [Errno 98] Address already in use
-                time.sleep(0.02)
-        self._servers[server.server_address] = server
-        server_thread = threading.Thread(target=server.serve_forever)
-        server_thread.daemon = True
-        server_thread.start()
-        error = SpineDBClient(server.server_address).open_db_map(db_url, upgrade, memory).get("error")
-        if error:
-            raise RuntimeError(error)
-        return server.server_address
-
-    def _shutdown_server(self, server_address):
-        server = self._servers.pop(server_address, None)
-        if server is None:
-            return False
-        SpineDBClient(server.server_address).close_db_map()
-        server.shutdown()
-        server.server_close()
-        return True
-
-    def _shutdown_servers(self):
-        return all(self._shutdown_server(server_address) for server_address in list(self._servers))
-
-    def _db_checkin(self, server_address):
-        server = self._servers.get(server_address)
-        if not server:
-            return
-        ordering = server.ordering
-        checkouts = self._checkouts.get(ordering["id"], dict())
-        full_checkouts = set(x for x, count in checkouts.items() if count == self._CHECKOUT_COMPLETE)
-        precursors = ordering["precursors"]
-        if precursors <= full_checkouts:
-            return
-        event = mp.Manager().Event()
-        self._waiters.setdefault(ordering["id"], {})[event] = precursors
-        return event
-
-    def _db_checkout(self, server_address):
-        server = self._servers.get(server_address)
-        if not server:
-            return
-        ordering = server.ordering
-        self._quick_db_checkout(ordering)
-
-    def _quick_db_checkout(self, ordering):
-        current = ordering["current"]
-        checkouts = self._checkouts.setdefault(ordering["id"], dict())
-        if current not in checkouts:
-            checkouts[current] = 1
-        elif checkouts[current] != self._CHECKOUT_COMPLETE:
-            checkouts[current] += 1
-        if checkouts[current] == ordering["part_count"]:
-            checkouts[current] = self._CHECKOUT_COMPLETE
-        full_checkouts = set(x for x, count in checkouts.items() if count == self._CHECKOUT_COMPLETE)
-        waiters = self._waiters.get(ordering["id"], dict())
-        done = [event for event, precursors in waiters.items() if precursors <= full_checkouts]
-        for event in done:
-            del waiters[event]
-            event.set()
-
-    def _cancel_db_checkout(self, server_address):
-        server = self._servers.get(server_address)
-        if not server:
-            return
-        ordering = server.ordering
-        checkouts = self._checkouts.get(ordering["id"], dict())
-        checkouts.pop(ordering["current"], None)
-
-
-class _ManagerRequestHandler:
-    def __init__(self, mngr_queue):
-        self._mngr_queue = mngr_queue
-
-    def _run_request(self, request, *args, **kwargs):
-        with mp.Manager() as manager:
-            output_queue = manager.Queue()
-            self._mngr_queue.put((output_queue, request, args, kwargs))
-            return output_queue.get()
-
-    def start_server(self, db_url, upgrade, memory, ordering):
-        return self._run_request("start_server", db_url, upgrade, memory, ordering)
-
-    def shutdown_server(self, server_address):
-        return self._run_request("shutdown_server", server_address)
-
-    def shutdown_servers(self):
-        return self._run_request("shutdown_servers")
-
-    def register_ordering(self, server_address, ordering):
-        return self._run_request("register_ordering", server_address, ordering)
-
-    def db_checkin(self, server_address):
-        event = self._run_request("db_checkin", server_address)
-        if event:
-            event.wait()
-
-    def db_checkout(self, server_address):
-        return self._run_request("db_checkout", server_address)
-
-    def quick_db_checkout(self, ordering):
-        return self._run_request("quick_db_checkout", ordering)
-
-    def cancel_db_checkout(self, server_address):
-        return self._run_request("cancel_db_checkout", server_address)
-
-
-class _DBWorker:
-    _CLOSE = "close"
-
-    def __init__(self, db_url, upgrade, memory, create=True):
-        self._db_url = db_url
-        self._upgrade = upgrade
-        self._memory = memory
-        self._create = create
-        self._db_map = None
-        self._lock = threading.Lock()
-        self._in_queue = Queue()
-        self._out_queue = Queue()
-        self._thread = threading.Thread(target=self._do_work)
-        self._thread.start()
-        error = self._out_queue.get()
-        if isinstance(error, Exception):
-            raise error
-
-    @property
-    def db_url(self):
-        return str(self._db_map.db_url)
-
-    def shutdown(self):
-        self._in_queue.put(self._CLOSE)
-        self._thread.join()
-
-    def _do_work(self):
-        try:
-            self._db_map = DatabaseMapping(
-                self._db_url, upgrade=self._upgrade, memory=self._memory, create=self._create
-            )
-            self._out_queue.put(None)
-        except Exception as error:  # pylint: disable=broad-except
-            self._out_queue.put(error)
-            return
-        while True:
-            input_ = self._in_queue.get()
-            if input_ == self._CLOSE:
-                self._db_map.connection.close()
-                break
-            request, args, kwargs = input_
-            handler = {
-                "query": self._do_query,
-                "filtered_query": self._do_filtered_query,
-                "import_data": self._do_import_data,
-                "export_data": self._do_export_data,
-                "call_method": self._do_call_method,
-                "apply_filters": self._do_apply_filters,
-                "clear_filters": self._do_clear_filters,
-            }[request]
-            result = handler(*args, **kwargs)
-            self._out_queue.put(result)
-
-    def run(self, request, args, kwargs):
-        with self._lock:
-            self._in_queue.put((request, args, kwargs))
-            return self._out_queue.get()
-
-    def _do_query(self, *args):
-        result = {}
-        for sq_name in args:
-            sq = getattr(self._db_map, sq_name, None)
-            if sq is None:
-                continue
-            result[sq_name] = [x._asdict() for x in self._db_map.query(sq)]
-        return dict(result=result)
-
-    def _do_filtered_query(self, **kwargs):
-        result = {}
-        for sq_name, filters in kwargs.items():
-            sq = getattr(self._db_map, sq_name, None)
-            if sq is None:
-                continue
-            qry = self._db_map.query(sq)
-            for field, value in filters.items():
-                qry = qry.filter_by(**{field: value})
-            result[sq_name] = [x._asdict() for x in qry]
-        return dict(result=result)
-
-    def _do_import_data(self, data, comment):
-        count, errors = import_data(self._db_map, unparse_value=_unparse_value, **data)
-        if count and comment:
-            try:
-                self._db_map.commit_session(comment)
-            except DBAPIError:
-                self._db_map.rollback_session()
-        return dict(result=(count, errors))
-
-    def _do_export_data(self, **kwargs):
-        return dict(result=export_data(self._db_map, parse_value=_parse_value, **kwargs))
-
-    def _do_call_method(self, method_name, *args, **kwargs):
-        method = getattr(self._db_map, method_name)
-        result = method(*args, **kwargs)
-        return dict(result=result)
-
-    def _do_clear_filters(self):
-        self._db_map.restore_entity_sq_maker()
-        self._db_map.restore_entity_class_sq_maker()
-        self._db_map.restore_parameter_definition_sq_maker()
-        self._db_map.restore_parameter_value_sq_maker()
-        self._db_map.restore_alternative_sq_maker()
-        self._db_map.restore_scenario_sq_maker()
-        self._db_map.restore_scenario_alternative_sq_maker()
-        return dict(result=True)
-
-    def _do_apply_filters(self, configs):
-        try:
-            apply_filter_stack(self._db_map, configs)
-            return dict(result=True)
-        except Exception as error:  # pylint: disable=broad-except
-            return dict(error=str(error))
-
-
-class _DBManager:
-    def __init__(self):
-        self._workers = {}
-
-    def open_db_map(self, server_address, db_url, upgrade, memory):
-        worker = self._workers.get(server_address)
-        if worker is None:
-            try:
-                worker = self._workers[server_address] = _DBWorker(db_url, upgrade, memory)
-            except Exception as error:  # pylint: disable=broad-except
-                return dict(error=str(error))
-        return dict(result=True)
-
-    def close_db_map(self, server_address):
-        worker = self._workers.pop(server_address, None)
-        if worker is None:
-            return dict(result=False)
-        worker.shutdown()
-        return dict(result=True)
-
-    def get_db_url(self, server_address):
-        worker = self._workers.get(server_address)
-        if worker is not None:
-            return worker.db_url
-
-    def _run_request(self, server_address, request, args=(), kwargs=None, create=True):
-        if kwargs is None:
-            kwargs = {}
-        worker = self._workers.get(server_address)
-        if worker is not None:
-            return worker.run(request, args, kwargs)
-
-    def query(self, server_address, *args):
-        return self._run_request(server_address, "query", args=args)
-
-    def filtered_query(self, server_address, **kwargs):
-        return self._run_request(server_address, "filtered_query", kwargs=kwargs)
-
-    def import_data(self, server_address, data, comment):
-        return self._run_request(server_address, "import_data", args=(data, comment))
-
-    def export_data(self, server_address, **kwargs):
-        return self._run_request(server_address, "export_data", kwargs=kwargs)
-
-    def call_method(self, server_address, method_name, *args, **kwargs):
-        return self._run_request(server_address, "call_method", args=(method_name, *args), kwargs=kwargs)
-
-    def apply_filters(self, server_address, configs):
-        return self._run_request(server_address, "apply_filters", args=(configs,))
-
-    def clear_filters(self, server_address):
-        return self._run_request(server_address, "clear_filters")
-
-
-_db_manager = _DBManager()
-
-
-class HandleDBMixin:
-    def get_db_url(self):
-        """
-        Returns:
-            str: The underlying db url
-        """
-        return _db_manager.get_db_url(self.server_address)
-
-    def query(self, *args):
-        """
-        Runs queries.
-
-        Returns:
-            dict: where result is a dict from subquery name to a list of items from thay subquery, if successful.
-        """
-        return _db_manager.query(self.server_address, *args)
-
-    def filtered_query(self, **kwargs):
-        """
-        Runs queries with filters.
-
-        Returns:
-            dict: where result is a dict from subquery name to a list of items from thay subquery, if successful.
-        """
-        return _db_manager.filtered_query(self.server_address, **kwargs)
-
-    def import_data(self, data, comment):
-        """Imports data and commit.
-
-        Args:
-            data (dict)
-            comment (str)
-        Returns:
-            dict: where result is a list of import errors, if successful.
-        """
-        return _db_manager.import_data(self.server_address, data, comment)
-
-    def export_data(self, **kwargs):
-        """Exports data.
-
-        Returns:
-            dict: where result is the data exported from the db
-        """
-        return _db_manager.export_data(self.server_address, **kwargs)
-
-    def call_method(self, method_name, *args, **kwargs):
-        """Calls a method from the DatabaseMapping class.
-
-        Args:
-            method_name (str): the method name
-            args: positional arguments passed to the method call
-            kwargs: keyword arguments passed to the method call
-
-        Returns:
-            dict: where result is the return value of the method
-        """
-        return _db_manager.call_method(self.server_address, method_name, *args, **kwargs)
-
-    def apply_filters(self, filters):
-        configs = [
-            {"scenario": scenario_filter_config, "tool": tool_filter_config, "alternatives": alternative_filter_config}[
-                key
-            ](value)
-            for key, value in filters.items()
-        ]
-        return _db_manager.apply_filters(self.server_address, configs)
-
-    def clear_filters(self):
-        return _db_manager.clear_filters(self.server_address)
-
-    def db_checkin(self):
-        _ManagerRequestHandler(self.server_manager_queue).db_checkin(self.server_address)
-        return dict(result=True)
-
-    def db_checkout(self):
-        _ManagerRequestHandler(self.server_manager_queue).db_checkout(self.server_address)
-        return dict(result=True)
-
-    def cancel_db_checkout(self):
-        _ManagerRequestHandler(self.server_manager_queue).cancel_db_checkout(self.server_address)
-        return dict(result=True)
-
-    def open_db_map(self, db_url, upgrade, memory):
-        return _db_manager.open_db_map(self.server_address, db_url, upgrade, memory)
-
-    def close_db_map(self):
-        return _db_manager.close_db_map(self.server_address)
-
-    def _get_response(self, request):
-        request, *extras = decode(request)
-        # NOTE: Clients should always send requests "get_api_version" and "get_db_url" in a format that is compatible
-        # with the legacy server -- to (based on the format of the answer) determine that it needs to be updated.
-        # That's why we don't expand the extras so far.
-        response = {"get_api_version": spinedb_api_version, "get_db_url": self.get_db_url()}.get(request)
-        if response is not None:
-            return response
-        try:
-            args, kwargs, client_version = extras
-        except ValueError:
-            client_version = 0
-        if client_version < _required_client_version:
-            return dict(error=1, result=_required_client_version)
-        handler = {
-            "query": self.query,
-            "filtered_query": self.filtered_query,
-            "import_data": self.import_data,
-            "export_data": self.export_data,
-            "call_method": self.call_method,
-            "apply_filters": self.apply_filters,
-            "clear_filters": self.clear_filters,
-            "db_checkin": self.db_checkin,
-            "db_checkout": self.db_checkout,
-            "cancel_db_checkout": self.cancel_db_checkout,
-            "open_db_map": self.open_db_map,
-            "close_db_map": self.close_db_map,
-        }.get(request)
-        if handler is None:
-            return dict(error=f"invalid request '{request}'")
-        try:
-            return handler(*args, **kwargs)
-        except Exception:  # pylint: disable=broad-except
-            return dict(error=traceback.format_exc())
-
-    def handle_request(self, request):
-        response = self._get_response(request)
-        return encode(response)
-
-
-class DBHandler(HandleDBMixin):
-    def __init__(self, db_url, upgrade=False, memory=False):
-        self.server_address = uuid.uuid4().hex
-        error = _db_manager.open_db_map(self.server_address, db_url, upgrade, memory).get("error")
-        if error:
-            raise RuntimeError(error)
-        atexit.register(self.close)
-
-    def close(self):
-        _db_manager.close_db_map(self.server_address)
-
-
-class DBRequestHandler(ReceiveAllMixing, HandleDBMixin, socketserver.BaseRequestHandler):
-    """
-    The request handler class for our server.
-    """
-
-    @property
-    def server_address(self):
-        return self.server.server_address
-
-    @property
-    def server_manager_queue(self):
-        return self.server.manager_queue
-
-    def handle(self):
-        request = self._recvall()
-        response = self.handle_request(request)
-        self.request.sendall(response + bytes(self._EOT, self._ENCODING))
-
-
-def quick_db_checkout(server_manager_queue, ordering):
-    _ManagerRequestHandler(server_manager_queue).quick_db_checkout(ordering)
-
-
-def start_spine_db_server(server_manager_queue, db_url, upgrade=False, memory=False, ordering=None):
-    """
-    Args:
-        db_url (str): Spine db url
-        upgrade (bool): Whether to upgrade db or not
-        memory (bool): Whether to use an in-memory database together with a persistent connection to it
-
-    Returns:
-        tuple: server address (e.g. (127.0.0.1, 54321))
-    """
-    handler = _ManagerRequestHandler(server_manager_queue)
-    server_address = handler.start_server(db_url, upgrade, memory, ordering)
-    return server_address
-
-
-def shutdown_spine_db_server(server_manager_queue, server_address):
-    handler = _ManagerRequestHandler(server_manager_queue)
-    handler.shutdown_server(server_address)
-
-
-@contextmanager
-def closing_spine_db_server(server_manager_queue, db_url, upgrade=False, memory=False, ordering=None):
-    server_address = start_spine_db_server(server_manager_queue, db_url, memory=memory, ordering=ordering)
-    host, port = server_address
-    try:
-        yield urlunsplit(("http", f"{host}:{port}", "", "", ""))
-    finally:
-        shutdown_spine_db_server(server_manager_queue, server_address)
-
-
-@contextmanager
-def db_server_manager():
-    mngr = _DBServerManager()
-    try:
-        yield mngr.queue
-    finally:
-        mngr.shutdown()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains the SpineDBServer class.
+
+"""
+
+from urllib.parse import urlunsplit
+from contextlib import contextmanager
+import socketserver
+import multiprocessing as mp
+from multiprocessing.queues import Queue as MPQueue
+import threading
+import atexit
+import traceback
+import time
+import uuid
+from queue import Queue
+from sqlalchemy.exc import DBAPIError
+from spinedb_api import __version__ as spinedb_api_version
+from .db_mapping import DatabaseMapping
+from .import_functions import import_data
+from .export_functions import export_data
+from .parameter_value import dump_db_value
+from .server_client_helpers import ReceiveAllMixing, encode, decode
+from .filters.scenario_filter import scenario_filter_config
+from .filters.tool_filter import tool_filter_config
+from .filters.alternative_filter import alternative_filter_config
+from .filters.tools import apply_filter_stack
+from .spine_db_client import SpineDBClient
+
+_required_client_version = 6
+
+
+def _parse_value(v, value_type=None):
+    return (v, value_type)
+
+
+def _unparse_value(value_and_type):
+    if isinstance(value_and_type, (tuple, list)) and len(value_and_type) == 2:
+        value, type_ = value_and_type
+        if value is None or (isinstance(value, bytes) and (isinstance(type_, str) or type_ is None)):
+            # Tuple of value and type ready to go
+            return value, type_
+    # JSON object
+    return dump_db_value(value_and_type)
+
+
+class SpineDBServer(socketserver.TCPServer):
+    def __init__(self, manager_queue, ordering, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.manager_queue = manager_queue
+        self.ordering = ordering
+
+
+class _DeepCopyableQueue(MPQueue):
+    def __init__(self, *args, **kwargs):
+        ctx = mp.get_context()
+        super().__init__(*args, **kwargs, ctx=ctx)
+
+    def __deepcopy__(self, _memo):
+        return self
+
+
+class _DBServerManager:
+    _SHUTDOWN = "shutdown"
+    _CHECKOUT_COMPLETE = "checkout_complete"
+
+    def __init__(self):
+        super().__init__()
+        self._servers = {}
+        self._checkouts = {}
+        self._waiters = {}
+        self._queue = _DeepCopyableQueue()
+        self._process = mp.Process(target=self._do_work)
+        self._process.start()
+
+    def shutdown(self):
+        self._queue.put(self._SHUTDOWN)
+        self._process.join()
+
+    @property
+    def queue(self):
+        return self._queue
+
+    @property
+    def _handlers(self):
+        return {
+            "start_server": self._start_server,
+            "shutdown_server": self._shutdown_server,
+            "shutdown_servers": self._shutdown_servers,
+            "db_checkin": self._db_checkin,
+            "db_checkout": self._db_checkout,
+            "quick_db_checkout": self._quick_db_checkout,
+            "cancel_db_checkout": self._cancel_db_checkout,
+        }
+
+    def _do_work(self):
+        while True:
+            msg = self._queue.get()
+            if msg == self._SHUTDOWN:
+                break
+            output_queue, request, args, kwargs = msg
+            handler = self._handlers[request]
+            result = handler(*args, **kwargs)
+            output_queue.put(result)
+        for server_address in list(self._servers):
+            self._shutdown_server(server_address)
+
+    def _start_server(self, db_url, upgrade, memory, ordering):
+        host = "127.0.0.1"
+        while True:
+            with socketserver.TCPServer((host, 0), None) as s:
+                port = s.server_address[1]
+            try:
+                server = SpineDBServer(self._queue, ordering, (host, port), DBRequestHandler)
+                break
+            except OSError:
+                # [Errno 98] Address already in use
+                time.sleep(0.02)
+        self._servers[server.server_address] = server
+        server_thread = threading.Thread(target=server.serve_forever)
+        server_thread.daemon = True
+        server_thread.start()
+        error = SpineDBClient(server.server_address).open_db_map(db_url, upgrade, memory).get("error")
+        if error:
+            raise RuntimeError(error)
+        return server.server_address
+
+    def _shutdown_server(self, server_address):
+        server = self._servers.pop(server_address, None)
+        if server is None:
+            return False
+        SpineDBClient(server.server_address).close_db_map()
+        server.shutdown()
+        server.server_close()
+        return True
+
+    def _shutdown_servers(self):
+        return all(self._shutdown_server(server_address) for server_address in list(self._servers))
+
+    def _db_checkin(self, server_address):
+        server = self._servers.get(server_address)
+        if not server:
+            return
+        ordering = server.ordering
+        checkouts = self._checkouts.get(ordering["id"], dict())
+        full_checkouts = set(x for x, count in checkouts.items() if count == self._CHECKOUT_COMPLETE)
+        precursors = ordering["precursors"]
+        if precursors <= full_checkouts:
+            return
+        event = mp.Manager().Event()
+        self._waiters.setdefault(ordering["id"], {})[event] = precursors
+        return event
+
+    def _db_checkout(self, server_address):
+        server = self._servers.get(server_address)
+        if not server:
+            return
+        ordering = server.ordering
+        self._quick_db_checkout(ordering)
+
+    def _quick_db_checkout(self, ordering):
+        current = ordering["current"]
+        checkouts = self._checkouts.setdefault(ordering["id"], dict())
+        if current not in checkouts:
+            checkouts[current] = 1
+        elif checkouts[current] != self._CHECKOUT_COMPLETE:
+            checkouts[current] += 1
+        if checkouts[current] == ordering["part_count"]:
+            checkouts[current] = self._CHECKOUT_COMPLETE
+        full_checkouts = set(x for x, count in checkouts.items() if count == self._CHECKOUT_COMPLETE)
+        waiters = self._waiters.get(ordering["id"], dict())
+        done = [event for event, precursors in waiters.items() if precursors <= full_checkouts]
+        for event in done:
+            del waiters[event]
+            event.set()
+
+    def _cancel_db_checkout(self, server_address):
+        server = self._servers.get(server_address)
+        if not server:
+            return
+        ordering = server.ordering
+        checkouts = self._checkouts.get(ordering["id"], dict())
+        checkouts.pop(ordering["current"], None)
+
+
+class _ManagerRequestHandler:
+    def __init__(self, mngr_queue):
+        self._mngr_queue = mngr_queue
+
+    def _run_request(self, request, *args, **kwargs):
+        with mp.Manager() as manager:
+            output_queue = manager.Queue()
+            self._mngr_queue.put((output_queue, request, args, kwargs))
+            return output_queue.get()
+
+    def start_server(self, db_url, upgrade, memory, ordering):
+        return self._run_request("start_server", db_url, upgrade, memory, ordering)
+
+    def shutdown_server(self, server_address):
+        return self._run_request("shutdown_server", server_address)
+
+    def shutdown_servers(self):
+        return self._run_request("shutdown_servers")
+
+    def register_ordering(self, server_address, ordering):
+        return self._run_request("register_ordering", server_address, ordering)
+
+    def db_checkin(self, server_address):
+        event = self._run_request("db_checkin", server_address)
+        if event:
+            event.wait()
+
+    def db_checkout(self, server_address):
+        return self._run_request("db_checkout", server_address)
+
+    def quick_db_checkout(self, ordering):
+        return self._run_request("quick_db_checkout", ordering)
+
+    def cancel_db_checkout(self, server_address):
+        return self._run_request("cancel_db_checkout", server_address)
+
+
+class _DBWorker:
+    _CLOSE = "close"
+
+    def __init__(self, db_url, upgrade, memory, create=True):
+        self._db_url = db_url
+        self._upgrade = upgrade
+        self._memory = memory
+        self._create = create
+        self._db_map = None
+        self._lock = threading.Lock()
+        self._in_queue = Queue()
+        self._out_queue = Queue()
+        self._thread = threading.Thread(target=self._do_work)
+        self._thread.start()
+        error = self._out_queue.get()
+        if isinstance(error, Exception):
+            raise error
+
+    @property
+    def db_url(self):
+        return str(self._db_map.db_url)
+
+    def shutdown(self):
+        self._in_queue.put(self._CLOSE)
+        self._thread.join()
+
+    def _do_work(self):
+        try:
+            self._db_map = DatabaseMapping(
+                self._db_url, upgrade=self._upgrade, memory=self._memory, create=self._create
+            )
+            self._out_queue.put(None)
+        except Exception as error:  # pylint: disable=broad-except
+            self._out_queue.put(error)
+            return
+        while True:
+            input_ = self._in_queue.get()
+            if input_ == self._CLOSE:
+                self._db_map.connection.close()
+                break
+            request, args, kwargs = input_
+            handler = {
+                "query": self._do_query,
+                "filtered_query": self._do_filtered_query,
+                "import_data": self._do_import_data,
+                "export_data": self._do_export_data,
+                "call_method": self._do_call_method,
+                "apply_filters": self._do_apply_filters,
+                "clear_filters": self._do_clear_filters,
+            }[request]
+            result = handler(*args, **kwargs)
+            self._out_queue.put(result)
+
+    def run(self, request, args, kwargs):
+        with self._lock:
+            self._in_queue.put((request, args, kwargs))
+            return self._out_queue.get()
+
+    def _do_query(self, *args):
+        result = {}
+        for sq_name in args:
+            sq = getattr(self._db_map, sq_name, None)
+            if sq is None:
+                continue
+            result[sq_name] = [x._asdict() for x in self._db_map.query(sq)]
+        return dict(result=result)
+
+    def _do_filtered_query(self, **kwargs):
+        result = {}
+        for sq_name, filters in kwargs.items():
+            sq = getattr(self._db_map, sq_name, None)
+            if sq is None:
+                continue
+            qry = self._db_map.query(sq)
+            for field, value in filters.items():
+                qry = qry.filter_by(**{field: value})
+            result[sq_name] = [x._asdict() for x in qry]
+        return dict(result=result)
+
+    def _do_import_data(self, data, comment):
+        count, errors = import_data(self._db_map, unparse_value=_unparse_value, **data)
+        if count and comment:
+            try:
+                self._db_map.commit_session(comment)
+            except DBAPIError:
+                self._db_map.rollback_session()
+        return dict(result=(count, errors))
+
+    def _do_export_data(self, **kwargs):
+        return dict(result=export_data(self._db_map, parse_value=_parse_value, **kwargs))
+
+    def _do_call_method(self, method_name, *args, **kwargs):
+        method = getattr(self._db_map, method_name)
+        result = method(*args, **kwargs)
+        return dict(result=result)
+
+    def _do_clear_filters(self):
+        self._db_map.restore_entity_sq_maker()
+        self._db_map.restore_entity_class_sq_maker()
+        self._db_map.restore_parameter_definition_sq_maker()
+        self._db_map.restore_parameter_value_sq_maker()
+        self._db_map.restore_alternative_sq_maker()
+        self._db_map.restore_scenario_sq_maker()
+        self._db_map.restore_scenario_alternative_sq_maker()
+        return dict(result=True)
+
+    def _do_apply_filters(self, configs):
+        try:
+            apply_filter_stack(self._db_map, configs)
+            return dict(result=True)
+        except Exception as error:  # pylint: disable=broad-except
+            return dict(error=str(error))
+
+
+class _DBManager:
+    def __init__(self):
+        self._workers = {}
+
+    def open_db_map(self, server_address, db_url, upgrade, memory):
+        worker = self._workers.get(server_address)
+        if worker is None:
+            try:
+                worker = self._workers[server_address] = _DBWorker(db_url, upgrade, memory)
+            except Exception as error:  # pylint: disable=broad-except
+                return dict(error=str(error))
+        return dict(result=True)
+
+    def close_db_map(self, server_address):
+        worker = self._workers.pop(server_address, None)
+        if worker is None:
+            return dict(result=False)
+        worker.shutdown()
+        return dict(result=True)
+
+    def get_db_url(self, server_address):
+        worker = self._workers.get(server_address)
+        if worker is not None:
+            return worker.db_url
+
+    def _run_request(self, server_address, request, args=(), kwargs=None, create=True):
+        if kwargs is None:
+            kwargs = {}
+        worker = self._workers.get(server_address)
+        if worker is not None:
+            return worker.run(request, args, kwargs)
+
+    def query(self, server_address, *args):
+        return self._run_request(server_address, "query", args=args)
+
+    def filtered_query(self, server_address, **kwargs):
+        return self._run_request(server_address, "filtered_query", kwargs=kwargs)
+
+    def import_data(self, server_address, data, comment):
+        return self._run_request(server_address, "import_data", args=(data, comment))
+
+    def export_data(self, server_address, **kwargs):
+        return self._run_request(server_address, "export_data", kwargs=kwargs)
+
+    def call_method(self, server_address, method_name, *args, **kwargs):
+        return self._run_request(server_address, "call_method", args=(method_name, *args), kwargs=kwargs)
+
+    def apply_filters(self, server_address, configs):
+        return self._run_request(server_address, "apply_filters", args=(configs,))
+
+    def clear_filters(self, server_address):
+        return self._run_request(server_address, "clear_filters")
+
+
+_db_manager = _DBManager()
+
+
+class HandleDBMixin:
+    def get_db_url(self):
+        """
+        Returns:
+            str: The underlying db url
+        """
+        return _db_manager.get_db_url(self.server_address)
+
+    def query(self, *args):
+        """
+        Runs queries.
+
+        Returns:
+            dict: where result is a dict from subquery name to a list of items from thay subquery, if successful.
+        """
+        return _db_manager.query(self.server_address, *args)
+
+    def filtered_query(self, **kwargs):
+        """
+        Runs queries with filters.
+
+        Returns:
+            dict: where result is a dict from subquery name to a list of items from thay subquery, if successful.
+        """
+        return _db_manager.filtered_query(self.server_address, **kwargs)
+
+    def import_data(self, data, comment):
+        """Imports data and commit.
+
+        Args:
+            data (dict)
+            comment (str)
+        Returns:
+            dict: where result is a list of import errors, if successful.
+        """
+        return _db_manager.import_data(self.server_address, data, comment)
+
+    def export_data(self, **kwargs):
+        """Exports data.
+
+        Returns:
+            dict: where result is the data exported from the db
+        """
+        return _db_manager.export_data(self.server_address, **kwargs)
+
+    def call_method(self, method_name, *args, **kwargs):
+        """Calls a method from the DatabaseMapping class.
+
+        Args:
+            method_name (str): the method name
+            args: positional arguments passed to the method call
+            kwargs: keyword arguments passed to the method call
+
+        Returns:
+            dict: where result is the return value of the method
+        """
+        return _db_manager.call_method(self.server_address, method_name, *args, **kwargs)
+
+    def apply_filters(self, filters):
+        configs = [
+            {"scenario": scenario_filter_config, "tool": tool_filter_config, "alternatives": alternative_filter_config}[
+                key
+            ](value)
+            for key, value in filters.items()
+        ]
+        return _db_manager.apply_filters(self.server_address, configs)
+
+    def clear_filters(self):
+        return _db_manager.clear_filters(self.server_address)
+
+    def db_checkin(self):
+        _ManagerRequestHandler(self.server_manager_queue).db_checkin(self.server_address)
+        return dict(result=True)
+
+    def db_checkout(self):
+        _ManagerRequestHandler(self.server_manager_queue).db_checkout(self.server_address)
+        return dict(result=True)
+
+    def cancel_db_checkout(self):
+        _ManagerRequestHandler(self.server_manager_queue).cancel_db_checkout(self.server_address)
+        return dict(result=True)
+
+    def open_db_map(self, db_url, upgrade, memory):
+        return _db_manager.open_db_map(self.server_address, db_url, upgrade, memory)
+
+    def close_db_map(self):
+        return _db_manager.close_db_map(self.server_address)
+
+    def _get_response(self, request):
+        request, *extras = decode(request)
+        # NOTE: Clients should always send requests "get_api_version" and "get_db_url" in a format that is compatible
+        # with the legacy server -- to (based on the format of the answer) determine that it needs to be updated.
+        # That's why we don't expand the extras so far.
+        response = {"get_api_version": spinedb_api_version, "get_db_url": self.get_db_url()}.get(request)
+        if response is not None:
+            return response
+        try:
+            args, kwargs, client_version = extras
+        except ValueError:
+            client_version = 0
+        if client_version < _required_client_version:
+            return dict(error=1, result=_required_client_version)
+        handler = {
+            "query": self.query,
+            "filtered_query": self.filtered_query,
+            "import_data": self.import_data,
+            "export_data": self.export_data,
+            "call_method": self.call_method,
+            "apply_filters": self.apply_filters,
+            "clear_filters": self.clear_filters,
+            "db_checkin": self.db_checkin,
+            "db_checkout": self.db_checkout,
+            "cancel_db_checkout": self.cancel_db_checkout,
+            "open_db_map": self.open_db_map,
+            "close_db_map": self.close_db_map,
+        }.get(request)
+        if handler is None:
+            return dict(error=f"invalid request '{request}'")
+        try:
+            return handler(*args, **kwargs)
+        except Exception:  # pylint: disable=broad-except
+            return dict(error=traceback.format_exc())
+
+    def handle_request(self, request):
+        response = self._get_response(request)
+        return encode(response)
+
+
+class DBHandler(HandleDBMixin):
+    def __init__(self, db_url, upgrade=False, memory=False):
+        self.server_address = uuid.uuid4().hex
+        error = _db_manager.open_db_map(self.server_address, db_url, upgrade, memory).get("error")
+        if error:
+            raise RuntimeError(error)
+        atexit.register(self.close)
+
+    def close(self):
+        _db_manager.close_db_map(self.server_address)
+
+
+class DBRequestHandler(ReceiveAllMixing, HandleDBMixin, socketserver.BaseRequestHandler):
+    """
+    The request handler class for our server.
+    """
+
+    @property
+    def server_address(self):
+        return self.server.server_address
+
+    @property
+    def server_manager_queue(self):
+        return self.server.manager_queue
+
+    def handle(self):
+        request = self._recvall()
+        response = self.handle_request(request)
+        self.request.sendall(response + bytes(self._EOT, self._ENCODING))
+
+
+def quick_db_checkout(server_manager_queue, ordering):
+    _ManagerRequestHandler(server_manager_queue).quick_db_checkout(ordering)
+
+
+def start_spine_db_server(server_manager_queue, db_url, upgrade=False, memory=False, ordering=None):
+    """
+    Args:
+        db_url (str): Spine db url
+        upgrade (bool): Whether to upgrade db or not
+        memory (bool): Whether to use an in-memory database together with a persistent connection to it
+
+    Returns:
+        tuple: server address (e.g. (127.0.0.1, 54321))
+    """
+    handler = _ManagerRequestHandler(server_manager_queue)
+    server_address = handler.start_server(db_url, upgrade, memory, ordering)
+    return server_address
+
+
+def shutdown_spine_db_server(server_manager_queue, server_address):
+    handler = _ManagerRequestHandler(server_manager_queue)
+    handler.shutdown_server(server_address)
+
+
+@contextmanager
+def closing_spine_db_server(server_manager_queue, db_url, upgrade=False, memory=False, ordering=None):
+    server_address = start_spine_db_server(server_manager_queue, db_url, memory=memory, ordering=ordering)
+    host, port = server_address
+    try:
+        yield urlunsplit(("http", f"{host}:{port}", "", "", ""))
+    finally:
+        shutdown_spine_db_server(server_manager_queue, server_address)
+
+
+@contextmanager
+def db_server_manager():
+    mngr = _DBServerManager()
+    try:
+        yield mngr.queue
+    finally:
+        mngr.shutdown()
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/__init__.py` & `spinedb_api-0.30.4/tests/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Init file for spine_io package. Intentionally empty.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests package for :mod:`spinedb_api`.
+
+"""
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/exporters/__init__.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/exporters/__init__.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Init file for spine_io.exporters package. Intentionally empty.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Init file for spine_io.exporters package. Intentionally empty.
+
+"""
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/exporters/csv_writer.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/exporters/csv_writer.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,68 +1,68 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Module contains a .csv writer implementation.
-
-"""
-import csv
-import os
-import os.path
-from .writer import Writer
-
-
-class CsvWriter(Writer):
-    def __init__(self, path, backup_file_name):
-        """
-        Args:
-            path (Path or str): path to output directory
-            backup_file_name (str): output file name if no table name is provided by the mappings
-        """
-        super().__init__()
-        self._path = path
-        self._default_table_name = backup_file_name
-        self._file = None
-        self._out = None
-        self._file_name = None
-        self._finished_files = set()
-
-    def finish_table(self):
-        """See base class."""
-        self._file.close()
-        self._finished_files.add(self._file_name)
-        self._file_name = None
-        self._file = None
-        self._out = None
-
-    def output_files(self):
-        """Returns absolute paths to files that have been written.
-
-        Returns:
-            set of str: file paths
-        """
-        return self._finished_files
-
-    def start_table(self, table_name, title_key):
-        """See base class."""
-        if table_name is None:
-            table_name = self._default_table_name
-        else:
-            table_name = table_name + ".csv"
-        self._file_name = os.path.join(self._path, table_name)
-        if self._file_name not in self._finished_files and os.path.exists(self._file_name):
-            os.remove(self._file_name)
-        self._file = open(self._file_name, "a", newline="")
-        self._out = csv.writer(self._file)
-        return True
-
-    def write_row(self, row):
-        """See base class."""
-        self._out.writerow(row)
-        return True
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Module contains a .csv writer implementation.
+
+"""
+import csv
+import os
+import os.path
+from .writer import Writer
+
+
+class CsvWriter(Writer):
+    def __init__(self, path, backup_file_name):
+        """
+        Args:
+            path (Path or str): path to output directory
+            backup_file_name (str): output file name if no table name is provided by the mappings
+        """
+        super().__init__()
+        self._path = path
+        self._default_table_name = backup_file_name
+        self._file = None
+        self._out = None
+        self._file_name = None
+        self._finished_files = set()
+
+    def finish_table(self):
+        """See base class."""
+        self._file.close()
+        self._finished_files.add(self._file_name)
+        self._file_name = None
+        self._file = None
+        self._out = None
+
+    def output_files(self):
+        """Returns absolute paths to files that have been written.
+
+        Returns:
+            set of str: file paths
+        """
+        return self._finished_files
+
+    def start_table(self, table_name, title_key):
+        """See base class."""
+        if table_name is None:
+            table_name = self._default_table_name
+        else:
+            table_name = table_name + ".csv"
+        self._file_name = os.path.join(self._path, table_name)
+        if self._file_name not in self._finished_files and os.path.exists(self._file_name):
+            os.remove(self._file_name)
+        self._file = open(self._file_name, "a", newline="")
+        self._out = csv.writer(self._file)
+        return True
+
+    def write_row(self, row):
+        """See base class."""
+        self._out.writerow(row)
+        return True
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/exporters/excel.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/exporters/excel.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,269 +1,269 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Framework for exporting a database to Excel file.
-
-"""
-from spinedb_api.export_mapping.group_functions import GroupOneOrNone
-from spinedb_api.export_mapping.export_mapping import (
-    Position,
-    AlternativeMapping,
-    AlternativeDescriptionMapping,
-    ObjectClassMapping,
-    ObjectGroupMapping,
-    ObjectMapping,
-    FixedValueMapping,
-    ScenarioMapping,
-    ScenarioAlternativeMapping,
-    ScenarioBeforeAlternativeMapping,
-    ScenarioDescriptionMapping,
-    ParameterDefinitionMapping,
-    ParameterValueIndexMapping,
-    ParameterValueTypeMapping,
-    ParameterValueMapping,
-    ExpandedParameterValueMapping,
-    RelationshipClassMapping,
-    RelationshipMapping,
-    RelationshipObjectMapping,
-)
-from ...parameter_value import from_database_to_dimension_count
-from .excel_writer import ExcelWriter
-from .writer import write
-
-# FIXME: Use multiple sheets if data doesn't fit
-
-
-class ExcelWriterWithPreamble(ExcelWriter):
-    _preamble = {}
-
-    def start_table(self, table_name, title_key):
-        """See base class."""
-        if super().start_table(table_name, title_key):
-            self._preamble = self._make_preamble(table_name, title_key)
-            return True
-        return False
-
-    @staticmethod
-    def _make_preamble(table_name, title_key):
-        if table_name in ("alternative", "scenario", "scenario_alternative"):
-            return {"sheet_type": table_name}
-        class_name = title_key.get("object_class_name") or title_key.get("relationship_class_name")
-        if table_name.endswith(",group"):
-            return {"sheet_type": "object_group", "class_name": class_name}
-        object_class_id_list = title_key.get("object_class_id_list")
-        if object_class_id_list is None:
-            entity_type = "object"
-            entity_dim_count = 1
-        else:
-            entity_type = "relationship"
-            entity_dim_count = len(object_class_id_list.split(","))
-        preamble = {
-            "sheet_type": "entity",
-            "entity_type": entity_type,
-            "class_name": class_name,
-            "entity_dim_count": entity_dim_count,
-        }
-        td = title_key.get("type_and_dimensions")
-        if td is not None:
-            preamble["value_type"] = td[0]
-            preamble["index_dim_count"] = td[1]
-        return preamble
-
-    def _set_current_sheet(self):
-        super()._set_current_sheet()
-        if not self._preamble:
-            return
-        for row in self._preamble.items():
-            self._current_sheet.append(row)
-        self._current_sheet.append([])
-
-
-def export_spine_database_to_xlsx(db_map, filepath):
-    """Writes data from a Spine database into an excel file.
-
-    Args:
-        db_map (spinedb_api.DatabaseMapping): database mapping.
-        filepath (str): destination path.
-    """
-    mappings = [_make_alternative_mapping(), _make_scenario_mapping(), _make_scenario_alternative_mapping()]
-    mappings.extend(_make_object_group_mappings(db_map))
-    mappings.extend(_make_parameter_value_mappings(db_map))
-    writer = ExcelWriterWithPreamble(filepath)
-    write(db_map, writer, *mappings, empty_data_header=False, group_fns=GroupOneOrNone.NAME)
-
-
-def _make_alternative_mapping():
-    root_mapping = FixedValueMapping(Position.table_name, value="alternative")
-    alternative_mapping = root_mapping.child = AlternativeMapping(0, header="alternative")
-    alternative_mapping.child = AlternativeDescriptionMapping(1, header="description")
-    return root_mapping
-
-
-def _make_scenario_mapping():
-    root_mapping = FixedValueMapping(Position.table_name, value="scenario")
-    scenario_mapping = root_mapping.child = ScenarioMapping(0, header="scenario")
-    scenario_mapping.child = ScenarioDescriptionMapping(1, header="description")
-    return root_mapping
-
-
-def _make_scenario_alternative_mapping():
-    root_mapping = FixedValueMapping(Position.table_name, value="scenario_alternative")
-    scenario_mapping = root_mapping.child = ScenarioMapping(0, header="scenario")
-    alternative_mapping = scenario_mapping.child = ScenarioAlternativeMapping(1, header="alternative")
-    alternative_mapping.child = ScenarioBeforeAlternativeMapping(2, header="before alternative")
-    return root_mapping
-
-
-def _make_object_group_mappings(db_map):
-    for obj_grp in db_map.query(db_map.ext_entity_group_sq).group_by(db_map.ext_entity_group_sq.c.class_name):
-        root_mapping = ObjectClassMapping(Position.table_name, filter_re=obj_grp.class_name)
-        group_mapping = root_mapping.child = FixedValueMapping(Position.table_name, value="group")
-        object_mapping = group_mapping.child = ObjectMapping(1, header="member")
-        object_mapping.child = ObjectGroupMapping(0, header="group")
-        yield root_mapping
-
-
-def _make_scalar_parameter_value_mapping(alt_pos=1):
-    alternative_mapping = AlternativeMapping(alt_pos, header="alternative")
-    param_def_mapping = alternative_mapping.child = ParameterDefinitionMapping(-1)
-    type_mapping = param_def_mapping.child = ParameterValueTypeMapping(Position.table_name, filter_re="single_value")
-    type_mapping.child = ParameterValueMapping(420)
-    return alternative_mapping
-
-
-def _make_indexed_parameter_value_mapping(alt_pos=-2, filter_re="array|time_pattern|time_series", dim_count=1):
-    alternative_mapping = AlternativeMapping(alt_pos, header="alternative")
-    param_def_mapping = alternative_mapping.child = ParameterDefinitionMapping(alt_pos - 1)
-    type_mapping = param_def_mapping.child = ParameterValueTypeMapping(Position.table_name, filter_re=filter_re)
-    parent_mapping = type_mapping
-    for k in range(dim_count):
-        index_mapping = parent_mapping.child = ParameterValueIndexMapping(k, header="index")
-        index_mapping.set_ignorable(True)
-        parent_mapping = index_mapping
-    parent_mapping.child = ExpandedParameterValueMapping(420)
-    return alternative_mapping
-
-
-def _make_object_mapping(object_class_name, pivoted=False):
-    root_mapping = ObjectClassMapping(Position.table_name, filter_re=f"^{object_class_name}$")
-    pos = 0 if not pivoted else -1
-    root_mapping.child = ObjectMapping(pos, header=object_class_name)
-    return root_mapping
-
-
-def _make_object_scalar_parameter_value_mapping(object_class_name):
-    root_mapping = _make_object_mapping(object_class_name)
-    object_mapping = root_mapping.child
-    object_mapping.child = _make_scalar_parameter_value_mapping(alt_pos=1)
-    return root_mapping
-
-
-def _make_object_indexed_parameter_value_mapping(object_class_name):
-    root_mapping = _make_object_mapping(object_class_name, pivoted=True)
-    object_mapping = root_mapping.child
-    object_mapping.child = _make_indexed_parameter_value_mapping(alt_pos=-2)
-    return root_mapping
-
-
-def _make_object_map_parameter_value_mapping(object_class_name, dim_count):
-    root_mapping = _make_object_mapping(object_class_name, pivoted=True)
-    object_mapping = root_mapping.child
-    filter_re = f"{dim_count}d_map"
-    object_mapping.child = _make_indexed_parameter_value_mapping(alt_pos=-2, filter_re=filter_re, dim_count=dim_count)
-    return root_mapping
-
-
-def _make_relationship_mapping(relationship_class_name, object_class_name_list, pivoted=False):
-    root_mapping = RelationshipClassMapping(Position.table_name, filter_re=f"^{relationship_class_name}$")
-    relationship_mapping = root_mapping.child = RelationshipMapping(Position.hidden)
-    parent_mapping = relationship_mapping
-    for d, class_name in enumerate(object_class_name_list):
-        if pivoted:
-            d = -(d + 1)
-        object_mapping = parent_mapping.child = RelationshipObjectMapping(d, header=class_name)
-        parent_mapping = object_mapping
-    return root_mapping
-
-
-def _make_relationship_scalar_parameter_value_mapping(relationship_class_name, object_class_name_list):
-    root_mapping = _make_relationship_mapping(relationship_class_name, object_class_name_list)
-    parent_mapping = root_mapping.flatten()[-1]
-    d = len(object_class_name_list)
-    parent_mapping.child = _make_scalar_parameter_value_mapping(alt_pos=d)
-    return root_mapping
-
-
-def _make_relationship_indexed_parameter_value_mapping(relationship_class_name, object_class_name_list):
-    root_mapping = _make_relationship_mapping(relationship_class_name, object_class_name_list, pivoted=True)
-    parent_mapping = root_mapping.flatten()[-1]
-    d = len(object_class_name_list) + 1
-    parent_mapping.child = _make_indexed_parameter_value_mapping(alt_pos=-d)
-    return root_mapping
-
-
-def _make_relationship_map_parameter_value_mapping(relationship_class_name, object_class_name_list, dim_count):
-    root_mapping = _make_relationship_mapping(relationship_class_name, object_class_name_list, pivoted=True)
-    parent_mapping = root_mapping.flatten()[-1]
-    d = len(object_class_name_list) + 1
-    filter_re = f"{dim_count}d_map"
-    parent_mapping.child = _make_indexed_parameter_value_mapping(alt_pos=-d, filter_re=filter_re, dim_count=dim_count)
-    return root_mapping
-
-
-def _make_parameter_value_mappings(db_map):
-    def db_value_type(type_):
-        return type_ if type_ in ("map", "time_series", "time_pattern", "array") else "single_value"
-
-    object_class_names = set()
-    relationship_class_names = set()
-    object_class_names_per_value_type = {}
-    relationship_classes_per_value_type = {}
-    for pval in db_map.query(db_map.object_parameter_value_sq):
-        key = (db_value_type(pval.type), from_database_to_dimension_count(pval.value, pval.type))
-        object_class_names_per_value_type.setdefault(key, set()).add(pval.object_class_name)
-        object_class_names.add(pval.object_class_name)
-    for pval in db_map.query(db_map.relationship_parameter_value_sq):
-        key = (db_value_type(pval.type), from_database_to_dimension_count(pval.value, pval.type))
-        object_class_name_list = tuple(pval.object_class_name_list.split(","))
-        relationship_classes_per_value_type.setdefault(key, set()).add(
-            (pval.relationship_class_name, object_class_name_list)
-        )
-        relationship_class_names.add(pval.relationship_class_name)
-    for object_class in db_map.query(db_map.object_class_sq):
-        if object_class.name in object_class_names:
-            continue
-        yield _make_object_mapping(object_class.name)
-    for relationship_class in db_map.query(db_map.wide_relationship_class_sq):
-        if relationship_class.name in relationship_class_names:
-            continue
-        object_class_name_list = tuple(relationship_class.object_class_name_list.split(","))
-        yield _make_relationship_mapping(relationship_class.name, object_class_name_list)
-    for (type_, dimension_count), object_class_names in object_class_names_per_value_type.items():
-        if type_ == "single_value":
-            for object_class_name in object_class_names:
-                yield _make_object_scalar_parameter_value_mapping(object_class_name)
-        elif type_ == "map":
-            for object_class_name in object_class_names:
-                yield _make_object_map_parameter_value_mapping(object_class_name, dimension_count)
-        else:
-            for object_class_name in object_class_names:
-                yield _make_object_indexed_parameter_value_mapping(object_class_name)
-    for (type_, dimension_count), relationship_classes in relationship_classes_per_value_type.items():
-        if type_ == "single_value":
-            for relationship_class in relationship_classes:
-                yield _make_relationship_scalar_parameter_value_mapping(*relationship_class)
-        elif type_ == "map":
-            for relationship_class in relationship_classes:
-                yield _make_relationship_map_parameter_value_mapping(*relationship_class, dimension_count)
-        else:
-            for relationship_class in relationship_classes:
-                yield _make_relationship_indexed_parameter_value_mapping(*relationship_class)
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Framework for exporting a database to Excel file.
+
+"""
+from spinedb_api.export_mapping.group_functions import GroupOneOrNone
+from spinedb_api.export_mapping.export_mapping import (
+    Position,
+    AlternativeMapping,
+    AlternativeDescriptionMapping,
+    ObjectClassMapping,
+    ObjectGroupMapping,
+    ObjectMapping,
+    FixedValueMapping,
+    ScenarioMapping,
+    ScenarioAlternativeMapping,
+    ScenarioBeforeAlternativeMapping,
+    ScenarioDescriptionMapping,
+    ParameterDefinitionMapping,
+    ParameterValueIndexMapping,
+    ParameterValueTypeMapping,
+    ParameterValueMapping,
+    ExpandedParameterValueMapping,
+    RelationshipClassMapping,
+    RelationshipMapping,
+    RelationshipObjectMapping,
+)
+from ...parameter_value import from_database_to_dimension_count
+from .excel_writer import ExcelWriter
+from .writer import write
+
+# FIXME: Use multiple sheets if data doesn't fit
+
+
+class ExcelWriterWithPreamble(ExcelWriter):
+    _preamble = {}
+
+    def start_table(self, table_name, title_key):
+        """See base class."""
+        if super().start_table(table_name, title_key):
+            self._preamble = self._make_preamble(table_name, title_key)
+            return True
+        return False
+
+    @staticmethod
+    def _make_preamble(table_name, title_key):
+        if table_name in ("alternative", "scenario", "scenario_alternative"):
+            return {"sheet_type": table_name}
+        class_name = title_key.get("object_class_name") or title_key.get("relationship_class_name")
+        if table_name.endswith(",group"):
+            return {"sheet_type": "object_group", "class_name": class_name}
+        object_class_id_list = title_key.get("object_class_id_list")
+        if object_class_id_list is None:
+            entity_type = "object"
+            entity_dim_count = 1
+        else:
+            entity_type = "relationship"
+            entity_dim_count = len(object_class_id_list.split(","))
+        preamble = {
+            "sheet_type": "entity",
+            "entity_type": entity_type,
+            "class_name": class_name,
+            "entity_dim_count": entity_dim_count,
+        }
+        td = title_key.get("type_and_dimensions")
+        if td is not None:
+            preamble["value_type"] = td[0]
+            preamble["index_dim_count"] = td[1]
+        return preamble
+
+    def _set_current_sheet(self):
+        super()._set_current_sheet()
+        if not self._preamble:
+            return
+        for row in self._preamble.items():
+            self._current_sheet.append(row)
+        self._current_sheet.append([])
+
+
+def export_spine_database_to_xlsx(db_map, filepath):
+    """Writes data from a Spine database into an excel file.
+
+    Args:
+        db_map (spinedb_api.DatabaseMapping): database mapping.
+        filepath (str): destination path.
+    """
+    mappings = [_make_alternative_mapping(), _make_scenario_mapping(), _make_scenario_alternative_mapping()]
+    mappings.extend(_make_object_group_mappings(db_map))
+    mappings.extend(_make_parameter_value_mappings(db_map))
+    writer = ExcelWriterWithPreamble(filepath)
+    write(db_map, writer, *mappings, empty_data_header=False, group_fns=GroupOneOrNone.NAME)
+
+
+def _make_alternative_mapping():
+    root_mapping = FixedValueMapping(Position.table_name, value="alternative")
+    alternative_mapping = root_mapping.child = AlternativeMapping(0, header="alternative")
+    alternative_mapping.child = AlternativeDescriptionMapping(1, header="description")
+    return root_mapping
+
+
+def _make_scenario_mapping():
+    root_mapping = FixedValueMapping(Position.table_name, value="scenario")
+    scenario_mapping = root_mapping.child = ScenarioMapping(0, header="scenario")
+    scenario_mapping.child = ScenarioDescriptionMapping(1, header="description")
+    return root_mapping
+
+
+def _make_scenario_alternative_mapping():
+    root_mapping = FixedValueMapping(Position.table_name, value="scenario_alternative")
+    scenario_mapping = root_mapping.child = ScenarioMapping(0, header="scenario")
+    alternative_mapping = scenario_mapping.child = ScenarioAlternativeMapping(1, header="alternative")
+    alternative_mapping.child = ScenarioBeforeAlternativeMapping(2, header="before alternative")
+    return root_mapping
+
+
+def _make_object_group_mappings(db_map):
+    for obj_grp in db_map.query(db_map.ext_entity_group_sq).group_by(db_map.ext_entity_group_sq.c.class_name):
+        root_mapping = ObjectClassMapping(Position.table_name, filter_re=obj_grp.class_name)
+        group_mapping = root_mapping.child = FixedValueMapping(Position.table_name, value="group")
+        object_mapping = group_mapping.child = ObjectMapping(1, header="member")
+        object_mapping.child = ObjectGroupMapping(0, header="group")
+        yield root_mapping
+
+
+def _make_scalar_parameter_value_mapping(alt_pos=1):
+    alternative_mapping = AlternativeMapping(alt_pos, header="alternative")
+    param_def_mapping = alternative_mapping.child = ParameterDefinitionMapping(-1)
+    type_mapping = param_def_mapping.child = ParameterValueTypeMapping(Position.table_name, filter_re="single_value")
+    type_mapping.child = ParameterValueMapping(420)
+    return alternative_mapping
+
+
+def _make_indexed_parameter_value_mapping(alt_pos=-2, filter_re="array|time_pattern|time_series", dim_count=1):
+    alternative_mapping = AlternativeMapping(alt_pos, header="alternative")
+    param_def_mapping = alternative_mapping.child = ParameterDefinitionMapping(alt_pos - 1)
+    type_mapping = param_def_mapping.child = ParameterValueTypeMapping(Position.table_name, filter_re=filter_re)
+    parent_mapping = type_mapping
+    for k in range(dim_count):
+        index_mapping = parent_mapping.child = ParameterValueIndexMapping(k, header="index")
+        index_mapping.set_ignorable(True)
+        parent_mapping = index_mapping
+    parent_mapping.child = ExpandedParameterValueMapping(420)
+    return alternative_mapping
+
+
+def _make_object_mapping(object_class_name, pivoted=False):
+    root_mapping = ObjectClassMapping(Position.table_name, filter_re=f"^{object_class_name}$")
+    pos = 0 if not pivoted else -1
+    root_mapping.child = ObjectMapping(pos, header=object_class_name)
+    return root_mapping
+
+
+def _make_object_scalar_parameter_value_mapping(object_class_name):
+    root_mapping = _make_object_mapping(object_class_name)
+    object_mapping = root_mapping.child
+    object_mapping.child = _make_scalar_parameter_value_mapping(alt_pos=1)
+    return root_mapping
+
+
+def _make_object_indexed_parameter_value_mapping(object_class_name):
+    root_mapping = _make_object_mapping(object_class_name, pivoted=True)
+    object_mapping = root_mapping.child
+    object_mapping.child = _make_indexed_parameter_value_mapping(alt_pos=-2)
+    return root_mapping
+
+
+def _make_object_map_parameter_value_mapping(object_class_name, dim_count):
+    root_mapping = _make_object_mapping(object_class_name, pivoted=True)
+    object_mapping = root_mapping.child
+    filter_re = f"{dim_count}d_map"
+    object_mapping.child = _make_indexed_parameter_value_mapping(alt_pos=-2, filter_re=filter_re, dim_count=dim_count)
+    return root_mapping
+
+
+def _make_relationship_mapping(relationship_class_name, object_class_name_list, pivoted=False):
+    root_mapping = RelationshipClassMapping(Position.table_name, filter_re=f"^{relationship_class_name}$")
+    relationship_mapping = root_mapping.child = RelationshipMapping(Position.hidden)
+    parent_mapping = relationship_mapping
+    for d, class_name in enumerate(object_class_name_list):
+        if pivoted:
+            d = -(d + 1)
+        object_mapping = parent_mapping.child = RelationshipObjectMapping(d, header=class_name)
+        parent_mapping = object_mapping
+    return root_mapping
+
+
+def _make_relationship_scalar_parameter_value_mapping(relationship_class_name, object_class_name_list):
+    root_mapping = _make_relationship_mapping(relationship_class_name, object_class_name_list)
+    parent_mapping = root_mapping.flatten()[-1]
+    d = len(object_class_name_list)
+    parent_mapping.child = _make_scalar_parameter_value_mapping(alt_pos=d)
+    return root_mapping
+
+
+def _make_relationship_indexed_parameter_value_mapping(relationship_class_name, object_class_name_list):
+    root_mapping = _make_relationship_mapping(relationship_class_name, object_class_name_list, pivoted=True)
+    parent_mapping = root_mapping.flatten()[-1]
+    d = len(object_class_name_list) + 1
+    parent_mapping.child = _make_indexed_parameter_value_mapping(alt_pos=-d)
+    return root_mapping
+
+
+def _make_relationship_map_parameter_value_mapping(relationship_class_name, object_class_name_list, dim_count):
+    root_mapping = _make_relationship_mapping(relationship_class_name, object_class_name_list, pivoted=True)
+    parent_mapping = root_mapping.flatten()[-1]
+    d = len(object_class_name_list) + 1
+    filter_re = f"{dim_count}d_map"
+    parent_mapping.child = _make_indexed_parameter_value_mapping(alt_pos=-d, filter_re=filter_re, dim_count=dim_count)
+    return root_mapping
+
+
+def _make_parameter_value_mappings(db_map):
+    def db_value_type(type_):
+        return type_ if type_ in ("map", "time_series", "time_pattern", "array") else "single_value"
+
+    object_class_names = set()
+    relationship_class_names = set()
+    object_class_names_per_value_type = {}
+    relationship_classes_per_value_type = {}
+    for pval in db_map.query(db_map.object_parameter_value_sq):
+        key = (db_value_type(pval.type), from_database_to_dimension_count(pval.value, pval.type))
+        object_class_names_per_value_type.setdefault(key, set()).add(pval.object_class_name)
+        object_class_names.add(pval.object_class_name)
+    for pval in db_map.query(db_map.relationship_parameter_value_sq):
+        key = (db_value_type(pval.type), from_database_to_dimension_count(pval.value, pval.type))
+        object_class_name_list = tuple(pval.object_class_name_list.split(","))
+        relationship_classes_per_value_type.setdefault(key, set()).add(
+            (pval.relationship_class_name, object_class_name_list)
+        )
+        relationship_class_names.add(pval.relationship_class_name)
+    for object_class in db_map.query(db_map.object_class_sq):
+        if object_class.name in object_class_names:
+            continue
+        yield _make_object_mapping(object_class.name)
+    for relationship_class in db_map.query(db_map.wide_relationship_class_sq):
+        if relationship_class.name in relationship_class_names:
+            continue
+        object_class_name_list = tuple(relationship_class.object_class_name_list.split(","))
+        yield _make_relationship_mapping(relationship_class.name, object_class_name_list)
+    for (type_, dimension_count), object_class_names in object_class_names_per_value_type.items():
+        if type_ == "single_value":
+            for object_class_name in object_class_names:
+                yield _make_object_scalar_parameter_value_mapping(object_class_name)
+        elif type_ == "map":
+            for object_class_name in object_class_names:
+                yield _make_object_map_parameter_value_mapping(object_class_name, dimension_count)
+        else:
+            for object_class_name in object_class_names:
+                yield _make_object_indexed_parameter_value_mapping(object_class_name)
+    for (type_, dimension_count), relationship_classes in relationship_classes_per_value_type.items():
+        if type_ == "single_value":
+            for relationship_class in relationship_classes:
+                yield _make_relationship_scalar_parameter_value_mapping(*relationship_class)
+        elif type_ == "map":
+            for relationship_class in relationship_classes:
+                yield _make_relationship_map_parameter_value_mapping(*relationship_class, dimension_count)
+        else:
+            for relationship_class in relationship_classes:
+                yield _make_relationship_indexed_parameter_value_mapping(*relationship_class)
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/exporters/excel_writer.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/exporters/excel_writer.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,115 +1,115 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-A writer for exporting Spine databases to Excel files.
-
-"""
-from pathlib import Path
-import re
-import numpy
-from openpyxl import load_workbook, Workbook
-from openpyxl.utils.exceptions import InvalidFileException
-from openpyxl.workbook.child import INVALID_TITLE_REGEX
-from .writer import Writer, WriterException
-
-
-class ExcelWriter(Writer):
-    def __init__(self, file_path):
-        """
-        Args:
-            file_path (str): path to output file
-        """
-        super().__init__()
-        self._file_path = file_path
-        self._workbook = None
-        self._current_sheet = None
-        self._removable_sheet_names = set()
-        self._next_table_name = None
-        self._default_sheet_title = None
-
-    def finish(self):
-        """See base class."""
-        if self._workbook is None:
-            return
-        for name in self._removable_sheet_names:
-            self._workbook.remove(self._workbook[name])
-        self._removable_sheet_names.clear()
-        if not self._workbook.worksheets:
-            self._workbook.create_sheet("Sheet1")
-        self._workbook.save(self._file_path)
-        self._workbook.close()
-        self._workbook = None
-
-    def finish_table(self):
-        """See base class."""
-        self._current_sheet = None
-
-    def start(self):
-        """See base class."""
-        if Path(self._file_path).exists():
-            try:
-                self._workbook = load_workbook(self._file_path)
-            except InvalidFileException as e:
-                raise WriterException(f"Cannot open Excel file: {e}")
-        else:
-            self._workbook = Workbook()
-            if not self._removable_sheet_names:
-                self._removable_sheet_names = set(self._workbook.sheetnames)
-
-    def start_table(self, table_name, title_key):
-        """See base class."""
-        self._next_table_name = re.sub(INVALID_TITLE_REGEX, "", table_name) if table_name is not None else table_name
-        return True
-
-    def _set_current_sheet(self):
-        """Gets an existing sheet from workbook or creates a new one if needed."""
-        if self._next_table_name is not None:
-            if self._next_table_name in self._workbook:
-                self._current_sheet = self._workbook[self._next_table_name]
-            else:
-                self._current_sheet = self._workbook.create_sheet(self._next_table_name)
-        else:
-            if self._default_sheet_title:
-                self._current_sheet = self._workbook[self._default_sheet_title]
-            else:
-                self._current_sheet = self._workbook.create_sheet(None)
-                self._default_sheet_title = self._current_sheet.title
-        self._removable_sheet_names.discard(self._current_sheet.title)
-
-    def write_row(self, row):
-        """See base class."""
-        if self._current_sheet is None:
-            self._set_current_sheet()
-        row = [_convert_to_excel(cell) for cell in row]
-        self._current_sheet.append(row)
-        return True
-
-
-def _convert_to_excel(x):
-    """
-    Converts parameter values to formats that are comprehensible to openpyxl.
-
-    Args:
-        x (Any): a parameter value
-
-    Returns:
-        float or str: Excel compatible value
-    """
-    if isinstance(x, numpy.float_):
-        if numpy.isnan(x):
-            return "nan"
-        return float(x)
-    if isinstance(x, numpy.int_):
-        return int(x)
-    if not isinstance(x, (float, int, str)) and x is not None:
-        return str(x)
-    return x
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+A writer for exporting Spine databases to Excel files.
+
+"""
+from pathlib import Path
+import re
+import numpy
+from openpyxl import load_workbook, Workbook
+from openpyxl.utils.exceptions import InvalidFileException
+from openpyxl.workbook.child import INVALID_TITLE_REGEX
+from .writer import Writer, WriterException
+
+
+class ExcelWriter(Writer):
+    def __init__(self, file_path):
+        """
+        Args:
+            file_path (str): path to output file
+        """
+        super().__init__()
+        self._file_path = file_path
+        self._workbook = None
+        self._current_sheet = None
+        self._removable_sheet_names = set()
+        self._next_table_name = None
+        self._default_sheet_title = None
+
+    def finish(self):
+        """See base class."""
+        if self._workbook is None:
+            return
+        for name in self._removable_sheet_names:
+            self._workbook.remove(self._workbook[name])
+        self._removable_sheet_names.clear()
+        if not self._workbook.worksheets:
+            self._workbook.create_sheet("Sheet1")
+        self._workbook.save(self._file_path)
+        self._workbook.close()
+        self._workbook = None
+
+    def finish_table(self):
+        """See base class."""
+        self._current_sheet = None
+
+    def start(self):
+        """See base class."""
+        if Path(self._file_path).exists():
+            try:
+                self._workbook = load_workbook(self._file_path)
+            except InvalidFileException as e:
+                raise WriterException(f"Cannot open Excel file: {e}")
+        else:
+            self._workbook = Workbook()
+            if not self._removable_sheet_names:
+                self._removable_sheet_names = set(self._workbook.sheetnames)
+
+    def start_table(self, table_name, title_key):
+        """See base class."""
+        self._next_table_name = re.sub(INVALID_TITLE_REGEX, "", table_name) if table_name is not None else table_name
+        return True
+
+    def _set_current_sheet(self):
+        """Gets an existing sheet from workbook or creates a new one if needed."""
+        if self._next_table_name is not None:
+            if self._next_table_name in self._workbook:
+                self._current_sheet = self._workbook[self._next_table_name]
+            else:
+                self._current_sheet = self._workbook.create_sheet(self._next_table_name)
+        else:
+            if self._default_sheet_title:
+                self._current_sheet = self._workbook[self._default_sheet_title]
+            else:
+                self._current_sheet = self._workbook.create_sheet(None)
+                self._default_sheet_title = self._current_sheet.title
+        self._removable_sheet_names.discard(self._current_sheet.title)
+
+    def write_row(self, row):
+        """See base class."""
+        if self._current_sheet is None:
+            self._set_current_sheet()
+        row = [_convert_to_excel(cell) for cell in row]
+        self._current_sheet.append(row)
+        return True
+
+
+def _convert_to_excel(x):
+    """
+    Converts parameter values to formats that are comprehensible to openpyxl.
+
+    Args:
+        x (Any): a parameter value
+
+    Returns:
+        float or str: Excel compatible value
+    """
+    if isinstance(x, numpy.float_):
+        if numpy.isnan(x):
+            return "nan"
+        return float(x)
+    if isinstance(x, numpy.int_):
+        return int(x)
+    if not isinstance(x, (float, int, str)) and x is not None:
+        return str(x)
+    return x
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/exporters/sql_writer.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/exporters/sql_writer.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,148 +1,148 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Module contains an SQL writer implementation.
-
-"""
-from sqlalchemy import Boolean, Column, create_engine, Float, Integer, MetaData, String, Table, DateTime
-from sqlalchemy.orm import Session
-from spinedb_api import parameter_value
-from .writer import Writer, WriterException
-
-
-class SqlWriter(Writer):
-    """Export writer that targets SQL databases."""
-
-    def __init__(self, database, overwrite_existing):
-        """
-        Args:
-            database (str): URL or path to output .sqlite file
-            overwrite_existing (bool): if True, overwrites tables in existing database, otherwise appends to the tables
-        """
-        super().__init__()
-        self._overwrite_existing = overwrite_existing
-        if database.find("://") < 0:
-            database = "sqlite:///" + database
-        self._engine = create_engine(database)
-        self._connection = self._engine.connect()
-        self._metadata = MetaData()
-        self._metadata.reflect(bind=self._engine)
-        self._session = Session(self._engine)
-        self._table_name = None
-        self._column_names = None
-        self._column_converters = None
-        self._table = None
-        self._finished_table_names = set()
-
-    def finish(self):
-        """Closes the database connection."""
-        self._session.close()
-        self._connection.close()
-
-    def finish_table(self):
-        """Commits current session."""
-        if self._column_names and self._table is None:
-            # Create an empty table if no rows were available in the database.
-            columns = [Column(name, String) for name in self._column_names]
-            self._table = Table(self._table_name, self._metadata, *columns)
-            self._table.create(self._engine)
-        self._finished_table_names.add(self._table_name)
-        self._session.commit()
-
-    def start_table(self, table_name, title_key):
-        """See base class."""
-        if not table_name:
-            raise WriterException("Cannot create anonymous SQL tables.")
-        self._table = self._metadata.tables.get(table_name)
-        if self._overwrite_existing and self._table is not None and table_name not in self._finished_table_names:
-            self._table.drop(self._engine)
-            self._metadata.remove(self._table)
-            self._table = None
-        self._table_name = table_name
-        self._column_names = None
-        return True
-
-    def write_row(self, row):
-        """See base class."""
-        if self._column_names is None:
-            # Expecting first row to contain column names as headers.
-            self._column_names = row
-            return True
-        if self._table is None:
-            # Build columns using the second row.
-            columns, self._column_converters = _database_columns_and_converters(self._column_names, row)
-            self._table = Table(self._table_name, self._metadata, *columns)
-            self._table.create(self._engine)
-        elif self._column_converters is None:
-            self._column_converters = _converters(row)
-        row = [convert(x) for convert, x in zip(self._column_converters, row)]
-        self._session.execute(self._table.insert().values(tuple(row)))
-        return True
-
-
-def _database_columns_and_converters(names, row):
-    """Creates columns for a database table as well as converters to convert a row to correct types.
-
-    Args:
-        names (list of str): column names
-        row (list): a data row for sniffing column types
-
-    Returns:
-        tuple: list of database columns and list of converter callables
-    """
-    types = []
-    converters = []
-    for x in row:
-        if isinstance(x, float):
-            types.append(Float)
-            converters.append(float)
-        elif isinstance(x, int):
-            types.append(Integer)
-            converters.append(int)
-        elif isinstance(x, bool):
-            types.append(Boolean)
-            converters.append(bool)
-        elif isinstance(x, parameter_value.DateTime):
-            types.append(DateTime)
-            converters.append(lambda v: v.value)
-        elif isinstance(x, parameter_value.Duration):
-            types.append(String)
-            converters.append(lambda v: parameter_value.relativedelta_to_duration(v.value))
-        else:
-            types.append(String)
-            converters.append(str)
-    return [Column(name, type_, nullable=True) for name, type_ in zip(names, types)], converters
-
-
-def _converters(row):
-    """Creates converters to convert a row to correct types.
-
-    Args:
-        row (list): a data row for sniffing column types
-
-    Returns:
-        list of Callable: list of converter callables
-    """
-    converters = []
-    for x in row:
-        if isinstance(x, float):
-            converters.append(float)
-        elif isinstance(x, int):
-            converters.append(int)
-        elif isinstance(x, bool):
-            converters.append(bool)
-        elif isinstance(x, parameter_value.DateTime):
-            converters.append(lambda v: v.value)
-        elif isinstance(x, parameter_value.Duration):
-            converters.append(lambda v: parameter_value.relativedelta_to_duration(v.value))
-        else:
-            converters.append(str)
-    return converters
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Module contains an SQL writer implementation.
+
+"""
+from sqlalchemy import Boolean, Column, create_engine, Float, Integer, MetaData, String, Table, DateTime
+from sqlalchemy.orm import Session
+from spinedb_api import parameter_value
+from .writer import Writer, WriterException
+
+
+class SqlWriter(Writer):
+    """Export writer that targets SQL databases."""
+
+    def __init__(self, database, overwrite_existing):
+        """
+        Args:
+            database (str): URL or path to output .sqlite file
+            overwrite_existing (bool): if True, overwrites tables in existing database, otherwise appends to the tables
+        """
+        super().__init__()
+        self._overwrite_existing = overwrite_existing
+        if database.find("://") < 0:
+            database = "sqlite:///" + database
+        self._engine = create_engine(database)
+        self._connection = self._engine.connect()
+        self._metadata = MetaData()
+        self._metadata.reflect(bind=self._engine)
+        self._session = Session(self._engine)
+        self._table_name = None
+        self._column_names = None
+        self._column_converters = None
+        self._table = None
+        self._finished_table_names = set()
+
+    def finish(self):
+        """Closes the database connection."""
+        self._session.close()
+        self._connection.close()
+
+    def finish_table(self):
+        """Commits current session."""
+        if self._column_names and self._table is None:
+            # Create an empty table if no rows were available in the database.
+            columns = [Column(name, String) for name in self._column_names]
+            self._table = Table(self._table_name, self._metadata, *columns)
+            self._table.create(self._engine)
+        self._finished_table_names.add(self._table_name)
+        self._session.commit()
+
+    def start_table(self, table_name, title_key):
+        """See base class."""
+        if not table_name:
+            raise WriterException("Cannot create anonymous SQL tables.")
+        self._table = self._metadata.tables.get(table_name)
+        if self._overwrite_existing and self._table is not None and table_name not in self._finished_table_names:
+            self._table.drop(self._engine)
+            self._metadata.remove(self._table)
+            self._table = None
+        self._table_name = table_name
+        self._column_names = None
+        return True
+
+    def write_row(self, row):
+        """See base class."""
+        if self._column_names is None:
+            # Expecting first row to contain column names as headers.
+            self._column_names = row
+            return True
+        if self._table is None:
+            # Build columns using the second row.
+            columns, self._column_converters = _database_columns_and_converters(self._column_names, row)
+            self._table = Table(self._table_name, self._metadata, *columns)
+            self._table.create(self._engine)
+        elif self._column_converters is None:
+            self._column_converters = _converters(row)
+        row = [convert(x) for convert, x in zip(self._column_converters, row)]
+        self._session.execute(self._table.insert().values(tuple(row)))
+        return True
+
+
+def _database_columns_and_converters(names, row):
+    """Creates columns for a database table as well as converters to convert a row to correct types.
+
+    Args:
+        names (list of str): column names
+        row (list): a data row for sniffing column types
+
+    Returns:
+        tuple: list of database columns and list of converter callables
+    """
+    types = []
+    converters = []
+    for x in row:
+        if isinstance(x, float):
+            types.append(Float)
+            converters.append(float)
+        elif isinstance(x, int):
+            types.append(Integer)
+            converters.append(int)
+        elif isinstance(x, bool):
+            types.append(Boolean)
+            converters.append(bool)
+        elif isinstance(x, parameter_value.DateTime):
+            types.append(DateTime)
+            converters.append(lambda v: v.value)
+        elif isinstance(x, parameter_value.Duration):
+            types.append(String)
+            converters.append(lambda v: parameter_value.relativedelta_to_duration(v.value))
+        else:
+            types.append(String)
+            converters.append(str)
+    return [Column(name, type_, nullable=True) for name, type_ in zip(names, types)], converters
+
+
+def _converters(row):
+    """Creates converters to convert a row to correct types.
+
+    Args:
+        row (list): a data row for sniffing column types
+
+    Returns:
+        list of Callable: list of converter callables
+    """
+    converters = []
+    for x in row:
+        if isinstance(x, float):
+            converters.append(float)
+        elif isinstance(x, int):
+            converters.append(int)
+        elif isinstance(x, bool):
+            converters.append(bool)
+        elif isinstance(x, parameter_value.DateTime):
+            converters.append(lambda v: v.value)
+        elif isinstance(x, parameter_value.Duration):
+            converters.append(lambda v: parameter_value.relativedelta_to_duration(v.value))
+        else:
+            converters.append(str)
+    return converters
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/exporters/writer.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/exporters/writer.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,142 +1,142 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Module contains the :class:`Writer` base class and functions to write tabular data.
-
-"""
-from contextlib import contextmanager
-from copy import copy
-from sqlalchemy.exc import OperationalError
-
-from spinedb_api import SpineDBAPIError
-from spinedb_api.export_mapping import rows, titles
-from spinedb_api.export_mapping.export_mapping import drop_non_positioned_tail
-from spinedb_api.export_mapping.group_functions import NoGroup
-
-
-def write(db_map, writer, *mappings, empty_data_header=True, max_tables=None, max_rows=None, group_fns=NoGroup.NAME):
-    """
-    Writes given mapping.
-
-    Args:
-        db_map (DatabaseMappingBase): database map
-        writer (Writer): target writer
-        mappings (Mapping): root mappings
-        empty_data_header (bool or Iterable of bool): True to write at least header rows even if there is no data,
-            False to write nothing; a list of booleans applies to each mapping individually
-        max_tables (int, optional): maximum number of tables to write
-        max_rows (int, optional): maximum number of rows/table to write
-        group_fns (str or Iterable of str): group function names for each mappings
-    """
-    if isinstance(empty_data_header, bool):
-        empty_data_header = len(mappings) * [empty_data_header]
-    if isinstance(group_fns, str):
-        group_fns = len(mappings) * [group_fns]
-    with _new_write(writer):
-        for mapping, header_for_empty_data, group_fn in zip(mappings, empty_data_header, group_fns):
-            mapping = drop_non_positioned_tail(copy(mapping))
-            for title, title_key in titles(mapping, db_map, limit=max_tables):
-                with _new_table(writer, title, title_key) as table_started:
-                    if not table_started:
-                        break
-                    try:
-                        if max_rows is None:
-                            for row in rows(mapping, db_map, title_key, header_for_empty_data, group_fn=group_fn):
-                                write_more = writer.write_row(row)
-                                if not write_more:
-                                    break
-                        else:
-                            for n, row in enumerate(
-                                rows(mapping, db_map, title_key, header_for_empty_data, group_fn=group_fn)
-                            ):
-                                write_more = writer.write_row(row)
-                                if not write_more or n + 1 == max_rows:
-                                    break
-                    except OperationalError as error:
-                        raise SpineDBAPIError(str(error))
-
-
-class Writer:
-    def finish(self):
-        """Finishes writing."""
-
-    def finish_table(self):
-        """Finishes writing the current table."""
-
-    def start(self):
-        """Prepares writer for writing."""
-
-    def start_table(self, table_name, title_key):
-        """
-        Starts a new table.
-
-        Args:
-            table_name (str): table's name
-            title_key (dict): table state dictionary
-
-        Returns:
-            bool: True if the table was successfully started, False otherwise
-        """
-        raise NotImplementedError()
-
-    def write_row(self, row):
-        """
-        Writes a row of data.
-
-        Args:
-            row (list): row elements
-
-        Returns:
-            bool: True if more rows can be written, False otherwise
-        """
-        raise NotImplementedError()
-
-
-class WriterException(Exception):
-    """Writer exception."""
-
-
-@contextmanager
-def _new_write(writer):
-    """
-    Manages writing contexts.
-
-    Args:
-        writer (Writer): a writer
-
-    Yields:
-        NoneType
-    """
-    try:
-        writer.start()
-        yield None
-    finally:
-        writer.finish()
-
-
-@contextmanager
-def _new_table(writer, table_name, title_key):
-    """
-    Manages table contexts.
-
-    Args:
-        writer (Writer): a writer
-        table_name (str): table's name
-        title_key (dict,optional)
-
-    Yields:
-        bool: whether or not the new table was successfully started
-    """
-    try:
-        table_started = writer.start_table(table_name, title_key)
-        yield table_started
-    finally:
-        writer.finish_table()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Module contains the :class:`Writer` base class and functions to write tabular data.
+
+"""
+from contextlib import contextmanager
+from copy import copy
+from sqlalchemy.exc import OperationalError
+
+from spinedb_api import SpineDBAPIError
+from spinedb_api.export_mapping import rows, titles
+from spinedb_api.export_mapping.export_mapping import drop_non_positioned_tail
+from spinedb_api.export_mapping.group_functions import NoGroup
+
+
+def write(db_map, writer, *mappings, empty_data_header=True, max_tables=None, max_rows=None, group_fns=NoGroup.NAME):
+    """
+    Writes given mapping.
+
+    Args:
+        db_map (DatabaseMappingBase): database map
+        writer (Writer): target writer
+        mappings (Mapping): root mappings
+        empty_data_header (bool or Iterable of bool): True to write at least header rows even if there is no data,
+            False to write nothing; a list of booleans applies to each mapping individually
+        max_tables (int, optional): maximum number of tables to write
+        max_rows (int, optional): maximum number of rows/table to write
+        group_fns (str or Iterable of str): group function names for each mappings
+    """
+    if isinstance(empty_data_header, bool):
+        empty_data_header = len(mappings) * [empty_data_header]
+    if isinstance(group_fns, str):
+        group_fns = len(mappings) * [group_fns]
+    with _new_write(writer):
+        for mapping, header_for_empty_data, group_fn in zip(mappings, empty_data_header, group_fns):
+            mapping = drop_non_positioned_tail(copy(mapping))
+            for title, title_key in titles(mapping, db_map, limit=max_tables):
+                with _new_table(writer, title, title_key) as table_started:
+                    if not table_started:
+                        break
+                    try:
+                        if max_rows is None:
+                            for row in rows(mapping, db_map, title_key, header_for_empty_data, group_fn=group_fn):
+                                write_more = writer.write_row(row)
+                                if not write_more:
+                                    break
+                        else:
+                            for n, row in enumerate(
+                                rows(mapping, db_map, title_key, header_for_empty_data, group_fn=group_fn)
+                            ):
+                                write_more = writer.write_row(row)
+                                if not write_more or n + 1 == max_rows:
+                                    break
+                    except OperationalError as error:
+                        raise SpineDBAPIError(str(error))
+
+
+class Writer:
+    def finish(self):
+        """Finishes writing."""
+
+    def finish_table(self):
+        """Finishes writing the current table."""
+
+    def start(self):
+        """Prepares writer for writing."""
+
+    def start_table(self, table_name, title_key):
+        """
+        Starts a new table.
+
+        Args:
+            table_name (str): table's name
+            title_key (dict): table state dictionary
+
+        Returns:
+            bool: True if the table was successfully started, False otherwise
+        """
+        raise NotImplementedError()
+
+    def write_row(self, row):
+        """
+        Writes a row of data.
+
+        Args:
+            row (list): row elements
+
+        Returns:
+            bool: True if more rows can be written, False otherwise
+        """
+        raise NotImplementedError()
+
+
+class WriterException(Exception):
+    """Writer exception."""
+
+
+@contextmanager
+def _new_write(writer):
+    """
+    Manages writing contexts.
+
+    Args:
+        writer (Writer): a writer
+
+    Yields:
+        NoneType
+    """
+    try:
+        writer.start()
+        yield None
+    finally:
+        writer.finish()
+
+
+@contextmanager
+def _new_table(writer, table_name, title_key):
+    """
+    Manages table contexts.
+
+    Args:
+        writer (Writer): a writer
+        table_name (str): table's name
+        title_key (dict,optional)
+
+    Yields:
+        bool: whether or not the new table was successfully started
+    """
+    try:
+        table_started = writer.start_table(table_name, title_key)
+        yield table_started
+    finally:
+        writer.finish_table()
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/gdx_utils.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/gdx_utils.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,63 +1,63 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Utility functions for .gdx import/export.
-
-"""
-
-import os
-import sys
-
-if sys.platform == "win32":
-    import winreg
-
-
-def _python_interpreter_bitness():
-    """Returns 64 for 64bit Python interpreter or 32 for 32bit interpreter."""
-    # As recommended in Python's docs:
-    # https://docs.python.org/3/library/platform.html#cross-platform
-    return 64 if sys.maxsize > 2 ** 32 else 32
-
-
-def _windows_dlls_exist(gams_path):
-    """Returns True if requred DLL files exist in given GAMS installation path."""
-    bitness = _python_interpreter_bitness()
-    # This DLL must exist on Windows installation
-    dll_name = "gdxdclib{}.dll".format(bitness)
-    dll_path = os.path.join(gams_path, dll_name)
-    return os.path.isfile(dll_path)
-
-
-def find_gams_directory():
-    """
-    Returns GAMS installation directory or None if not found.
-
-    On Windows systems, this function looks for `gams.location` in registry,
-    then checks the ``PATH`` environment variable.
-    On other systems, only the ``PATH`` environment variable is checked.
-
-    Returns:
-        str: a path to GAMS installation directory or None if not found.
-    """
-    if sys.platform == "win32":
-        try:
-            with winreg.OpenKey(winreg.HKEY_CLASSES_ROOT, "gams.location") as gams_location_key:
-                gams_path, _ = winreg.QueryValueEx(gams_location_key, "")
-                if _windows_dlls_exist(gams_path):
-                    return gams_path
-        except FileNotFoundError:
-            pass
-    executable_paths = os.get_exec_path()
-    for path in executable_paths:
-        if "gams" in path.casefold():
-            return path
-    return None
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Utility functions for .gdx import/export.
+
+"""
+
+import os
+import sys
+
+if sys.platform == "win32":
+    import winreg
+
+
+def _python_interpreter_bitness():
+    """Returns 64 for 64bit Python interpreter or 32 for 32bit interpreter."""
+    # As recommended in Python's docs:
+    # https://docs.python.org/3/library/platform.html#cross-platform
+    return 64 if sys.maxsize > 2 ** 32 else 32
+
+
+def _windows_dlls_exist(gams_path):
+    """Returns True if requred DLL files exist in given GAMS installation path."""
+    bitness = _python_interpreter_bitness()
+    # This DLL must exist on Windows installation
+    dll_name = "gdxdclib{}.dll".format(bitness)
+    dll_path = os.path.join(gams_path, dll_name)
+    return os.path.isfile(dll_path)
+
+
+def find_gams_directory():
+    """
+    Returns GAMS installation directory or None if not found.
+
+    On Windows systems, this function looks for `gams.location` in registry,
+    then checks the ``PATH`` environment variable.
+    On other systems, only the ``PATH`` environment variable is checked.
+
+    Returns:
+        str: a path to GAMS installation directory or None if not found.
+    """
+    if sys.platform == "win32":
+        try:
+            with winreg.OpenKey(winreg.HKEY_CLASSES_ROOT, "gams.location") as gams_location_key:
+                gams_path, _ = winreg.QueryValueEx(gams_location_key, "")
+                if _windows_dlls_exist(gams_path):
+                    return gams_path
+        except FileNotFoundError:
+            pass
+    executable_paths = os.get_exec_path()
+    for path in executable_paths:
+        if "gams" in path.casefold():
+            return path
+    return None
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/importers/__init__.py` & `spinedb_api-0.30.4/tests/spine_io/exporters/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Intentionally empty.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Init file for tests.spine_io.exporters package. Intentionally empty.
+
+"""
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/importers/csv_reader.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/importers/csv_reader.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,185 +1,185 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains CSVConnector class and a help function.
-
-"""
-
-
-import csv
-from itertools import islice
-import chardet
-from .reader import SourceConnection
-
-
-class CSVConnector(SourceConnection):
-    """Template class to read data from another QThread."""
-
-    DISPLAY_NAME = "Text/CSV"
-    """name of data source, ex: "Text/CSV""" ""
-
-    _ENCODINGS = ["utf-8", "utf-16", "utf-32", "ascii", "iso-8859-1", "iso-8859-2"]
-    """List of available text encodings"""
-
-    OPTIONS = {
-        "encoding": {"type": list, "label": "Encoding", "Items": _ENCODINGS, "default": _ENCODINGS[0]},
-        "delimiter": {"type": list, "label": "Delimiter", "Items": [",", ";", "Tab"], "default": ","},
-        "delimiter_custom": {"type": str, "label": "Custom Delimiter", "MaxLength": 1, "default": ""},
-        "quotechar": {"type": str, "label": "Quotechar", "MaxLength": 1, "default": ""},
-        "has_header": {"type": bool, "label": "Has header", "default": False},
-        "skip": {"type": int, "label": "Skip rows", "Minimum": 0, "default": 0},
-    }
-    """dict with option specification for source."""
-
-    FILE_EXTENSIONS = "*.csv"
-
-    def __init__(self, settings):
-        super().__init__(settings)
-        self._filename = None
-
-    def connect_to_source(self, source, **extras):
-        """saves filepath
-
-        Args:
-            source (str): filepath
-            **extras: ignored
-        """
-        self._filename = source
-
-    def disconnect(self):
-        """Disconnect from connected source."""
-
-    def get_tables(self):
-        """
-        Returns a mapping from file name to options.
-
-        Returns:
-            dict
-        """
-        options = {}
-        # try to find options for file
-        with open(self._filename, "rb") as input_file:
-            sniff_result = chardet.detect(input_file.read(1024))
-        sniffed_encoding = sniff_result["encoding"]
-        if sniffed_encoding is not None:
-            sniffed_encoding = sniffed_encoding.lower()
-        # The sniffed encoding is not always correct. We may still need to try other options too.
-        if sniffed_encoding in self._ENCODINGS:
-            try_encodings = [sniffed_encoding] + [
-                encoding for encoding in self._ENCODINGS if encoding != sniffed_encoding
-            ]
-        else:
-            try_encodings = self._ENCODINGS
-        options["encoding"] = try_encodings[0]
-        for encoding in try_encodings:
-            with open(self._filename, encoding=encoding) as csvfile:
-                try:
-                    dialect = csv.Sniffer().sniff(csvfile.read(1024))
-                    if dialect.delimiter in [",", ";"]:
-                        options["delimiter"] = dialect.delimiter
-                    elif dialect.delimiter == "\t":
-                        options["delimiter"] = "Tab"
-                    else:
-                        options["delimiter_custom"] = dialect.delimiter
-                    options.update({"quotechar": dialect.quotechar, "skip": 0})
-                except csv.Error:
-                    pass
-                except UnicodeDecodeError:
-                    continue
-                try:
-                    options["has_header"] = csv.Sniffer().has_header(csvfile.read(1024))
-                except csv.Error:
-                    pass
-                options["encoding"] = encoding
-                break
-        return {"data": {"options": options}}
-
-    @staticmethod
-    def parse_options(options):
-        """Parses options dict to dialect and quotechar options for csv.reader
-
-        Arguments:
-            options (dict): dict with options:
-                "encoding": file text encoding
-                "delimiter": file delimiter
-                "quotechar": file quotechar
-                "has_header": if first row should be treated as a header
-                "skip": how many rows should be skipped
-
-        Returns:
-            tuple(dict, bool, integer): tuple dialect for csv.reader,
-                                        quotechar for csv.reader and
-                                        number of rows to skip
-        """
-        encoding = options.get("encoding", None)
-        delimiter = options.get("delimiter_custom", "")
-        if not delimiter:
-            delimiter = options.get("delimiter", ",")
-        if not delimiter:
-            delimiter = ","
-        elif delimiter == "Tab":
-            delimiter = "\t"
-        dialect = {"delimiter": delimiter}
-        quotechar = options.get("quotechar", None)
-        if quotechar:
-            dialect.update({"quotechar": quotechar})
-        has_header = options.get("has_header", False)
-        skip = options.get("skip", 0)
-        return encoding, dialect, has_header, skip
-
-    def file_iterator(self, options, max_rows):
-        """creates an iterator that reads max_rows number of rows from text file
-
-        Arguments:
-            options (dict): dict with options:
-            max_rows (integer): max number of rows to read, if -1 then read all rows
-
-        Returns:
-            iterator: iterator of csv file
-        """
-        if not self._filename:
-            return []
-        encoding, dialect, _has_header, skip = self.parse_options(options)
-        if max_rows == -1:
-            max_rows = None
-        else:
-            max_rows += skip
-        with open(self._filename, encoding=encoding) as text_file:
-            csv_reader = csv.reader(text_file, **dialect)
-            csv_reader = islice(csv_reader, skip, max_rows)
-            yield from csv_reader
-
-    def get_data_iterator(self, table, options, max_rows=-1):
-        """Creates an iterator for the file in self.filename.
-
-        Arguments:
-            table (string): ignored, used in abstract IOWorker class
-            options (dict): dict with options
-            max_rows (int): how many rows of data to read, if -1 read all rows (default: {-1})
-
-        Returns:
-            tuple:
-        """
-        csv_iter = self.file_iterator(options, max_rows)
-        try:
-            first_row = next(csv_iter)
-        except StopIteration:
-            return iter([]), []
-        has_header = options.get("has_header", False)
-        if has_header:
-            # Very good, we already have the first row
-            header = first_row
-        else:
-            header = []
-            # reset iterator
-            csv_iter = self.file_iterator(options, max_rows)
-        return csv_iter, header
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains CSVConnector class and a help function.
+
+"""
+
+
+import csv
+from itertools import islice
+import chardet
+from .reader import SourceConnection
+
+
+class CSVConnector(SourceConnection):
+    """Template class to read data from another QThread."""
+
+    DISPLAY_NAME = "Text/CSV"
+    """name of data source, ex: "Text/CSV""" ""
+
+    _ENCODINGS = ["utf-8", "utf-16", "utf-32", "ascii", "iso-8859-1", "iso-8859-2"]
+    """List of available text encodings"""
+
+    OPTIONS = {
+        "encoding": {"type": list, "label": "Encoding", "Items": _ENCODINGS, "default": _ENCODINGS[0]},
+        "delimiter": {"type": list, "label": "Delimiter", "Items": [",", ";", "Tab"], "default": ","},
+        "delimiter_custom": {"type": str, "label": "Custom Delimiter", "MaxLength": 1, "default": ""},
+        "quotechar": {"type": str, "label": "Quotechar", "MaxLength": 1, "default": ""},
+        "has_header": {"type": bool, "label": "Has header", "default": False},
+        "skip": {"type": int, "label": "Skip rows", "Minimum": 0, "default": 0},
+    }
+    """dict with option specification for source."""
+
+    FILE_EXTENSIONS = "*.csv"
+
+    def __init__(self, settings):
+        super().__init__(settings)
+        self._filename = None
+
+    def connect_to_source(self, source, **extras):
+        """saves filepath
+
+        Args:
+            source (str): filepath
+            **extras: ignored
+        """
+        self._filename = source
+
+    def disconnect(self):
+        """Disconnect from connected source."""
+
+    def get_tables(self):
+        """
+        Returns a mapping from file name to options.
+
+        Returns:
+            dict
+        """
+        options = {}
+        # try to find options for file
+        with open(self._filename, "rb") as input_file:
+            sniff_result = chardet.detect(input_file.read(1024))
+        sniffed_encoding = sniff_result["encoding"]
+        if sniffed_encoding is not None:
+            sniffed_encoding = sniffed_encoding.lower()
+        # The sniffed encoding is not always correct. We may still need to try other options too.
+        if sniffed_encoding in self._ENCODINGS:
+            try_encodings = [sniffed_encoding] + [
+                encoding for encoding in self._ENCODINGS if encoding != sniffed_encoding
+            ]
+        else:
+            try_encodings = self._ENCODINGS
+        options["encoding"] = try_encodings[0]
+        for encoding in try_encodings:
+            with open(self._filename, encoding=encoding) as csvfile:
+                try:
+                    dialect = csv.Sniffer().sniff(csvfile.read(1024))
+                    if dialect.delimiter in [",", ";"]:
+                        options["delimiter"] = dialect.delimiter
+                    elif dialect.delimiter == "\t":
+                        options["delimiter"] = "Tab"
+                    else:
+                        options["delimiter_custom"] = dialect.delimiter
+                    options.update({"quotechar": dialect.quotechar, "skip": 0})
+                except csv.Error:
+                    pass
+                except UnicodeDecodeError:
+                    continue
+                try:
+                    options["has_header"] = csv.Sniffer().has_header(csvfile.read(1024))
+                except csv.Error:
+                    pass
+                options["encoding"] = encoding
+                break
+        return {"data": {"options": options}}
+
+    @staticmethod
+    def parse_options(options):
+        """Parses options dict to dialect and quotechar options for csv.reader
+
+        Arguments:
+            options (dict): dict with options:
+                "encoding": file text encoding
+                "delimiter": file delimiter
+                "quotechar": file quotechar
+                "has_header": if first row should be treated as a header
+                "skip": how many rows should be skipped
+
+        Returns:
+            tuple(dict, bool, integer): tuple dialect for csv.reader,
+                                        quotechar for csv.reader and
+                                        number of rows to skip
+        """
+        encoding = options.get("encoding", None)
+        delimiter = options.get("delimiter_custom", "")
+        if not delimiter:
+            delimiter = options.get("delimiter", ",")
+        if not delimiter:
+            delimiter = ","
+        elif delimiter == "Tab":
+            delimiter = "\t"
+        dialect = {"delimiter": delimiter}
+        quotechar = options.get("quotechar", None)
+        if quotechar:
+            dialect.update({"quotechar": quotechar})
+        has_header = options.get("has_header", False)
+        skip = options.get("skip", 0)
+        return encoding, dialect, has_header, skip
+
+    def file_iterator(self, options, max_rows):
+        """creates an iterator that reads max_rows number of rows from text file
+
+        Arguments:
+            options (dict): dict with options:
+            max_rows (integer): max number of rows to read, if -1 then read all rows
+
+        Returns:
+            iterator: iterator of csv file
+        """
+        if not self._filename:
+            return []
+        encoding, dialect, _has_header, skip = self.parse_options(options)
+        if max_rows == -1:
+            max_rows = None
+        else:
+            max_rows += skip
+        with open(self._filename, encoding=encoding) as text_file:
+            csv_reader = csv.reader(text_file, **dialect)
+            csv_reader = islice(csv_reader, skip, max_rows)
+            yield from csv_reader
+
+    def get_data_iterator(self, table, options, max_rows=-1):
+        """Creates an iterator for the file in self.filename.
+
+        Arguments:
+            table (string): ignored, used in abstract IOWorker class
+            options (dict): dict with options
+            max_rows (int): how many rows of data to read, if -1 read all rows (default: {-1})
+
+        Returns:
+            tuple:
+        """
+        csv_iter = self.file_iterator(options, max_rows)
+        try:
+            first_row = next(csv_iter)
+        except StopIteration:
+            return iter([]), []
+        has_header = options.get("has_header", False)
+        if has_header:
+            # Very good, we already have the first row
+            header = first_row
+        else:
+            header = []
+            # reset iterator
+            csv_iter = self.file_iterator(options, max_rows)
+        return csv_iter, header
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/importers/datapackage_reader.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/importers/datapackage_reader.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,112 +1,112 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains DataPackageConnector class.
-
-"""
-import threading
-from itertools import chain
-
-from datapackage import Package
-from .reader import SourceConnection
-
-
-class DataPackageConnector(SourceConnection):
-    """Template class to read data from another QThread."""
-
-    # name of data source, ex: "Text/CSV"
-    DISPLAY_NAME = "Datapackage"
-
-    # dict with option specification for source.
-    OPTIONS = {"has_header": {"type": bool, "label": "Has header", "default": True}}
-
-    FILE_EXTENSIONS = "*.json"
-
-    def __init__(self, settings):
-        super().__init__(settings)
-        self._filename = None
-        self._datapackage = None
-        self._resource_name_lock = threading.Lock()
-
-    def __getstate__(self):
-        """Builds a state that can be pickled.
-
-        Returns:
-            dict: picklable representation of the connector
-        """
-        state = self.__dict__.copy()
-        del state["_resource_name_lock"]
-        return state
-
-    def __setstate__(self, state):
-        """Restores connector from pickled state.
-
-        Args:
-            state (dict): pickled state
-        """
-        self.__dict__.update(state)
-        self._resource_name_lock = threading.Lock()
-
-    def connect_to_source(self, source, **extras):
-        """Creates datapackage.
-
-        Args:
-            source (str): filepath of a datapackage.json file
-            **extras: ignored
-        """
-        if source:
-            self._datapackage = Package(source)
-            self._filename = source
-
-    def disconnect(self):
-        """Disconnect from connected source."""
-        if self._datapackage:
-            self._datapackage = None
-        self._filename = None
-
-    def get_tables(self):
-        """Returns resources' mappings and their options.
-
-        Returns:
-            dict: key is resource name, value is mapping and options.
-        """
-        if not self._datapackage:
-            return {}
-        tables = {}
-        for resource in self._datapackage.resources:
-            with self._resource_name_lock:
-                if resource.name is None:
-                    resource.infer()
-            tables[resource.name] = {"options": {}}  # FIXME?
-        return tables
-
-    def get_data_iterator(self, table, options, max_rows=-1):
-        """
-        Return data read from data source table in table. If max_rows is
-        specified only that number of rows.
-        """
-        if not self._datapackage:
-            return iter([]), []
-
-        has_header = options.get("has_header", True)
-        for resource in self._datapackage.resources:
-            with self._resource_name_lock:
-                if resource.name is None:
-                    resource.infer()
-            if table == resource.name:
-                iterator = (item for row, item in enumerate(resource.iter(cast=False)) if row != max_rows)
-                if has_header:
-                    header = resource.schema.field_names
-                    return iterator, header
-                return chain([resource.headers], iterator), None
-        # table not found
-        return iter([]), []
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains DataPackageConnector class.
+
+"""
+import threading
+from itertools import chain
+
+from datapackage import Package
+from .reader import SourceConnection
+
+
+class DataPackageConnector(SourceConnection):
+    """Template class to read data from another QThread."""
+
+    # name of data source, ex: "Text/CSV"
+    DISPLAY_NAME = "Datapackage"
+
+    # dict with option specification for source.
+    OPTIONS = {"has_header": {"type": bool, "label": "Has header", "default": True}}
+
+    FILE_EXTENSIONS = "*.json"
+
+    def __init__(self, settings):
+        super().__init__(settings)
+        self._filename = None
+        self._datapackage = None
+        self._resource_name_lock = threading.Lock()
+
+    def __getstate__(self):
+        """Builds a state that can be pickled.
+
+        Returns:
+            dict: picklable representation of the connector
+        """
+        state = self.__dict__.copy()
+        del state["_resource_name_lock"]
+        return state
+
+    def __setstate__(self, state):
+        """Restores connector from pickled state.
+
+        Args:
+            state (dict): pickled state
+        """
+        self.__dict__.update(state)
+        self._resource_name_lock = threading.Lock()
+
+    def connect_to_source(self, source, **extras):
+        """Creates datapackage.
+
+        Args:
+            source (str): filepath of a datapackage.json file
+            **extras: ignored
+        """
+        if source:
+            self._datapackage = Package(source)
+            self._filename = source
+
+    def disconnect(self):
+        """Disconnect from connected source."""
+        if self._datapackage:
+            self._datapackage = None
+        self._filename = None
+
+    def get_tables(self):
+        """Returns resources' mappings and their options.
+
+        Returns:
+            dict: key is resource name, value is mapping and options.
+        """
+        if not self._datapackage:
+            return {}
+        tables = {}
+        for resource in self._datapackage.resources:
+            with self._resource_name_lock:
+                if resource.name is None:
+                    resource.infer()
+            tables[resource.name] = {"options": {}}  # FIXME?
+        return tables
+
+    def get_data_iterator(self, table, options, max_rows=-1):
+        """
+        Return data read from data source table in table. If max_rows is
+        specified only that number of rows.
+        """
+        if not self._datapackage:
+            return iter([]), []
+
+        has_header = options.get("has_header", True)
+        for resource in self._datapackage.resources:
+            with self._resource_name_lock:
+                if resource.name is None:
+                    resource.infer()
+            if table == resource.name:
+                iterator = (item for row, item in enumerate(resource.iter(cast=False)) if row != max_rows)
+                if has_header:
+                    header = resource.schema.field_names
+                    return iterator, header
+                return chain([resource.headers], iterator), None
+        # table not found
+        return iter([]), []
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/importers/excel_reader.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/importers/excel_reader.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,274 +1,274 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains ExcelConnector class and a help function.
-
-"""
-
-from itertools import islice, takewhile, chain
-import io
-from openpyxl import load_workbook
-from .reader import SourceConnection
-
-
-class ExcelConnector(SourceConnection):
-    """Template class to read data from another QThread."""
-
-    # name of data source, ex: "Text/CSV"
-    DISPLAY_NAME = "Excel"
-
-    # dict with option specification for source.
-    OPTIONS = {
-        "header": {"type": bool, "label": "Has header", "default": False},
-        "row": {"type": int, "label": "Skip rows", "Minimum": 0, "default": 0},
-        "column": {"type": int, "label": "Skip columns", "Minimum": 0, "default": 0},
-        "read_until_col": {"type": bool, "label": "Read until empty column on first row", "default": False},
-        "read_until_row": {"type": bool, "label": "Read until empty row on first column", "default": False},
-    }
-
-    FILE_EXTENSIONS = "*.xlsx;;*.xlsm;;*.xltx;;*.xltm"
-
-    def __init__(self, settings):
-        super().__init__(settings)
-        self._filename = None
-        self._wb = None
-
-    def connect_to_source(self, source, **extras):
-        """Connects to Excel file.
-
-        Args:
-            source (str): path to file
-            **extras: ignored
-        """
-        if source:
-            self._filename = source
-            # NOTE: there seems to be no way of closing the workbook
-            # when read_only=True, read file into memory first and then
-            # open to avoid locking file while toolbox is running.
-            with open(self._filename, "rb") as bin_file:
-                in_mem_file = io.BytesIO(bin_file.read())
-            self._wb = load_workbook(in_mem_file, read_only=True, data_only=True)
-
-    def disconnect(self):
-        """Disconnect from connected source."""
-        if self._wb:
-            self._wb.close()
-            self._wb = None
-        self._filename = None
-
-    def create_default_mapping(self):
-        """See base class"""
-        default_mapping = {}
-        table_mappings = default_mapping["table_mappings"] = {}
-        table_options = default_mapping["table_options"] = {}
-        selected_tables = default_mapping["selected_tables"] = []
-        for sheet in self.get_tables():
-            map_dict, option = create_mapping_from_sheet(self._wb[sheet])
-            if map_dict:
-                table_mappings[sheet] = [map_dict]
-                selected_tables.append(sheet)
-            if option:
-                table_options[sheet] = option
-        return default_mapping
-
-    def get_tables(self):
-        """Method that should return Excel sheets as mappings and their options.
-
-        Returns:
-            dict: Sheets as mappings and options for each sheet or an empty dictionary if no workbook.
-
-        Raises:
-            Exception: If something goes wrong.
-        """
-        if not self._wb:
-            return {}
-        try:
-            return self._wb.sheetnames
-        except Exception as error:
-            raise error
-
-    def get_data_iterator(self, table, options, max_rows=-1):
-        """
-        Return data read from data source table in table. If max_rows is
-        specified only that number of rows.
-        """
-        if not self._wb:
-            return iter([]), []
-        if table not in self._wb:
-            # table not found
-            return iter([]), []
-        worksheet = self._wb[table]
-        # get options
-        has_header = options.get("header", False)
-        skip_rows = options.get("row", 0)
-        skip_columns = options.get("column", 0)
-        stop_at_empty_col = options.get("read_until_col", False)
-        stop_at_empty_row = options.get("read_until_row", False)
-        if max_rows == -1:
-            max_rows = None
-        else:
-            max_rows += skip_rows
-        rows = islice(worksheet.iter_rows(), skip_rows, max_rows)
-        first_row = next(rows, None)
-        if first_row is None:
-            return iter([]), []
-        read_to_col = None
-        if stop_at_empty_col:
-            for i, column in enumerate(islice(first_row, skip_columns, None)):
-                if column.value is None:
-                    read_to_col = i + skip_columns
-                    break
-        if has_header:
-            header = [c.value for c in islice(first_row, skip_columns, read_to_col)]
-        else:
-            header = []
-            rows = chain((first_row,), rows)
-        # iterator for selected columns and skipped rows
-        data_iterator = (list(cell.value for cell in islice(row, skip_columns, read_to_col)) for row in rows)
-        if stop_at_empty_row:
-            # add condition to iterator
-            data_iterator = takewhile(any, data_iterator)
-        return data_iterator, header
-
-
-def get_mapped_data_from_xlsx(filepath):
-    """Returns mapped data from given Excel file assuming it has the default Spine Excel format.
-
-    Args:
-        filepath (str): path to Excel file
-    """
-    connector = ExcelConnector(None)
-    connector.connect_to_source(filepath)
-    mapping = connector.create_default_mapping()
-    table_mappings = {
-        name: m for name, m in mapping.get("table_mappings", {}).items() if name in mapping["selected_tables"]
-    }
-    table_options = {
-        name: options
-        for name, options in mapping.get("table_options", {}).items()
-        if name in mapping["selected_tables"]
-    }
-    table_types = table_row_types = dict.fromkeys(mapping["selected_tables"], {})
-    mapped_data, errors = connector.get_mapped_data(
-        table_mappings, table_options, table_types, {}, table_row_types, max_rows=-1
-    )
-    connector.disconnect()
-    return mapped_data, errors
-
-
-def create_mapping_from_sheet(ws):
-    """
-    Checks if sheet has the default Spine Excel format, if so creates a
-    mapping object for each sheet.
-    """
-    metadata = _get_metadata(ws)
-    if not metadata or "sheet_type" not in metadata:
-        return None, None
-    header_row = len(metadata) + 2
-    if metadata["sheet_type"] == "entity":
-        index_dim_count = metadata.get("index_dim_count")
-        if index_dim_count is not None:
-            index_dim_count = int(index_dim_count)
-        header = _get_header(ws, header_row, index_dim_count)
-        if not header:
-            return None, None
-        return _create_entity_mappings(metadata, header, index_dim_count)
-    options = {"header": True, "row": len(metadata) + 1, "read_until_col": True, "read_until_row": True}
-    if metadata["sheet_type"] == "object_group":
-        map_dict = {"map_type": "ObjectGroup", "name": metadata["class_name"], "groups": 0, "members": 1}
-        return map_dict, options
-    if metadata["sheet_type"] == "alternative":
-        map_dict = {"map_type": "Alternative", "name": 0}
-        return map_dict, options
-    if metadata["sheet_type"] == "scenario":
-        map_dict = {"map_type": "Scenario", "name": 0, "active": 1}
-        return map_dict, options
-    if metadata["sheet_type"] == "scenario_alternative":
-        map_dict = {
-            "map_type": "ScenarioAlternative",
-            "name": 0,
-            "scenario_name": 0,
-            "alternative_name": 1,
-            "before_alternative_name": 2,
-        }
-        return map_dict, options
-    return None, None
-
-
-def _get_metadata(ws):
-    metadata = {}
-    for key, value in ws.iter_rows(min_row=1, max_row=10, max_col=2, values_only=True):
-        if not key:
-            break
-        metadata[key] = value
-    return metadata
-
-
-def _get_header(ws, header_row, index_dim_count):
-    if index_dim_count:
-        # Vertical header
-        header = []
-        header_column = index_dim_count
-        row = header_row
-        while True:
-            label = ws.cell(row=row, column=header_column).value
-            if label == "index":
-                break
-            header.append(label)
-            row += 1
-        return header
-    # Horizontal
-    header = []
-    column = 1
-    while True:
-        label = ws.cell(row=header_row, column=column).value
-        if not label:
-            break
-        header.append(label)
-        column += 1
-    return header
-
-
-def _create_entity_mappings(metadata, header, index_dim_count):
-    class_name = metadata["class_name"]
-    entity_type = metadata["entity_type"]
-    map_dict = {"name": class_name}
-    ent_alt_map_type = "row" if index_dim_count else "column"
-    if entity_type == "object":
-        map_dict["map_type"] = "ObjectClass"
-        map_dict["objects"] = {"map_type": ent_alt_map_type, "reference": 0}
-    elif entity_type == "relationship":
-        entity_dim_count = int(metadata["entity_dim_count"])
-        map_dict["map_type"] = "RelationshipClass"
-        map_dict["object_classes"] = header[:entity_dim_count]
-        map_dict["objects"] = [{"map_type": ent_alt_map_type, "reference": i} for i in range(entity_dim_count)]
-    else:
-        return None, None
-    value_type = metadata.get("value_type")
-    if value_type is not None:
-        value = {"value_type": value_type}
-        if index_dim_count:
-            value["extra_dimensions"] = list(range(index_dim_count))
-        p_ref = len(header) if index_dim_count else -1
-        map_dict["parameters"] = {
-            "map_type": "ParameterValue",
-            "name": {"map_type": "row", "reference": p_ref},
-            "value": value,
-            "alternative_name": {"map_type": ent_alt_map_type, "reference": header.index("alternative")},
-        }
-    options = {
-        "header": not index_dim_count,
-        "row": len(metadata) + 1,
-        "read_until_col": not index_dim_count,
-        "read_until_row": not index_dim_count,
-    }
-    return map_dict, options
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains ExcelConnector class and a help function.
+
+"""
+
+from itertools import islice, takewhile, chain
+import io
+from openpyxl import load_workbook
+from .reader import SourceConnection
+
+
+class ExcelConnector(SourceConnection):
+    """Template class to read data from another QThread."""
+
+    # name of data source, ex: "Text/CSV"
+    DISPLAY_NAME = "Excel"
+
+    # dict with option specification for source.
+    OPTIONS = {
+        "header": {"type": bool, "label": "Has header", "default": False},
+        "row": {"type": int, "label": "Skip rows", "Minimum": 0, "default": 0},
+        "column": {"type": int, "label": "Skip columns", "Minimum": 0, "default": 0},
+        "read_until_col": {"type": bool, "label": "Read until empty column on first row", "default": False},
+        "read_until_row": {"type": bool, "label": "Read until empty row on first column", "default": False},
+    }
+
+    FILE_EXTENSIONS = "*.xlsx;;*.xlsm;;*.xltx;;*.xltm"
+
+    def __init__(self, settings):
+        super().__init__(settings)
+        self._filename = None
+        self._wb = None
+
+    def connect_to_source(self, source, **extras):
+        """Connects to Excel file.
+
+        Args:
+            source (str): path to file
+            **extras: ignored
+        """
+        if source:
+            self._filename = source
+            # NOTE: there seems to be no way of closing the workbook
+            # when read_only=True, read file into memory first and then
+            # open to avoid locking file while toolbox is running.
+            with open(self._filename, "rb") as bin_file:
+                in_mem_file = io.BytesIO(bin_file.read())
+            self._wb = load_workbook(in_mem_file, read_only=True, data_only=True)
+
+    def disconnect(self):
+        """Disconnect from connected source."""
+        if self._wb:
+            self._wb.close()
+            self._wb = None
+        self._filename = None
+
+    def create_default_mapping(self):
+        """See base class"""
+        default_mapping = {}
+        table_mappings = default_mapping["table_mappings"] = {}
+        table_options = default_mapping["table_options"] = {}
+        selected_tables = default_mapping["selected_tables"] = []
+        for sheet in self.get_tables():
+            map_dict, option = create_mapping_from_sheet(self._wb[sheet])
+            if map_dict:
+                table_mappings[sheet] = [map_dict]
+                selected_tables.append(sheet)
+            if option:
+                table_options[sheet] = option
+        return default_mapping
+
+    def get_tables(self):
+        """Method that should return Excel sheets as mappings and their options.
+
+        Returns:
+            dict: Sheets as mappings and options for each sheet or an empty dictionary if no workbook.
+
+        Raises:
+            Exception: If something goes wrong.
+        """
+        if not self._wb:
+            return {}
+        try:
+            return self._wb.sheetnames
+        except Exception as error:
+            raise error
+
+    def get_data_iterator(self, table, options, max_rows=-1):
+        """
+        Return data read from data source table in table. If max_rows is
+        specified only that number of rows.
+        """
+        if not self._wb:
+            return iter([]), []
+        if table not in self._wb:
+            # table not found
+            return iter([]), []
+        worksheet = self._wb[table]
+        # get options
+        has_header = options.get("header", False)
+        skip_rows = options.get("row", 0)
+        skip_columns = options.get("column", 0)
+        stop_at_empty_col = options.get("read_until_col", False)
+        stop_at_empty_row = options.get("read_until_row", False)
+        if max_rows == -1:
+            max_rows = None
+        else:
+            max_rows += skip_rows
+        rows = islice(worksheet.iter_rows(), skip_rows, max_rows)
+        first_row = next(rows, None)
+        if first_row is None:
+            return iter([]), []
+        read_to_col = None
+        if stop_at_empty_col:
+            for i, column in enumerate(islice(first_row, skip_columns, None)):
+                if column.value is None:
+                    read_to_col = i + skip_columns
+                    break
+        if has_header:
+            header = [c.value for c in islice(first_row, skip_columns, read_to_col)]
+        else:
+            header = []
+            rows = chain((first_row,), rows)
+        # iterator for selected columns and skipped rows
+        data_iterator = (list(cell.value for cell in islice(row, skip_columns, read_to_col)) for row in rows)
+        if stop_at_empty_row:
+            # add condition to iterator
+            data_iterator = takewhile(any, data_iterator)
+        return data_iterator, header
+
+
+def get_mapped_data_from_xlsx(filepath):
+    """Returns mapped data from given Excel file assuming it has the default Spine Excel format.
+
+    Args:
+        filepath (str): path to Excel file
+    """
+    connector = ExcelConnector(None)
+    connector.connect_to_source(filepath)
+    mapping = connector.create_default_mapping()
+    table_mappings = {
+        name: m for name, m in mapping.get("table_mappings", {}).items() if name in mapping["selected_tables"]
+    }
+    table_options = {
+        name: options
+        for name, options in mapping.get("table_options", {}).items()
+        if name in mapping["selected_tables"]
+    }
+    table_types = table_row_types = dict.fromkeys(mapping["selected_tables"], {})
+    mapped_data, errors = connector.get_mapped_data(
+        table_mappings, table_options, table_types, {}, table_row_types, max_rows=-1
+    )
+    connector.disconnect()
+    return mapped_data, errors
+
+
+def create_mapping_from_sheet(ws):
+    """
+    Checks if sheet has the default Spine Excel format, if so creates a
+    mapping object for each sheet.
+    """
+    metadata = _get_metadata(ws)
+    if not metadata or "sheet_type" not in metadata:
+        return None, None
+    header_row = len(metadata) + 2
+    if metadata["sheet_type"] == "entity":
+        index_dim_count = metadata.get("index_dim_count")
+        if index_dim_count is not None:
+            index_dim_count = int(index_dim_count)
+        header = _get_header(ws, header_row, index_dim_count)
+        if not header:
+            return None, None
+        return _create_entity_mappings(metadata, header, index_dim_count)
+    options = {"header": True, "row": len(metadata) + 1, "read_until_col": True, "read_until_row": True}
+    if metadata["sheet_type"] == "object_group":
+        map_dict = {"map_type": "ObjectGroup", "name": metadata["class_name"], "groups": 0, "members": 1}
+        return map_dict, options
+    if metadata["sheet_type"] == "alternative":
+        map_dict = {"map_type": "Alternative", "name": 0}
+        return map_dict, options
+    if metadata["sheet_type"] == "scenario":
+        map_dict = {"map_type": "Scenario", "name": 0, "active": 1}
+        return map_dict, options
+    if metadata["sheet_type"] == "scenario_alternative":
+        map_dict = {
+            "map_type": "ScenarioAlternative",
+            "name": 0,
+            "scenario_name": 0,
+            "alternative_name": 1,
+            "before_alternative_name": 2,
+        }
+        return map_dict, options
+    return None, None
+
+
+def _get_metadata(ws):
+    metadata = {}
+    for key, value in ws.iter_rows(min_row=1, max_row=10, max_col=2, values_only=True):
+        if not key:
+            break
+        metadata[key] = value
+    return metadata
+
+
+def _get_header(ws, header_row, index_dim_count):
+    if index_dim_count:
+        # Vertical header
+        header = []
+        header_column = index_dim_count
+        row = header_row
+        while True:
+            label = ws.cell(row=row, column=header_column).value
+            if label == "index":
+                break
+            header.append(label)
+            row += 1
+        return header
+    # Horizontal
+    header = []
+    column = 1
+    while True:
+        label = ws.cell(row=header_row, column=column).value
+        if not label:
+            break
+        header.append(label)
+        column += 1
+    return header
+
+
+def _create_entity_mappings(metadata, header, index_dim_count):
+    class_name = metadata["class_name"]
+    entity_type = metadata["entity_type"]
+    map_dict = {"name": class_name}
+    ent_alt_map_type = "row" if index_dim_count else "column"
+    if entity_type == "object":
+        map_dict["map_type"] = "ObjectClass"
+        map_dict["objects"] = {"map_type": ent_alt_map_type, "reference": 0}
+    elif entity_type == "relationship":
+        entity_dim_count = int(metadata["entity_dim_count"])
+        map_dict["map_type"] = "RelationshipClass"
+        map_dict["object_classes"] = header[:entity_dim_count]
+        map_dict["objects"] = [{"map_type": ent_alt_map_type, "reference": i} for i in range(entity_dim_count)]
+    else:
+        return None, None
+    value_type = metadata.get("value_type")
+    if value_type is not None:
+        value = {"value_type": value_type}
+        if index_dim_count:
+            value["extra_dimensions"] = list(range(index_dim_count))
+        p_ref = len(header) if index_dim_count else -1
+        map_dict["parameters"] = {
+            "map_type": "ParameterValue",
+            "name": {"map_type": "row", "reference": p_ref},
+            "value": value,
+            "alternative_name": {"map_type": ent_alt_map_type, "reference": header.index("alternative")},
+        }
+    options = {
+        "header": not index_dim_count,
+        "row": len(metadata) + 1,
+        "read_until_col": not index_dim_count,
+        "read_until_row": not index_dim_count,
+    }
+    return map_dict, options
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/importers/gdx_connector.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/importers/gdx_connector.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,121 +1,121 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains GDXConnector class and a help function.
-
-"""
-
-from gdx2py import GdxFile, GAMSParameter, GAMSScalar, GAMSSet
-
-from spinedb_api import SpineDBAPIError
-from .reader import SourceConnection
-from ..gdx_utils import find_gams_directory
-
-
-class GdxConnector(SourceConnection):
-    """Template class to read data from another QThread."""
-
-    DISPLAY_NAME = "Gdx"
-    """name of data source"""
-
-    OPTIONS = {}
-    """dict with option specification for source"""
-
-    FILE_EXTENSIONS = "*.gdx"
-    """File extensions for modal widget that returns source object and action (OK, CANCEL)."""
-
-    def __init__(self, settings):
-        """
-        Args:
-            settings (dict): a dict from "gams_directory" to GAMS path; if the argument is None
-                or the path is empty or None, a default path is used
-        """
-        super().__init__(settings)
-        self._filename = None
-        self._gdx_file = None
-        gams_directory = settings.get("gams_directory") if settings is not None else None
-        if gams_directory is not None and gams_directory:
-            self._gams_dir = gams_directory
-        else:
-            self._gams_dir = find_gams_directory()
-
-    def __exit__(self, exc_type, exc_value, traceback):
-        self.disconnect()
-
-    def __del__(self):
-        self.disconnect()
-
-    def connect_to_source(self, source, **extras):
-        """
-        Connects to given .gdx file.
-
-        Args:
-            source (str): path to .gdx file.
-            **extras: ignored
-        """
-        if self._gams_dir is None:
-            raise IOError(f"Could not find GAMS directory. Make sure you have GAMS installed.")
-        self._filename = source
-        self._gdx_file = GdxFile(source, gams_dir=self._gams_dir)
-
-    def disconnect(self):
-        """Disconnects from connected source."""
-        if self._gdx_file is not None:
-            self._gdx_file.close()
-
-    def get_tables(self):
-        """
-        Returns a list of table names.
-
-        GAMS scalars are also regarded as tables.
-
-        Returns:
-            list(str): Table names in list
-        """
-        tables = []
-        for symbol in self._gdx_file:
-            tables.append(symbol[0])
-        return tables
-
-    def get_data_iterator(self, table, options, max_rows=-1):
-        """Creates an iterator for the data source
-
-        Arguments:
-            table (string): table name
-            options (dict): dict with options
-
-        Keyword Arguments:
-            max_rows (int): ignored
-
-        Returns:
-            tuple: data iterator, list of column names, number of columns
-        """
-        if table not in self._gdx_file:
-            return iter([]), []
-        symbol = self._gdx_file[table]
-        if symbol is None:
-            raise SpineDBAPIError(f"the type of '{table}' is not supported.")
-        if isinstance(symbol, GAMSScalar):
-            return iter([[float(symbol)]]), ["Value"]
-        domains = symbol.domain if symbol.domain is not None else symbol.dimension * [None]
-        header = [domain if domain is not None else f"dim{i}" for i, domain in enumerate(domains)]
-        if isinstance(symbol, GAMSSet):
-            if symbol.elements and isinstance(symbol.elements[0], str):
-                return iter([[key] for key in symbol.elements]), header
-            return iter(list(keys) for keys in symbol.elements), header
-        if isinstance(symbol, GAMSParameter):
-            header.append("Value")
-            symbol_keys = list(symbol.keys())
-            if symbol_keys and isinstance(symbol_keys[0], str):
-                return iter([keys] + [value] for keys, value in zip(symbol_keys, symbol.values())), header
-            return iter(list(keys) + [value] for keys, value in zip(symbol_keys, symbol.values())), header
-        raise RuntimeError("Unknown GAMS symbol type.")
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains GDXConnector class and a help function.
+
+"""
+
+from gdx2py import GdxFile, GAMSParameter, GAMSScalar, GAMSSet
+
+from spinedb_api import SpineDBAPIError
+from .reader import SourceConnection
+from ..gdx_utils import find_gams_directory
+
+
+class GdxConnector(SourceConnection):
+    """Template class to read data from another QThread."""
+
+    DISPLAY_NAME = "Gdx"
+    """name of data source"""
+
+    OPTIONS = {}
+    """dict with option specification for source"""
+
+    FILE_EXTENSIONS = "*.gdx"
+    """File extensions for modal widget that returns source object and action (OK, CANCEL)."""
+
+    def __init__(self, settings):
+        """
+        Args:
+            settings (dict): a dict from "gams_directory" to GAMS path; if the argument is None
+                or the path is empty or None, a default path is used
+        """
+        super().__init__(settings)
+        self._filename = None
+        self._gdx_file = None
+        gams_directory = settings.get("gams_directory") if settings is not None else None
+        if gams_directory is not None and gams_directory:
+            self._gams_dir = gams_directory
+        else:
+            self._gams_dir = find_gams_directory()
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        self.disconnect()
+
+    def __del__(self):
+        self.disconnect()
+
+    def connect_to_source(self, source, **extras):
+        """
+        Connects to given .gdx file.
+
+        Args:
+            source (str): path to .gdx file.
+            **extras: ignored
+        """
+        if self._gams_dir is None:
+            raise IOError(f"Could not find GAMS directory. Make sure you have GAMS installed.")
+        self._filename = source
+        self._gdx_file = GdxFile(source, gams_dir=self._gams_dir)
+
+    def disconnect(self):
+        """Disconnects from connected source."""
+        if self._gdx_file is not None:
+            self._gdx_file.close()
+
+    def get_tables(self):
+        """
+        Returns a list of table names.
+
+        GAMS scalars are also regarded as tables.
+
+        Returns:
+            list(str): Table names in list
+        """
+        tables = []
+        for symbol in self._gdx_file:
+            tables.append(symbol[0])
+        return tables
+
+    def get_data_iterator(self, table, options, max_rows=-1):
+        """Creates an iterator for the data source
+
+        Arguments:
+            table (string): table name
+            options (dict): dict with options
+
+        Keyword Arguments:
+            max_rows (int): ignored
+
+        Returns:
+            tuple: data iterator, list of column names, number of columns
+        """
+        if table not in self._gdx_file:
+            return iter([]), []
+        symbol = self._gdx_file[table]
+        if symbol is None:
+            raise SpineDBAPIError(f"the type of '{table}' is not supported.")
+        if isinstance(symbol, GAMSScalar):
+            return iter([[float(symbol)]]), ["Value"]
+        domains = symbol.domain if symbol.domain is not None else symbol.dimension * [None]
+        header = [domain if domain is not None else f"dim{i}" for i, domain in enumerate(domains)]
+        if isinstance(symbol, GAMSSet):
+            if symbol.elements and isinstance(symbol.elements[0], str):
+                return iter([[key] for key in symbol.elements]), header
+            return iter(list(keys) for keys in symbol.elements), header
+        if isinstance(symbol, GAMSParameter):
+            header.append("Value")
+            symbol_keys = list(symbol.keys())
+            if symbol_keys and isinstance(symbol_keys[0], str):
+                return iter([keys] + [value] for keys, value in zip(symbol_keys, symbol.values())), header
+            return iter(list(keys) + [value] for keys, value in zip(symbol_keys, symbol.values())), header
+        raise RuntimeError("Unknown GAMS symbol type.")
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/importers/json_reader.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/importers/json_reader.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,101 +1,101 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains JSONConnector class.
-
-"""
-
-import sys
-import os
-import ijson
-from .reader import SourceConnection
-
-
-class JSONConnector(SourceConnection):
-    """Template class to read data from another QThread."""
-
-    # name of data source, ex: "Text/CSV"
-    DISPLAY_NAME = "JSON"
-
-    # dict with option specification for source.
-    OPTIONS = {"max_depth": {"type": int, "label": "Maximum depth", "default": 8}}
-
-    # File extensions for modal widget that that returns source object and action (OK, CANCEL)
-    FILE_EXTENSIONS = "*.json"
-
-    def __init__(self, settings):
-        super().__init__(settings)
-        self._filename = None
-        self._root_prefix = None
-
-    def connect_to_source(self, source, **extras):
-        """saves filepath
-
-        Args:
-            source (str): filepath
-            **extras: ignored
-        """
-        self._filename = source
-        self._root_prefix = os.path.splitext(os.path.basename(source))[0]
-
-    def disconnect(self):
-        """Disconnect from connected source."""
-
-    def get_tables(self):
-        prefixes = dict()
-        with open(self._filename) as f:
-            for prefix, event, _ in ijson.parse(f):
-                if event in ("start_map", "start_array"):
-                    prefixes[".".join([self._root_prefix, prefix])] = None
-        return [self._root_prefix] + list(prefixes.keys())[1:]
-
-    def file_iterator(self, table, options, max_rows=-1):
-        if max_rows == -1:
-            max_rows = sys.maxsize
-        max_depth = options.get("max_depth", self.OPTIONS["max_depth"]["default"])
-        prefix = ".".join(table.split(".")[1:])
-        with open(self._filename, "rb") as f:
-            row = 0
-            for obj in ijson.items(f, prefix):
-                for x in _tabulize_json(obj):
-                    if row > max_rows:
-                        return
-                    yield x[:max_depth]
-                    row += 1
-
-    def get_data_iterator(self, table, options, max_rows=-1):
-        """
-        Returns data read from data source table in table. If max_rows is
-        specified only that number of rows.
-        """
-        return self.file_iterator(table, options, max_rows=max_rows), []
-
-
-def _tabulize_json(obj):
-    if isinstance(obj, dict):
-        yield from _tabulize_json_object(obj)
-    elif isinstance(obj, list):
-        yield from _tabulize_json_array(obj)
-    else:
-        yield [obj]
-
-
-def _tabulize_json_object(obj):
-    for key, value in obj.items():
-        for x in _tabulize_json(value):
-            yield [key] + x
-
-
-def _tabulize_json_array(arr):
-    for i, item in enumerate(arr):
-        for x in _tabulize_json(item):
-            yield [i] + x
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains JSONConnector class.
+
+"""
+
+import sys
+import os
+import ijson
+from .reader import SourceConnection
+
+
+class JSONConnector(SourceConnection):
+    """Template class to read data from another QThread."""
+
+    # name of data source, ex: "Text/CSV"
+    DISPLAY_NAME = "JSON"
+
+    # dict with option specification for source.
+    OPTIONS = {"max_depth": {"type": int, "label": "Maximum depth", "default": 8}}
+
+    # File extensions for modal widget that that returns source object and action (OK, CANCEL)
+    FILE_EXTENSIONS = "*.json"
+
+    def __init__(self, settings):
+        super().__init__(settings)
+        self._filename = None
+        self._root_prefix = None
+
+    def connect_to_source(self, source, **extras):
+        """saves filepath
+
+        Args:
+            source (str): filepath
+            **extras: ignored
+        """
+        self._filename = source
+        self._root_prefix = os.path.splitext(os.path.basename(source))[0]
+
+    def disconnect(self):
+        """Disconnect from connected source."""
+
+    def get_tables(self):
+        prefixes = dict()
+        with open(self._filename) as f:
+            for prefix, event, _ in ijson.parse(f):
+                if event in ("start_map", "start_array"):
+                    prefixes[".".join([self._root_prefix, prefix])] = None
+        return [self._root_prefix] + list(prefixes.keys())[1:]
+
+    def file_iterator(self, table, options, max_rows=-1):
+        if max_rows == -1:
+            max_rows = sys.maxsize
+        max_depth = options.get("max_depth", self.OPTIONS["max_depth"]["default"])
+        prefix = ".".join(table.split(".")[1:])
+        with open(self._filename, "rb") as f:
+            row = 0
+            for obj in ijson.items(f, prefix):
+                for x in _tabulize_json(obj):
+                    if row > max_rows:
+                        return
+                    yield x[:max_depth]
+                    row += 1
+
+    def get_data_iterator(self, table, options, max_rows=-1):
+        """
+        Returns data read from data source table in table. If max_rows is
+        specified only that number of rows.
+        """
+        return self.file_iterator(table, options, max_rows=max_rows), []
+
+
+def _tabulize_json(obj):
+    if isinstance(obj, dict):
+        yield from _tabulize_json_object(obj)
+    elif isinstance(obj, list):
+        yield from _tabulize_json_array(obj)
+    else:
+        yield [obj]
+
+
+def _tabulize_json_object(obj):
+    for key, value in obj.items():
+        for x in _tabulize_json(value):
+            yield [key] + x
+
+
+def _tabulize_json_array(arr):
+    for i, item in enumerate(arr):
+        for x in _tabulize_json(item):
+            yield [i] + x
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/importers/reader.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/importers/reader.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,159 +1,159 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains a class template for a data source connector used in import ui.
-
-"""
-
-from itertools import islice
-from spinedb_api.import_mapping.generator import get_mapped_data, identity
-from spinedb_api.import_mapping.import_mapping_compat import parse_named_mapping_spec
-from spinedb_api import DateTime, Duration, ParameterValueFormatError
-
-TYPE_STRING_TO_CLASS = {"string": str, "datetime": DateTime, "duration": Duration, "float": float, "boolean": bool}
-
-TYPE_CLASS_TO_STRING = {type_class: string for string, type_class in TYPE_STRING_TO_CLASS.items()}
-
-
-class SourceConnection:
-    """Template class to read data from another QThread."""
-
-    # name of data source, ex: "Text/CSV"
-    DISPLAY_NAME = "unnamed source"
-
-    # dict with option specification for source.
-    OPTIONS = {}
-    BASE_OPTIONS = {
-        "max_rows": {
-            "type": int,
-            "label": "Max rows",
-            "Minimum": -1,
-            "Maximum": 16777215,
-            "SpecialValueText": "unrestricted",
-            "default": -1,
-        }
-    }
-
-    # File extensions for modal widget that that returns action (OK, CANCEL) and source object
-    FILE_EXTENSIONS = NotImplemented
-
-    def __init__(self, settings):
-        """
-        Args:
-            settings (dict, optional): connector specific settings or None
-        """
-
-    def connect_to_source(self, source, **extras):
-        """Connects to source, ex: connecting to a database where source is a connection string.
-
-        Args:
-            source (str): file path or URL to connect to
-            **extras: additional source specific connection data
-        """
-        raise NotImplementedError()
-
-    def disconnect(self):
-        """Disconnect from connected source."""
-        raise NotImplementedError()
-
-    def get_tables(self):
-        """Method that should return a list of table names, list(str)
-
-        Raises:
-            NotImplementedError: [description]
-        """
-        raise NotImplementedError()
-
-    def get_data_iterator(self, table, options, max_rows=-1):
-        """
-        Function that should return a data iterator and data header.
-        """
-        raise NotImplementedError()
-
-    @staticmethod
-    def _resolve_max_rows(options, max_rows=-1):
-        options_max_rows = options.get("max_rows", -1)
-        if options_max_rows == -1:
-            return max_rows
-        if max_rows == -1:
-            return options_max_rows
-        return min(max_rows, options_max_rows)
-
-    def get_data(self, table, options, max_rows=-1, start=0):
-        """
-        Return data read from data source table in table. If max_rows is
-        specified only that number of rows.
-        """
-        max_rows = self._resolve_max_rows(options, max_rows)
-        data_iter, header = self.get_data_iterator(table, options, max_rows)
-        data_iter = islice(data_iter, start, None)
-        data = list(data_iter)
-        return data, header
-
-    def get_mapped_data(
-        self,
-        tables_mappings,
-        table_options,
-        table_column_convert_specs,
-        table_default_column_convert_fns,
-        table_row_convert_specs,
-        unparse_value=identity,
-        max_rows=-1,
-    ):
-        """
-        Reads all mappings in dict tables_mappings, where key is name of table
-        and value is the mappings for that table.
-
-        Args:
-            tables_mappings (dict): mapping from table name to list of import mappings
-            table_options (dict): mapping from table name to table-specific import options
-            table_column_convert_specs (dict): mapping from table name to column data type conversion settings
-            table_default_column_convert_fns (dict): mapping from table name to
-                default column data type converter
-            table_row_convert_specs (dict): mapping from table name to row data type conversion settings
-            unparse_value (Callable): callable that converts imported values to database representation
-            max_rows (int): maximum number of source rows to map
-
-        Returns:
-            tuple: mapped data and a list of errors, if any
-        """
-        mapped_data = {}
-        errors = []
-        for table, named_mapping_specs in tables_mappings.items():
-            column_convert_fns = table_column_convert_specs.get(table, {})
-            default_column_convert_fn = table_default_column_convert_fns.get(table)
-            row_convert_fns = table_row_convert_specs.get(table, {})
-            options = table_options.get(table, {})
-            table_max_rows = self._resolve_max_rows(options, max_rows)
-            data_source, header = self.get_data_iterator(table, options, table_max_rows)
-            mappings = []
-            for named_mapping_spec in named_mapping_specs:
-                _, mapping = parse_named_mapping_spec(named_mapping_spec)
-                mappings.append(mapping)
-            try:
-                data, t_errors = get_mapped_data(
-                    data_source,
-                    mappings,
-                    header,
-                    table,
-                    column_convert_fns,
-                    default_column_convert_fn,
-                    row_convert_fns,
-                    unparse_value,
-                )
-            except ParameterValueFormatError as error:
-                errors.append(str(error))
-                continue
-            for key, value in data.items():
-                mapped_data.setdefault(key, []).extend(value)
-            errors.extend([(table, err) for err in t_errors])
-        return mapped_data, errors
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains a class template for a data source connector used in import ui.
+
+"""
+
+from itertools import islice
+from spinedb_api.import_mapping.generator import get_mapped_data, identity
+from spinedb_api.import_mapping.import_mapping_compat import parse_named_mapping_spec
+from spinedb_api import DateTime, Duration, ParameterValueFormatError
+
+TYPE_STRING_TO_CLASS = {"string": str, "datetime": DateTime, "duration": Duration, "float": float, "boolean": bool}
+
+TYPE_CLASS_TO_STRING = {type_class: string for string, type_class in TYPE_STRING_TO_CLASS.items()}
+
+
+class SourceConnection:
+    """Template class to read data from another QThread."""
+
+    # name of data source, ex: "Text/CSV"
+    DISPLAY_NAME = "unnamed source"
+
+    # dict with option specification for source.
+    OPTIONS = {}
+    BASE_OPTIONS = {
+        "max_rows": {
+            "type": int,
+            "label": "Max rows",
+            "Minimum": -1,
+            "Maximum": 16777215,
+            "SpecialValueText": "unrestricted",
+            "default": -1,
+        }
+    }
+
+    # File extensions for modal widget that that returns action (OK, CANCEL) and source object
+    FILE_EXTENSIONS = NotImplemented
+
+    def __init__(self, settings):
+        """
+        Args:
+            settings (dict, optional): connector specific settings or None
+        """
+
+    def connect_to_source(self, source, **extras):
+        """Connects to source, ex: connecting to a database where source is a connection string.
+
+        Args:
+            source (str): file path or URL to connect to
+            **extras: additional source specific connection data
+        """
+        raise NotImplementedError()
+
+    def disconnect(self):
+        """Disconnect from connected source."""
+        raise NotImplementedError()
+
+    def get_tables(self):
+        """Method that should return a list of table names, list(str)
+
+        Raises:
+            NotImplementedError: [description]
+        """
+        raise NotImplementedError()
+
+    def get_data_iterator(self, table, options, max_rows=-1):
+        """
+        Function that should return a data iterator and data header.
+        """
+        raise NotImplementedError()
+
+    @staticmethod
+    def _resolve_max_rows(options, max_rows=-1):
+        options_max_rows = options.get("max_rows", -1)
+        if options_max_rows == -1:
+            return max_rows
+        if max_rows == -1:
+            return options_max_rows
+        return min(max_rows, options_max_rows)
+
+    def get_data(self, table, options, max_rows=-1, start=0):
+        """
+        Return data read from data source table in table. If max_rows is
+        specified only that number of rows.
+        """
+        max_rows = self._resolve_max_rows(options, max_rows)
+        data_iter, header = self.get_data_iterator(table, options, max_rows)
+        data_iter = islice(data_iter, start, None)
+        data = list(data_iter)
+        return data, header
+
+    def get_mapped_data(
+        self,
+        tables_mappings,
+        table_options,
+        table_column_convert_specs,
+        table_default_column_convert_fns,
+        table_row_convert_specs,
+        unparse_value=identity,
+        max_rows=-1,
+    ):
+        """
+        Reads all mappings in dict tables_mappings, where key is name of table
+        and value is the mappings for that table.
+
+        Args:
+            tables_mappings (dict): mapping from table name to list of import mappings
+            table_options (dict): mapping from table name to table-specific import options
+            table_column_convert_specs (dict): mapping from table name to column data type conversion settings
+            table_default_column_convert_fns (dict): mapping from table name to
+                default column data type converter
+            table_row_convert_specs (dict): mapping from table name to row data type conversion settings
+            unparse_value (Callable): callable that converts imported values to database representation
+            max_rows (int): maximum number of source rows to map
+
+        Returns:
+            tuple: mapped data and a list of errors, if any
+        """
+        mapped_data = {}
+        errors = []
+        for table, named_mapping_specs in tables_mappings.items():
+            column_convert_fns = table_column_convert_specs.get(table, {})
+            default_column_convert_fn = table_default_column_convert_fns.get(table)
+            row_convert_fns = table_row_convert_specs.get(table, {})
+            options = table_options.get(table, {})
+            table_max_rows = self._resolve_max_rows(options, max_rows)
+            data_source, header = self.get_data_iterator(table, options, table_max_rows)
+            mappings = []
+            for named_mapping_spec in named_mapping_specs:
+                _, mapping = parse_named_mapping_spec(named_mapping_spec)
+                mappings.append(mapping)
+            try:
+                data, t_errors = get_mapped_data(
+                    data_source,
+                    mappings,
+                    header,
+                    table,
+                    column_convert_fns,
+                    default_column_convert_fn,
+                    row_convert_fns,
+                    unparse_value,
+                )
+            except ParameterValueFormatError as error:
+                errors.append(str(error))
+                continue
+            for key, value in data.items():
+                mapped_data.setdefault(key, []).extend(value)
+            errors.extend([(table, err) for err in t_errors])
+        return mapped_data, errors
```

### Comparing `spinedb_api-0.30.3/spinedb_api/spine_io/importers/sqlalchemy_connector.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/importers/sqlalchemy_connector.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,94 +1,94 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-""" Contains SqlAlchemyConnector class. """
-
-
-from sqlalchemy import create_engine, MetaData
-from sqlalchemy.orm import Session
-from .reader import SourceConnection
-
-
-class SqlAlchemyConnector(SourceConnection):
-    """Template class to read data from another QThread."""
-
-    DISPLAY_NAME = "SqlAlchemy"
-    """name of data source"""
-
-    OPTIONS = {}
-    """dict with option specification for source."""
-
-    FILE_EXTENSIONS = "*.sqlite"
-
-    def __init__(self, settings):
-        super().__init__(settings)
-        self._connection_string = None
-        self._engine = None
-        self._connection = None
-        self._session = None
-        self._schema = None
-        self._metadata = None
-
-    def connect_to_source(self, source, **extras):
-        """Saves source.
-
-        Args:
-            source (str): url
-            **extras: optional database schema
-        """
-        self._connection_string = source
-        self._engine = create_engine(source)
-        self._connection = self._engine.connect()
-        self._session = Session(self._engine)
-        self._schema = extras.get("schema")
-        self._metadata = MetaData(schema=self._schema)
-        self._metadata.reflect(bind=self._engine)
-
-    def disconnect(self):
-        """Disconnect from connected source."""
-        self._metadata = None
-        self._schema = None
-        self._session.close()
-        self._session = None
-        self._connection.close()
-        self._connection_string = None
-        self._engine = None
-        self._connection = None
-
-    def get_tables(self):
-        """Method that should return a list of table names, list(str)
-
-        Returns:
-            list of str: Table names in list
-        """
-        tables = list(self._engine.table_names(schema=self._schema))
-        return tables
-
-    def get_data_iterator(self, table, options, max_rows=-1):
-        """Creates an iterator for the database connection.
-
-        Args:
-            table (str): table name
-            options (dict): dict with options, not used
-            max_rows (int): how many rows of data to read, if -1 read all rows (default: {-1})
-
-        Returns:
-            tuple: iterator, header, column count
-        """
-        if self._schema is not None:
-            table = self._schema + "." + table
-        db_table = self._metadata.tables[table]
-        header = [str(name) for name in db_table.columns.keys()]
-
-        query = self._session.query(db_table)
-        if max_rows > 0:
-            query = query.limit(max_rows)
-
-        return query, header
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+""" Contains SqlAlchemyConnector class. """
+
+
+from sqlalchemy import create_engine, MetaData
+from sqlalchemy.orm import Session
+from .reader import SourceConnection
+
+
+class SqlAlchemyConnector(SourceConnection):
+    """Template class to read data from another QThread."""
+
+    DISPLAY_NAME = "SqlAlchemy"
+    """name of data source"""
+
+    OPTIONS = {}
+    """dict with option specification for source."""
+
+    FILE_EXTENSIONS = "*.sqlite"
+
+    def __init__(self, settings):
+        super().__init__(settings)
+        self._connection_string = None
+        self._engine = None
+        self._connection = None
+        self._session = None
+        self._schema = None
+        self._metadata = None
+
+    def connect_to_source(self, source, **extras):
+        """Saves source.
+
+        Args:
+            source (str): url
+            **extras: optional database schema
+        """
+        self._connection_string = source
+        self._engine = create_engine(source)
+        self._connection = self._engine.connect()
+        self._session = Session(self._engine)
+        self._schema = extras.get("schema")
+        self._metadata = MetaData(schema=self._schema)
+        self._metadata.reflect(bind=self._engine)
+
+    def disconnect(self):
+        """Disconnect from connected source."""
+        self._metadata = None
+        self._schema = None
+        self._session.close()
+        self._session = None
+        self._connection.close()
+        self._connection_string = None
+        self._engine = None
+        self._connection = None
+
+    def get_tables(self):
+        """Method that should return a list of table names, list(str)
+
+        Returns:
+            list of str: Table names in list
+        """
+        tables = list(self._engine.table_names(schema=self._schema))
+        return tables
+
+    def get_data_iterator(self, table, options, max_rows=-1):
+        """Creates an iterator for the database connection.
+
+        Args:
+            table (str): table name
+            options (dict): dict with options, not used
+            max_rows (int): how many rows of data to read, if -1 read all rows (default: {-1})
+
+        Returns:
+            tuple: iterator, header, column count
+        """
+        if self._schema is not None:
+            table = self._schema + "." + table
+        db_table = self._metadata.tables[table]
+        header = [str(name) for name in db_table.columns.keys()]
+
+        query = self._session.query(db_table)
+        if max_rows > 0:
+            query = query.limit(max_rows)
+
+        return query, header
```

### Comparing `spinedb_api-0.30.3/spinedb_api.egg-info/PKG-INFO` & `spinedb_api-0.30.4/spinedb_api.egg-info/PKG-INFO`

 * *Files 21% similar despite different names*

```diff
@@ -1,104 +1,104 @@
-Metadata-Version: 2.1
-Name: spinedb-api
-Version: 0.30.3
-Summary: An API to talk to Spine databases.
-Author-email: Spine Project consortium <spine_info@vtt.fi>
-License: LGPL-3.0-or-later
-Project-URL: Repository, https://github.com/spine-tools/Spine-Database-API
-Keywords: energy system modelling,workflow,optimisation,database
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
-Classifier: Operating System :: OS Independent
-Requires-Python: <3.12,>=3.8.1
-Description-Content-Type: text/markdown
-License-File: COPYING
-License-File: COPYING.LESSER
-Requires-Dist: sqlalchemy<1.4,>=1.3
-Requires-Dist: alembic>=1.7
-Requires-Dist: faker>=8.1.2
-Requires-Dist: datapackage>=1.15.2
-Requires-Dist: python-dateutil>=2.8.1
-Requires-Dist: numpy>=1.20.2
-Requires-Dist: scipy>=1.7.1
-Requires-Dist: openpyxl!=3.1.1,>=3.0.7
-Requires-Dist: gdx2py>=2.1.1
-Requires-Dist: ijson>=3.1.4
-Requires-Dist: chardet>=4.0.0
-Requires-Dist: pymysql>=1.0.2
-Requires-Dist: psycopg2
-Requires-Dist: cx_Oracle
-Provides-Extra: dev
-Requires-Dist: coverage[toml]; extra == "dev"
-
-# Spine Database API
-
-[![Documentation Status](https://readthedocs.org/projects/spine-database-api/badge/?version=latest)](https://spine-database-api.readthedocs.io/en/latest/?badge=latest)
-[![Unit tests](https://github.com/spine-tools/Spine-Database-API/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/Spine-Database-API/actions?query=workflow%3A"Unit+tests")
-[![codecov](https://codecov.io/gh/spine-tools/Spine-Database-API/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/Spine-Database-API)
-[![PyPI version](https://badge.fury.io/py/spinedb-api.svg)](https://badge.fury.io/py/spinedb-api)
-
-A Python package to access and manipulate Spine databases in a customary, unified way.
-
-## License
-
-Spine Database API is released under the GNU Lesser General Public License (LGPL) license. All accompanying
-documentation and manual are released under the Creative Commons BY-SA 4.0 license.
-
-## Getting started
-
-### Installation
-
-To install the package run:
-
-    $ pip install spinedb_api
-
-To upgrade to the most recent version, run:
-
-    $ pip install --upgrade spinedb_api
-
-You can also specify a branch, or a tag, for instance:
-
-    $ pip install spinedb_api==0.12.1
-
-To install the latest development version use the Git repository url:
-
-    $ pip install --upgrade git+https://github.com/spine-tools/Spine-Database-API.git
-
-
-## Building the documentation
-
-Source files for the documentation can be found in `docs/source` directory. In order to 
-build the HTML docs, you need to install the additional documentation building requirements
-by running:
-
-    $ pip install -r dev-requirements.txt 
-
-This installs Sphinx (among other things), which is required in building the documentation.
-When Sphinx is installed, you can build the HTML pages from the source files by running:
-
-    > docs\make.bat html
-    
-or
-
-    $ pushd docs
-    $ make html
-    $ popd
-    
-depending on your operating system.        
- 
-After running the build, the index page can be found in `docs/build/html/index.html`.
-
-&nbsp;
-<hr>
-<center>
-<table width=500px frame="none">
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
-<tr>
-<td valign="middle" width=100px>
-<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
-<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
-</table>
-</center>
+Metadata-Version: 2.1
+Name: spinedb_api
+Version: 0.30.4
+Summary: An API to talk to Spine databases.
+Author-email: Spine Project consortium <spine_info@vtt.fi>
+License: LGPL-3.0-or-later
+Project-URL: Repository, https://github.com/spine-tools/Spine-Database-API
+Keywords: energy system modelling,workflow,optimisation,database
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
+Classifier: Operating System :: OS Independent
+Requires-Python: <3.12,>=3.8.1
+Description-Content-Type: text/markdown
+License-File: COPYING
+License-File: COPYING.LESSER
+Requires-Dist: sqlalchemy<1.4,>=1.3
+Requires-Dist: alembic>=1.7
+Requires-Dist: faker>=8.1.2
+Requires-Dist: datapackage>=1.15.2
+Requires-Dist: python-dateutil>=2.8.1
+Requires-Dist: numpy>=1.20.2
+Requires-Dist: scipy>=1.7.1
+Requires-Dist: openpyxl!=3.1.1,>=3.0.7
+Requires-Dist: gdx2py>=2.1.1
+Requires-Dist: ijson>=3.1.4
+Requires-Dist: chardet>=4.0.0
+Requires-Dist: pymysql>=1.0.2
+Requires-Dist: psycopg2
+Requires-Dist: cx_Oracle
+Provides-Extra: dev
+Requires-Dist: coverage[toml]; extra == "dev"
+
+# Spine Database API
+
+[![Documentation Status](https://readthedocs.org/projects/spine-database-api/badge/?version=latest)](https://spine-database-api.readthedocs.io/en/latest/?badge=latest)
+[![Unit tests](https://github.com/spine-tools/Spine-Database-API/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/Spine-Database-API/actions?query=workflow%3A"Unit+tests")
+[![codecov](https://codecov.io/gh/spine-tools/Spine-Database-API/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/Spine-Database-API)
+[![PyPI version](https://badge.fury.io/py/spinedb-api.svg)](https://badge.fury.io/py/spinedb-api)
+
+A Python package to access and manipulate Spine databases in a customary, unified way.
+
+## License
+
+Spine Database API is released under the GNU Lesser General Public License (LGPL) license. All accompanying
+documentation and manual are released under the Creative Commons BY-SA 4.0 license.
+
+## Getting started
+
+### Installation
+
+To install the package run:
+
+    $ pip install spinedb_api
+
+To upgrade to the most recent version, run:
+
+    $ pip install --upgrade spinedb_api
+
+You can also specify a branch, or a tag, for instance:
+
+    $ pip install spinedb_api==0.12.1
+
+To install the latest development version use the Git repository url:
+
+    $ pip install --upgrade git+https://github.com/spine-tools/Spine-Database-API.git
+
+
+## Building the documentation
+
+Source files for the documentation can be found in `docs/source` directory. In order to 
+build the HTML docs, you need to install the additional documentation building requirements
+by running:
+
+    $ pip install -r dev-requirements.txt 
+
+This installs Sphinx (among other things), which is required in building the documentation.
+When Sphinx is installed, you can build the HTML pages from the source files by running:
+
+    > docs\make.bat html
+    
+or
+
+    $ pushd docs
+    $ make html
+    $ popd
+    
+depending on your operating system.        
+ 
+After running the build, the index page can be found in `docs/build/html/index.html`.
+
+&nbsp;
+<hr>
+<center>
+<table width=500px frame="none">
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from European Climate, Infrastructure and Environment Executive Agency under the European Union’s HORIZON Research and Innovation Actions under grant agreement N°101095998.</td>
+<tr>
+<td valign="middle" width=100px>
+<img src=fig/eu-emblem-low-res.jpg alt="EU emblem" width=100%></td>
+<td valign="middle">This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 774629.</td>
+</table>
+</center>
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: spinedb-api Version: 0.30.3 Summary: An API to talk
+Metadata-Version: 2.1 Name: spinedb_api Version: 0.30.4 Summary: An API to talk
 to Spine databases. Author-email: Spine Project consortium
 vtt.fi> License: LGPL-3.0-or-later Project-URL: Repository, https://github.com/
 spine-tools/Spine-Database-API Keywords: energy system
 modelling,workflow,optimisation,database Classifier: Programming Language ::
 Python :: 3 Classifier: License :: OSI Approved :: GNU Lesser General Public
 License v3 (LGPLv3) Classifier: Operating System :: OS Independent Requires-
 Python: <3.12,>=3.8.1 Description-Content-Type: text/markdown License-File:
```

### Comparing `spinedb_api-0.30.3/spinedb_api.egg-info/SOURCES.txt` & `spinedb_api-0.30.4/spinedb_api.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.3/tests/__init__.py` & `spinedb_api-0.30.4/tests/spine_io/importers/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests package for :mod:`spinedb_api`.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Init file for tests.spine_io.importers package. Intentionally empty.
+
+"""
```

### Comparing `spinedb_api-0.30.3/tests/export_mapping/__init__.py` & `spinedb_api-0.30.4/spinedb_api/spine_io/importers/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,15 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Intentionally empty.
+
+"""
```

### Comparing `spinedb_api-0.30.3/tests/export_mapping/test_export_mapping.py` & `spinedb_api-0.30.4/tests/export_mapping/test_export_mapping.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,1667 +1,1667 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Unit tests for export mappings.
-
-"""
-
-import unittest
-from spinedb_api import (
-    DatabaseMapping,
-    DiffDatabaseMapping,
-    import_alternatives,
-    import_features,
-    import_object_classes,
-    import_object_parameter_values,
-    import_object_parameters,
-    import_objects,
-    import_parameter_value_lists,
-    import_relationship_classes,
-    import_relationships,
-    import_scenario_alternatives,
-    import_scenarios,
-    import_tool_features,
-    import_tool_feature_methods,
-    import_tools,
-    Map,
-)
-from spinedb_api.import_functions import import_object_groups
-from spinedb_api.mapping import Position, to_dict
-from spinedb_api.export_mapping import (
-    rows,
-    titles,
-    object_parameter_default_value_export,
-    object_parameter_export,
-    relationship_export,
-    relationship_parameter_export,
-)
-from spinedb_api.export_mapping.export_mapping import (
-    AlternativeMapping,
-    drop_non_positioned_tail,
-    FixedValueMapping,
-    ExpandedParameterValueMapping,
-    ExpandedParameterDefaultValueMapping,
-    FeatureEntityClassMapping,
-    FeatureParameterDefinitionMapping,
-    from_dict,
-    ObjectGroupMapping,
-    ObjectGroupObjectMapping,
-    ObjectMapping,
-    ObjectClassMapping,
-    ParameterDefaultValueMapping,
-    ParameterDefaultValueIndexMapping,
-    ParameterDefinitionMapping,
-    ParameterValueIndexMapping,
-    ParameterValueListMapping,
-    ParameterValueListValueMapping,
-    ParameterValueMapping,
-    ParameterValueTypeMapping,
-    RelationshipClassMapping,
-    RelationshipClassObjectClassMapping,
-    RelationshipMapping,
-    RelationshipObjectMapping,
-    ScenarioActiveFlagMapping,
-    ScenarioAlternativeMapping,
-    ScenarioMapping,
-    ToolMapping,
-    ToolFeatureEntityClassMapping,
-    ToolFeatureParameterDefinitionMapping,
-    ToolFeatureRequiredFlagMapping,
-    ToolFeatureMethodEntityClassMapping,
-    ToolFeatureMethodMethodMapping,
-    ToolFeatureMethodParameterDefinitionMapping,
-)
-from spinedb_api.mapping import unflatten
-
-
-class TestExportMapping(unittest.TestCase):
-    def test_export_empty_table(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        object_class_mapping = ObjectClassMapping(0)
-        self.assertEqual(list(rows(object_class_mapping, db_map)), [])
-        db_map.connection.close()
-
-    def test_export_single_object_class(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("object_class",))
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        self.assertEqual(list(rows(object_class_mapping, db_map)), [["object_class"]])
-        db_map.connection.close()
-
-    def test_export_objects(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_objects(
-            db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31"), ("oc3", "o32"), ("oc3", "o33"))
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        object_class_mapping.child = ObjectMapping(1)
-        self.assertEqual(
-            list(rows(object_class_mapping, db_map)),
-            [["oc1", "o11"], ["oc1", "o12"], ["oc2", "o21"], ["oc3", "o31"], ["oc3", "o32"], ["oc3", "o33"]],
-        )
-        db_map.connection.close()
-
-    def test_hidden_tail(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1",))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        object_class_mapping.child = ObjectMapping(Position.hidden)
-        self.assertEqual(list(rows(object_class_mapping, db_map)), [["oc1"], ["oc1"]])
-        db_map.connection.close()
-
-    def test_pivot_without_values(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1",))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(-1)
-        object_class_mapping.child = ObjectMapping(Position.hidden)
-        self.assertEqual(list(rows(object_class_mapping, db_map)), [])
-        db_map.connection.close()
-
-    def test_hidden_tail_pivoted(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p1"), ("oc", "p2")))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(db_map, (("oc", "o1", "p1", -11.0), ("oc", "o1", "p2", -12.0)))
-        db_map.commit_session("Add test data.")
-        root_mapping = unflatten(
-            [
-                ObjectClassMapping(0),
-                ParameterDefinitionMapping(-1),
-                ObjectMapping(1),
-                AlternativeMapping(2),
-                ParameterValueMapping(Position.hidden),
-            ]
-        )
-        expected = [[None, None, "p1", "p2"], ["oc", "o1", "Base", "Base"]]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_hidden_leaf_item_in_regular_table_valid(self):
-        object_class_mapping = ObjectClassMapping(0)
-        object_class_mapping.child = ObjectMapping(Position.hidden)
-        self.assertEqual(object_class_mapping.check_validity(), [])
-
-    def test_hidden_leaf_item_in_pivot_table_not_valid(self):
-        object_class_mapping = ObjectClassMapping(-1)
-        object_class_mapping.child = ObjectMapping(Position.hidden)
-        self.assertEqual(object_class_mapping.check_validity(), ["Cannot be pivoted."])
-
-    def test_object_groups(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2"), ("oc", "o3"), ("oc", "g1"), ("oc", "g2")))
-        import_object_groups(db_map, (("oc", "g1", "o1"), ("oc", "g1", "o2"), ("oc", "g2", "o3")))
-        db_map.commit_session("Add test data.")
-        flattened = [ObjectClassMapping(0), ObjectGroupMapping(1)]
-        mapping = unflatten(flattened)
-        self.assertEqual(list(rows(mapping, db_map)), [["oc", "g1"], ["oc", "g2"]])
-        db_map.connection.close()
-
-    def test_object_groups_with_objects(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2"), ("oc", "o3"), ("oc", "g1"), ("oc", "g2")))
-        import_object_groups(db_map, (("oc", "g1", "o1"), ("oc", "g1", "o2"), ("oc", "g2", "o3")))
-        db_map.commit_session("Add test data.")
-        flattened = [ObjectClassMapping(0), ObjectGroupMapping(1), ObjectGroupObjectMapping(2)]
-        mapping = unflatten(flattened)
-        self.assertEqual(list(rows(mapping, db_map)), [["oc", "g1", "o1"], ["oc", "g1", "o2"], ["oc", "g2", "o3"]])
-        db_map.connection.close()
-
-    def test_object_groups_with_parameter_values(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2"), ("oc", "o3"), ("oc", "g1"), ("oc", "g2")))
-        import_object_groups(db_map, (("oc", "g1", "o1"), ("oc", "g1", "o2"), ("oc", "g2", "o3")))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_object_parameter_values(
-            db_map, (("oc", "o1", "p", -11.0), ("oc", "o2", "p", -12.0), ("oc", "o3", "p", -13.0))
-        )
-        db_map.commit_session("Add test data.")
-        flattened = [
-            ObjectClassMapping(0),
-            ObjectGroupMapping(1),
-            ObjectGroupObjectMapping(2),
-            ParameterDefinitionMapping(Position.hidden),
-            AlternativeMapping(Position.hidden),
-            ParameterValueMapping(3),
-        ]
-        mapping = unflatten(flattened)
-        self.assertEqual(
-            list(rows(mapping, db_map)),
-            [["oc", "g1", "o1", -11.0], ["oc", "g1", "o2", -12.0], ["oc", "g2", "o3", -13.0]],
-        )
-        db_map.connection.close()
-
-    def test_export_parameter_definitions(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        parameter_definition_mapping = ParameterDefinitionMapping(1)
-        parameter_definition_mapping.child = ObjectMapping(2)
-        object_class_mapping.child = parameter_definition_mapping
-        expected = [
-            ["oc1", "p11", "o11"],
-            ["oc1", "p11", "o12"],
-            ["oc1", "p12", "o11"],
-            ["oc1", "p12", "o12"],
-            ["oc2", "p21", "o21"],
-        ]
-        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_export_single_parameter_value_when_there_are_multiple_objects(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
-        import_object_parameter_values(db_map, (("oc1", "o11", "p12", -11.0),))
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        parameter_definition_mapping = ParameterDefinitionMapping(1)
-        alternative_mapping = AlternativeMapping(Position.hidden)
-        parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(2)
-        alternative_mapping.child = object_mapping
-        value_mapping = ParameterValueMapping(3)
-        object_mapping.child = value_mapping
-        object_class_mapping.child = parameter_definition_mapping
-        self.assertEqual(list(rows(object_class_mapping, db_map)), [["oc1", "p12", "o11", -11.0]])
-        db_map.connection.close()
-
-    def test_export_single_parameter_value_pivoted_by_object_name(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1",))
-        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12")))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc1", "o11", "p11", -11.0),
-                ("oc1", "o11", "p12", -12.0),
-                ("oc1", "o12", "p11", -21.0),
-                ("oc1", "o12", "p12", -22.0),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        parameter_definition_mapping = ParameterDefinitionMapping(1)
-        alternative_mapping = AlternativeMapping(Position.hidden)
-        parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(-1)
-        alternative_mapping.child = object_mapping
-        value_mapping = ParameterValueMapping(-2)
-        object_mapping.child = value_mapping
-        object_class_mapping.child = parameter_definition_mapping
-        expected = [[None, None, "o11", "o12"], ["oc1", "p11", -11.0, -21.0], ["oc1", "p12", -12.0, -22.0]]
-        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_minimum_pivot_index_need_not_be_minus_one(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_alternatives(db_map, ("alt",))
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o"),))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o", "p", Map(["A", "B"], [-1.1, -2.2]), "Base"),
-                ("oc", "o", "p", Map(["A", "B"], [-5.5, -6.6]), "alt"),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(1, 2, Position.hidden, 0, -2, Position.hidden, 4, [Position.hidden], [3])
-        expected = [
-            [None, None, None, None, "Base", "alt"],
-            ["o", "oc", "p", "A", -1.1, -5.5],
-            ["o", "oc", "p", "B", -2.2, -6.6],
-        ]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_pivot_row_order(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1",))
-        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12")))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc1", "o11", "p11", -11.0),
-                ("oc1", "o11", "p12", -12.0),
-                ("oc1", "o12", "p11", -21.0),
-                ("oc1", "o12", "p12", -22.0),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        parameter_definition_mapping = ParameterDefinitionMapping(-1)
-        alternative_mapping = AlternativeMapping(Position.hidden)
-        object_mapping = ObjectMapping(-2)
-        value_mapping = ParameterValueMapping(3)
-        mappings = [
-            object_class_mapping,
-            parameter_definition_mapping,
-            alternative_mapping,
-            object_mapping,
-            value_mapping,
-        ]
-        root = unflatten(mappings)
-        expected = [
-            [None, "p11", "p11", "p12", "p12"],
-            [None, "o11", "o12", "o11", "o12"],
-            ["oc1", -11.0, -21.0, -12.0, -22.0],
-        ]
-        self.assertEqual(list(rows(root, db_map)), expected)
-        parameter_definition_mapping.position = -2
-        object_mapping.position = -1
-        expected = [
-            [None, "o11", "o11", "o12", "o12"],
-            [None, "p11", "p12", "p11", "p12"],
-            ["oc1", -11.0, -12.0, -21.0, -22.0],
-        ]
-        self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
-
-    def test_export_parameter_indexes(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p1"), ("oc", "p2")))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o1", "p1", Map(["a", "b"], [5.0, 5.0])),
-                ("oc", "o1", "p2", Map(["c", "d"], [5.0, 5.0])),
-                ("oc", "o2", "p1", Map(["e", "f"], [5.0, 5.0])),
-                ("oc", "o2", "p2", Map(["g", "h"], [5.0, 5.0])),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        parameter_definition_mapping = ParameterDefinitionMapping(2)
-        alternative_mapping = AlternativeMapping(Position.hidden)
-        parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(1)
-        alternative_mapping.child = object_mapping
-        index_mapping = ParameterValueIndexMapping(3)
-        object_mapping.child = index_mapping
-        object_class_mapping.child = parameter_definition_mapping
-        expected = [
-            ["oc", "o1", "p1", "a"],
-            ["oc", "o1", "p1", "b"],
-            ["oc", "o1", "p2", "c"],
-            ["oc", "o1", "p2", "d"],
-            ["oc", "o2", "p1", "e"],
-            ["oc", "o2", "p1", "f"],
-            ["oc", "o2", "p2", "g"],
-            ["oc", "o2", "p2", "h"],
-        ]
-        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_export_nested_parameter_indexes(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o1", "p", Map(["A", "B"], [23.0, Map(["a", "b"], [-1.1, -2.2])])),
-                ("oc", "o2", "p", Map(["C", "D"], [Map(["c", "d"], [-3.3, -4.4]), 2.3])),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        parameter_definition_mapping = ParameterDefinitionMapping(2)
-        alternative_mapping = AlternativeMapping(Position.hidden)
-        parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(1)
-        alternative_mapping.child = object_mapping
-        index_mapping_1 = ParameterValueIndexMapping(3)
-        index_mapping_2 = ParameterValueIndexMapping(4)
-        index_mapping_1.child = index_mapping_2
-        object_mapping.child = index_mapping_1
-        object_class_mapping.child = parameter_definition_mapping
-        expected = [
-            ["oc", "o1", "p", "A", None],
-            ["oc", "o1", "p", "B", "a"],
-            ["oc", "o1", "p", "B", "b"],
-            ["oc", "o2", "p", "C", "c"],
-            ["oc", "o2", "p", "C", "d"],
-            ["oc", "o2", "p", "D", None],
-        ]
-        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_export_nested_map_values_only(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o1", "p", Map(["A", "B"], [23.0, Map(["a", "b"], [-1.1, -2.2])])),
-                ("oc", "o2", "p", Map(["C", "D"], [Map(["c", "d"], [-3.3, -4.4]), 2.3])),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(Position.hidden)
-        parameter_definition_mapping = ParameterDefinitionMapping(Position.hidden)
-        object_mapping = ObjectMapping(Position.hidden)
-        parameter_definition_mapping.child = object_mapping
-        alternative_mapping = AlternativeMapping(Position.hidden)
-        object_mapping.child = alternative_mapping
-        index_mapping_1 = ParameterValueIndexMapping(Position.hidden)
-        index_mapping_2 = ParameterValueIndexMapping(Position.hidden)
-        value_mapping = ExpandedParameterValueMapping(0)
-        index_mapping_2.child = value_mapping
-        index_mapping_1.child = index_mapping_2
-        alternative_mapping.child = index_mapping_1
-        object_class_mapping.child = parameter_definition_mapping
-        expected = [[23.0], [-1.1], [-2.2], [-3.3], [-4.4], [2.3]]
-        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_full_pivot_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o1", "p", Map(["A", "B"], [Map(["a", "b"], [-1.1, -2.2]), Map(["a", "b"], [-3.3, -4.4])])),
-                ("oc", "o2", "p", Map(["A", "B"], [Map(["a", "b"], [-5.5, -6.6]), Map(["a", "b"], [-7.7, -8.8])])),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        parameter_definition_mapping = ParameterDefinitionMapping(1)
-        alternative_mapping = AlternativeMapping(Position.hidden)
-        parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(-1)
-        alternative_mapping.child = object_mapping
-        index_mapping_1 = ParameterValueIndexMapping(2)
-        index_mapping_2 = ParameterValueIndexMapping(3)
-        value_mapping = ExpandedParameterValueMapping(-2)
-        index_mapping_2.child = value_mapping
-        index_mapping_1.child = index_mapping_2
-        object_mapping.child = index_mapping_1
-        object_class_mapping.child = parameter_definition_mapping
-        expected = [
-            [None, None, None, None, "o1", "o2"],
-            ["oc", "p", "A", "a", -1.1, -5.5],
-            ["oc", "p", "A", "b", -2.2, -6.6],
-            ["oc", "p", "B", "a", -3.3, -7.7],
-            ["oc", "p", "B", "b", -4.4, -8.8],
-        ]
-        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_full_pivot_table_with_hidden_columns(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map, (("oc", "o1", "p", Map(["A", "B"], [-1.1, -2.2])), ("oc", "o2", "p", Map(["A", "B"], [-5.5, -6.6])))
-        )
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 2, Position.hidden, -1, 3, Position.hidden, 5, [Position.hidden], [4])
-        expected = [
-            [None, None, None, None, None, "o1", "o2"],
-            ["oc", None, "p", "Base", "A", -1.1, -5.5],
-            ["oc", None, "p", "Base", "B", -2.2, -6.6],
-        ]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_objects_as_pivot_header_for_indexed_values_with_alternatives(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_alternatives(db_map, ("alt",))
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o1", "p", Map(["A", "B"], [-1.1, -2.2]), "Base"),
-                ("oc", "o1", "p", Map(["A", "B"], [-3.3, -4.4]), "alt"),
-                ("oc", "o2", "p", Map(["A", "B"], [-5.5, -6.6]), "Base"),
-                ("oc", "o2", "p", Map(["A", "B"], [-7.7, -8.8]), "alt"),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 2, Position.hidden, -1, 3, Position.hidden, 5, [Position.hidden], [4])
-        expected = [
-            [None, None, None, None, None, "o1", "o2"],
-            ["oc", None, "p", "Base", "A", -1.1, -5.5],
-            ["oc", None, "p", "Base", "B", -2.2, -6.6],
-            ["oc", None, "p", "alt", "A", -3.3, -7.7],
-            ["oc", None, "p", "alt", "B", -4.4, -8.8],
-        ]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_objects_and_indexes_as_pivot_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map, (("oc", "o1", "p", Map(["A", "B"], [-1.1, -2.2])), ("oc", "o2", "p", Map(["A", "B"], [-3.3, -4.4])))
-        )
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 2, Position.hidden, -1, 3, Position.hidden, 4, [Position.hidden], [-2])
-        expected = [
-            [None, None, None, None, "o1", "o1", "o2", "o2"],
-            [None, None, None, None, "A", "B", "A", "B"],
-            ["oc", None, "p", "Base", -1.1, -2.2, -3.3, -4.4],
-        ]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_objects_and_indexes_as_pivot_header_with_multiple_alternatives_and_parameters(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_alternatives(db_map, ("alt",))
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p1"),))
-        import_object_parameters(db_map, (("oc", "p2"),))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o1", "p1", Map(["A", "B"], [-1.1, -2.2]), "Base"),
-                ("oc", "o1", "p1", Map(["A", "B"], [-3.3, -4.4]), "alt"),
-                ("oc", "o1", "p2", Map(["A", "B"], [-5.5, -6.6]), "Base"),
-                ("oc", "o1", "p2", Map(["A", "B"], [-7.7, -8.8]), "alt"),
-                ("oc", "o2", "p1", Map(["A", "B"], [-9.9, -10.1]), "Base"),
-                ("oc", "o2", "p1", Map(["A", "B"], [-11.1, -12.2]), "alt"),
-                ("oc", "o2", "p2", Map(["A", "B"], [-13.3, -14.4]), "Base"),
-                ("oc", "o2", "p2", Map(["A", "B"], [-15.5, -16.6]), "alt"),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 1, Position.hidden, -1, -2, Position.hidden, 2, [Position.hidden], [-3])
-        expected = [
-            [None, None, "o1", "o1", "o1", "o1", "o2", "o2", "o2", "o2"],
-            [None, None, "Base", "Base", "alt", "alt", "Base", "Base", "alt", "alt"],
-            [None, None, "A", "B", "A", "B", "A", "B", "A", "B"],
-            ["oc", "p1", -1.1, -2.2, -3.3, -4.4, -9.9, -10.1, -11.1, -12.2],
-            ["oc", "p2", -5.5, -6.6, -7.7, -8.8, -13.3, -14.4, -15.5, -16.6],
-        ]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_empty_column_while_pivoted_handled_gracefully(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_alternatives(db_map, ("alt",))
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o"),))
-        db_map.commit_session("Add test data.")
-        mapping = ObjectClassMapping(0)
-        definition = ParameterDefinitionMapping(1)
-        value_list = ParameterValueListMapping(2)
-        object_ = ObjectMapping(-1)
-        value_list.child = object_
-        definition.child = value_list
-        mapping.child = definition
-        self.assertEqual(list(rows(mapping, db_map)), [])
-        db_map.connection.close()
-
-    def test_object_classes_as_header_row_and_objects_in_columns(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_objects(
-            db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31"), ("oc3", "o32"), ("oc3", "o33"))
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(-1)
-        object_class_mapping.child = ObjectMapping(0)
-        self.assertEqual(
-            list(rows(object_class_mapping, db_map)),
-            [["oc1", "oc2", "oc3"], ["o11", "o21", "o31"], ["o12", None, "o32"], [None, None, "o33"]],
-        )
-        db_map.connection.close()
-
-    def test_object_classes_as_table_names(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_objects(
-            db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31"), ("oc3", "o32"), ("oc3", "o33"))
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(Position.table_name)
-        object_class_mapping.child = ObjectMapping(0)
-        tables = dict()
-        for title, title_key in titles(object_class_mapping, db_map):
-            tables[title] = list(rows(object_class_mapping, db_map, title_key))
-        self.assertEqual(tables, {"oc1": [["o11"], ["o12"]], "oc2": [["o21"]], "oc3": [["o31"], ["o32"], ["o33"]]})
-        db_map.connection.close()
-
-    def test_object_class_and_parameter_definition_as_table_name(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_object_parameters(db_map, (("oc1", "p11"), ("oc2", "p21"), ("oc2", "p22")))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31")))
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(Position.table_name)
-        definition_mapping = ParameterDefinitionMapping(Position.table_name)
-        object_mapping = ObjectMapping(0)
-        object_class_mapping.child = definition_mapping
-        definition_mapping.child = object_mapping
-        tables = dict()
-        for title, title_key in titles(object_class_mapping, db_map):
-            tables[title] = list(rows(object_class_mapping, db_map, title_key))
-        self.assertEqual(
-            tables, {"oc1,p11": [["o11"], ["o12"]], "oc2,p21": [["o21"]], "oc2,p22": [["o21"]], "oc3": [["o31"]]}
-        )
-        db_map.connection.close()
-
-    def test_object_relationship_name_as_table_name(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_objects(db_map, (("oc1", "o1"), ("oc1", "o2"), ("oc2", "O")))
-        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
-        import_relationships(db_map, (("rc", ("o1", "O")), ("rc", ("o2", "O"))))
-        db_map.commit_session("Add test data.")
-        mappings = relationship_export(0, Position.table_name, [1, 2], [Position.table_name, 3])
-        tables = dict()
-        for title, title_key in titles(mappings, db_map):
-            tables[title] = list(rows(mappings, db_map, title_key))
-        self.assertEqual(
-            tables, {"rc_o1__O,o1": [["rc", "oc1", "oc2", "O"]], "rc_o2__O,o2": [["rc", "oc1", "oc2", "O"]]}
-        )
-        db_map.connection.close()
-
-    def test_parameter_definitions_with_value_lists(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_parameter_value_lists(db_map, (("vl1", -1.0), ("vl2", -2.0)))
-        import_object_parameters(db_map, (("oc", "p1", None, "vl1"), ("oc", "p2")))
-        db_map.commit_session("Add test data.")
-        class_mapping = ObjectClassMapping(0)
-        definition_mapping = ParameterDefinitionMapping(1)
-        value_list_mapping = ParameterValueListMapping(2)
-        definition_mapping.child = value_list_mapping
-        class_mapping.child = definition_mapping
-        tables = dict()
-        for title, title_key in titles(class_mapping, db_map):
-            tables[title] = list(rows(class_mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["oc", "p1", "vl1"]]})
-        db_map.connection.close()
-
-    def test_parameter_definitions_and_values_and_value_lists(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_parameter_value_lists(db_map, (("vl", -1.0),))
-        import_object_parameters(db_map, (("oc", "p1", None, "vl"), ("oc", "p2")))
-        import_objects(db_map, (("oc", "o"),))
-        import_object_parameter_values(db_map, (("oc", "o", "p1", -1.0), ("oc", "o", "p2", 5.0)))
-        db_map.commit_session("Add test data.")
-        flattened = [
-            ObjectClassMapping(0),
-            ParameterDefinitionMapping(1),
-            AlternativeMapping(Position.hidden),
-            ParameterValueListMapping(2),
-            ObjectMapping(3),
-            ParameterValueMapping(4),
-        ]
-        mapping = unflatten(flattened)
-        tables = dict()
-        for title, title_key in titles(mapping, db_map):
-            tables[title] = list(rows(mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["oc", "p1", "vl", "o", -1.0]]})
-        db_map.connection.close()
-
-    def test_parameter_definitions_and_values_and_ignorable_value_lists(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_parameter_value_lists(db_map, (("vl", -1.0),))
-        import_object_parameters(db_map, (("oc", "p1", None, "vl"), ("oc", "p2")))
-        import_objects(db_map, (("oc", "o"),))
-        import_object_parameter_values(db_map, (("oc", "o", "p1", -1.0), ("oc", "o", "p2", 5.0)))
-        db_map.commit_session("Add test data.")
-        value_list_mapping = ParameterValueListMapping(2)
-        value_list_mapping.set_ignorable(True)
-        flattened = [
-            ObjectClassMapping(0),
-            ParameterDefinitionMapping(1),
-            AlternativeMapping(Position.hidden),
-            value_list_mapping,
-            ObjectMapping(3),
-            ParameterValueMapping(4),
-        ]
-        mapping = unflatten(flattened)
-        tables = dict()
-        for title, title_key in titles(mapping, db_map):
-            tables[title] = list(rows(mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["oc", "p1", "vl", "o", -1.0], ["oc", "p2", None, "o", 5.0]]})
-        db_map.connection.close()
-
-    def test_parameter_value_lists(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_parameter_value_lists(db_map, (("vl1", -1.0), ("vl2", -2.0)))
-        db_map.commit_session("Add test data.")
-        value_list_mapping = ParameterValueListMapping(0)
-        tables = dict()
-        for title, title_key in titles(value_list_mapping, db_map):
-            tables[title] = list(rows(value_list_mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["vl1"], ["vl2"]]})
-        db_map.connection.close()
-
-    def test_parameter_value_list_values(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_parameter_value_lists(db_map, (("vl1", -1.0), ("vl2", -2.0)))
-        db_map.commit_session("Add test data.")
-        value_list_mapping = ParameterValueListMapping(Position.table_name)
-        value_mapping = ParameterValueListValueMapping(0)
-        value_list_mapping.child = value_mapping
-        tables = dict()
-        for title, title_key in titles(value_list_mapping, db_map):
-            tables[title] = list(rows(value_list_mapping, db_map, title_key))
-        self.assertEqual(tables, {"vl1": [[-1.0]], "vl2": [[-2.0]]})
-        db_map.connection.close()
-
-    def test_no_item_declared_as_title_gives_full_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_object_parameters(db_map, (("oc1", "p11"), ("oc2", "p21"), ("oc2", "p22")))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31")))
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(Position.hidden)
-        definition_mapping = ParameterDefinitionMapping(Position.hidden)
-        object_mapping = ObjectMapping(0)
-        object_class_mapping.child = definition_mapping
-        definition_mapping.child = object_mapping
-        tables = dict()
-        for title, title_key in titles(object_class_mapping, db_map):
-            tables[title] = list(rows(object_class_mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["o11"], ["o12"], ["o21"], ["o21"]]})
-        db_map.connection.close()
-
-    def test_missing_values_for_alternatives(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p1"), ("oc", "p2")))
-        import_alternatives(db_map, ("alt1", "alt2"))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o1", "p1", -1.1, "alt1"),
-                ("oc", "o1", "p1", -1.2, "alt2"),
-                ("oc", "o1", "p2", -2.2, "alt1"),
-                ("oc", "o2", "p2", -5.5, "alt2"),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        definition_mapping = ParameterDefinitionMapping(2)
-        object_mapping = ObjectMapping(1)
-        alternative_mapping = AlternativeMapping(3)
-        value_mapping = ParameterValueMapping(4)
-        object_class_mapping.child = definition_mapping
-        definition_mapping.child = object_mapping
-        object_mapping.child = alternative_mapping
-        alternative_mapping.child = value_mapping
-        expected = [
-            ["oc", "o1", "p1", "alt1", -1.1],
-            ["oc", "o1", "p1", "alt2", -1.2],
-            ["oc", "o1", "p2", "alt1", -2.2],
-            ["oc", "o2", "p2", "alt2", -5.5],
-        ]
-        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_export_relationship_classes(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_relationship_classes(
-            db_map, (("rc1", ("oc1",)), ("rc2", ("oc3", "oc2")), ("rc3", ("oc2", "oc3", "oc1")))
-        )
-        db_map.commit_session("Add test data.")
-        relationship_class_mapping = RelationshipClassMapping(0)
-        self.assertEqual(list(rows(relationship_class_mapping, db_map)), [["rc1"], ["rc2"], ["rc3"]])
-        db_map.connection.close()
-
-    def test_export_relationships(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
-        import_relationship_classes(db_map, (("rc1", ("oc1",)), ("rc2", ("oc2", "oc1"))))
-        import_relationships(db_map, (("rc1", ("o11",)), ("rc2", ("o21", "o11")), ("rc2", ("o21", "o12"))))
-        db_map.commit_session("Add test data.")
-        relationship_class_mapping = RelationshipClassMapping(0)
-        relationship_mapping = RelationshipMapping(1)
-        relationship_class_mapping.child = relationship_mapping
-        expected = [["rc1", "rc1_o11"], ["rc2", "rc2_o21__o11"], ["rc2", "rc2_o21__o12"]]
-        self.assertEqual(list(rows(relationship_class_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_relationships_with_different_dimensions(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc2", "o22")))
-        import_relationship_classes(db_map, (("rc1D", ("oc1",)), ("rc2D", ("oc1", "oc2"))))
-        import_relationships(db_map, (("rc1D", ("o11",)), ("rc1D", ("o12",))))
-        import_relationships(
-            db_map,
-            (("rc2D", ("o11", "o21")), ("rc2D", ("o11", "o22")), ("rc2D", ("o12", "o21")), ("rc2D", ("o12", "o22"))),
-        )
-        db_map.commit_session("Add test data.")
-        relationship_class_mapping = RelationshipClassMapping(0)
-        object_class_mapping1 = RelationshipClassObjectClassMapping(1)
-        object_class_mapping2 = RelationshipClassObjectClassMapping(2)
-        relationship_mapping = RelationshipMapping(Position.hidden)
-        object_mapping1 = RelationshipObjectMapping(3)
-        object_mapping2 = RelationshipObjectMapping(4)
-        object_mapping1.child = object_mapping2
-        relationship_mapping.child = object_mapping1
-        object_class_mapping2.child = relationship_mapping
-        object_class_mapping1.child = object_class_mapping2
-        relationship_class_mapping.child = object_class_mapping1
-        tables = dict()
-        for title, title_key in titles(relationship_class_mapping, db_map):
-            tables[title] = list(rows(relationship_class_mapping, db_map, title_key))
-        expected = [
-            ["rc1D", "oc1", "", "o11", ""],
-            ["rc1D", "oc1", "", "o12", ""],
-            ["rc2D", "oc1", "oc2", "o11", "o21"],
-            ["rc2D", "oc1", "oc2", "o11", "o22"],
-            ["rc2D", "oc1", "oc2", "o12", "o21"],
-            ["rc2D", "oc1", "oc2", "o12", "o22"],
-        ]
-        self.assertEqual(tables[None], expected)
-        db_map.connection.close()
-
-    def test_default_parameter_values(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_object_parameters(db_map, (("oc1", "p11", 3.14), ("oc2", "p21", 14.3), ("oc2", "p22", -1.0)))
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        definition_mapping = ParameterDefinitionMapping(1)
-        default_value_mapping = ParameterDefaultValueMapping(2)
-        definition_mapping.child = default_value_mapping
-        object_class_mapping.child = definition_mapping
-        table = list(rows(object_class_mapping, db_map))
-        self.assertEqual(table, [["oc1", "p11", 3.14], ["oc2", "p21", 14.3], ["oc2", "p22", -1.0]])
-        db_map.connection.close()
-
-    def test_indexed_default_parameter_values(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_object_parameters(
-            db_map,
-            (
-                ("oc1", "p11", Map(["a", "b"], [-6.28, -3.14])),
-                ("oc2", "p21", Map(["A", "B"], [1.1, 2.2])),
-                ("oc2", "p22", Map(["D"], [-1.0])),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        definition_mapping = ParameterDefinitionMapping(1)
-        index_mapping = ParameterDefaultValueIndexMapping(2)
-        value_mapping = ExpandedParameterDefaultValueMapping(3)
-        index_mapping.child = value_mapping
-        definition_mapping.child = index_mapping
-        object_class_mapping.child = definition_mapping
-        table = list(rows(object_class_mapping, db_map))
-        expected = [
-            ["oc1", "p11", "a", -6.28],
-            ["oc1", "p11", "b", -3.14],
-            ["oc2", "p21", "A", 1.1],
-            ["oc2", "p21", "B", 2.2],
-            ["oc2", "p22", "D", -1.0],
-        ]
-        self.assertEqual(table, expected)
-        db_map.connection.close()
-
-    def test_replace_parameter_indexes_by_external_data(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p1"),))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map, (("oc", "o1", "p1", Map(["a", "b"], [5.0, -5.0])), ("oc", "o2", "p1", Map(["a", "b"], [2.0, -2.0])))
-        )
-        db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        parameter_definition_mapping = ParameterDefinitionMapping(2)
-        alternative_mapping = AlternativeMapping(Position.hidden)
-        parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(1)
-        alternative_mapping.child = object_mapping
-        index_mapping = ParameterValueIndexMapping(3)
-        value_mapping = ExpandedParameterValueMapping(4)
-        index_mapping.child = value_mapping
-        index_mapping.replace_data(["c", "d"])
-        object_mapping.child = index_mapping
-        object_class_mapping.child = parameter_definition_mapping
-        expected = [
-            ["oc", "o1", "p1", "c", 5.0],
-            ["oc", "o1", "p1", "d", -5.0],
-            ["oc", "o2", "p1", "c", 2.0],
-            ["oc", "o2", "p1", "d", -2.0],
-        ]
-        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_constant_mapping_as_title(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        db_map.commit_session("Add test data.")
-        constant_mapping = FixedValueMapping(Position.table_name, "title_text")
-        object_class_mapping = ObjectClassMapping(0)
-        constant_mapping.child = object_class_mapping
-        tables = dict()
-        for title, title_key in titles(constant_mapping, db_map):
-            tables[title] = list(rows(constant_mapping, db_map, title_key))
-        self.assertEqual(tables, {"title_text": [["oc1"], ["oc2"], ["oc3"]]})
-        db_map.connection.close()
-
-    def test_scenario_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_scenarios(db_map, ("s1", "s2"))
-        db_map.commit_session("Add test data.")
-        scenario_mapping = ScenarioMapping(0)
-        tables = dict()
-        for title, title_key in titles(scenario_mapping, db_map):
-            tables[title] = list(rows(scenario_mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["s1"], ["s2"]]})
-        db_map.connection.close()
-
-    def test_scenario_active_flag_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_scenarios(db_map, (("s1", True), ("s2", False)))
-        db_map.commit_session("Add test data.")
-        scenario_mapping = ScenarioMapping(0)
-        active_flag_mapping = ScenarioActiveFlagMapping(1)
-        scenario_mapping.child = active_flag_mapping
-        tables = dict()
-        for title, title_key in titles(scenario_mapping, db_map):
-            tables[title] = list(rows(scenario_mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["s1", True], ["s2", False]]})
-        db_map.connection.close()
-
-    def test_scenario_alternative_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_alternatives(db_map, ("a1", "a2", "a3"))
-        import_scenarios(db_map, ("s1", "s2", "empty"))
-        import_scenario_alternatives(db_map, (("s1", "a2"), ("s1", "a1", "a2"), ("s2", "a2"), ("s2", "a3", "a2")))
-        db_map.commit_session("Add test data.")
-        scenario_mapping = ScenarioMapping(0)
-        scenario_alternative_mapping = ScenarioAlternativeMapping(1)
-        scenario_mapping.child = scenario_alternative_mapping
-        tables = dict()
-        for title, title_key in titles(scenario_mapping, db_map):
-            tables[title] = list(rows(scenario_mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["s1", "a1"], ["s1", "a2"], ["s2", "a2"], ["s2", "a3"]]})
-        db_map.connection.close()
-
-    def test_tool_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_tools(db_map, ("tool1", "tool2"))
-        db_map.commit_session("Add test data.")
-        tool_mapping = ToolMapping(0)
-        tables = dict()
-        for title, title_key in titles(tool_mapping, db_map):
-            tables[title] = list(rows(tool_mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["tool1"], ["tool2"]]})
-        db_map.connection.close()
-
-    def test_feature_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_parameter_value_lists(db_map, (("features", "feat1"), ("features", "feat2")))
-        import_object_parameters(
-            db_map,
-            (
-                ("oc1", "p1", "feat1", "features"),
-                ("oc1", "p2", "feat1", "features"),
-                ("oc2", "p3", "feat2", "features"),
-            ),
-        )
-        import_features(db_map, (("oc1", "p2"), ("oc2", "p3")))
-        db_map.commit_session("Add test data.")
-        class_mapping = FeatureEntityClassMapping(0)
-        parameter_mapping = FeatureParameterDefinitionMapping(1)
-        class_mapping.child = parameter_mapping
-        tables = dict()
-        for title, title_key in titles(class_mapping, db_map):
-            tables[title] = list(rows(class_mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["oc1", "p2"], ["oc2", "p3"]]})
-        db_map.connection.close()
-
-    def test_tool_feature_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_parameter_value_lists(db_map, (("features", "feat1"), ("features", "feat2")))
-        import_object_parameters(
-            db_map,
-            (
-                ("oc1", "p1", "feat1", "features"),
-                ("oc1", "p2", "feat1", "features"),
-                ("oc2", "p3", "feat2", "features"),
-            ),
-        )
-        import_features(db_map, (("oc1", "p1"), ("oc1", "p2"), ("oc2", "p3")))
-        import_tools(db_map, ("tool1", "tool2"))
-        import_tool_features(
-            db_map, (("tool1", "oc1", "p1", True), ("tool1", "oc2", "p3", False), ("tool2", "oc1", "p1", True))
-        )
-        db_map.commit_session("Add test data.")
-        mapping = unflatten(
-            [
-                ToolMapping(Position.table_name),
-                ToolFeatureEntityClassMapping(0),
-                ToolFeatureParameterDefinitionMapping(1),
-                ToolFeatureRequiredFlagMapping(2),
-            ]
-        )
-        tables = dict()
-        for title, title_key in titles(mapping, db_map):
-            tables[title] = list(rows(mapping, db_map, title_key))
-        expected = {"tool1": [["oc1", "p1", True], ["oc2", "p3", False]], "tool2": [["oc1", "p1", True]]}
-        self.assertEqual(tables, expected)
-        db_map.connection.close()
-
-    def test_tool_feature_method_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_parameter_value_lists(db_map, (("features", "feat1"), ("features", "feat2")))
-        import_object_parameters(
-            db_map,
-            (
-                ("oc1", "p1", "feat1", "features"),
-                ("oc1", "p2", "feat1", "features"),
-                ("oc2", "p3", "feat2", "features"),
-            ),
-        )
-        import_features(db_map, (("oc1", "p1"), ("oc1", "p2"), ("oc2", "p3")))
-        import_tools(db_map, ("tool1", "tool2"))
-        import_tool_features(
-            db_map, (("tool1", "oc1", "p1", True), ("tool1", "oc2", "p3", False), ("tool2", "oc1", "p1", True))
-        )
-        import_tool_feature_methods(
-            db_map,
-            (
-                ("tool1", "oc1", "p1", "feat1"),
-                ("tool1", "oc1", "p1", "feat2"),
-                ("tool2", "oc1", "p1", "feat1"),
-                ("tool2", "oc1", "p1", "feat2"),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        mapping = unflatten(
-            [
-                ToolMapping(Position.table_name),
-                ToolFeatureMethodEntityClassMapping(0),
-                ToolFeatureMethodParameterDefinitionMapping(1),
-                ToolFeatureMethodMethodMapping(2),
-            ]
-        )
-        tables = dict()
-        for title, title_key in titles(mapping, db_map):
-            tables[title] = list(rows(mapping, db_map, title_key))
-        expected = {
-            "tool1": [["oc1", "p1", "feat1"], ["oc1", "p1", "feat2"]],
-            "tool2": [["oc1", "p1", "feat1"], ["oc1", "p1", "feat2"]],
-        }
-        self.assertEqual(tables, expected)
-        db_map.connection.close()
-
-    def test_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        root = unflatten([ObjectClassMapping(0, header="class"), ObjectMapping(1, header="entity")])
-        expected = [["class", "entity"], ["oc", "o1"]]
-        self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
-
-    def test_header_without_data_still_creates_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root = unflatten([ObjectClassMapping(0, header="class"), ObjectMapping(1, header="object")])
-        expected = [["class", "object"]]
-        self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
-
-    def test_header_in_half_pivot_table_without_data_still_creates_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root = unflatten([ObjectClassMapping(-1, header="class"), ObjectMapping(9, header="object")])
-        expected = [["class"]]
-        self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
-
-    def test_header_in_pivot_table_without_data_still_creates_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root = unflatten(
-            [
-                ObjectClassMapping(-1, header="class"),
-                ParameterDefinitionMapping(0, header="parameter"),
-                ObjectMapping(-2, header="object"),
-                AlternativeMapping(1, header="alternative"),
-                ParameterValueMapping(0),
-            ]
-        )
-        expected = [[None, "class"], ["parameter", "alternative"]]
-        self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
-
-    def test_disabled_empty_data_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root = unflatten([ObjectClassMapping(0, header="class"), ObjectMapping(1, header="object")])
-        expected = []
-        self.assertEqual(list(rows(root, db_map, empty_data_header=False)), expected)
-        db_map.connection.close()
-
-    def test_disabled_empty_data_header_in_pivot_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root = unflatten([ObjectClassMapping(-1, header="class"), ObjectMapping(0)])
-        expected = []
-        self.assertEqual(list(rows(root, db_map, empty_data_header=False)), expected)
-        db_map.connection.close()
-
-    def test_header_position(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        root = unflatten([ObjectClassMapping(Position.header), ObjectMapping(0)])
-        expected = [["oc"], ["o1"]]
-        self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
-
-    def test_header_position_with_relationships(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_objects(db_map, (("oc1", "o11"), ("oc2", "o21")))
-        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
-        import_relationships(db_map, (("rc", ("o11", "o21")),))
-        db_map.commit_session("Add test data.")
-        root = unflatten(
-            [
-                RelationshipClassMapping(0),
-                RelationshipClassObjectClassMapping(Position.header),
-                RelationshipClassObjectClassMapping(Position.header),
-                RelationshipMapping(1),
-                RelationshipObjectMapping(2),
-                RelationshipObjectMapping(3),
-            ]
-        )
-        expected = [["", "", "oc1", "oc2"], ["rc", "rc_o11__o21", "o11", "o21"]]
-        self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
-
-    def test_header_position_with_relationships_but_no_data(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
-        db_map.commit_session("Add test data.")
-        root = unflatten(
-            [
-                RelationshipClassMapping(0),
-                RelationshipClassObjectClassMapping(Position.header),
-                RelationshipClassObjectClassMapping(Position.header),
-                RelationshipMapping(1),
-                RelationshipObjectMapping(2),
-                RelationshipObjectMapping(3),
-            ]
-        )
-        expected = [["", "", "oc1", "oc2"]]
-        self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
-
-    def test_header_and_pivot(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_alternatives(db_map, ("alt",))
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p1"),))
-        import_object_parameters(db_map, (("oc", "p2"),))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o1", "p1", Map(["A", "B"], [-1.1, -2.2]), "Base"),
-                ("oc", "o1", "p1", Map(["A", "B"], [-3.3, -4.4]), "alt"),
-                ("oc", "o1", "p2", Map(["A", "B"], [-5.5, -6.6]), "Base"),
-                ("oc", "o1", "p2", Map(["A", "B"], [-7.7, -8.8]), "alt"),
-                ("oc", "o2", "p1", Map(["A", "B"], [-9.9, -10.1]), "Base"),
-                ("oc", "o2", "p1", Map(["A", "B"], [-11.1, -12.2]), "alt"),
-                ("oc", "o2", "p2", Map(["A", "B"], [-13.3, -14.4]), "Base"),
-                ("oc", "o2", "p2", Map(["A", "B"], [-15.5, -16.6]), "alt"),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        mapping = unflatten(
-            [
-                ObjectClassMapping(0, header="class"),
-                ParameterDefinitionMapping(1, header="parameter"),
-                ObjectMapping(-1, header="object"),
-                AlternativeMapping(-2, header="alternative"),
-                ParameterValueIndexMapping(-3, header=""),
-                ExpandedParameterValueMapping(2, header="value"),
-            ]
-        )
-        expected = [
-            [None, "object", "o1", "o1", "o1", "o1", "o2", "o2", "o2", "o2"],
-            [None, "alternative", "Base", "Base", "alt", "alt", "Base", "Base", "alt", "alt"],
-            ["class", "parameter", "A", "B", "A", "B", "A", "B", "A", "B"],
-            ["oc", "p1", -1.1, -2.2, -3.3, -4.4, -9.9, -10.1, -11.1, -12.2],
-            ["oc", "p2", -5.5, -6.6, -7.7, -8.8, -13.3, -14.4, -15.5, -16.6],
-        ]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_pivot_without_left_hand_side_has_padding_column_for_headers(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_alternatives(db_map, ("alt",))
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p1"),))
-        import_object_parameters(db_map, (("oc", "p2"),))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o1", "p1", Map(["A", "B"], [-1.1, -2.2]), "Base"),
-                ("oc", "o1", "p1", Map(["A", "B"], [-3.3, -4.4]), "alt"),
-                ("oc", "o1", "p2", Map(["A", "B"], [-5.5, -6.6]), "Base"),
-                ("oc", "o1", "p2", Map(["A", "B"], [-7.7, -8.8]), "alt"),
-                ("oc", "o2", "p1", Map(["A", "B"], [-9.9, -10.1]), "Base"),
-                ("oc", "o2", "p1", Map(["A", "B"], [-11.1, -12.2]), "alt"),
-                ("oc", "o2", "p2", Map(["A", "B"], [-13.3, -14.4]), "Base"),
-                ("oc", "o2", "p2", Map(["A", "B"], [-15.5, -16.6]), "alt"),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        mapping = unflatten(
-            [
-                ObjectClassMapping(Position.header),
-                ParameterDefinitionMapping(Position.hidden, header="parameter"),
-                ObjectMapping(-1),
-                AlternativeMapping(-2, header="alternative"),
-                ParameterValueIndexMapping(-3, header="index"),
-                ExpandedParameterValueMapping(2, header="value"),
-            ]
-        )
-        expected = [
-            ["oc", "o1", "o1", "o1", "o1", "o2", "o2", "o2", "o2"],
-            ["alternative", "Base", "Base", "alt", "alt", "Base", "Base", "alt", "alt"],
-            ["index", "A", "B", "A", "B", "A", "B", "A", "B"],
-            [None, -1.1, -2.2, -3.3, -4.4, -9.9, -10.1, -11.1, -12.2],
-            [None, -5.5, -6.6, -7.7, -8.8, -13.3, -14.4, -15.5, -16.6],
-        ]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_count_mappings(self):
-        object_class_mapping = ObjectClassMapping(2)
-        parameter_definition_mapping = ParameterDefinitionMapping(0)
-        object_mapping = ObjectMapping(1)
-        parameter_definition_mapping.child = object_mapping
-        object_class_mapping.child = parameter_definition_mapping
-        self.assertEqual(object_class_mapping.count_mappings(), 3)
-
-    def test_flatten(self):
-        object_class_mapping = ObjectClassMapping(2)
-        parameter_definition_mapping = ParameterDefinitionMapping(0)
-        object_mapping = ObjectMapping(1)
-        parameter_definition_mapping.child = object_mapping
-        object_class_mapping.child = parameter_definition_mapping
-        mappings = object_class_mapping.flatten()
-        self.assertEqual(mappings, [object_class_mapping, parameter_definition_mapping, object_mapping])
-
-    def test_unflatten_sets_last_mappings_child_to_none(self):
-        object_class_mapping = ObjectClassMapping(2)
-        object_mapping = ObjectMapping(1)
-        object_class_mapping.child = object_mapping
-        mapping_list = object_class_mapping.flatten()
-        root = unflatten(mapping_list[:1])
-        self.assertIsNone(root.child)
-
-    def test_has_titles(self):
-        object_class_mapping = ObjectClassMapping(0)
-        parameter_definition_mapping = ParameterDefinitionMapping(Position.table_name)
-        object_mapping = ObjectMapping(1)
-        parameter_definition_mapping.child = object_mapping
-        object_class_mapping.child = parameter_definition_mapping
-        self.assertTrue(object_class_mapping.has_titles())
-
-    def test_drop_non_positioned_tail(self):
-        object_class_mapping = ObjectClassMapping(0)
-        parameter_definition_mapping = ParameterDefinitionMapping(Position.hidden)
-        object_mapping = ObjectMapping(1)
-        alternative_mapping = AlternativeMapping(Position.hidden)
-        value_mapping = ParameterValueMapping(Position.hidden)
-        alternative_mapping.child = value_mapping
-        object_mapping.child = alternative_mapping
-        parameter_definition_mapping.child = object_mapping
-        object_class_mapping.child = parameter_definition_mapping
-        tail_cut_mapping = drop_non_positioned_tail(object_class_mapping)
-        flattened = tail_cut_mapping.flatten()
-        self.assertEqual(flattened, [object_class_mapping, parameter_definition_mapping, object_mapping])
-
-    def test_serialization(self):
-        highlight_dimension = 5
-        mappings = [
-            ObjectClassMapping(0),
-            RelationshipClassMapping(Position.table_name, highlight_dimension=highlight_dimension),
-            RelationshipClassObjectClassMapping(2),
-            ParameterDefinitionMapping(1),
-            ObjectMapping(-1),
-            RelationshipMapping(Position.hidden),
-            RelationshipObjectMapping(-1),
-            AlternativeMapping(3),
-            ParameterValueMapping(4),
-            ParameterValueIndexMapping(5),
-            ParameterValueTypeMapping(6),
-            ExpandedParameterValueMapping(7),
-            FixedValueMapping(8, "gaga"),
-        ]
-        expected_positions = [m.position for m in mappings]
-        expected_types = [type(m) for m in mappings]
-        root = unflatten(mappings)
-        serialized = to_dict(root)
-        deserialized = from_dict(serialized).flatten()
-        self.assertEqual([type(m) for m in deserialized], expected_types)
-        self.assertEqual([m.position for m in deserialized], expected_positions)
-        for m in deserialized:
-            if isinstance(m, FixedValueMapping):
-                self.assertEqual(m.value, "gaga")
-            elif isinstance(m, RelationshipClassMapping):
-                self.assertEqual(m.highlight_dimension, highlight_dimension)
-
-    def test_setting_ignorable_flag(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        db_map.commit_session("Add test data.")
-        object_mapping = ObjectMapping(1)
-        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
-        object_mapping.set_ignorable(True)
-        self.assertTrue(object_mapping.is_ignorable())
-        expected = [["oc", None]]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_unsetting_ignorable_flag(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        object_mapping = ObjectMapping(1)
-        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
-        object_mapping.set_ignorable(True)
-        self.assertTrue(object_mapping.is_ignorable())
-        object_mapping.set_ignorable(False)
-        self.assertFalse(object_mapping.is_ignorable())
-        expected = [["oc", "o1"]]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_filter(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
-        db_map.commit_session("Add test data.")
-        object_mapping = ObjectMapping(1)
-        object_mapping.filter_re = "o1"
-        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
-        expected = [["oc", "o1"]]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_hidden_tail_filter(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_objects(db_map, (("oc1", "o1"), ("oc2", "o2")))
-        db_map.commit_session("Add test data.")
-        object_mapping = ObjectMapping(Position.hidden)
-        object_mapping.filter_re = "o1"
-        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
-        expected = [["oc1"]]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_index_names(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o"),))
-        import_object_parameter_values(db_map, (("oc", "o", "p", Map(["a"], [5.0], index_name="index")),))
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 2, Position.hidden, 1, 3, Position.hidden, 5, [Position.header], [4])
-        expected = [["", "", "", "", "index", ""], ["oc", "o", "p", "Base", "a", 5.0]]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_default_value_index_names_with_nested_map(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(
-            db_map, (("oc", "p", Map(["A"], [Map(["b"], [2.3], index_name="idx2")], index_name="idx1")),)
-        )
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_default_value_export(
-            0, 1, Position.hidden, 4, [Position.header, Position.header], [2, 3]
-        )
-        expected = [["", "", "idx1", "idx2", ""], ["oc", "p", "A", "b", 2.3]]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_multiple_index_names_with_empty_database(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        mapping = relationship_parameter_export(
-            0, 4, Position.hidden, 1, [2], [3], 5, Position.hidden, 8, [Position.header, Position.header], [6, 7]
-        )
-        expected = [9 * [""]]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_parameter_default_value_type(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_object_parameters(db_map, (("oc1", "p11", 3.14), ("oc2", "p21", 14.3), ("oc2", "p22", -1.0)))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_default_value_export(0, 1, 2, 3, None, None)
-        expected = [
-            ["oc1", "p11", "single_value", 3.14],
-            ["oc2", "p21", "single_value", 14.3],
-            ["oc2", "p22", "single_value", -1.0],
-        ]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_map_with_more_dimensions_than_index_mappings(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o"),))
-        import_object_parameter_values(db_map, (("oc", "o", "p", Map(["A"], [Map(["b"], [2.3])])),))
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(
-            0, 1, Position.hidden, 2, Position.hidden, Position.hidden, 4, [Position.hidden], [3]
-        )
-        expected = [["oc", "p", "o", "A", "map"]]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_default_map_value_with_more_dimensions_than_index_mappings(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p", Map(["A"], [Map(["b"], [2.3])])),))
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_default_value_export(0, 1, Position.hidden, 3, [Position.hidden], [2])
-        expected = [["oc", "p", "A", "map"]]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_map_with_single_value_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o"),))
-        import_object_parameter_values(db_map, (("oc", "o", "p", Map(["A"], [2.3])),))
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 1, Position.hidden, 2, Position.hidden, Position.hidden, 3, None, None)
-        expected = [["oc", "p", "o", "map"]]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_default_map_value_with_single_value_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p", Map(["A"], [2.3])),))
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_default_value_export(0, 1, Position.hidden, 2, None, None)
-        expected = [["oc", "p", "map"]]
-        self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_table_gets_exported_even_without_parameter_values(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(Position.header, Position.table_name, object_position=0, value_position=1)
-        tables = dict()
-        for title, title_key in titles(mapping, db_map):
-            tables[title] = list(rows(mapping, db_map, title_key))
-        expected = {"p": [["oc", ""]]}
-        self.assertEqual(tables, expected)
-        db_map.connection.close()
-
-    def test_relationship_class_object_classes_parameters(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_relationship_classes(db_map, (("rc", ("oc",)),))
-        db_map.commit_session("Add test data")
-        root_mapping = unflatten(
-            [
-                RelationshipClassMapping(0, highlight_dimension=0),
-                RelationshipClassObjectClassMapping(1),
-                ParameterDefinitionMapping(2),
-            ]
-        )
-        expected = [["rc", "oc", "p"]]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_relationship_class_object_classes_parameters_multiple_dimensions(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
-        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
-        db_map.commit_session("Add test data")
-        root_mapping = unflatten(
-            [
-                RelationshipClassMapping(0, highlight_dimension=0),
-                RelationshipClassObjectClassMapping(1),
-                RelationshipClassObjectClassMapping(3),
-                ParameterDefinitionMapping(2),
-            ]
-        )
-        expected = [["rc", "oc1", "p11", "oc2"], ["rc", "oc1", "p12", "oc2"]]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_highlight_relationship_objects(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_objects(
-            db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc2", "o22"), ("oc3", "o31"), ("oc3", "o32"))
-        )
-        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
-        import_relationships(db_map, (("rc", ("o11", "o21")), ("rc", ("o12", "o22"))))
-        db_map.commit_session("Add test data")
-        root_mapping = unflatten(
-            [
-                RelationshipClassMapping(0, highlight_dimension=0),
-                RelationshipClassObjectClassMapping(1),
-                RelationshipClassObjectClassMapping(2),
-                RelationshipMapping(3),
-                RelationshipObjectMapping(4),
-                RelationshipObjectMapping(5),
-            ]
-        )
-        expected = [
-            ["rc", "oc1", "oc2", "rc_o11__o21", "o11", "o21"],
-            ["rc", "oc1", "oc2", "rc_o12__o22", "o12", "o22"],
-        ]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_export_object_parameters_while_exporting_relationships(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o"),))
-        import_object_parameter_values(db_map, (("oc", "o", "p", 23.0),))
-        import_relationship_classes(db_map, (("rc", ("oc",)),))
-        import_relationships(db_map, (("rc", ("o",)),))
-        db_map.commit_session("Add test data")
-        root_mapping = unflatten(
-            [
-                RelationshipClassMapping(0, highlight_dimension=0),
-                RelationshipClassObjectClassMapping(1),
-                RelationshipMapping(2),
-                RelationshipObjectMapping(3),
-                ParameterDefinitionMapping(4),
-                AlternativeMapping(5),
-                ParameterValueMapping(6),
-            ]
-        )
-        expected = [["rc", "oc", "rc_o", "o", "p", "Base", 23.0]]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_export_default_values_of_object_parameters_while_exporting_relationships(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p", 23.0),))
-        import_objects(db_map, (("oc", "o"),))
-        import_relationship_classes(db_map, (("rc", ("oc",)),))
-        import_relationships(db_map, (("rc", ("o",)),))
-        db_map.commit_session("Add test data")
-        root_mapping = unflatten(
-            [
-                RelationshipClassMapping(0, highlight_dimension=0),
-                RelationshipClassObjectClassMapping(1),
-                ParameterDefinitionMapping(2),
-                ParameterDefaultValueMapping(3),
-            ]
-        )
-        expected = [["rc", "oc", "p", 23.0]]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-    def test_export_object_parameters_while_exporting_relationships_with_multiple_parameters_and_classes2(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
-        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21"), ("oc3", "p31")))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc2", "o22"), ("oc3", "o31")))
-        import_object_parameter_values(db_map, (("oc1", "o11", "p11", 1.1),))
-        import_object_parameter_values(db_map, (("oc1", "o11", "p12", 2.2),))
-        import_object_parameter_values(db_map, (("oc1", "o12", "p11", 3.3),))
-        import_object_parameter_values(db_map, (("oc1", "o12", "p12", 4.4),))
-        import_object_parameter_values(db_map, (("oc2", "o21", "p21", 5.5),))
-        import_object_parameter_values(db_map, (("oc2", "o22", "p21", 6.6),))
-        import_object_parameter_values(db_map, (("oc3", "o31", "p31", 7.7),))
-        import_relationship_classes(db_map, (("rc12", ("oc1", "oc2")),))
-        import_relationship_classes(db_map, (("rc23", ("oc2", "oc3")),))
-        import_relationships(db_map, (("rc12", ("o11", "o21")),))
-        import_relationships(db_map, (("rc12", ("o12", "o21")),))
-        import_relationships(db_map, (("rc23", ("o21", "o31")),))
-        db_map.commit_session("Add test data")
-        root_mapping = unflatten(
-            [
-                RelationshipClassMapping(0, highlight_dimension=1),
-                RelationshipClassObjectClassMapping(1),
-                RelationshipClassObjectClassMapping(2),
-                RelationshipMapping(3),
-                RelationshipObjectMapping(4),
-                RelationshipObjectMapping(5),
-                ParameterDefinitionMapping(6),
-                AlternativeMapping(7),
-                ParameterValueMapping(8),
-            ]
-        )
-        expected = [
-            ["rc12", "oc1", "oc2", "rc12_o11__o21", "o11", "o21", "p21", "Base", 5.5],
-            ["rc12", "oc1", "oc2", "rc12_o12__o21", "o12", "o21", "p21", "Base", 5.5],
-            ["rc23", "oc2", "oc3", "rc23_o21__o31", "o21", "o31", "p31", "Base", 7.7],
-        ]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Unit tests for export mappings.
+
+"""
+
+import unittest
+from spinedb_api import (
+    DatabaseMapping,
+    DiffDatabaseMapping,
+    import_alternatives,
+    import_features,
+    import_object_classes,
+    import_object_parameter_values,
+    import_object_parameters,
+    import_objects,
+    import_parameter_value_lists,
+    import_relationship_classes,
+    import_relationships,
+    import_scenario_alternatives,
+    import_scenarios,
+    import_tool_features,
+    import_tool_feature_methods,
+    import_tools,
+    Map,
+)
+from spinedb_api.import_functions import import_object_groups
+from spinedb_api.mapping import Position, to_dict
+from spinedb_api.export_mapping import (
+    rows,
+    titles,
+    object_parameter_default_value_export,
+    object_parameter_export,
+    relationship_export,
+    relationship_parameter_export,
+)
+from spinedb_api.export_mapping.export_mapping import (
+    AlternativeMapping,
+    drop_non_positioned_tail,
+    FixedValueMapping,
+    ExpandedParameterValueMapping,
+    ExpandedParameterDefaultValueMapping,
+    FeatureEntityClassMapping,
+    FeatureParameterDefinitionMapping,
+    from_dict,
+    ObjectGroupMapping,
+    ObjectGroupObjectMapping,
+    ObjectMapping,
+    ObjectClassMapping,
+    ParameterDefaultValueMapping,
+    ParameterDefaultValueIndexMapping,
+    ParameterDefinitionMapping,
+    ParameterValueIndexMapping,
+    ParameterValueListMapping,
+    ParameterValueListValueMapping,
+    ParameterValueMapping,
+    ParameterValueTypeMapping,
+    RelationshipClassMapping,
+    RelationshipClassObjectClassMapping,
+    RelationshipMapping,
+    RelationshipObjectMapping,
+    ScenarioActiveFlagMapping,
+    ScenarioAlternativeMapping,
+    ScenarioMapping,
+    ToolMapping,
+    ToolFeatureEntityClassMapping,
+    ToolFeatureParameterDefinitionMapping,
+    ToolFeatureRequiredFlagMapping,
+    ToolFeatureMethodEntityClassMapping,
+    ToolFeatureMethodMethodMapping,
+    ToolFeatureMethodParameterDefinitionMapping,
+)
+from spinedb_api.mapping import unflatten
+
+
+class TestExportMapping(unittest.TestCase):
+    def test_export_empty_table(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        object_class_mapping = ObjectClassMapping(0)
+        self.assertEqual(list(rows(object_class_mapping, db_map)), [])
+        db_map.connection.close()
+
+    def test_export_single_object_class(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("object_class",))
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        self.assertEqual(list(rows(object_class_mapping, db_map)), [["object_class"]])
+        db_map.connection.close()
+
+    def test_export_objects(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_objects(
+            db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31"), ("oc3", "o32"), ("oc3", "o33"))
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping.child = ObjectMapping(1)
+        self.assertEqual(
+            list(rows(object_class_mapping, db_map)),
+            [["oc1", "o11"], ["oc1", "o12"], ["oc2", "o21"], ["oc3", "o31"], ["oc3", "o32"], ["oc3", "o33"]],
+        )
+        db_map.connection.close()
+
+    def test_hidden_tail(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1",))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping.child = ObjectMapping(Position.hidden)
+        self.assertEqual(list(rows(object_class_mapping, db_map)), [["oc1"], ["oc1"]])
+        db_map.connection.close()
+
+    def test_pivot_without_values(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1",))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(-1)
+        object_class_mapping.child = ObjectMapping(Position.hidden)
+        self.assertEqual(list(rows(object_class_mapping, db_map)), [])
+        db_map.connection.close()
+
+    def test_hidden_tail_pivoted(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p1"), ("oc", "p2")))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(db_map, (("oc", "o1", "p1", -11.0), ("oc", "o1", "p2", -12.0)))
+        db_map.commit_session("Add test data.")
+        root_mapping = unflatten(
+            [
+                ObjectClassMapping(0),
+                ParameterDefinitionMapping(-1),
+                ObjectMapping(1),
+                AlternativeMapping(2),
+                ParameterValueMapping(Position.hidden),
+            ]
+        )
+        expected = [[None, None, "p1", "p2"], ["oc", "o1", "Base", "Base"]]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_hidden_leaf_item_in_regular_table_valid(self):
+        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping.child = ObjectMapping(Position.hidden)
+        self.assertEqual(object_class_mapping.check_validity(), [])
+
+    def test_hidden_leaf_item_in_pivot_table_not_valid(self):
+        object_class_mapping = ObjectClassMapping(-1)
+        object_class_mapping.child = ObjectMapping(Position.hidden)
+        self.assertEqual(object_class_mapping.check_validity(), ["Cannot be pivoted."])
+
+    def test_object_groups(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2"), ("oc", "o3"), ("oc", "g1"), ("oc", "g2")))
+        import_object_groups(db_map, (("oc", "g1", "o1"), ("oc", "g1", "o2"), ("oc", "g2", "o3")))
+        db_map.commit_session("Add test data.")
+        flattened = [ObjectClassMapping(0), ObjectGroupMapping(1)]
+        mapping = unflatten(flattened)
+        self.assertEqual(list(rows(mapping, db_map)), [["oc", "g1"], ["oc", "g2"]])
+        db_map.connection.close()
+
+    def test_object_groups_with_objects(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2"), ("oc", "o3"), ("oc", "g1"), ("oc", "g2")))
+        import_object_groups(db_map, (("oc", "g1", "o1"), ("oc", "g1", "o2"), ("oc", "g2", "o3")))
+        db_map.commit_session("Add test data.")
+        flattened = [ObjectClassMapping(0), ObjectGroupMapping(1), ObjectGroupObjectMapping(2)]
+        mapping = unflatten(flattened)
+        self.assertEqual(list(rows(mapping, db_map)), [["oc", "g1", "o1"], ["oc", "g1", "o2"], ["oc", "g2", "o3"]])
+        db_map.connection.close()
+
+    def test_object_groups_with_parameter_values(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2"), ("oc", "o3"), ("oc", "g1"), ("oc", "g2")))
+        import_object_groups(db_map, (("oc", "g1", "o1"), ("oc", "g1", "o2"), ("oc", "g2", "o3")))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_object_parameter_values(
+            db_map, (("oc", "o1", "p", -11.0), ("oc", "o2", "p", -12.0), ("oc", "o3", "p", -13.0))
+        )
+        db_map.commit_session("Add test data.")
+        flattened = [
+            ObjectClassMapping(0),
+            ObjectGroupMapping(1),
+            ObjectGroupObjectMapping(2),
+            ParameterDefinitionMapping(Position.hidden),
+            AlternativeMapping(Position.hidden),
+            ParameterValueMapping(3),
+        ]
+        mapping = unflatten(flattened)
+        self.assertEqual(
+            list(rows(mapping, db_map)),
+            [["oc", "g1", "o1", -11.0], ["oc", "g1", "o2", -12.0], ["oc", "g2", "o3", -13.0]],
+        )
+        db_map.connection.close()
+
+    def test_export_parameter_definitions(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        parameter_definition_mapping = ParameterDefinitionMapping(1)
+        parameter_definition_mapping.child = ObjectMapping(2)
+        object_class_mapping.child = parameter_definition_mapping
+        expected = [
+            ["oc1", "p11", "o11"],
+            ["oc1", "p11", "o12"],
+            ["oc1", "p12", "o11"],
+            ["oc1", "p12", "o12"],
+            ["oc2", "p21", "o21"],
+        ]
+        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_export_single_parameter_value_when_there_are_multiple_objects(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
+        import_object_parameter_values(db_map, (("oc1", "o11", "p12", -11.0),))
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        parameter_definition_mapping = ParameterDefinitionMapping(1)
+        alternative_mapping = AlternativeMapping(Position.hidden)
+        parameter_definition_mapping.child = alternative_mapping
+        object_mapping = ObjectMapping(2)
+        alternative_mapping.child = object_mapping
+        value_mapping = ParameterValueMapping(3)
+        object_mapping.child = value_mapping
+        object_class_mapping.child = parameter_definition_mapping
+        self.assertEqual(list(rows(object_class_mapping, db_map)), [["oc1", "p12", "o11", -11.0]])
+        db_map.connection.close()
+
+    def test_export_single_parameter_value_pivoted_by_object_name(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1",))
+        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12")))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc1", "o11", "p11", -11.0),
+                ("oc1", "o11", "p12", -12.0),
+                ("oc1", "o12", "p11", -21.0),
+                ("oc1", "o12", "p12", -22.0),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        parameter_definition_mapping = ParameterDefinitionMapping(1)
+        alternative_mapping = AlternativeMapping(Position.hidden)
+        parameter_definition_mapping.child = alternative_mapping
+        object_mapping = ObjectMapping(-1)
+        alternative_mapping.child = object_mapping
+        value_mapping = ParameterValueMapping(-2)
+        object_mapping.child = value_mapping
+        object_class_mapping.child = parameter_definition_mapping
+        expected = [[None, None, "o11", "o12"], ["oc1", "p11", -11.0, -21.0], ["oc1", "p12", -12.0, -22.0]]
+        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_minimum_pivot_index_need_not_be_minus_one(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_alternatives(db_map, ("alt",))
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o"),))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o", "p", Map(["A", "B"], [-1.1, -2.2]), "Base"),
+                ("oc", "o", "p", Map(["A", "B"], [-5.5, -6.6]), "alt"),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_export(1, 2, Position.hidden, 0, -2, Position.hidden, 4, [Position.hidden], [3])
+        expected = [
+            [None, None, None, None, "Base", "alt"],
+            ["o", "oc", "p", "A", -1.1, -5.5],
+            ["o", "oc", "p", "B", -2.2, -6.6],
+        ]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_pivot_row_order(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1",))
+        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12")))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc1", "o11", "p11", -11.0),
+                ("oc1", "o11", "p12", -12.0),
+                ("oc1", "o12", "p11", -21.0),
+                ("oc1", "o12", "p12", -22.0),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        parameter_definition_mapping = ParameterDefinitionMapping(-1)
+        alternative_mapping = AlternativeMapping(Position.hidden)
+        object_mapping = ObjectMapping(-2)
+        value_mapping = ParameterValueMapping(3)
+        mappings = [
+            object_class_mapping,
+            parameter_definition_mapping,
+            alternative_mapping,
+            object_mapping,
+            value_mapping,
+        ]
+        root = unflatten(mappings)
+        expected = [
+            [None, "p11", "p11", "p12", "p12"],
+            [None, "o11", "o12", "o11", "o12"],
+            ["oc1", -11.0, -21.0, -12.0, -22.0],
+        ]
+        self.assertEqual(list(rows(root, db_map)), expected)
+        parameter_definition_mapping.position = -2
+        object_mapping.position = -1
+        expected = [
+            [None, "o11", "o11", "o12", "o12"],
+            [None, "p11", "p12", "p11", "p12"],
+            ["oc1", -11.0, -12.0, -21.0, -22.0],
+        ]
+        self.assertEqual(list(rows(root, db_map)), expected)
+        db_map.connection.close()
+
+    def test_export_parameter_indexes(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p1"), ("oc", "p2")))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o1", "p1", Map(["a", "b"], [5.0, 5.0])),
+                ("oc", "o1", "p2", Map(["c", "d"], [5.0, 5.0])),
+                ("oc", "o2", "p1", Map(["e", "f"], [5.0, 5.0])),
+                ("oc", "o2", "p2", Map(["g", "h"], [5.0, 5.0])),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        parameter_definition_mapping = ParameterDefinitionMapping(2)
+        alternative_mapping = AlternativeMapping(Position.hidden)
+        parameter_definition_mapping.child = alternative_mapping
+        object_mapping = ObjectMapping(1)
+        alternative_mapping.child = object_mapping
+        index_mapping = ParameterValueIndexMapping(3)
+        object_mapping.child = index_mapping
+        object_class_mapping.child = parameter_definition_mapping
+        expected = [
+            ["oc", "o1", "p1", "a"],
+            ["oc", "o1", "p1", "b"],
+            ["oc", "o1", "p2", "c"],
+            ["oc", "o1", "p2", "d"],
+            ["oc", "o2", "p1", "e"],
+            ["oc", "o2", "p1", "f"],
+            ["oc", "o2", "p2", "g"],
+            ["oc", "o2", "p2", "h"],
+        ]
+        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_export_nested_parameter_indexes(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o1", "p", Map(["A", "B"], [23.0, Map(["a", "b"], [-1.1, -2.2])])),
+                ("oc", "o2", "p", Map(["C", "D"], [Map(["c", "d"], [-3.3, -4.4]), 2.3])),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        parameter_definition_mapping = ParameterDefinitionMapping(2)
+        alternative_mapping = AlternativeMapping(Position.hidden)
+        parameter_definition_mapping.child = alternative_mapping
+        object_mapping = ObjectMapping(1)
+        alternative_mapping.child = object_mapping
+        index_mapping_1 = ParameterValueIndexMapping(3)
+        index_mapping_2 = ParameterValueIndexMapping(4)
+        index_mapping_1.child = index_mapping_2
+        object_mapping.child = index_mapping_1
+        object_class_mapping.child = parameter_definition_mapping
+        expected = [
+            ["oc", "o1", "p", "A", None],
+            ["oc", "o1", "p", "B", "a"],
+            ["oc", "o1", "p", "B", "b"],
+            ["oc", "o2", "p", "C", "c"],
+            ["oc", "o2", "p", "C", "d"],
+            ["oc", "o2", "p", "D", None],
+        ]
+        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_export_nested_map_values_only(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o1", "p", Map(["A", "B"], [23.0, Map(["a", "b"], [-1.1, -2.2])])),
+                ("oc", "o2", "p", Map(["C", "D"], [Map(["c", "d"], [-3.3, -4.4]), 2.3])),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(Position.hidden)
+        parameter_definition_mapping = ParameterDefinitionMapping(Position.hidden)
+        object_mapping = ObjectMapping(Position.hidden)
+        parameter_definition_mapping.child = object_mapping
+        alternative_mapping = AlternativeMapping(Position.hidden)
+        object_mapping.child = alternative_mapping
+        index_mapping_1 = ParameterValueIndexMapping(Position.hidden)
+        index_mapping_2 = ParameterValueIndexMapping(Position.hidden)
+        value_mapping = ExpandedParameterValueMapping(0)
+        index_mapping_2.child = value_mapping
+        index_mapping_1.child = index_mapping_2
+        alternative_mapping.child = index_mapping_1
+        object_class_mapping.child = parameter_definition_mapping
+        expected = [[23.0], [-1.1], [-2.2], [-3.3], [-4.4], [2.3]]
+        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_full_pivot_table(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o1", "p", Map(["A", "B"], [Map(["a", "b"], [-1.1, -2.2]), Map(["a", "b"], [-3.3, -4.4])])),
+                ("oc", "o2", "p", Map(["A", "B"], [Map(["a", "b"], [-5.5, -6.6]), Map(["a", "b"], [-7.7, -8.8])])),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        parameter_definition_mapping = ParameterDefinitionMapping(1)
+        alternative_mapping = AlternativeMapping(Position.hidden)
+        parameter_definition_mapping.child = alternative_mapping
+        object_mapping = ObjectMapping(-1)
+        alternative_mapping.child = object_mapping
+        index_mapping_1 = ParameterValueIndexMapping(2)
+        index_mapping_2 = ParameterValueIndexMapping(3)
+        value_mapping = ExpandedParameterValueMapping(-2)
+        index_mapping_2.child = value_mapping
+        index_mapping_1.child = index_mapping_2
+        object_mapping.child = index_mapping_1
+        object_class_mapping.child = parameter_definition_mapping
+        expected = [
+            [None, None, None, None, "o1", "o2"],
+            ["oc", "p", "A", "a", -1.1, -5.5],
+            ["oc", "p", "A", "b", -2.2, -6.6],
+            ["oc", "p", "B", "a", -3.3, -7.7],
+            ["oc", "p", "B", "b", -4.4, -8.8],
+        ]
+        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_full_pivot_table_with_hidden_columns(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map, (("oc", "o1", "p", Map(["A", "B"], [-1.1, -2.2])), ("oc", "o2", "p", Map(["A", "B"], [-5.5, -6.6])))
+        )
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_export(0, 2, Position.hidden, -1, 3, Position.hidden, 5, [Position.hidden], [4])
+        expected = [
+            [None, None, None, None, None, "o1", "o2"],
+            ["oc", None, "p", "Base", "A", -1.1, -5.5],
+            ["oc", None, "p", "Base", "B", -2.2, -6.6],
+        ]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_objects_as_pivot_header_for_indexed_values_with_alternatives(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_alternatives(db_map, ("alt",))
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o1", "p", Map(["A", "B"], [-1.1, -2.2]), "Base"),
+                ("oc", "o1", "p", Map(["A", "B"], [-3.3, -4.4]), "alt"),
+                ("oc", "o2", "p", Map(["A", "B"], [-5.5, -6.6]), "Base"),
+                ("oc", "o2", "p", Map(["A", "B"], [-7.7, -8.8]), "alt"),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_export(0, 2, Position.hidden, -1, 3, Position.hidden, 5, [Position.hidden], [4])
+        expected = [
+            [None, None, None, None, None, "o1", "o2"],
+            ["oc", None, "p", "Base", "A", -1.1, -5.5],
+            ["oc", None, "p", "Base", "B", -2.2, -6.6],
+            ["oc", None, "p", "alt", "A", -3.3, -7.7],
+            ["oc", None, "p", "alt", "B", -4.4, -8.8],
+        ]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_objects_and_indexes_as_pivot_header(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map, (("oc", "o1", "p", Map(["A", "B"], [-1.1, -2.2])), ("oc", "o2", "p", Map(["A", "B"], [-3.3, -4.4])))
+        )
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_export(0, 2, Position.hidden, -1, 3, Position.hidden, 4, [Position.hidden], [-2])
+        expected = [
+            [None, None, None, None, "o1", "o1", "o2", "o2"],
+            [None, None, None, None, "A", "B", "A", "B"],
+            ["oc", None, "p", "Base", -1.1, -2.2, -3.3, -4.4],
+        ]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_objects_and_indexes_as_pivot_header_with_multiple_alternatives_and_parameters(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_alternatives(db_map, ("alt",))
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p1"),))
+        import_object_parameters(db_map, (("oc", "p2"),))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o1", "p1", Map(["A", "B"], [-1.1, -2.2]), "Base"),
+                ("oc", "o1", "p1", Map(["A", "B"], [-3.3, -4.4]), "alt"),
+                ("oc", "o1", "p2", Map(["A", "B"], [-5.5, -6.6]), "Base"),
+                ("oc", "o1", "p2", Map(["A", "B"], [-7.7, -8.8]), "alt"),
+                ("oc", "o2", "p1", Map(["A", "B"], [-9.9, -10.1]), "Base"),
+                ("oc", "o2", "p1", Map(["A", "B"], [-11.1, -12.2]), "alt"),
+                ("oc", "o2", "p2", Map(["A", "B"], [-13.3, -14.4]), "Base"),
+                ("oc", "o2", "p2", Map(["A", "B"], [-15.5, -16.6]), "alt"),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_export(0, 1, Position.hidden, -1, -2, Position.hidden, 2, [Position.hidden], [-3])
+        expected = [
+            [None, None, "o1", "o1", "o1", "o1", "o2", "o2", "o2", "o2"],
+            [None, None, "Base", "Base", "alt", "alt", "Base", "Base", "alt", "alt"],
+            [None, None, "A", "B", "A", "B", "A", "B", "A", "B"],
+            ["oc", "p1", -1.1, -2.2, -3.3, -4.4, -9.9, -10.1, -11.1, -12.2],
+            ["oc", "p2", -5.5, -6.6, -7.7, -8.8, -13.3, -14.4, -15.5, -16.6],
+        ]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_empty_column_while_pivoted_handled_gracefully(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_alternatives(db_map, ("alt",))
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o"),))
+        db_map.commit_session("Add test data.")
+        mapping = ObjectClassMapping(0)
+        definition = ParameterDefinitionMapping(1)
+        value_list = ParameterValueListMapping(2)
+        object_ = ObjectMapping(-1)
+        value_list.child = object_
+        definition.child = value_list
+        mapping.child = definition
+        self.assertEqual(list(rows(mapping, db_map)), [])
+        db_map.connection.close()
+
+    def test_object_classes_as_header_row_and_objects_in_columns(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_objects(
+            db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31"), ("oc3", "o32"), ("oc3", "o33"))
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(-1)
+        object_class_mapping.child = ObjectMapping(0)
+        self.assertEqual(
+            list(rows(object_class_mapping, db_map)),
+            [["oc1", "oc2", "oc3"], ["o11", "o21", "o31"], ["o12", None, "o32"], [None, None, "o33"]],
+        )
+        db_map.connection.close()
+
+    def test_object_classes_as_table_names(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_objects(
+            db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31"), ("oc3", "o32"), ("oc3", "o33"))
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(Position.table_name)
+        object_class_mapping.child = ObjectMapping(0)
+        tables = dict()
+        for title, title_key in titles(object_class_mapping, db_map):
+            tables[title] = list(rows(object_class_mapping, db_map, title_key))
+        self.assertEqual(tables, {"oc1": [["o11"], ["o12"]], "oc2": [["o21"]], "oc3": [["o31"], ["o32"], ["o33"]]})
+        db_map.connection.close()
+
+    def test_object_class_and_parameter_definition_as_table_name(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_object_parameters(db_map, (("oc1", "p11"), ("oc2", "p21"), ("oc2", "p22")))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31")))
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(Position.table_name)
+        definition_mapping = ParameterDefinitionMapping(Position.table_name)
+        object_mapping = ObjectMapping(0)
+        object_class_mapping.child = definition_mapping
+        definition_mapping.child = object_mapping
+        tables = dict()
+        for title, title_key in titles(object_class_mapping, db_map):
+            tables[title] = list(rows(object_class_mapping, db_map, title_key))
+        self.assertEqual(
+            tables, {"oc1,p11": [["o11"], ["o12"]], "oc2,p21": [["o21"]], "oc2,p22": [["o21"]], "oc3": [["o31"]]}
+        )
+        db_map.connection.close()
+
+    def test_object_relationship_name_as_table_name(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_objects(db_map, (("oc1", "o1"), ("oc1", "o2"), ("oc2", "O")))
+        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
+        import_relationships(db_map, (("rc", ("o1", "O")), ("rc", ("o2", "O"))))
+        db_map.commit_session("Add test data.")
+        mappings = relationship_export(0, Position.table_name, [1, 2], [Position.table_name, 3])
+        tables = dict()
+        for title, title_key in titles(mappings, db_map):
+            tables[title] = list(rows(mappings, db_map, title_key))
+        self.assertEqual(
+            tables, {"rc_o1__O,o1": [["rc", "oc1", "oc2", "O"]], "rc_o2__O,o2": [["rc", "oc1", "oc2", "O"]]}
+        )
+        db_map.connection.close()
+
+    def test_parameter_definitions_with_value_lists(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_parameter_value_lists(db_map, (("vl1", -1.0), ("vl2", -2.0)))
+        import_object_parameters(db_map, (("oc", "p1", None, "vl1"), ("oc", "p2")))
+        db_map.commit_session("Add test data.")
+        class_mapping = ObjectClassMapping(0)
+        definition_mapping = ParameterDefinitionMapping(1)
+        value_list_mapping = ParameterValueListMapping(2)
+        definition_mapping.child = value_list_mapping
+        class_mapping.child = definition_mapping
+        tables = dict()
+        for title, title_key in titles(class_mapping, db_map):
+            tables[title] = list(rows(class_mapping, db_map, title_key))
+        self.assertEqual(tables, {None: [["oc", "p1", "vl1"]]})
+        db_map.connection.close()
+
+    def test_parameter_definitions_and_values_and_value_lists(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_parameter_value_lists(db_map, (("vl", -1.0),))
+        import_object_parameters(db_map, (("oc", "p1", None, "vl"), ("oc", "p2")))
+        import_objects(db_map, (("oc", "o"),))
+        import_object_parameter_values(db_map, (("oc", "o", "p1", -1.0), ("oc", "o", "p2", 5.0)))
+        db_map.commit_session("Add test data.")
+        flattened = [
+            ObjectClassMapping(0),
+            ParameterDefinitionMapping(1),
+            AlternativeMapping(Position.hidden),
+            ParameterValueListMapping(2),
+            ObjectMapping(3),
+            ParameterValueMapping(4),
+        ]
+        mapping = unflatten(flattened)
+        tables = dict()
+        for title, title_key in titles(mapping, db_map):
+            tables[title] = list(rows(mapping, db_map, title_key))
+        self.assertEqual(tables, {None: [["oc", "p1", "vl", "o", -1.0]]})
+        db_map.connection.close()
+
+    def test_parameter_definitions_and_values_and_ignorable_value_lists(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_parameter_value_lists(db_map, (("vl", -1.0),))
+        import_object_parameters(db_map, (("oc", "p1", None, "vl"), ("oc", "p2")))
+        import_objects(db_map, (("oc", "o"),))
+        import_object_parameter_values(db_map, (("oc", "o", "p1", -1.0), ("oc", "o", "p2", 5.0)))
+        db_map.commit_session("Add test data.")
+        value_list_mapping = ParameterValueListMapping(2)
+        value_list_mapping.set_ignorable(True)
+        flattened = [
+            ObjectClassMapping(0),
+            ParameterDefinitionMapping(1),
+            AlternativeMapping(Position.hidden),
+            value_list_mapping,
+            ObjectMapping(3),
+            ParameterValueMapping(4),
+        ]
+        mapping = unflatten(flattened)
+        tables = dict()
+        for title, title_key in titles(mapping, db_map):
+            tables[title] = list(rows(mapping, db_map, title_key))
+        self.assertEqual(tables, {None: [["oc", "p1", "vl", "o", -1.0], ["oc", "p2", None, "o", 5.0]]})
+        db_map.connection.close()
+
+    def test_parameter_value_lists(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_parameter_value_lists(db_map, (("vl1", -1.0), ("vl2", -2.0)))
+        db_map.commit_session("Add test data.")
+        value_list_mapping = ParameterValueListMapping(0)
+        tables = dict()
+        for title, title_key in titles(value_list_mapping, db_map):
+            tables[title] = list(rows(value_list_mapping, db_map, title_key))
+        self.assertEqual(tables, {None: [["vl1"], ["vl2"]]})
+        db_map.connection.close()
+
+    def test_parameter_value_list_values(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_parameter_value_lists(db_map, (("vl1", -1.0), ("vl2", -2.0)))
+        db_map.commit_session("Add test data.")
+        value_list_mapping = ParameterValueListMapping(Position.table_name)
+        value_mapping = ParameterValueListValueMapping(0)
+        value_list_mapping.child = value_mapping
+        tables = dict()
+        for title, title_key in titles(value_list_mapping, db_map):
+            tables[title] = list(rows(value_list_mapping, db_map, title_key))
+        self.assertEqual(tables, {"vl1": [[-1.0]], "vl2": [[-2.0]]})
+        db_map.connection.close()
+
+    def test_no_item_declared_as_title_gives_full_table(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_object_parameters(db_map, (("oc1", "p11"), ("oc2", "p21"), ("oc2", "p22")))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31")))
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(Position.hidden)
+        definition_mapping = ParameterDefinitionMapping(Position.hidden)
+        object_mapping = ObjectMapping(0)
+        object_class_mapping.child = definition_mapping
+        definition_mapping.child = object_mapping
+        tables = dict()
+        for title, title_key in titles(object_class_mapping, db_map):
+            tables[title] = list(rows(object_class_mapping, db_map, title_key))
+        self.assertEqual(tables, {None: [["o11"], ["o12"], ["o21"], ["o21"]]})
+        db_map.connection.close()
+
+    def test_missing_values_for_alternatives(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p1"), ("oc", "p2")))
+        import_alternatives(db_map, ("alt1", "alt2"))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o1", "p1", -1.1, "alt1"),
+                ("oc", "o1", "p1", -1.2, "alt2"),
+                ("oc", "o1", "p2", -2.2, "alt1"),
+                ("oc", "o2", "p2", -5.5, "alt2"),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        definition_mapping = ParameterDefinitionMapping(2)
+        object_mapping = ObjectMapping(1)
+        alternative_mapping = AlternativeMapping(3)
+        value_mapping = ParameterValueMapping(4)
+        object_class_mapping.child = definition_mapping
+        definition_mapping.child = object_mapping
+        object_mapping.child = alternative_mapping
+        alternative_mapping.child = value_mapping
+        expected = [
+            ["oc", "o1", "p1", "alt1", -1.1],
+            ["oc", "o1", "p1", "alt2", -1.2],
+            ["oc", "o1", "p2", "alt1", -2.2],
+            ["oc", "o2", "p2", "alt2", -5.5],
+        ]
+        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_export_relationship_classes(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_relationship_classes(
+            db_map, (("rc1", ("oc1",)), ("rc2", ("oc3", "oc2")), ("rc3", ("oc2", "oc3", "oc1")))
+        )
+        db_map.commit_session("Add test data.")
+        relationship_class_mapping = RelationshipClassMapping(0)
+        self.assertEqual(list(rows(relationship_class_mapping, db_map)), [["rc1"], ["rc2"], ["rc3"]])
+        db_map.connection.close()
+
+    def test_export_relationships(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
+        import_relationship_classes(db_map, (("rc1", ("oc1",)), ("rc2", ("oc2", "oc1"))))
+        import_relationships(db_map, (("rc1", ("o11",)), ("rc2", ("o21", "o11")), ("rc2", ("o21", "o12"))))
+        db_map.commit_session("Add test data.")
+        relationship_class_mapping = RelationshipClassMapping(0)
+        relationship_mapping = RelationshipMapping(1)
+        relationship_class_mapping.child = relationship_mapping
+        expected = [["rc1", "rc1_o11"], ["rc2", "rc2_o21__o11"], ["rc2", "rc2_o21__o12"]]
+        self.assertEqual(list(rows(relationship_class_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_relationships_with_different_dimensions(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc2", "o22")))
+        import_relationship_classes(db_map, (("rc1D", ("oc1",)), ("rc2D", ("oc1", "oc2"))))
+        import_relationships(db_map, (("rc1D", ("o11",)), ("rc1D", ("o12",))))
+        import_relationships(
+            db_map,
+            (("rc2D", ("o11", "o21")), ("rc2D", ("o11", "o22")), ("rc2D", ("o12", "o21")), ("rc2D", ("o12", "o22"))),
+        )
+        db_map.commit_session("Add test data.")
+        relationship_class_mapping = RelationshipClassMapping(0)
+        object_class_mapping1 = RelationshipClassObjectClassMapping(1)
+        object_class_mapping2 = RelationshipClassObjectClassMapping(2)
+        relationship_mapping = RelationshipMapping(Position.hidden)
+        object_mapping1 = RelationshipObjectMapping(3)
+        object_mapping2 = RelationshipObjectMapping(4)
+        object_mapping1.child = object_mapping2
+        relationship_mapping.child = object_mapping1
+        object_class_mapping2.child = relationship_mapping
+        object_class_mapping1.child = object_class_mapping2
+        relationship_class_mapping.child = object_class_mapping1
+        tables = dict()
+        for title, title_key in titles(relationship_class_mapping, db_map):
+            tables[title] = list(rows(relationship_class_mapping, db_map, title_key))
+        expected = [
+            ["rc1D", "oc1", "", "o11", ""],
+            ["rc1D", "oc1", "", "o12", ""],
+            ["rc2D", "oc1", "oc2", "o11", "o21"],
+            ["rc2D", "oc1", "oc2", "o11", "o22"],
+            ["rc2D", "oc1", "oc2", "o12", "o21"],
+            ["rc2D", "oc1", "oc2", "o12", "o22"],
+        ]
+        self.assertEqual(tables[None], expected)
+        db_map.connection.close()
+
+    def test_default_parameter_values(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_object_parameters(db_map, (("oc1", "p11", 3.14), ("oc2", "p21", 14.3), ("oc2", "p22", -1.0)))
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        definition_mapping = ParameterDefinitionMapping(1)
+        default_value_mapping = ParameterDefaultValueMapping(2)
+        definition_mapping.child = default_value_mapping
+        object_class_mapping.child = definition_mapping
+        table = list(rows(object_class_mapping, db_map))
+        self.assertEqual(table, [["oc1", "p11", 3.14], ["oc2", "p21", 14.3], ["oc2", "p22", -1.0]])
+        db_map.connection.close()
+
+    def test_indexed_default_parameter_values(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_object_parameters(
+            db_map,
+            (
+                ("oc1", "p11", Map(["a", "b"], [-6.28, -3.14])),
+                ("oc2", "p21", Map(["A", "B"], [1.1, 2.2])),
+                ("oc2", "p22", Map(["D"], [-1.0])),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        definition_mapping = ParameterDefinitionMapping(1)
+        index_mapping = ParameterDefaultValueIndexMapping(2)
+        value_mapping = ExpandedParameterDefaultValueMapping(3)
+        index_mapping.child = value_mapping
+        definition_mapping.child = index_mapping
+        object_class_mapping.child = definition_mapping
+        table = list(rows(object_class_mapping, db_map))
+        expected = [
+            ["oc1", "p11", "a", -6.28],
+            ["oc1", "p11", "b", -3.14],
+            ["oc2", "p21", "A", 1.1],
+            ["oc2", "p21", "B", 2.2],
+            ["oc2", "p22", "D", -1.0],
+        ]
+        self.assertEqual(table, expected)
+        db_map.connection.close()
+
+    def test_replace_parameter_indexes_by_external_data(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p1"),))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map, (("oc", "o1", "p1", Map(["a", "b"], [5.0, -5.0])), ("oc", "o2", "p1", Map(["a", "b"], [2.0, -2.0])))
+        )
+        db_map.commit_session("Add test data.")
+        object_class_mapping = ObjectClassMapping(0)
+        parameter_definition_mapping = ParameterDefinitionMapping(2)
+        alternative_mapping = AlternativeMapping(Position.hidden)
+        parameter_definition_mapping.child = alternative_mapping
+        object_mapping = ObjectMapping(1)
+        alternative_mapping.child = object_mapping
+        index_mapping = ParameterValueIndexMapping(3)
+        value_mapping = ExpandedParameterValueMapping(4)
+        index_mapping.child = value_mapping
+        index_mapping.replace_data(["c", "d"])
+        object_mapping.child = index_mapping
+        object_class_mapping.child = parameter_definition_mapping
+        expected = [
+            ["oc", "o1", "p1", "c", 5.0],
+            ["oc", "o1", "p1", "d", -5.0],
+            ["oc", "o2", "p1", "c", 2.0],
+            ["oc", "o2", "p1", "d", -2.0],
+        ]
+        self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_constant_mapping_as_title(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        db_map.commit_session("Add test data.")
+        constant_mapping = FixedValueMapping(Position.table_name, "title_text")
+        object_class_mapping = ObjectClassMapping(0)
+        constant_mapping.child = object_class_mapping
+        tables = dict()
+        for title, title_key in titles(constant_mapping, db_map):
+            tables[title] = list(rows(constant_mapping, db_map, title_key))
+        self.assertEqual(tables, {"title_text": [["oc1"], ["oc2"], ["oc3"]]})
+        db_map.connection.close()
+
+    def test_scenario_mapping(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_scenarios(db_map, ("s1", "s2"))
+        db_map.commit_session("Add test data.")
+        scenario_mapping = ScenarioMapping(0)
+        tables = dict()
+        for title, title_key in titles(scenario_mapping, db_map):
+            tables[title] = list(rows(scenario_mapping, db_map, title_key))
+        self.assertEqual(tables, {None: [["s1"], ["s2"]]})
+        db_map.connection.close()
+
+    def test_scenario_active_flag_mapping(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_scenarios(db_map, (("s1", True), ("s2", False)))
+        db_map.commit_session("Add test data.")
+        scenario_mapping = ScenarioMapping(0)
+        active_flag_mapping = ScenarioActiveFlagMapping(1)
+        scenario_mapping.child = active_flag_mapping
+        tables = dict()
+        for title, title_key in titles(scenario_mapping, db_map):
+            tables[title] = list(rows(scenario_mapping, db_map, title_key))
+        self.assertEqual(tables, {None: [["s1", True], ["s2", False]]})
+        db_map.connection.close()
+
+    def test_scenario_alternative_mapping(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_alternatives(db_map, ("a1", "a2", "a3"))
+        import_scenarios(db_map, ("s1", "s2", "empty"))
+        import_scenario_alternatives(db_map, (("s1", "a2"), ("s1", "a1", "a2"), ("s2", "a2"), ("s2", "a3", "a2")))
+        db_map.commit_session("Add test data.")
+        scenario_mapping = ScenarioMapping(0)
+        scenario_alternative_mapping = ScenarioAlternativeMapping(1)
+        scenario_mapping.child = scenario_alternative_mapping
+        tables = dict()
+        for title, title_key in titles(scenario_mapping, db_map):
+            tables[title] = list(rows(scenario_mapping, db_map, title_key))
+        self.assertEqual(tables, {None: [["s1", "a1"], ["s1", "a2"], ["s2", "a2"], ["s2", "a3"]]})
+        db_map.connection.close()
+
+    def test_tool_mapping(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_tools(db_map, ("tool1", "tool2"))
+        db_map.commit_session("Add test data.")
+        tool_mapping = ToolMapping(0)
+        tables = dict()
+        for title, title_key in titles(tool_mapping, db_map):
+            tables[title] = list(rows(tool_mapping, db_map, title_key))
+        self.assertEqual(tables, {None: [["tool1"], ["tool2"]]})
+        db_map.connection.close()
+
+    def test_feature_mapping(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_parameter_value_lists(db_map, (("features", "feat1"), ("features", "feat2")))
+        import_object_parameters(
+            db_map,
+            (
+                ("oc1", "p1", "feat1", "features"),
+                ("oc1", "p2", "feat1", "features"),
+                ("oc2", "p3", "feat2", "features"),
+            ),
+        )
+        import_features(db_map, (("oc1", "p2"), ("oc2", "p3")))
+        db_map.commit_session("Add test data.")
+        class_mapping = FeatureEntityClassMapping(0)
+        parameter_mapping = FeatureParameterDefinitionMapping(1)
+        class_mapping.child = parameter_mapping
+        tables = dict()
+        for title, title_key in titles(class_mapping, db_map):
+            tables[title] = list(rows(class_mapping, db_map, title_key))
+        self.assertEqual(tables, {None: [["oc1", "p2"], ["oc2", "p3"]]})
+        db_map.connection.close()
+
+    def test_tool_feature_mapping(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_parameter_value_lists(db_map, (("features", "feat1"), ("features", "feat2")))
+        import_object_parameters(
+            db_map,
+            (
+                ("oc1", "p1", "feat1", "features"),
+                ("oc1", "p2", "feat1", "features"),
+                ("oc2", "p3", "feat2", "features"),
+            ),
+        )
+        import_features(db_map, (("oc1", "p1"), ("oc1", "p2"), ("oc2", "p3")))
+        import_tools(db_map, ("tool1", "tool2"))
+        import_tool_features(
+            db_map, (("tool1", "oc1", "p1", True), ("tool1", "oc2", "p3", False), ("tool2", "oc1", "p1", True))
+        )
+        db_map.commit_session("Add test data.")
+        mapping = unflatten(
+            [
+                ToolMapping(Position.table_name),
+                ToolFeatureEntityClassMapping(0),
+                ToolFeatureParameterDefinitionMapping(1),
+                ToolFeatureRequiredFlagMapping(2),
+            ]
+        )
+        tables = dict()
+        for title, title_key in titles(mapping, db_map):
+            tables[title] = list(rows(mapping, db_map, title_key))
+        expected = {"tool1": [["oc1", "p1", True], ["oc2", "p3", False]], "tool2": [["oc1", "p1", True]]}
+        self.assertEqual(tables, expected)
+        db_map.connection.close()
+
+    def test_tool_feature_method_mapping(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_parameter_value_lists(db_map, (("features", "feat1"), ("features", "feat2")))
+        import_object_parameters(
+            db_map,
+            (
+                ("oc1", "p1", "feat1", "features"),
+                ("oc1", "p2", "feat1", "features"),
+                ("oc2", "p3", "feat2", "features"),
+            ),
+        )
+        import_features(db_map, (("oc1", "p1"), ("oc1", "p2"), ("oc2", "p3")))
+        import_tools(db_map, ("tool1", "tool2"))
+        import_tool_features(
+            db_map, (("tool1", "oc1", "p1", True), ("tool1", "oc2", "p3", False), ("tool2", "oc1", "p1", True))
+        )
+        import_tool_feature_methods(
+            db_map,
+            (
+                ("tool1", "oc1", "p1", "feat1"),
+                ("tool1", "oc1", "p1", "feat2"),
+                ("tool2", "oc1", "p1", "feat1"),
+                ("tool2", "oc1", "p1", "feat2"),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        mapping = unflatten(
+            [
+                ToolMapping(Position.table_name),
+                ToolFeatureMethodEntityClassMapping(0),
+                ToolFeatureMethodParameterDefinitionMapping(1),
+                ToolFeatureMethodMethodMapping(2),
+            ]
+        )
+        tables = dict()
+        for title, title_key in titles(mapping, db_map):
+            tables[title] = list(rows(mapping, db_map, title_key))
+        expected = {
+            "tool1": [["oc1", "p1", "feat1"], ["oc1", "p1", "feat2"]],
+            "tool2": [["oc1", "p1", "feat1"], ["oc1", "p1", "feat2"]],
+        }
+        self.assertEqual(tables, expected)
+        db_map.connection.close()
+
+    def test_header(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        root = unflatten([ObjectClassMapping(0, header="class"), ObjectMapping(1, header="entity")])
+        expected = [["class", "entity"], ["oc", "o1"]]
+        self.assertEqual(list(rows(root, db_map)), expected)
+        db_map.connection.close()
+
+    def test_header_without_data_still_creates_header(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        root = unflatten([ObjectClassMapping(0, header="class"), ObjectMapping(1, header="object")])
+        expected = [["class", "object"]]
+        self.assertEqual(list(rows(root, db_map)), expected)
+        db_map.connection.close()
+
+    def test_header_in_half_pivot_table_without_data_still_creates_header(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        root = unflatten([ObjectClassMapping(-1, header="class"), ObjectMapping(9, header="object")])
+        expected = [["class"]]
+        self.assertEqual(list(rows(root, db_map)), expected)
+        db_map.connection.close()
+
+    def test_header_in_pivot_table_without_data_still_creates_header(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        root = unflatten(
+            [
+                ObjectClassMapping(-1, header="class"),
+                ParameterDefinitionMapping(0, header="parameter"),
+                ObjectMapping(-2, header="object"),
+                AlternativeMapping(1, header="alternative"),
+                ParameterValueMapping(0),
+            ]
+        )
+        expected = [[None, "class"], ["parameter", "alternative"]]
+        self.assertEqual(list(rows(root, db_map)), expected)
+        db_map.connection.close()
+
+    def test_disabled_empty_data_header(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        root = unflatten([ObjectClassMapping(0, header="class"), ObjectMapping(1, header="object")])
+        expected = []
+        self.assertEqual(list(rows(root, db_map, empty_data_header=False)), expected)
+        db_map.connection.close()
+
+    def test_disabled_empty_data_header_in_pivot_table(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        root = unflatten([ObjectClassMapping(-1, header="class"), ObjectMapping(0)])
+        expected = []
+        self.assertEqual(list(rows(root, db_map, empty_data_header=False)), expected)
+        db_map.connection.close()
+
+    def test_header_position(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        root = unflatten([ObjectClassMapping(Position.header), ObjectMapping(0)])
+        expected = [["oc"], ["o1"]]
+        self.assertEqual(list(rows(root, db_map)), expected)
+        db_map.connection.close()
+
+    def test_header_position_with_relationships(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_objects(db_map, (("oc1", "o11"), ("oc2", "o21")))
+        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
+        import_relationships(db_map, (("rc", ("o11", "o21")),))
+        db_map.commit_session("Add test data.")
+        root = unflatten(
+            [
+                RelationshipClassMapping(0),
+                RelationshipClassObjectClassMapping(Position.header),
+                RelationshipClassObjectClassMapping(Position.header),
+                RelationshipMapping(1),
+                RelationshipObjectMapping(2),
+                RelationshipObjectMapping(3),
+            ]
+        )
+        expected = [["", "", "oc1", "oc2"], ["rc", "rc_o11__o21", "o11", "o21"]]
+        self.assertEqual(list(rows(root, db_map)), expected)
+        db_map.connection.close()
+
+    def test_header_position_with_relationships_but_no_data(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
+        db_map.commit_session("Add test data.")
+        root = unflatten(
+            [
+                RelationshipClassMapping(0),
+                RelationshipClassObjectClassMapping(Position.header),
+                RelationshipClassObjectClassMapping(Position.header),
+                RelationshipMapping(1),
+                RelationshipObjectMapping(2),
+                RelationshipObjectMapping(3),
+            ]
+        )
+        expected = [["", "", "oc1", "oc2"]]
+        self.assertEqual(list(rows(root, db_map)), expected)
+        db_map.connection.close()
+
+    def test_header_and_pivot(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_alternatives(db_map, ("alt",))
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p1"),))
+        import_object_parameters(db_map, (("oc", "p2"),))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o1", "p1", Map(["A", "B"], [-1.1, -2.2]), "Base"),
+                ("oc", "o1", "p1", Map(["A", "B"], [-3.3, -4.4]), "alt"),
+                ("oc", "o1", "p2", Map(["A", "B"], [-5.5, -6.6]), "Base"),
+                ("oc", "o1", "p2", Map(["A", "B"], [-7.7, -8.8]), "alt"),
+                ("oc", "o2", "p1", Map(["A", "B"], [-9.9, -10.1]), "Base"),
+                ("oc", "o2", "p1", Map(["A", "B"], [-11.1, -12.2]), "alt"),
+                ("oc", "o2", "p2", Map(["A", "B"], [-13.3, -14.4]), "Base"),
+                ("oc", "o2", "p2", Map(["A", "B"], [-15.5, -16.6]), "alt"),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        mapping = unflatten(
+            [
+                ObjectClassMapping(0, header="class"),
+                ParameterDefinitionMapping(1, header="parameter"),
+                ObjectMapping(-1, header="object"),
+                AlternativeMapping(-2, header="alternative"),
+                ParameterValueIndexMapping(-3, header=""),
+                ExpandedParameterValueMapping(2, header="value"),
+            ]
+        )
+        expected = [
+            [None, "object", "o1", "o1", "o1", "o1", "o2", "o2", "o2", "o2"],
+            [None, "alternative", "Base", "Base", "alt", "alt", "Base", "Base", "alt", "alt"],
+            ["class", "parameter", "A", "B", "A", "B", "A", "B", "A", "B"],
+            ["oc", "p1", -1.1, -2.2, -3.3, -4.4, -9.9, -10.1, -11.1, -12.2],
+            ["oc", "p2", -5.5, -6.6, -7.7, -8.8, -13.3, -14.4, -15.5, -16.6],
+        ]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_pivot_without_left_hand_side_has_padding_column_for_headers(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_alternatives(db_map, ("alt",))
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p1"),))
+        import_object_parameters(db_map, (("oc", "p2"),))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o1", "p1", Map(["A", "B"], [-1.1, -2.2]), "Base"),
+                ("oc", "o1", "p1", Map(["A", "B"], [-3.3, -4.4]), "alt"),
+                ("oc", "o1", "p2", Map(["A", "B"], [-5.5, -6.6]), "Base"),
+                ("oc", "o1", "p2", Map(["A", "B"], [-7.7, -8.8]), "alt"),
+                ("oc", "o2", "p1", Map(["A", "B"], [-9.9, -10.1]), "Base"),
+                ("oc", "o2", "p1", Map(["A", "B"], [-11.1, -12.2]), "alt"),
+                ("oc", "o2", "p2", Map(["A", "B"], [-13.3, -14.4]), "Base"),
+                ("oc", "o2", "p2", Map(["A", "B"], [-15.5, -16.6]), "alt"),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        mapping = unflatten(
+            [
+                ObjectClassMapping(Position.header),
+                ParameterDefinitionMapping(Position.hidden, header="parameter"),
+                ObjectMapping(-1),
+                AlternativeMapping(-2, header="alternative"),
+                ParameterValueIndexMapping(-3, header="index"),
+                ExpandedParameterValueMapping(2, header="value"),
+            ]
+        )
+        expected = [
+            ["oc", "o1", "o1", "o1", "o1", "o2", "o2", "o2", "o2"],
+            ["alternative", "Base", "Base", "alt", "alt", "Base", "Base", "alt", "alt"],
+            ["index", "A", "B", "A", "B", "A", "B", "A", "B"],
+            [None, -1.1, -2.2, -3.3, -4.4, -9.9, -10.1, -11.1, -12.2],
+            [None, -5.5, -6.6, -7.7, -8.8, -13.3, -14.4, -15.5, -16.6],
+        ]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_count_mappings(self):
+        object_class_mapping = ObjectClassMapping(2)
+        parameter_definition_mapping = ParameterDefinitionMapping(0)
+        object_mapping = ObjectMapping(1)
+        parameter_definition_mapping.child = object_mapping
+        object_class_mapping.child = parameter_definition_mapping
+        self.assertEqual(object_class_mapping.count_mappings(), 3)
+
+    def test_flatten(self):
+        object_class_mapping = ObjectClassMapping(2)
+        parameter_definition_mapping = ParameterDefinitionMapping(0)
+        object_mapping = ObjectMapping(1)
+        parameter_definition_mapping.child = object_mapping
+        object_class_mapping.child = parameter_definition_mapping
+        mappings = object_class_mapping.flatten()
+        self.assertEqual(mappings, [object_class_mapping, parameter_definition_mapping, object_mapping])
+
+    def test_unflatten_sets_last_mappings_child_to_none(self):
+        object_class_mapping = ObjectClassMapping(2)
+        object_mapping = ObjectMapping(1)
+        object_class_mapping.child = object_mapping
+        mapping_list = object_class_mapping.flatten()
+        root = unflatten(mapping_list[:1])
+        self.assertIsNone(root.child)
+
+    def test_has_titles(self):
+        object_class_mapping = ObjectClassMapping(0)
+        parameter_definition_mapping = ParameterDefinitionMapping(Position.table_name)
+        object_mapping = ObjectMapping(1)
+        parameter_definition_mapping.child = object_mapping
+        object_class_mapping.child = parameter_definition_mapping
+        self.assertTrue(object_class_mapping.has_titles())
+
+    def test_drop_non_positioned_tail(self):
+        object_class_mapping = ObjectClassMapping(0)
+        parameter_definition_mapping = ParameterDefinitionMapping(Position.hidden)
+        object_mapping = ObjectMapping(1)
+        alternative_mapping = AlternativeMapping(Position.hidden)
+        value_mapping = ParameterValueMapping(Position.hidden)
+        alternative_mapping.child = value_mapping
+        object_mapping.child = alternative_mapping
+        parameter_definition_mapping.child = object_mapping
+        object_class_mapping.child = parameter_definition_mapping
+        tail_cut_mapping = drop_non_positioned_tail(object_class_mapping)
+        flattened = tail_cut_mapping.flatten()
+        self.assertEqual(flattened, [object_class_mapping, parameter_definition_mapping, object_mapping])
+
+    def test_serialization(self):
+        highlight_dimension = 5
+        mappings = [
+            ObjectClassMapping(0),
+            RelationshipClassMapping(Position.table_name, highlight_dimension=highlight_dimension),
+            RelationshipClassObjectClassMapping(2),
+            ParameterDefinitionMapping(1),
+            ObjectMapping(-1),
+            RelationshipMapping(Position.hidden),
+            RelationshipObjectMapping(-1),
+            AlternativeMapping(3),
+            ParameterValueMapping(4),
+            ParameterValueIndexMapping(5),
+            ParameterValueTypeMapping(6),
+            ExpandedParameterValueMapping(7),
+            FixedValueMapping(8, "gaga"),
+        ]
+        expected_positions = [m.position for m in mappings]
+        expected_types = [type(m) for m in mappings]
+        root = unflatten(mappings)
+        serialized = to_dict(root)
+        deserialized = from_dict(serialized).flatten()
+        self.assertEqual([type(m) for m in deserialized], expected_types)
+        self.assertEqual([m.position for m in deserialized], expected_positions)
+        for m in deserialized:
+            if isinstance(m, FixedValueMapping):
+                self.assertEqual(m.value, "gaga")
+            elif isinstance(m, RelationshipClassMapping):
+                self.assertEqual(m.highlight_dimension, highlight_dimension)
+
+    def test_setting_ignorable_flag(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        db_map.commit_session("Add test data.")
+        object_mapping = ObjectMapping(1)
+        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
+        object_mapping.set_ignorable(True)
+        self.assertTrue(object_mapping.is_ignorable())
+        expected = [["oc", None]]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_unsetting_ignorable_flag(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        object_mapping = ObjectMapping(1)
+        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
+        object_mapping.set_ignorable(True)
+        self.assertTrue(object_mapping.is_ignorable())
+        object_mapping.set_ignorable(False)
+        self.assertFalse(object_mapping.is_ignorable())
+        expected = [["oc", "o1"]]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_filter(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
+        db_map.commit_session("Add test data.")
+        object_mapping = ObjectMapping(1)
+        object_mapping.filter_re = "o1"
+        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
+        expected = [["oc", "o1"]]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_hidden_tail_filter(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_objects(db_map, (("oc1", "o1"), ("oc2", "o2")))
+        db_map.commit_session("Add test data.")
+        object_mapping = ObjectMapping(Position.hidden)
+        object_mapping.filter_re = "o1"
+        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
+        expected = [["oc1"]]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_index_names(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o"),))
+        import_object_parameter_values(db_map, (("oc", "o", "p", Map(["a"], [5.0], index_name="index")),))
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_export(0, 2, Position.hidden, 1, 3, Position.hidden, 5, [Position.header], [4])
+        expected = [["", "", "", "", "index", ""], ["oc", "o", "p", "Base", "a", 5.0]]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_default_value_index_names_with_nested_map(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(
+            db_map, (("oc", "p", Map(["A"], [Map(["b"], [2.3], index_name="idx2")], index_name="idx1")),)
+        )
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_default_value_export(
+            0, 1, Position.hidden, 4, [Position.header, Position.header], [2, 3]
+        )
+        expected = [["", "", "idx1", "idx2", ""], ["oc", "p", "A", "b", 2.3]]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_multiple_index_names_with_empty_database(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        mapping = relationship_parameter_export(
+            0, 4, Position.hidden, 1, [2], [3], 5, Position.hidden, 8, [Position.header, Position.header], [6, 7]
+        )
+        expected = [9 * [""]]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_parameter_default_value_type(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_object_parameters(db_map, (("oc1", "p11", 3.14), ("oc2", "p21", 14.3), ("oc2", "p22", -1.0)))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_parameter_default_value_export(0, 1, 2, 3, None, None)
+        expected = [
+            ["oc1", "p11", "single_value", 3.14],
+            ["oc2", "p21", "single_value", 14.3],
+            ["oc2", "p22", "single_value", -1.0],
+        ]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_map_with_more_dimensions_than_index_mappings(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o"),))
+        import_object_parameter_values(db_map, (("oc", "o", "p", Map(["A"], [Map(["b"], [2.3])])),))
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_export(
+            0, 1, Position.hidden, 2, Position.hidden, Position.hidden, 4, [Position.hidden], [3]
+        )
+        expected = [["oc", "p", "o", "A", "map"]]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_default_map_value_with_more_dimensions_than_index_mappings(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p", Map(["A"], [Map(["b"], [2.3])])),))
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_default_value_export(0, 1, Position.hidden, 3, [Position.hidden], [2])
+        expected = [["oc", "p", "A", "map"]]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_map_with_single_value_mapping(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o"),))
+        import_object_parameter_values(db_map, (("oc", "o", "p", Map(["A"], [2.3])),))
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_export(0, 1, Position.hidden, 2, Position.hidden, Position.hidden, 3, None, None)
+        expected = [["oc", "p", "o", "map"]]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_default_map_value_with_single_value_mapping(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p", Map(["A"], [2.3])),))
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_default_value_export(0, 1, Position.hidden, 2, None, None)
+        expected = [["oc", "p", "map"]]
+        self.assertEqual(list(rows(mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_table_gets_exported_even_without_parameter_values(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        db_map.commit_session("Add test data.")
+        mapping = object_parameter_export(Position.header, Position.table_name, object_position=0, value_position=1)
+        tables = dict()
+        for title, title_key in titles(mapping, db_map):
+            tables[title] = list(rows(mapping, db_map, title_key))
+        expected = {"p": [["oc", ""]]}
+        self.assertEqual(tables, expected)
+        db_map.connection.close()
+
+    def test_relationship_class_object_classes_parameters(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_relationship_classes(db_map, (("rc", ("oc",)),))
+        db_map.commit_session("Add test data")
+        root_mapping = unflatten(
+            [
+                RelationshipClassMapping(0, highlight_dimension=0),
+                RelationshipClassObjectClassMapping(1),
+                ParameterDefinitionMapping(2),
+            ]
+        )
+        expected = [["rc", "oc", "p"]]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_relationship_class_object_classes_parameters_multiple_dimensions(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
+        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
+        db_map.commit_session("Add test data")
+        root_mapping = unflatten(
+            [
+                RelationshipClassMapping(0, highlight_dimension=0),
+                RelationshipClassObjectClassMapping(1),
+                RelationshipClassObjectClassMapping(3),
+                ParameterDefinitionMapping(2),
+            ]
+        )
+        expected = [["rc", "oc1", "p11", "oc2"], ["rc", "oc1", "p12", "oc2"]]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_highlight_relationship_objects(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_objects(
+            db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc2", "o22"), ("oc3", "o31"), ("oc3", "o32"))
+        )
+        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
+        import_relationships(db_map, (("rc", ("o11", "o21")), ("rc", ("o12", "o22"))))
+        db_map.commit_session("Add test data")
+        root_mapping = unflatten(
+            [
+                RelationshipClassMapping(0, highlight_dimension=0),
+                RelationshipClassObjectClassMapping(1),
+                RelationshipClassObjectClassMapping(2),
+                RelationshipMapping(3),
+                RelationshipObjectMapping(4),
+                RelationshipObjectMapping(5),
+            ]
+        )
+        expected = [
+            ["rc", "oc1", "oc2", "rc_o11__o21", "o11", "o21"],
+            ["rc", "oc1", "oc2", "rc_o12__o22", "o12", "o22"],
+        ]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_export_object_parameters_while_exporting_relationships(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o"),))
+        import_object_parameter_values(db_map, (("oc", "o", "p", 23.0),))
+        import_relationship_classes(db_map, (("rc", ("oc",)),))
+        import_relationships(db_map, (("rc", ("o",)),))
+        db_map.commit_session("Add test data")
+        root_mapping = unflatten(
+            [
+                RelationshipClassMapping(0, highlight_dimension=0),
+                RelationshipClassObjectClassMapping(1),
+                RelationshipMapping(2),
+                RelationshipObjectMapping(3),
+                ParameterDefinitionMapping(4),
+                AlternativeMapping(5),
+                ParameterValueMapping(6),
+            ]
+        )
+        expected = [["rc", "oc", "rc_o", "o", "p", "Base", 23.0]]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_export_default_values_of_object_parameters_while_exporting_relationships(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p", 23.0),))
+        import_objects(db_map, (("oc", "o"),))
+        import_relationship_classes(db_map, (("rc", ("oc",)),))
+        import_relationships(db_map, (("rc", ("o",)),))
+        db_map.commit_session("Add test data")
+        root_mapping = unflatten(
+            [
+                RelationshipClassMapping(0, highlight_dimension=0),
+                RelationshipClassObjectClassMapping(1),
+                ParameterDefinitionMapping(2),
+                ParameterDefaultValueMapping(3),
+            ]
+        )
+        expected = [["rc", "oc", "p", 23.0]]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+    def test_export_object_parameters_while_exporting_relationships_with_multiple_parameters_and_classes2(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2", "oc3"))
+        import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21"), ("oc3", "p31")))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc2", "o22"), ("oc3", "o31")))
+        import_object_parameter_values(db_map, (("oc1", "o11", "p11", 1.1),))
+        import_object_parameter_values(db_map, (("oc1", "o11", "p12", 2.2),))
+        import_object_parameter_values(db_map, (("oc1", "o12", "p11", 3.3),))
+        import_object_parameter_values(db_map, (("oc1", "o12", "p12", 4.4),))
+        import_object_parameter_values(db_map, (("oc2", "o21", "p21", 5.5),))
+        import_object_parameter_values(db_map, (("oc2", "o22", "p21", 6.6),))
+        import_object_parameter_values(db_map, (("oc3", "o31", "p31", 7.7),))
+        import_relationship_classes(db_map, (("rc12", ("oc1", "oc2")),))
+        import_relationship_classes(db_map, (("rc23", ("oc2", "oc3")),))
+        import_relationships(db_map, (("rc12", ("o11", "o21")),))
+        import_relationships(db_map, (("rc12", ("o12", "o21")),))
+        import_relationships(db_map, (("rc23", ("o21", "o31")),))
+        db_map.commit_session("Add test data")
+        root_mapping = unflatten(
+            [
+                RelationshipClassMapping(0, highlight_dimension=1),
+                RelationshipClassObjectClassMapping(1),
+                RelationshipClassObjectClassMapping(2),
+                RelationshipMapping(3),
+                RelationshipObjectMapping(4),
+                RelationshipObjectMapping(5),
+                ParameterDefinitionMapping(6),
+                AlternativeMapping(7),
+                ParameterValueMapping(8),
+            ]
+        )
+        expected = [
+            ["rc12", "oc1", "oc2", "rc12_o11__o21", "o11", "o21", "p21", "Base", 5.5],
+            ["rc12", "oc1", "oc2", "rc12_o12__o21", "o12", "o21", "p21", "Base", 5.5],
+            ["rc23", "oc2", "oc3", "rc23_o21__o31", "o21", "o31", "p31", "Base", 7.7],
+        ]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/export_mapping/test_pivot.py` & `spinedb_api-0.30.4/tests/export_mapping/test_pivot.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,237 +1,237 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Contains unit tests for the ``pivot`` module.
-
-"""
-import unittest
-import numpy
-from spinedb_api.export_mapping.pivot import make_pivot
-
-
-class TestPivot(unittest.TestCase):
-    def test_pivot(self):
-        table = [
-            ["A", "a", "1", -1.1],
-            ["A", "a", "2", -2.2],
-            ["A", "b", "1", -3.3],
-            ["A", "b", "2", -4.4],
-            ["A", "b", "3", -5.5],
-            ["B", "a", "2", -6.6],
-            ["B", "b", "2", -7.7],
-            ["B", "c", "3", -8.8],
-            ["C", "a", "1", -9.9],
-        ]
-        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2]))
-        expected = [
-            ["H", "1", "2", "3"],
-            ["A", -1.1, -2.2, None],
-            ["A", -3.3, -4.4, -5.5],
-            ["B", None, -6.6, None],
-            ["B", None, -7.7, None],
-            ["B", None, None, -8.8],
-            ["C", -9.9, None, None],
-        ]
-        self.assertEqual(pivot_table, expected)
-
-    def test_half_pivot(self):
-        table = [
-            ["A", "a", "1", -1.1],
-            ["A", "a", "2", -2.2],
-            ["A", "b", "1", -3.3],
-            ["A", "b", "2", -4.4],
-            ["A", "b", "3", -5.5],
-            ["B", "a", "2", -6.6],
-            ["B", "b", "2", -7.7],
-            ["B", "c", "3", -8.8],
-            ["C", "a", "1", -9.9],
-        ]
-        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [], [], [0, 1, 2]))
-        expected = [
-            ["H", "A", "A", "A", "A", "A", "B", "B", "B", "C"],
-            ["h", "a", "a", "b", "b", "b", "a", "b", "c", "a"],
-            ["#", "1", "2", "1", "2", "3", "2", "2", "3", "1"],
-            [None, -1.1, -2.2, -3.3, -4.4, -5.5, -6.6, -7.7, -8.8, -9.9],
-        ]
-        self.assertEqual(pivot_table, expected)
-
-    def test_pivot_group_concat(self):
-        table = [
-            ["A", "a", "1", -1.1],
-            ["A", "a", "2", -2.2],
-            ["A", "b", "1", -3.3],
-            ["A", "b", "2", -4.4],
-            ["A", "b", "3", -5.5],
-            ["B", "a", "2", -6.6],
-            ["B", "b", "2", -7.7],
-            ["B", "c", "3", -8.8],
-            ["C", "a", "1", -9.9],
-        ]
-        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "concat"))
-        expected = [
-            ["H", "1", "2", "3"],
-            ["A", "-1.1,-3.3", "-2.2,-4.4", "-5.5"],
-            ["B", "", "-6.6,-7.7", "-8.8"],
-            ["C", "-9.9", "", ""],
-        ]
-        self.assertEqual(pivot_table, expected)
-
-    def test_pivot_group_sum(self):
-        table = [
-            ["A", "a", "1", -1.1],
-            ["A", "a", "2", -2.2],
-            ["A", "b", "1", -3.3],
-            ["A", "b", "2", -4.4],
-            ["A", "b", "3", -5.5],
-            ["B", "a", "2", -6.6],
-            ["B", "b", "2", -7.7],
-            ["B", "c", "3", -8.8],
-            ["C", "a", "1", -9.9],
-        ]
-        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "sum"))
-        expected = [
-            ["H", "1", "2", "3"],
-            ["A", -1.1 - 3.3, -2.2 - 4.4, -5.5],
-            ["B", numpy.nan, -6.6 - 7.7, -8.8],
-            ["C", -9.9, numpy.nan, numpy.nan],
-        ]
-        self.assertEqual(pivot_table, expected)
-
-    def test_pivot_group_mean(self):
-        table = [
-            ["A", "a", "1", -1.1],
-            ["A", "a", "2", -2.2],
-            ["A", "b", "1", -3.3],
-            ["A", "b", "2", -4.4],
-            ["A", "b", "3", -5.5],
-            ["B", "a", "2", -6.6],
-            ["B", "b", "2", -7.7],
-            ["B", "c", "3", -8.8],
-            ["C", "a", "1", -9.9],
-        ]
-        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "mean"))
-        expected = [
-            ["H", "1", "2", "3"],
-            ["A", (-1.1 - 3.3) / 2.0, (-2.2 - 4.4) / 2.0, -5.5],
-            ["B", numpy.nan, (-6.6 - 7.7) / 2.0, -8.8],
-            ["C", -9.9, numpy.nan, numpy.nan],
-        ]
-        self.assertEqual(pivot_table, expected)
-
-    def test_pivot_group_min(self):
-        table = [
-            ["A", "a", "1", -1.1],
-            ["A", "a", "2", -2.2],
-            ["A", "b", "1", -3.3],
-            ["A", "b", "2", -4.4],
-            ["A", "b", "3", -5.5],
-            ["B", "a", "2", -6.6],
-            ["B", "b", "2", -7.7],
-            ["B", "c", "3", -8.8],
-            ["C", "a", "1", -9.9],
-        ]
-        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "min"))
-        expected = [
-            ["H", "1", "2", "3"],
-            ["A", -3.3, -4.4, -5.5],
-            ["B", numpy.nan, -7.7, -8.8],
-            ["C", -9.9, numpy.nan, numpy.nan],
-        ]
-        self.assertEqual(pivot_table, expected)
-
-    def test_pivot_group_max(self):
-        table = [
-            ["A", "a", "1", -1.1],
-            ["A", "a", "2", -2.2],
-            ["A", "b", "1", -3.3],
-            ["A", "b", "2", -4.4],
-            ["A", "b", "3", -5.5],
-            ["B", "a", "2", -6.6],
-            ["B", "b", "2", -7.7],
-            ["B", "c", "3", -8.8],
-            ["C", "a", "1", -9.9],
-        ]
-        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "max"))
-        expected = [
-            ["H", "1", "2", "3"],
-            ["A", -1.1, -2.2, -5.5],
-            ["B", numpy.nan, -6.6, -8.8],
-            ["C", -9.9, numpy.nan, numpy.nan],
-        ]
-        self.assertEqual(pivot_table, expected)
-
-    def test_pivot_group_one_or_none(self):
-        table = [
-            ["A", "a", "1", -1.1],
-            ["A", "a", "2", -2.2],
-            ["A", "b", "1", -3.3],
-            ["A", "b", "2", -4.4],
-            ["A", "b", "3", -5.5],
-            ["B", "a", "2", -6.6],
-            ["B", "b", "2", -7.7],
-            ["B", "c", "3", -8.8],
-            ["C", "a", "1", -9.9],
-        ]
-        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "one_or_none"))
-        expected = [["H", "1", "2", "3"], ["A", None, None, -5.5], ["B", None, None, -8.8], ["C", -9.9, None, None]]
-        self.assertEqual(pivot_table, expected)
-
-    def test_Nones_in_regular_keys(self):
-        table = [
-            ["A", "a", "1", -1.1],
-            ["A", "a", "2", -2.2],
-            ["A", "b", "1", -3.3],
-            ["A", "b", "2", -4.4],
-            ["A", "b", "3", -5.5],
-            [None, "a", "2", -6.6],
-            [None, "b", "2", -7.7],
-            [None, "c", "3", -8.8],
-            ["C", "a", "1", -9.9],
-        ]
-        pivot_table = list(make_pivot(table, None, 3, [0], [1], [2]))
-        expected = [
-            [None, "1", "2", "3"],
-            [None, None, -6.6, None],
-            [None, None, -7.7, None],
-            [None, None, None, -8.8],
-            ["A", -1.1, -2.2, None],
-            ["A", -3.3, -4.4, -5.5],
-            ["C", -9.9, None, None],
-        ]
-        self.assertEqual(pivot_table, expected)
-
-    def test_Nones_in_pivot_keys(self):
-        table = [
-            ["A", "a", "1", -1.1],
-            ["A", "a", None, -2.2],
-            ["A", "b", "1", -3.3],
-            ["A", "b", None, -4.4],
-            ["A", "b", "3", -5.5],
-            ["B", "a", None, -6.6],
-            ["B", "b", None, -7.7],
-            ["B", "c", "3", -8.8],
-            ["C", "a", "1", -9.9],
-        ]
-        pivot_table = list(make_pivot(table, None, 3, [0], [1], [2]))
-        expected = [
-            [None, None, "1", "3"],
-            ["A", -2.2, -1.1, None],
-            ["A", -4.4, -3.3, -5.5],
-            ["B", -6.6, None, None],
-            ["B", -7.7, None, None],
-            ["B", None, None, -8.8],
-            ["C", None, -9.9, None],
-        ]
-        self.assertEqual(pivot_table, expected)
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Contains unit tests for the ``pivot`` module.
+
+"""
+import unittest
+import numpy
+from spinedb_api.export_mapping.pivot import make_pivot
+
+
+class TestPivot(unittest.TestCase):
+    def test_pivot(self):
+        table = [
+            ["A", "a", "1", -1.1],
+            ["A", "a", "2", -2.2],
+            ["A", "b", "1", -3.3],
+            ["A", "b", "2", -4.4],
+            ["A", "b", "3", -5.5],
+            ["B", "a", "2", -6.6],
+            ["B", "b", "2", -7.7],
+            ["B", "c", "3", -8.8],
+            ["C", "a", "1", -9.9],
+        ]
+        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2]))
+        expected = [
+            ["H", "1", "2", "3"],
+            ["A", -1.1, -2.2, None],
+            ["A", -3.3, -4.4, -5.5],
+            ["B", None, -6.6, None],
+            ["B", None, -7.7, None],
+            ["B", None, None, -8.8],
+            ["C", -9.9, None, None],
+        ]
+        self.assertEqual(pivot_table, expected)
+
+    def test_half_pivot(self):
+        table = [
+            ["A", "a", "1", -1.1],
+            ["A", "a", "2", -2.2],
+            ["A", "b", "1", -3.3],
+            ["A", "b", "2", -4.4],
+            ["A", "b", "3", -5.5],
+            ["B", "a", "2", -6.6],
+            ["B", "b", "2", -7.7],
+            ["B", "c", "3", -8.8],
+            ["C", "a", "1", -9.9],
+        ]
+        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [], [], [0, 1, 2]))
+        expected = [
+            ["H", "A", "A", "A", "A", "A", "B", "B", "B", "C"],
+            ["h", "a", "a", "b", "b", "b", "a", "b", "c", "a"],
+            ["#", "1", "2", "1", "2", "3", "2", "2", "3", "1"],
+            [None, -1.1, -2.2, -3.3, -4.4, -5.5, -6.6, -7.7, -8.8, -9.9],
+        ]
+        self.assertEqual(pivot_table, expected)
+
+    def test_pivot_group_concat(self):
+        table = [
+            ["A", "a", "1", -1.1],
+            ["A", "a", "2", -2.2],
+            ["A", "b", "1", -3.3],
+            ["A", "b", "2", -4.4],
+            ["A", "b", "3", -5.5],
+            ["B", "a", "2", -6.6],
+            ["B", "b", "2", -7.7],
+            ["B", "c", "3", -8.8],
+            ["C", "a", "1", -9.9],
+        ]
+        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "concat"))
+        expected = [
+            ["H", "1", "2", "3"],
+            ["A", "-1.1,-3.3", "-2.2,-4.4", "-5.5"],
+            ["B", "", "-6.6,-7.7", "-8.8"],
+            ["C", "-9.9", "", ""],
+        ]
+        self.assertEqual(pivot_table, expected)
+
+    def test_pivot_group_sum(self):
+        table = [
+            ["A", "a", "1", -1.1],
+            ["A", "a", "2", -2.2],
+            ["A", "b", "1", -3.3],
+            ["A", "b", "2", -4.4],
+            ["A", "b", "3", -5.5],
+            ["B", "a", "2", -6.6],
+            ["B", "b", "2", -7.7],
+            ["B", "c", "3", -8.8],
+            ["C", "a", "1", -9.9],
+        ]
+        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "sum"))
+        expected = [
+            ["H", "1", "2", "3"],
+            ["A", -1.1 - 3.3, -2.2 - 4.4, -5.5],
+            ["B", numpy.nan, -6.6 - 7.7, -8.8],
+            ["C", -9.9, numpy.nan, numpy.nan],
+        ]
+        self.assertEqual(pivot_table, expected)
+
+    def test_pivot_group_mean(self):
+        table = [
+            ["A", "a", "1", -1.1],
+            ["A", "a", "2", -2.2],
+            ["A", "b", "1", -3.3],
+            ["A", "b", "2", -4.4],
+            ["A", "b", "3", -5.5],
+            ["B", "a", "2", -6.6],
+            ["B", "b", "2", -7.7],
+            ["B", "c", "3", -8.8],
+            ["C", "a", "1", -9.9],
+        ]
+        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "mean"))
+        expected = [
+            ["H", "1", "2", "3"],
+            ["A", (-1.1 - 3.3) / 2.0, (-2.2 - 4.4) / 2.0, -5.5],
+            ["B", numpy.nan, (-6.6 - 7.7) / 2.0, -8.8],
+            ["C", -9.9, numpy.nan, numpy.nan],
+        ]
+        self.assertEqual(pivot_table, expected)
+
+    def test_pivot_group_min(self):
+        table = [
+            ["A", "a", "1", -1.1],
+            ["A", "a", "2", -2.2],
+            ["A", "b", "1", -3.3],
+            ["A", "b", "2", -4.4],
+            ["A", "b", "3", -5.5],
+            ["B", "a", "2", -6.6],
+            ["B", "b", "2", -7.7],
+            ["B", "c", "3", -8.8],
+            ["C", "a", "1", -9.9],
+        ]
+        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "min"))
+        expected = [
+            ["H", "1", "2", "3"],
+            ["A", -3.3, -4.4, -5.5],
+            ["B", numpy.nan, -7.7, -8.8],
+            ["C", -9.9, numpy.nan, numpy.nan],
+        ]
+        self.assertEqual(pivot_table, expected)
+
+    def test_pivot_group_max(self):
+        table = [
+            ["A", "a", "1", -1.1],
+            ["A", "a", "2", -2.2],
+            ["A", "b", "1", -3.3],
+            ["A", "b", "2", -4.4],
+            ["A", "b", "3", -5.5],
+            ["B", "a", "2", -6.6],
+            ["B", "b", "2", -7.7],
+            ["B", "c", "3", -8.8],
+            ["C", "a", "1", -9.9],
+        ]
+        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "max"))
+        expected = [
+            ["H", "1", "2", "3"],
+            ["A", -1.1, -2.2, -5.5],
+            ["B", numpy.nan, -6.6, -8.8],
+            ["C", -9.9, numpy.nan, numpy.nan],
+        ]
+        self.assertEqual(pivot_table, expected)
+
+    def test_pivot_group_one_or_none(self):
+        table = [
+            ["A", "a", "1", -1.1],
+            ["A", "a", "2", -2.2],
+            ["A", "b", "1", -3.3],
+            ["A", "b", "2", -4.4],
+            ["A", "b", "3", -5.5],
+            ["B", "a", "2", -6.6],
+            ["B", "b", "2", -7.7],
+            ["B", "c", "3", -8.8],
+            ["C", "a", "1", -9.9],
+        ]
+        pivot_table = list(make_pivot(table, ["H", "h", "#", "xx"], 3, [0], [1], [2], "one_or_none"))
+        expected = [["H", "1", "2", "3"], ["A", None, None, -5.5], ["B", None, None, -8.8], ["C", -9.9, None, None]]
+        self.assertEqual(pivot_table, expected)
+
+    def test_Nones_in_regular_keys(self):
+        table = [
+            ["A", "a", "1", -1.1],
+            ["A", "a", "2", -2.2],
+            ["A", "b", "1", -3.3],
+            ["A", "b", "2", -4.4],
+            ["A", "b", "3", -5.5],
+            [None, "a", "2", -6.6],
+            [None, "b", "2", -7.7],
+            [None, "c", "3", -8.8],
+            ["C", "a", "1", -9.9],
+        ]
+        pivot_table = list(make_pivot(table, None, 3, [0], [1], [2]))
+        expected = [
+            [None, "1", "2", "3"],
+            [None, None, -6.6, None],
+            [None, None, -7.7, None],
+            [None, None, None, -8.8],
+            ["A", -1.1, -2.2, None],
+            ["A", -3.3, -4.4, -5.5],
+            ["C", -9.9, None, None],
+        ]
+        self.assertEqual(pivot_table, expected)
+
+    def test_Nones_in_pivot_keys(self):
+        table = [
+            ["A", "a", "1", -1.1],
+            ["A", "a", None, -2.2],
+            ["A", "b", "1", -3.3],
+            ["A", "b", None, -4.4],
+            ["A", "b", "3", -5.5],
+            ["B", "a", None, -6.6],
+            ["B", "b", None, -7.7],
+            ["B", "c", "3", -8.8],
+            ["C", "a", "1", -9.9],
+        ]
+        pivot_table = list(make_pivot(table, None, 3, [0], [1], [2]))
+        expected = [
+            [None, None, "1", "3"],
+            ["A", -2.2, -1.1, None],
+            ["A", -4.4, -3.3, -5.5],
+            ["B", -6.6, None, None],
+            ["B", -7.7, None, None],
+            ["B", None, None, -8.8],
+            ["C", None, -9.9, None],
+        ]
+        self.assertEqual(pivot_table, expected)
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/export_mapping/test_settings.py` & `spinedb_api-0.30.4/tests/export_mapping/test_settings.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,341 +1,341 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Unit tests for export settings.
-
-"""
-
-import unittest
-import numpy
-from spinedb_api import (
-    DatabaseMapping,
-    import_object_classes,
-    import_object_parameters,
-    import_object_parameter_values,
-    import_objects,
-    import_relationship_classes,
-    import_relationship_parameter_values,
-    import_relationship_parameters,
-    import_relationships,
-    TimeSeriesFixedResolution,
-)
-from spinedb_api.export_mapping import rows
-from spinedb_api.export_mapping.settings import (
-    relationship_export,
-    set_relationship_dimensions,
-    object_parameter_export,
-    set_parameter_dimensions,
-    relationship_parameter_default_value_export,
-    set_parameter_default_value_dimensions,
-    object_parameter_default_value_export,
-    relationship_parameter_export,
-    relationship_object_parameter_default_value_export,
-    relationship_object_parameter_export,
-)
-from spinedb_api.export_mapping.export_mapping import (
-    Position,
-    RelationshipClassMapping,
-    RelationshipClassObjectClassMapping,
-    RelationshipMapping,
-    RelationshipObjectMapping,
-    ExpandedParameterValueMapping,
-    ParameterValueIndexMapping,
-    IndexNameMapping,
-    ParameterValueTypeMapping,
-    ParameterValueMapping,
-    ExpandedParameterDefaultValueMapping,
-    ParameterDefaultValueIndexMapping,
-    DefaultValueIndexNameMapping,
-    ParameterDefaultValueTypeMapping,
-    ParameterDefaultValueMapping,
-)
-
-
-class TestRelationshipParameterExport(unittest.TestCase):
-    def test_export_with_parameter_values(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_objects(db_map, (("oc1", "o1"), ("oc2", "o2"), ("oc2", "o3")))
-        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
-        import_relationship_parameters(db_map, (("rc", "p"),))
-        import_relationships(db_map, (("rc", ("o1", "o2")), ("rc", ("o1", "o3"))))
-        import_relationship_parameter_values(
-            db_map,
-            (
-                (
-                    "rc",
-                    ("o1", "o2"),
-                    "p",
-                    TimeSeriesFixedResolution("2022-06-22T11:00", "1h", [-1.1, -2.2], False, False),
-                ),
-                (
-                    "rc",
-                    ("o1", "o3"),
-                    "p",
-                    TimeSeriesFixedResolution("2022-06-22T11:00", "1h", [-3.3, -4.4], False, False),
-                ),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        root_mapping = relationship_parameter_export(
-            object_positions=[-1, -2], value_position=-3, index_name_positions=[Position.hidden], index_positions=[0]
-        )
-        expected = [
-            [None, "o1", "o1"],
-            [None, "o2", "o3"],
-            [numpy.datetime64("2022-06-22T11:00:00"), -1.1, -3.3],
-            [numpy.datetime64("2022-06-22T12:00:00"), -2.2, -4.4],
-        ]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
-
-
-class TestRelationshipObjectParameterDefaultValueExport(unittest.TestCase):
-    def setUp(self):
-        self._db_map = DatabaseMapping("sqlite://", create=True)
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_export_with_two_dimensions(self):
-        import_object_classes(self._db_map, ("oc1", "oc2"))
-        import_object_parameters(
-            self._db_map, (("oc1", "p11", 2.3), ("oc1", "p12", 5.0), ("oc2", "p21", "shouldn't show"))
-        )
-        import_relationship_classes(self._db_map, (("rc", ("oc1", "oc2")),))
-        import_relationship_parameters(self._db_map, (("rc", "rc_p", "dummy"),))
-        self._db_map.commit_session("Add test data.")
-        root_mapping = relationship_object_parameter_default_value_export(
-            relationship_class_position=0,
-            definition_position=1,
-            object_class_positions=[2, 3],
-            value_position=4,
-            value_type_position=5,
-            index_name_positions=None,
-            index_positions=None,
-            highlight_dimension=0,
-        )
-        expected = [["rc", "p11", "oc1", "oc2", 2.3, "single_value"], ["rc", "p12", "oc1", "oc2", 5.0, "single_value"]]
-        self.assertEqual(list(rows(root_mapping, self._db_map)), expected)
-
-
-class TestRelationshipObjectParameterExport(unittest.TestCase):
-    def setUp(self):
-        self._db_map = DatabaseMapping("sqlite://", create=True)
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_export_with_two_dimensions(self):
-        import_object_classes(self._db_map, ("oc1", "oc2"))
-        import_object_parameters(self._db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
-        import_objects(self._db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
-        import_object_parameter_values(
-            self._db_map,
-            (
-                ("oc1", "o11", "p11", 2.3),
-                ("oc1", "o12", "p11", -2.3),
-                ("oc1", "o12", "p12", -5.0),
-                ("oc2", "o21", "p21", "shouldn't show"),
-            ),
-        )
-        import_relationship_classes(self._db_map, (("rc", ("oc1", "oc2")),))
-        import_relationship_parameters(self._db_map, (("rc", "rc_p"),))
-        import_relationships(self._db_map, (("rc", ("o11", "o21")), ("rc", ("o12", "o21"))))
-        import_relationship_parameter_values(self._db_map, (("rc", ("o11", "o21"), "rc_p", "dummy"),))
-        self._db_map.commit_session("Add test data.")
-        root_mapping = relationship_object_parameter_export(
-            relationship_class_position=0,
-            definition_position=1,
-            value_list_position=Position.hidden,
-            relationship_position=2,
-            object_class_positions=[3, 4],
-            object_positions=[5, 6],
-            alternative_position=7,
-            value_type_position=8,
-            value_position=9,
-            highlight_dimension=0,
-        )
-        set_relationship_dimensions(root_mapping, 2)
-        expected = [
-            ["rc", "p11", "rc_o11__o21", "oc1", "oc2", "o11", "o21", "Base", "single_value", 2.3],
-            ["rc", "p11", "rc_o12__o21", "oc1", "oc2", "o12", "o21", "Base", "single_value", -2.3],
-            ["rc", "p12", "rc_o12__o21", "oc1", "oc2", "o12", "o21", "Base", "single_value", -5.0],
-        ]
-        self.assertEqual(list(rows(root_mapping, self._db_map)), expected)
-
-
-class TestSetRelationshipDimensions(unittest.TestCase):
-    def test_change_dimensions_from_zero_to_one(self):
-        mapping = relationship_export(0, 1)
-        self.assertEqual(mapping.count_mappings(), 2)
-        set_relationship_dimensions(mapping, 1)
-        self.assertEqual(mapping.count_mappings(), 4)
-        flattened = mapping.flatten()
-        classes = [type(mapping) for mapping in flattened]
-        self.assertEqual(
-            classes,
-            [
-                RelationshipClassMapping,
-                RelationshipClassObjectClassMapping,
-                RelationshipMapping,
-                RelationshipObjectMapping,
-            ],
-        )
-        positions = [mapping.position for mapping in flattened]
-        self.assertEqual(positions, [0, Position.hidden, 1, Position.hidden])
-
-    def test_change_dimension_from_one_to_zero(self):
-        mapping = relationship_export(0, 1, [2], [3])
-        self.assertEqual(mapping.count_mappings(), 4)
-        set_relationship_dimensions(mapping, 0)
-        self.assertEqual(mapping.count_mappings(), 2)
-        flattened = mapping.flatten()
-        classes = [type(mapping) for mapping in flattened]
-        self.assertEqual(classes, [RelationshipClassMapping, RelationshipMapping])
-        positions = [mapping.position for mapping in flattened]
-        self.assertEqual(positions, [0, 1])
-
-    def test_increase_dimensions(self):
-        mapping = relationship_export(0, 1, [2], [3])
-        self.assertEqual(mapping.count_mappings(), 4)
-        set_relationship_dimensions(mapping, 2)
-        self.assertEqual(mapping.count_mappings(), 6)
-        flattened = mapping.flatten()
-        classes = [type(mapping) for mapping in flattened]
-        self.assertEqual(
-            classes,
-            [
-                RelationshipClassMapping,
-                RelationshipClassObjectClassMapping,
-                RelationshipClassObjectClassMapping,
-                RelationshipMapping,
-                RelationshipObjectMapping,
-                RelationshipObjectMapping,
-            ],
-        )
-        positions = [mapping.position for mapping in flattened]
-        self.assertEqual(positions, [0, 2, Position.hidden, 1, 3, Position.hidden])
-
-    def test_decrease_dimensions(self):
-        mapping = relationship_export(0, 1, [2, 3], [4, 5])
-        self.assertEqual(mapping.count_mappings(), 6)
-        set_relationship_dimensions(mapping, 1)
-        self.assertEqual(mapping.count_mappings(), 4)
-        flattened = mapping.flatten()
-        classes = [type(mapping) for mapping in flattened]
-        self.assertEqual(
-            classes,
-            [
-                RelationshipClassMapping,
-                RelationshipClassObjectClassMapping,
-                RelationshipMapping,
-                RelationshipObjectMapping,
-            ],
-        )
-        positions = [mapping.position for mapping in flattened]
-        self.assertEqual(positions, [0, 2, 1, 4])
-
-
-class TestSetParameterDimensions(unittest.TestCase):
-    def test_set_dimensions_from_zero_to_one(self):
-        root_mapping = object_parameter_export()
-        set_parameter_dimensions(root_mapping, 1)
-        expected_types = [
-            ExpandedParameterValueMapping,
-            ParameterValueIndexMapping,
-            IndexNameMapping,
-            ParameterValueTypeMapping,
-        ]
-        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
-            self.assertIsInstance(mapping, expected_type)
-
-    def test_set_default_value_dimensions_from_zero_to_one(self):
-        root_mapping = relationship_parameter_default_value_export()
-        set_parameter_default_value_dimensions(root_mapping, 1)
-        expected_types = [
-            ExpandedParameterDefaultValueMapping,
-            ParameterDefaultValueIndexMapping,
-            DefaultValueIndexNameMapping,
-            ParameterDefaultValueTypeMapping,
-        ]
-        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
-            self.assertIsInstance(mapping, expected_type)
-
-    def test_set_dimensions_from_one_to_zero(self):
-        root_mapping = relationship_parameter_export(index_name_positions=[0], index_positions=[1])
-        set_parameter_dimensions(root_mapping, 0)
-        expected_types = [ParameterValueMapping, ParameterValueTypeMapping]
-        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
-            self.assertIsInstance(mapping, expected_type)
-
-    def test_set_default_value_dimensions_from_one_to_zero(self):
-        root_mapping = object_parameter_default_value_export(index_name_positions=[0], index_positions=[1])
-        set_parameter_default_value_dimensions(root_mapping, 0)
-        expected_types = [ParameterDefaultValueMapping, ParameterDefaultValueTypeMapping]
-        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
-            self.assertIsInstance(mapping, expected_type)
-
-    def test_set_dimensions_from_one_to_two(self):
-        root_mapping = relationship_parameter_export(index_name_positions=[0], index_positions=[1])
-        set_parameter_dimensions(root_mapping, 2)
-        expected_types = [
-            ExpandedParameterValueMapping,
-            ParameterValueIndexMapping,
-            IndexNameMapping,
-            ParameterValueIndexMapping,
-            IndexNameMapping,
-            ParameterValueTypeMapping,
-        ]
-        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
-            self.assertIsInstance(mapping, expected_type)
-
-    def test_set_default_value_dimensions_from_one_to_two(self):
-        root_mapping = relationship_parameter_default_value_export(index_name_positions=[0], index_positions=[1])
-        set_parameter_default_value_dimensions(root_mapping, 2)
-        expected_types = [
-            ExpandedParameterDefaultValueMapping,
-            ParameterDefaultValueIndexMapping,
-            DefaultValueIndexNameMapping,
-            ParameterDefaultValueIndexMapping,
-            DefaultValueIndexNameMapping,
-            ParameterDefaultValueTypeMapping,
-        ]
-        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
-            self.assertIsInstance(mapping, expected_type)
-
-    def test_set_dimensions_from_two_to_one(self):
-        root_mapping = relationship_parameter_export(index_name_positions=[0, 2], index_positions=[1, 3])
-        set_parameter_dimensions(root_mapping, 1)
-        expected_types = [
-            ExpandedParameterValueMapping,
-            ParameterValueIndexMapping,
-            IndexNameMapping,
-            ParameterValueTypeMapping,
-        ]
-        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
-            self.assertIsInstance(mapping, expected_type)
-
-    def test_set_default_value_dimensions_from_two_to_one(self):
-        root_mapping = relationship_parameter_default_value_export(index_name_positions=[0, 2], index_positions=[1, 3])
-        set_parameter_default_value_dimensions(root_mapping, 1)
-        expected_types = [
-            ExpandedParameterDefaultValueMapping,
-            ParameterDefaultValueIndexMapping,
-            DefaultValueIndexNameMapping,
-            ParameterDefaultValueTypeMapping,
-        ]
-        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
-            self.assertIsInstance(mapping, expected_type)
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Unit tests for export settings.
+
+"""
+
+import unittest
+import numpy
+from spinedb_api import (
+    DatabaseMapping,
+    import_object_classes,
+    import_object_parameters,
+    import_object_parameter_values,
+    import_objects,
+    import_relationship_classes,
+    import_relationship_parameter_values,
+    import_relationship_parameters,
+    import_relationships,
+    TimeSeriesFixedResolution,
+)
+from spinedb_api.export_mapping import rows
+from spinedb_api.export_mapping.settings import (
+    relationship_export,
+    set_relationship_dimensions,
+    object_parameter_export,
+    set_parameter_dimensions,
+    relationship_parameter_default_value_export,
+    set_parameter_default_value_dimensions,
+    object_parameter_default_value_export,
+    relationship_parameter_export,
+    relationship_object_parameter_default_value_export,
+    relationship_object_parameter_export,
+)
+from spinedb_api.export_mapping.export_mapping import (
+    Position,
+    RelationshipClassMapping,
+    RelationshipClassObjectClassMapping,
+    RelationshipMapping,
+    RelationshipObjectMapping,
+    ExpandedParameterValueMapping,
+    ParameterValueIndexMapping,
+    IndexNameMapping,
+    ParameterValueTypeMapping,
+    ParameterValueMapping,
+    ExpandedParameterDefaultValueMapping,
+    ParameterDefaultValueIndexMapping,
+    DefaultValueIndexNameMapping,
+    ParameterDefaultValueTypeMapping,
+    ParameterDefaultValueMapping,
+)
+
+
+class TestRelationshipParameterExport(unittest.TestCase):
+    def test_export_with_parameter_values(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_objects(db_map, (("oc1", "o1"), ("oc2", "o2"), ("oc2", "o3")))
+        import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
+        import_relationship_parameters(db_map, (("rc", "p"),))
+        import_relationships(db_map, (("rc", ("o1", "o2")), ("rc", ("o1", "o3"))))
+        import_relationship_parameter_values(
+            db_map,
+            (
+                (
+                    "rc",
+                    ("o1", "o2"),
+                    "p",
+                    TimeSeriesFixedResolution("2022-06-22T11:00", "1h", [-1.1, -2.2], False, False),
+                ),
+                (
+                    "rc",
+                    ("o1", "o3"),
+                    "p",
+                    TimeSeriesFixedResolution("2022-06-22T11:00", "1h", [-3.3, -4.4], False, False),
+                ),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        root_mapping = relationship_parameter_export(
+            object_positions=[-1, -2], value_position=-3, index_name_positions=[Position.hidden], index_positions=[0]
+        )
+        expected = [
+            [None, "o1", "o1"],
+            [None, "o2", "o3"],
+            [numpy.datetime64("2022-06-22T11:00:00"), -1.1, -3.3],
+            [numpy.datetime64("2022-06-22T12:00:00"), -2.2, -4.4],
+        ]
+        self.assertEqual(list(rows(root_mapping, db_map)), expected)
+        db_map.connection.close()
+
+
+class TestRelationshipObjectParameterDefaultValueExport(unittest.TestCase):
+    def setUp(self):
+        self._db_map = DatabaseMapping("sqlite://", create=True)
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_export_with_two_dimensions(self):
+        import_object_classes(self._db_map, ("oc1", "oc2"))
+        import_object_parameters(
+            self._db_map, (("oc1", "p11", 2.3), ("oc1", "p12", 5.0), ("oc2", "p21", "shouldn't show"))
+        )
+        import_relationship_classes(self._db_map, (("rc", ("oc1", "oc2")),))
+        import_relationship_parameters(self._db_map, (("rc", "rc_p", "dummy"),))
+        self._db_map.commit_session("Add test data.")
+        root_mapping = relationship_object_parameter_default_value_export(
+            relationship_class_position=0,
+            definition_position=1,
+            object_class_positions=[2, 3],
+            value_position=4,
+            value_type_position=5,
+            index_name_positions=None,
+            index_positions=None,
+            highlight_dimension=0,
+        )
+        expected = [["rc", "p11", "oc1", "oc2", 2.3, "single_value"], ["rc", "p12", "oc1", "oc2", 5.0, "single_value"]]
+        self.assertEqual(list(rows(root_mapping, self._db_map)), expected)
+
+
+class TestRelationshipObjectParameterExport(unittest.TestCase):
+    def setUp(self):
+        self._db_map = DatabaseMapping("sqlite://", create=True)
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_export_with_two_dimensions(self):
+        import_object_classes(self._db_map, ("oc1", "oc2"))
+        import_object_parameters(self._db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
+        import_objects(self._db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
+        import_object_parameter_values(
+            self._db_map,
+            (
+                ("oc1", "o11", "p11", 2.3),
+                ("oc1", "o12", "p11", -2.3),
+                ("oc1", "o12", "p12", -5.0),
+                ("oc2", "o21", "p21", "shouldn't show"),
+            ),
+        )
+        import_relationship_classes(self._db_map, (("rc", ("oc1", "oc2")),))
+        import_relationship_parameters(self._db_map, (("rc", "rc_p"),))
+        import_relationships(self._db_map, (("rc", ("o11", "o21")), ("rc", ("o12", "o21"))))
+        import_relationship_parameter_values(self._db_map, (("rc", ("o11", "o21"), "rc_p", "dummy"),))
+        self._db_map.commit_session("Add test data.")
+        root_mapping = relationship_object_parameter_export(
+            relationship_class_position=0,
+            definition_position=1,
+            value_list_position=Position.hidden,
+            relationship_position=2,
+            object_class_positions=[3, 4],
+            object_positions=[5, 6],
+            alternative_position=7,
+            value_type_position=8,
+            value_position=9,
+            highlight_dimension=0,
+        )
+        set_relationship_dimensions(root_mapping, 2)
+        expected = [
+            ["rc", "p11", "rc_o11__o21", "oc1", "oc2", "o11", "o21", "Base", "single_value", 2.3],
+            ["rc", "p11", "rc_o12__o21", "oc1", "oc2", "o12", "o21", "Base", "single_value", -2.3],
+            ["rc", "p12", "rc_o12__o21", "oc1", "oc2", "o12", "o21", "Base", "single_value", -5.0],
+        ]
+        self.assertEqual(list(rows(root_mapping, self._db_map)), expected)
+
+
+class TestSetRelationshipDimensions(unittest.TestCase):
+    def test_change_dimensions_from_zero_to_one(self):
+        mapping = relationship_export(0, 1)
+        self.assertEqual(mapping.count_mappings(), 2)
+        set_relationship_dimensions(mapping, 1)
+        self.assertEqual(mapping.count_mappings(), 4)
+        flattened = mapping.flatten()
+        classes = [type(mapping) for mapping in flattened]
+        self.assertEqual(
+            classes,
+            [
+                RelationshipClassMapping,
+                RelationshipClassObjectClassMapping,
+                RelationshipMapping,
+                RelationshipObjectMapping,
+            ],
+        )
+        positions = [mapping.position for mapping in flattened]
+        self.assertEqual(positions, [0, Position.hidden, 1, Position.hidden])
+
+    def test_change_dimension_from_one_to_zero(self):
+        mapping = relationship_export(0, 1, [2], [3])
+        self.assertEqual(mapping.count_mappings(), 4)
+        set_relationship_dimensions(mapping, 0)
+        self.assertEqual(mapping.count_mappings(), 2)
+        flattened = mapping.flatten()
+        classes = [type(mapping) for mapping in flattened]
+        self.assertEqual(classes, [RelationshipClassMapping, RelationshipMapping])
+        positions = [mapping.position for mapping in flattened]
+        self.assertEqual(positions, [0, 1])
+
+    def test_increase_dimensions(self):
+        mapping = relationship_export(0, 1, [2], [3])
+        self.assertEqual(mapping.count_mappings(), 4)
+        set_relationship_dimensions(mapping, 2)
+        self.assertEqual(mapping.count_mappings(), 6)
+        flattened = mapping.flatten()
+        classes = [type(mapping) for mapping in flattened]
+        self.assertEqual(
+            classes,
+            [
+                RelationshipClassMapping,
+                RelationshipClassObjectClassMapping,
+                RelationshipClassObjectClassMapping,
+                RelationshipMapping,
+                RelationshipObjectMapping,
+                RelationshipObjectMapping,
+            ],
+        )
+        positions = [mapping.position for mapping in flattened]
+        self.assertEqual(positions, [0, 2, Position.hidden, 1, 3, Position.hidden])
+
+    def test_decrease_dimensions(self):
+        mapping = relationship_export(0, 1, [2, 3], [4, 5])
+        self.assertEqual(mapping.count_mappings(), 6)
+        set_relationship_dimensions(mapping, 1)
+        self.assertEqual(mapping.count_mappings(), 4)
+        flattened = mapping.flatten()
+        classes = [type(mapping) for mapping in flattened]
+        self.assertEqual(
+            classes,
+            [
+                RelationshipClassMapping,
+                RelationshipClassObjectClassMapping,
+                RelationshipMapping,
+                RelationshipObjectMapping,
+            ],
+        )
+        positions = [mapping.position for mapping in flattened]
+        self.assertEqual(positions, [0, 2, 1, 4])
+
+
+class TestSetParameterDimensions(unittest.TestCase):
+    def test_set_dimensions_from_zero_to_one(self):
+        root_mapping = object_parameter_export()
+        set_parameter_dimensions(root_mapping, 1)
+        expected_types = [
+            ExpandedParameterValueMapping,
+            ParameterValueIndexMapping,
+            IndexNameMapping,
+            ParameterValueTypeMapping,
+        ]
+        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
+            self.assertIsInstance(mapping, expected_type)
+
+    def test_set_default_value_dimensions_from_zero_to_one(self):
+        root_mapping = relationship_parameter_default_value_export()
+        set_parameter_default_value_dimensions(root_mapping, 1)
+        expected_types = [
+            ExpandedParameterDefaultValueMapping,
+            ParameterDefaultValueIndexMapping,
+            DefaultValueIndexNameMapping,
+            ParameterDefaultValueTypeMapping,
+        ]
+        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
+            self.assertIsInstance(mapping, expected_type)
+
+    def test_set_dimensions_from_one_to_zero(self):
+        root_mapping = relationship_parameter_export(index_name_positions=[0], index_positions=[1])
+        set_parameter_dimensions(root_mapping, 0)
+        expected_types = [ParameterValueMapping, ParameterValueTypeMapping]
+        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
+            self.assertIsInstance(mapping, expected_type)
+
+    def test_set_default_value_dimensions_from_one_to_zero(self):
+        root_mapping = object_parameter_default_value_export(index_name_positions=[0], index_positions=[1])
+        set_parameter_default_value_dimensions(root_mapping, 0)
+        expected_types = [ParameterDefaultValueMapping, ParameterDefaultValueTypeMapping]
+        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
+            self.assertIsInstance(mapping, expected_type)
+
+    def test_set_dimensions_from_one_to_two(self):
+        root_mapping = relationship_parameter_export(index_name_positions=[0], index_positions=[1])
+        set_parameter_dimensions(root_mapping, 2)
+        expected_types = [
+            ExpandedParameterValueMapping,
+            ParameterValueIndexMapping,
+            IndexNameMapping,
+            ParameterValueIndexMapping,
+            IndexNameMapping,
+            ParameterValueTypeMapping,
+        ]
+        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
+            self.assertIsInstance(mapping, expected_type)
+
+    def test_set_default_value_dimensions_from_one_to_two(self):
+        root_mapping = relationship_parameter_default_value_export(index_name_positions=[0], index_positions=[1])
+        set_parameter_default_value_dimensions(root_mapping, 2)
+        expected_types = [
+            ExpandedParameterDefaultValueMapping,
+            ParameterDefaultValueIndexMapping,
+            DefaultValueIndexNameMapping,
+            ParameterDefaultValueIndexMapping,
+            DefaultValueIndexNameMapping,
+            ParameterDefaultValueTypeMapping,
+        ]
+        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
+            self.assertIsInstance(mapping, expected_type)
+
+    def test_set_dimensions_from_two_to_one(self):
+        root_mapping = relationship_parameter_export(index_name_positions=[0, 2], index_positions=[1, 3])
+        set_parameter_dimensions(root_mapping, 1)
+        expected_types = [
+            ExpandedParameterValueMapping,
+            ParameterValueIndexMapping,
+            IndexNameMapping,
+            ParameterValueTypeMapping,
+        ]
+        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
+            self.assertIsInstance(mapping, expected_type)
+
+    def test_set_default_value_dimensions_from_two_to_one(self):
+        root_mapping = relationship_parameter_default_value_export(index_name_positions=[0, 2], index_positions=[1, 3])
+        set_parameter_default_value_dimensions(root_mapping, 1)
+        expected_types = [
+            ExpandedParameterDefaultValueMapping,
+            ParameterDefaultValueIndexMapping,
+            DefaultValueIndexNameMapping,
+            ParameterDefaultValueTypeMapping,
+        ]
+        for expected_type, mapping in zip(expected_types, reversed(root_mapping.flatten())):
+            self.assertIsInstance(mapping, expected_type)
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/filters/__init__.py` & `spinedb_api-0.30.4/spinedb_api/filters/__init__.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
```

### Comparing `spinedb_api-0.30.3/tests/filters/test_alternative_filter.py` & `spinedb_api-0.30.4/tests/filters/test_alternative_filter.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,176 +1,176 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for ``alternative_filter`` module.
-
-"""
-from pathlib import Path
-from tempfile import TemporaryDirectory
-import unittest
-from sqlalchemy.engine.url import URL
-from spinedb_api import (
-    apply_alternative_filter_to_parameter_value_sq,
-    create_new_spine_database,
-    DatabaseMapping,
-    DiffDatabaseMapping,
-    import_alternatives,
-    import_object_classes,
-    import_object_parameter_values,
-    import_object_parameters,
-    import_objects,
-)
-from spinedb_api.filters.alternative_filter import (
-    alternative_filter_config,
-    alternative_filter_from_dict,
-    alternative_filter_config_to_shorthand,
-    alternative_filter_shorthand_to_config,
-    alternative_names_from_dict,
-)
-
-
-class TestAlternativeFilter(unittest.TestCase):
-    _db_url = None
-    _temp_dir = None
-
-    @classmethod
-    def setUpClass(cls):
-        cls._temp_dir = TemporaryDirectory()
-        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_scenario_filter_mapping.sqlite").as_posix())
-
-    def setUp(self):
-        create_new_spine_database(self._db_url)
-        self._out_map = DiffDatabaseMapping(self._db_url)
-        self._db_map = DatabaseMapping(self._db_url)
-        self._diff_db_map = DiffDatabaseMapping(self._db_url)
-
-    def tearDown(self):
-        self._out_map.connection.close()
-        self._db_map.connection.close()
-        self._diff_db_map.connection.close()
-
-    def test_alternative_filter_without_scenarios_or_alternatives(self):
-        self._build_data_without_alternatives()
-        self._out_map.commit_session("Add test data")
-        for db_map in [self._db_map, self._diff_db_map]:
-            apply_alternative_filter_to_parameter_value_sq(db_map, [])
-            parameters = db_map.query(db_map.parameter_value_sq).all()
-            self.assertEqual(parameters, [])
-
-    def test_alternative_filter_without_scenarios_or_alternatives_uncommitted_data(self):
-        self._build_data_without_alternatives()
-        apply_alternative_filter_to_parameter_value_sq(self._out_map, alternatives=[])
-        parameters = self._out_map.query(self._out_map.parameter_value_sq).all()
-        self.assertEqual(parameters, [])
-        self._out_map.rollback_session()
-
-    def test_alternative_filter(self):
-        self._build_data_with_single_alternative()
-        self._out_map.commit_session("Add test data")
-        for db_map in [self._db_map, self._diff_db_map]:
-            apply_alternative_filter_to_parameter_value_sq(db_map, ["alternative"])
-            parameters = db_map.query(db_map.parameter_value_sq).all()
-            self.assertEqual(len(parameters), 1)
-            self.assertEqual(parameters[0].value, b"23.0")
-
-    def test_alternative_filter_uncommitted_data(self):
-        self._build_data_with_single_alternative()
-        apply_alternative_filter_to_parameter_value_sq(self._out_map, ["alternative"])
-        parameters = self._out_map.query(self._out_map.parameter_value_sq).all()
-        self.assertEqual(len(parameters), 1)
-        self.assertEqual(parameters[0].value, b"23.0")
-        self._out_map.rollback_session()
-
-    def test_alternative_filter_from_dict(self):
-        self._build_data_with_single_alternative()
-        self._out_map.commit_session("Add test data")
-        config = alternative_filter_config(["alternative"])
-        alternative_filter_from_dict(self._db_map, config)
-        parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
-        self.assertEqual(len(parameters), 1)
-        self.assertEqual(parameters[0].value, b"23.0")
-
-    def _build_data_without_alternatives(self):
-        import_object_classes(self._out_map, ["object_class"])
-        import_objects(self._out_map, [("object_class", "object")])
-        import_object_parameters(self._out_map, [("object_class", "parameter")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 23.0)])
-
-    def _build_data_with_single_alternative(self):
-        import_alternatives(self._out_map, ["alternative"])
-        import_object_classes(self._out_map, ["object_class"])
-        import_objects(self._out_map, [("object_class", "object")])
-        import_object_parameters(self._out_map, [("object_class", "parameter")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", -1.0)])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 23.0, "alternative")])
-
-
-class TestAlternativeFilterWithMemoryDatabase(unittest.TestCase):
-    def setUp(self):
-        self._db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(self._db_map, ["object_class"])
-        import_objects(self._db_map, [("object_class", "object")])
-        import_object_parameters(self._db_map, [("object_class", "parameter")])
-        import_object_parameter_values(self._db_map, [("object_class", "object", "parameter", -1.0)])
-        self._db_map.commit_session("Add initial data.")
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_alternative_names_with_colons(self):
-        self._add_value_in_alternative(23.0, "new@2023-23-23T11:12:13")
-        config = alternative_filter_config(["new@2023-23-23T11:12:13"])
-        alternative_filter_from_dict(self._db_map, config)
-        parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
-        self.assertEqual(len(parameters), 1)
-        self.assertEqual(parameters[0].value, b"23.0")
-
-    def test_multiple_alternatives(self):
-        self._add_value_in_alternative(23.0, "new@2023-23-23T11:12:13")
-        self._add_value_in_alternative(101.1, "new@2005-05-05T22:23:24")
-        config = alternative_filter_config(["new@2005-05-05T22:23:24", "new@2023-23-23T11:12:13"])
-        alternative_filter_from_dict(self._db_map, config)
-        parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
-        self.assertEqual(len(parameters), 2)
-        self.assertEqual(parameters[0].value, b"23.0")
-        self.assertEqual(parameters[1].value, b"101.1")
-
-    def _add_value_in_alternative(self, value, alternative):
-        import_alternatives(self._db_map, [alternative])
-        import_object_parameter_values(self._db_map, [("object_class", "object", "parameter", value, alternative)])
-        self._db_map.commit_session(f"Add value in {alternative}")
-
-
-class TestAlternativeFilterWithoutDatabase(unittest.TestCase):
-    def test_alternative_filter_config(self):
-        config = alternative_filter_config(["alternative1", "alternative2"])
-        self.assertEqual(config, {"type": "alternative_filter", "alternatives": ["alternative1", "alternative2"]})
-
-    def test_alternative_names_from_dict(self):
-        config = alternative_filter_config(["alternative1", "alternative2"])
-        self.assertEqual(alternative_names_from_dict(config), ["alternative1", "alternative2"])
-
-    def test_alternative_filter_config_to_shorthand(self):
-        config = alternative_filter_config(["alternative1", "alternative2"])
-        shorthand = alternative_filter_config_to_shorthand(config)
-        self.assertEqual(shorthand, "alternatives:'alternative1':'alternative2'")
-
-    def test_alternative_filter_shorthand_to_config(self):
-        config = alternative_filter_shorthand_to_config("alternatives:'alternative1':'alternative2'")
-        self.assertEqual(config, {"type": "alternative_filter", "alternatives": ["alternative1", "alternative2"]})
-
-    def test_quoted_alternative_names(self):
-        config = alternative_filter_shorthand_to_config("alternatives:'alt:er:na:ti:ve':'alternative2'")
-        self.assertEqual(config, {"type": "alternative_filter", "alternatives": ["alt:er:na:ti:ve", "alternative2"]})
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for ``alternative_filter`` module.
+
+"""
+from pathlib import Path
+from tempfile import TemporaryDirectory
+import unittest
+from sqlalchemy.engine.url import URL
+from spinedb_api import (
+    apply_alternative_filter_to_parameter_value_sq,
+    create_new_spine_database,
+    DatabaseMapping,
+    DiffDatabaseMapping,
+    import_alternatives,
+    import_object_classes,
+    import_object_parameter_values,
+    import_object_parameters,
+    import_objects,
+)
+from spinedb_api.filters.alternative_filter import (
+    alternative_filter_config,
+    alternative_filter_from_dict,
+    alternative_filter_config_to_shorthand,
+    alternative_filter_shorthand_to_config,
+    alternative_names_from_dict,
+)
+
+
+class TestAlternativeFilter(unittest.TestCase):
+    _db_url = None
+    _temp_dir = None
+
+    @classmethod
+    def setUpClass(cls):
+        cls._temp_dir = TemporaryDirectory()
+        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_scenario_filter_mapping.sqlite").as_posix())
+
+    def setUp(self):
+        create_new_spine_database(self._db_url)
+        self._out_map = DiffDatabaseMapping(self._db_url)
+        self._db_map = DatabaseMapping(self._db_url)
+        self._diff_db_map = DiffDatabaseMapping(self._db_url)
+
+    def tearDown(self):
+        self._out_map.connection.close()
+        self._db_map.connection.close()
+        self._diff_db_map.connection.close()
+
+    def test_alternative_filter_without_scenarios_or_alternatives(self):
+        self._build_data_without_alternatives()
+        self._out_map.commit_session("Add test data")
+        for db_map in [self._db_map, self._diff_db_map]:
+            apply_alternative_filter_to_parameter_value_sq(db_map, [])
+            parameters = db_map.query(db_map.parameter_value_sq).all()
+            self.assertEqual(parameters, [])
+
+    def test_alternative_filter_without_scenarios_or_alternatives_uncommitted_data(self):
+        self._build_data_without_alternatives()
+        apply_alternative_filter_to_parameter_value_sq(self._out_map, alternatives=[])
+        parameters = self._out_map.query(self._out_map.parameter_value_sq).all()
+        self.assertEqual(parameters, [])
+        self._out_map.rollback_session()
+
+    def test_alternative_filter(self):
+        self._build_data_with_single_alternative()
+        self._out_map.commit_session("Add test data")
+        for db_map in [self._db_map, self._diff_db_map]:
+            apply_alternative_filter_to_parameter_value_sq(db_map, ["alternative"])
+            parameters = db_map.query(db_map.parameter_value_sq).all()
+            self.assertEqual(len(parameters), 1)
+            self.assertEqual(parameters[0].value, b"23.0")
+
+    def test_alternative_filter_uncommitted_data(self):
+        self._build_data_with_single_alternative()
+        apply_alternative_filter_to_parameter_value_sq(self._out_map, ["alternative"])
+        parameters = self._out_map.query(self._out_map.parameter_value_sq).all()
+        self.assertEqual(len(parameters), 1)
+        self.assertEqual(parameters[0].value, b"23.0")
+        self._out_map.rollback_session()
+
+    def test_alternative_filter_from_dict(self):
+        self._build_data_with_single_alternative()
+        self._out_map.commit_session("Add test data")
+        config = alternative_filter_config(["alternative"])
+        alternative_filter_from_dict(self._db_map, config)
+        parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
+        self.assertEqual(len(parameters), 1)
+        self.assertEqual(parameters[0].value, b"23.0")
+
+    def _build_data_without_alternatives(self):
+        import_object_classes(self._out_map, ["object_class"])
+        import_objects(self._out_map, [("object_class", "object")])
+        import_object_parameters(self._out_map, [("object_class", "parameter")])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 23.0)])
+
+    def _build_data_with_single_alternative(self):
+        import_alternatives(self._out_map, ["alternative"])
+        import_object_classes(self._out_map, ["object_class"])
+        import_objects(self._out_map, [("object_class", "object")])
+        import_object_parameters(self._out_map, [("object_class", "parameter")])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", -1.0)])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 23.0, "alternative")])
+
+
+class TestAlternativeFilterWithMemoryDatabase(unittest.TestCase):
+    def setUp(self):
+        self._db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(self._db_map, ["object_class"])
+        import_objects(self._db_map, [("object_class", "object")])
+        import_object_parameters(self._db_map, [("object_class", "parameter")])
+        import_object_parameter_values(self._db_map, [("object_class", "object", "parameter", -1.0)])
+        self._db_map.commit_session("Add initial data.")
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_alternative_names_with_colons(self):
+        self._add_value_in_alternative(23.0, "new@2023-23-23T11:12:13")
+        config = alternative_filter_config(["new@2023-23-23T11:12:13"])
+        alternative_filter_from_dict(self._db_map, config)
+        parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
+        self.assertEqual(len(parameters), 1)
+        self.assertEqual(parameters[0].value, b"23.0")
+
+    def test_multiple_alternatives(self):
+        self._add_value_in_alternative(23.0, "new@2023-23-23T11:12:13")
+        self._add_value_in_alternative(101.1, "new@2005-05-05T22:23:24")
+        config = alternative_filter_config(["new@2005-05-05T22:23:24", "new@2023-23-23T11:12:13"])
+        alternative_filter_from_dict(self._db_map, config)
+        parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
+        self.assertEqual(len(parameters), 2)
+        self.assertEqual(parameters[0].value, b"23.0")
+        self.assertEqual(parameters[1].value, b"101.1")
+
+    def _add_value_in_alternative(self, value, alternative):
+        import_alternatives(self._db_map, [alternative])
+        import_object_parameter_values(self._db_map, [("object_class", "object", "parameter", value, alternative)])
+        self._db_map.commit_session(f"Add value in {alternative}")
+
+
+class TestAlternativeFilterWithoutDatabase(unittest.TestCase):
+    def test_alternative_filter_config(self):
+        config = alternative_filter_config(["alternative1", "alternative2"])
+        self.assertEqual(config, {"type": "alternative_filter", "alternatives": ["alternative1", "alternative2"]})
+
+    def test_alternative_names_from_dict(self):
+        config = alternative_filter_config(["alternative1", "alternative2"])
+        self.assertEqual(alternative_names_from_dict(config), ["alternative1", "alternative2"])
+
+    def test_alternative_filter_config_to_shorthand(self):
+        config = alternative_filter_config(["alternative1", "alternative2"])
+        shorthand = alternative_filter_config_to_shorthand(config)
+        self.assertEqual(shorthand, "alternatives:'alternative1':'alternative2'")
+
+    def test_alternative_filter_shorthand_to_config(self):
+        config = alternative_filter_shorthand_to_config("alternatives:'alternative1':'alternative2'")
+        self.assertEqual(config, {"type": "alternative_filter", "alternatives": ["alternative1", "alternative2"]})
+
+    def test_quoted_alternative_names(self):
+        config = alternative_filter_shorthand_to_config("alternatives:'alt:er:na:ti:ve':'alternative2'")
+        self.assertEqual(config, {"type": "alternative_filter", "alternatives": ["alt:er:na:ti:ve", "alternative2"]})
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/filters/test_renamer.py` & `spinedb_api-0.30.4/tests/filters/test_renamer.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,260 +1,260 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for ``renamer`` module.
-
-"""
-from pathlib import Path
-from tempfile import TemporaryDirectory
-import unittest
-from sqlalchemy.engine.url import URL
-from spinedb_api import (
-    apply_renaming_to_parameter_definition_sq,
-    apply_renaming_to_entity_class_sq,
-    create_new_spine_database,
-    DatabaseMapping,
-    DiffDatabaseMapping,
-    import_object_classes,
-    import_object_parameters,
-    import_relationship_classes,
-)
-from spinedb_api.filters.renamer import (
-    entity_class_renamer_config,
-    entity_class_renamer_config_to_shorthand,
-    entity_class_renamer_from_dict,
-    entity_class_renamer_shorthand_to_config,
-    parameter_renamer_config,
-    parameter_renamer_config_to_shorthand,
-    parameter_renamer_from_dict,
-    parameter_renamer_shorthand_to_config,
-)
-
-
-class TestEntityClassRenamer(unittest.TestCase):
-    @classmethod
-    def setUpClass(cls):
-        cls._temp_dir = TemporaryDirectory()
-        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_entity_class_renamer.sqlite").as_posix())
-
-    def setUp(self):
-        create_new_spine_database(self._db_url)
-        self._out_map = DiffDatabaseMapping(self._db_url)
-        self._db_map = DatabaseMapping(self._db_url)
-
-    def tearDown(self):
-        self._out_map.connection.close()
-        self._db_map.connection.close()
-
-    def test_renaming_empty_database(self):
-        apply_renaming_to_entity_class_sq(self._db_map, {"some_name": "another_name"})
-        classes = list(self._db_map.query(self._db_map.entity_class_sq).all())
-        self.assertEqual(classes, [])
-
-    def test_renaming_singe_entity_class(self):
-        import_object_classes(self._out_map, ("old_name",))
-        self._out_map.commit_session("Add test data")
-        apply_renaming_to_entity_class_sq(self._db_map, {"old_name": "new_name"})
-        classes = list(self._db_map.query(self._db_map.entity_class_sq).all())
-        self.assertEqual(len(classes), 1)
-        class_row = classes[0]
-        keys = tuple(class_row.keys())
-        expected_keys = ("id", "type_id", "name", "description", "display_order", "display_icon", "hidden", "commit_id")
-        self.assertEqual(len(keys), len(expected_keys))
-        for expected_key in expected_keys:
-            self.assertIn(expected_key, keys)
-        self.assertEqual(class_row.name, "new_name")
-
-    def test_renaming_singe_relationship_class(self):
-        import_object_classes(self._out_map, ("object_class",))
-        import_relationship_classes(self._out_map, (("old_name", ("object_class",)),))
-        self._out_map.commit_session("Add test data")
-        apply_renaming_to_entity_class_sq(self._db_map, {"old_name": "new_name"})
-        classes = list(self._db_map.query(self._db_map.relationship_class_sq).all())
-        self.assertEqual(len(classes), 1)
-        self.assertEqual(classes[0].name, "new_name")
-
-    def test_renaming_multiple_entity_classes(self):
-        import_object_classes(self._out_map, ("object_class1", "object_class2"))
-        import_relationship_classes(
-            self._out_map,
-            (
-                ("relationship_class1", ("object_class1", "object_class2")),
-                ("relationship_class2", ("object_class2", "object_class1")),
-            ),
-        )
-        self._out_map.commit_session("Add test data")
-        apply_renaming_to_entity_class_sq(
-            self._db_map, {"object_class1": "new_object_class", "relationship_class1": "new_relationship_class"}
-        )
-        object_classes = list(self._db_map.query(self._db_map.object_class_sq).all())
-        self.assertEqual(len(object_classes), 2)
-        names = [row.name for row in object_classes]
-        for expected_name in ["new_object_class", "object_class2"]:
-            self.assertIn(expected_name, names)
-        relationship_classes = list(self._db_map.query(self._db_map.wide_relationship_class_sq).all())
-        self.assertEqual(len(relationship_classes), 2)
-        names = [row.name for row in relationship_classes]
-        for expected_name in ["new_relationship_class", "relationship_class2"]:
-            self.assertIn(expected_name, names)
-        object_class_names = [row.object_class_name_list for row in relationship_classes]
-        for expected_names in ["new_object_class,object_class2", "object_class2,new_object_class"]:
-            self.assertIn(expected_names, object_class_names)
-
-    def test_entity_class_renamer_config(self):
-        config = entity_class_renamer_config(class1="renamed1", class2="renamed2")
-        self.assertEqual(
-            config, {"type": "entity_class_renamer", "name_map": {"class1": "renamed1", "class2": "renamed2"}}
-        )
-
-    def test_entity_class_renamer_from_dict(self):
-        import_object_classes(self._out_map, ("old_name",))
-        self._out_map.commit_session("Add test data")
-        config = entity_class_renamer_config(old_name="new_name")
-        entity_class_renamer_from_dict(self._db_map, config)
-        classes = list(self._db_map.query(self._db_map.entity_class_sq).all())
-        self.assertEqual(len(classes), 1)
-        class_row = classes[0]
-        keys = tuple(class_row.keys())
-        expected_keys = ("id", "type_id", "name", "description", "display_order", "display_icon", "hidden", "commit_id")
-        self.assertEqual(len(keys), len(expected_keys))
-        for expected_key in expected_keys:
-            self.assertIn(expected_key, keys)
-        self.assertEqual(class_row.name, "new_name")
-
-
-class TestEntityClassRenamerWithoutDatabase(unittest.TestCase):
-    def test_entity_class_renamer_config_to_shorthand(self):
-        config = entity_class_renamer_config(class1="renamed1", class2="renamed2")
-        shorthand = entity_class_renamer_config_to_shorthand(config)
-        self.assertEqual(shorthand, "entity_class_rename:class1:renamed1:class2:renamed2")
-
-    def test_entity_class_renamer_shorthand_to_config(self):
-        config = entity_class_renamer_shorthand_to_config("entity_class_rename:class1:renamed1:class2:renamed2")
-        self.assertEqual(
-            config, {"type": "entity_class_renamer", "name_map": {"class1": "renamed1", "class2": "renamed2"}}
-        )
-
-
-class TestParameterRenamer(unittest.TestCase):
-    @classmethod
-    def setUpClass(cls):
-        cls._temp_dir = TemporaryDirectory()
-        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_parameter_renamer.sqlite").as_posix())
-
-    def setUp(self):
-        create_new_spine_database(self._db_url)
-        self._out_map = DiffDatabaseMapping(self._db_url)
-        self._db_map = DatabaseMapping(self._db_url)
-
-    def tearDown(self):
-        self._out_map.connection.close()
-        self._db_map.connection.close()
-
-    def test_renaming_empty_database(self):
-        apply_renaming_to_parameter_definition_sq(self._db_map, {"some_name": "another_name"})
-        classes = list(self._db_map.query(self._db_map.parameter_definition_sq).all())
-        self.assertEqual(classes, [])
-
-    def test_renaming_single_parameter(self):
-        import_object_classes(self._out_map, ("object_class",))
-        import_object_parameters(self._out_map, (("object_class", "old_name"),))
-        self._out_map.commit_session("Add test data")
-        apply_renaming_to_parameter_definition_sq(self._db_map, {"object_class": {"old_name": "new_name"}})
-        parameters = list(self._db_map.query(self._db_map.parameter_definition_sq).all())
-        self.assertEqual(len(parameters), 1)
-        parameter_row = parameters[0]
-        keys = tuple(parameter_row.keys())
-        expected_keys = (
-            "id",
-            "name",
-            "description",
-            "entity_class_id",
-            "object_class_id",
-            "relationship_class_id",
-            "default_value",
-            "default_type",
-            "list_value_id",
-            "commit_id",
-            "parameter_value_list_id",
-        )
-        self.assertEqual(len(keys), len(expected_keys))
-        for expected_key in expected_keys:
-            self.assertIn(expected_key, keys)
-        self.assertEqual(parameter_row.name, "new_name")
-
-    def test_renaming_applies_to_correct_parameter(self):
-        import_object_classes(self._out_map, ("oc1", "oc2"))
-        import_object_parameters(self._out_map, (("oc1", "param"), ("oc2", "param")))
-        self._out_map.commit_session("Add test data")
-        apply_renaming_to_parameter_definition_sq(self._db_map, {"oc2": {"param": "new_name"}})
-        parameters = list(self._db_map.query(self._db_map.entity_parameter_definition_sq).all())
-        self.assertEqual(len(parameters), 2)
-        for parameter_row in parameters:
-            if parameter_row.entity_class_name == "oc2":
-                self.assertEqual(parameter_row.parameter_name, "new_name")
-            else:
-                self.assertEqual(parameter_row.parameter_name, "param")
-
-    def test_parameter_renamer_config(self):
-        config = parameter_renamer_config({"class": {"parameter1": "renamed1", "parameter2": "renamed2"}})
-        self.assertEqual(
-            config,
-            {"type": "parameter_renamer", "name_map": {"class": {"parameter1": "renamed1", "parameter2": "renamed2"}}},
-        )
-
-    def test_parameter_renamer_from_dict(self):
-        import_object_classes(self._out_map, ("object_class",))
-        import_object_parameters(self._out_map, (("object_class", "old_name"),))
-        self._out_map.commit_session("Add test data")
-        config = parameter_renamer_config({"object_class": {"old_name": "new_name"}})
-        parameter_renamer_from_dict(self._db_map, config)
-        parameters = list(self._db_map.query(self._db_map.parameter_definition_sq).all())
-        self.assertEqual(len(parameters), 1)
-        parameter_row = parameters[0]
-        keys = tuple(parameter_row.keys())
-        expected_keys = (
-            "id",
-            "name",
-            "description",
-            "entity_class_id",
-            "object_class_id",
-            "relationship_class_id",
-            "default_value",
-            "default_type",
-            "list_value_id",
-            "commit_id",
-            "parameter_value_list_id",
-        )
-        self.assertEqual(len(keys), len(expected_keys))
-        for expected_key in expected_keys:
-            self.assertIn(expected_key, keys)
-        self.assertEqual(parameter_row.name, "new_name")
-
-
-class TestParameterRenamerWithoutDatabase(unittest.TestCase):
-    def test_parameter_renamer_config_to_shorthand(self):
-        config = parameter_renamer_config({"class": {"parameter1": "renamed1", "parameter2": "renamed2"}})
-        shorthand = parameter_renamer_config_to_shorthand(config)
-        self.assertEqual(shorthand, "parameter_rename:class:parameter1:renamed1:class:parameter2:renamed2")
-
-    def test_parameter_renamer_shorthand_to_config(self):
-        config = parameter_renamer_shorthand_to_config(
-            "parameter_rename:class:parameter1:renamed1:class:parameter2:renamed2"
-        )
-        self.assertEqual(
-            config,
-            {"type": "parameter_renamer", "name_map": {"class": {"parameter1": "renamed1", "parameter2": "renamed2"}}},
-        )
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for ``renamer`` module.
+
+"""
+from pathlib import Path
+from tempfile import TemporaryDirectory
+import unittest
+from sqlalchemy.engine.url import URL
+from spinedb_api import (
+    apply_renaming_to_parameter_definition_sq,
+    apply_renaming_to_entity_class_sq,
+    create_new_spine_database,
+    DatabaseMapping,
+    DiffDatabaseMapping,
+    import_object_classes,
+    import_object_parameters,
+    import_relationship_classes,
+)
+from spinedb_api.filters.renamer import (
+    entity_class_renamer_config,
+    entity_class_renamer_config_to_shorthand,
+    entity_class_renamer_from_dict,
+    entity_class_renamer_shorthand_to_config,
+    parameter_renamer_config,
+    parameter_renamer_config_to_shorthand,
+    parameter_renamer_from_dict,
+    parameter_renamer_shorthand_to_config,
+)
+
+
+class TestEntityClassRenamer(unittest.TestCase):
+    @classmethod
+    def setUpClass(cls):
+        cls._temp_dir = TemporaryDirectory()
+        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_entity_class_renamer.sqlite").as_posix())
+
+    def setUp(self):
+        create_new_spine_database(self._db_url)
+        self._out_map = DiffDatabaseMapping(self._db_url)
+        self._db_map = DatabaseMapping(self._db_url)
+
+    def tearDown(self):
+        self._out_map.connection.close()
+        self._db_map.connection.close()
+
+    def test_renaming_empty_database(self):
+        apply_renaming_to_entity_class_sq(self._db_map, {"some_name": "another_name"})
+        classes = list(self._db_map.query(self._db_map.entity_class_sq).all())
+        self.assertEqual(classes, [])
+
+    def test_renaming_singe_entity_class(self):
+        import_object_classes(self._out_map, ("old_name",))
+        self._out_map.commit_session("Add test data")
+        apply_renaming_to_entity_class_sq(self._db_map, {"old_name": "new_name"})
+        classes = list(self._db_map.query(self._db_map.entity_class_sq).all())
+        self.assertEqual(len(classes), 1)
+        class_row = classes[0]
+        keys = tuple(class_row.keys())
+        expected_keys = ("id", "type_id", "name", "description", "display_order", "display_icon", "hidden", "commit_id")
+        self.assertEqual(len(keys), len(expected_keys))
+        for expected_key in expected_keys:
+            self.assertIn(expected_key, keys)
+        self.assertEqual(class_row.name, "new_name")
+
+    def test_renaming_singe_relationship_class(self):
+        import_object_classes(self._out_map, ("object_class",))
+        import_relationship_classes(self._out_map, (("old_name", ("object_class",)),))
+        self._out_map.commit_session("Add test data")
+        apply_renaming_to_entity_class_sq(self._db_map, {"old_name": "new_name"})
+        classes = list(self._db_map.query(self._db_map.relationship_class_sq).all())
+        self.assertEqual(len(classes), 1)
+        self.assertEqual(classes[0].name, "new_name")
+
+    def test_renaming_multiple_entity_classes(self):
+        import_object_classes(self._out_map, ("object_class1", "object_class2"))
+        import_relationship_classes(
+            self._out_map,
+            (
+                ("relationship_class1", ("object_class1", "object_class2")),
+                ("relationship_class2", ("object_class2", "object_class1")),
+            ),
+        )
+        self._out_map.commit_session("Add test data")
+        apply_renaming_to_entity_class_sq(
+            self._db_map, {"object_class1": "new_object_class", "relationship_class1": "new_relationship_class"}
+        )
+        object_classes = list(self._db_map.query(self._db_map.object_class_sq).all())
+        self.assertEqual(len(object_classes), 2)
+        names = [row.name for row in object_classes]
+        for expected_name in ["new_object_class", "object_class2"]:
+            self.assertIn(expected_name, names)
+        relationship_classes = list(self._db_map.query(self._db_map.wide_relationship_class_sq).all())
+        self.assertEqual(len(relationship_classes), 2)
+        names = [row.name for row in relationship_classes]
+        for expected_name in ["new_relationship_class", "relationship_class2"]:
+            self.assertIn(expected_name, names)
+        object_class_names = [row.object_class_name_list for row in relationship_classes]
+        for expected_names in ["new_object_class,object_class2", "object_class2,new_object_class"]:
+            self.assertIn(expected_names, object_class_names)
+
+    def test_entity_class_renamer_config(self):
+        config = entity_class_renamer_config(class1="renamed1", class2="renamed2")
+        self.assertEqual(
+            config, {"type": "entity_class_renamer", "name_map": {"class1": "renamed1", "class2": "renamed2"}}
+        )
+
+    def test_entity_class_renamer_from_dict(self):
+        import_object_classes(self._out_map, ("old_name",))
+        self._out_map.commit_session("Add test data")
+        config = entity_class_renamer_config(old_name="new_name")
+        entity_class_renamer_from_dict(self._db_map, config)
+        classes = list(self._db_map.query(self._db_map.entity_class_sq).all())
+        self.assertEqual(len(classes), 1)
+        class_row = classes[0]
+        keys = tuple(class_row.keys())
+        expected_keys = ("id", "type_id", "name", "description", "display_order", "display_icon", "hidden", "commit_id")
+        self.assertEqual(len(keys), len(expected_keys))
+        for expected_key in expected_keys:
+            self.assertIn(expected_key, keys)
+        self.assertEqual(class_row.name, "new_name")
+
+
+class TestEntityClassRenamerWithoutDatabase(unittest.TestCase):
+    def test_entity_class_renamer_config_to_shorthand(self):
+        config = entity_class_renamer_config(class1="renamed1", class2="renamed2")
+        shorthand = entity_class_renamer_config_to_shorthand(config)
+        self.assertEqual(shorthand, "entity_class_rename:class1:renamed1:class2:renamed2")
+
+    def test_entity_class_renamer_shorthand_to_config(self):
+        config = entity_class_renamer_shorthand_to_config("entity_class_rename:class1:renamed1:class2:renamed2")
+        self.assertEqual(
+            config, {"type": "entity_class_renamer", "name_map": {"class1": "renamed1", "class2": "renamed2"}}
+        )
+
+
+class TestParameterRenamer(unittest.TestCase):
+    @classmethod
+    def setUpClass(cls):
+        cls._temp_dir = TemporaryDirectory()
+        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_parameter_renamer.sqlite").as_posix())
+
+    def setUp(self):
+        create_new_spine_database(self._db_url)
+        self._out_map = DiffDatabaseMapping(self._db_url)
+        self._db_map = DatabaseMapping(self._db_url)
+
+    def tearDown(self):
+        self._out_map.connection.close()
+        self._db_map.connection.close()
+
+    def test_renaming_empty_database(self):
+        apply_renaming_to_parameter_definition_sq(self._db_map, {"some_name": "another_name"})
+        classes = list(self._db_map.query(self._db_map.parameter_definition_sq).all())
+        self.assertEqual(classes, [])
+
+    def test_renaming_single_parameter(self):
+        import_object_classes(self._out_map, ("object_class",))
+        import_object_parameters(self._out_map, (("object_class", "old_name"),))
+        self._out_map.commit_session("Add test data")
+        apply_renaming_to_parameter_definition_sq(self._db_map, {"object_class": {"old_name": "new_name"}})
+        parameters = list(self._db_map.query(self._db_map.parameter_definition_sq).all())
+        self.assertEqual(len(parameters), 1)
+        parameter_row = parameters[0]
+        keys = tuple(parameter_row.keys())
+        expected_keys = (
+            "id",
+            "name",
+            "description",
+            "entity_class_id",
+            "object_class_id",
+            "relationship_class_id",
+            "default_value",
+            "default_type",
+            "list_value_id",
+            "commit_id",
+            "parameter_value_list_id",
+        )
+        self.assertEqual(len(keys), len(expected_keys))
+        for expected_key in expected_keys:
+            self.assertIn(expected_key, keys)
+        self.assertEqual(parameter_row.name, "new_name")
+
+    def test_renaming_applies_to_correct_parameter(self):
+        import_object_classes(self._out_map, ("oc1", "oc2"))
+        import_object_parameters(self._out_map, (("oc1", "param"), ("oc2", "param")))
+        self._out_map.commit_session("Add test data")
+        apply_renaming_to_parameter_definition_sq(self._db_map, {"oc2": {"param": "new_name"}})
+        parameters = list(self._db_map.query(self._db_map.entity_parameter_definition_sq).all())
+        self.assertEqual(len(parameters), 2)
+        for parameter_row in parameters:
+            if parameter_row.entity_class_name == "oc2":
+                self.assertEqual(parameter_row.parameter_name, "new_name")
+            else:
+                self.assertEqual(parameter_row.parameter_name, "param")
+
+    def test_parameter_renamer_config(self):
+        config = parameter_renamer_config({"class": {"parameter1": "renamed1", "parameter2": "renamed2"}})
+        self.assertEqual(
+            config,
+            {"type": "parameter_renamer", "name_map": {"class": {"parameter1": "renamed1", "parameter2": "renamed2"}}},
+        )
+
+    def test_parameter_renamer_from_dict(self):
+        import_object_classes(self._out_map, ("object_class",))
+        import_object_parameters(self._out_map, (("object_class", "old_name"),))
+        self._out_map.commit_session("Add test data")
+        config = parameter_renamer_config({"object_class": {"old_name": "new_name"}})
+        parameter_renamer_from_dict(self._db_map, config)
+        parameters = list(self._db_map.query(self._db_map.parameter_definition_sq).all())
+        self.assertEqual(len(parameters), 1)
+        parameter_row = parameters[0]
+        keys = tuple(parameter_row.keys())
+        expected_keys = (
+            "id",
+            "name",
+            "description",
+            "entity_class_id",
+            "object_class_id",
+            "relationship_class_id",
+            "default_value",
+            "default_type",
+            "list_value_id",
+            "commit_id",
+            "parameter_value_list_id",
+        )
+        self.assertEqual(len(keys), len(expected_keys))
+        for expected_key in expected_keys:
+            self.assertIn(expected_key, keys)
+        self.assertEqual(parameter_row.name, "new_name")
+
+
+class TestParameterRenamerWithoutDatabase(unittest.TestCase):
+    def test_parameter_renamer_config_to_shorthand(self):
+        config = parameter_renamer_config({"class": {"parameter1": "renamed1", "parameter2": "renamed2"}})
+        shorthand = parameter_renamer_config_to_shorthand(config)
+        self.assertEqual(shorthand, "parameter_rename:class:parameter1:renamed1:class:parameter2:renamed2")
+
+    def test_parameter_renamer_shorthand_to_config(self):
+        config = parameter_renamer_shorthand_to_config(
+            "parameter_rename:class:parameter1:renamed1:class:parameter2:renamed2"
+        )
+        self.assertEqual(
+            config,
+            {"type": "parameter_renamer", "name_map": {"class": {"parameter1": "renamed1", "parameter2": "renamed2"}}},
+        )
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/filters/test_scenario_filter.py` & `spinedb_api-0.30.4/tests/filters/test_scenario_filter.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,439 +1,439 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for ``alternative_value_filter`` module.
-
-"""
-from pathlib import Path
-from tempfile import TemporaryDirectory
-import unittest
-from sqlalchemy.engine.url import URL
-from spinedb_api import (
-    apply_scenario_filter_to_subqueries,
-    create_new_spine_database,
-    DatabaseMapping,
-    DiffDatabaseMapping,
-    import_alternatives,
-    import_object_classes,
-    import_object_parameter_values,
-    import_object_parameters,
-    import_objects,
-    import_relationship_classes,
-    import_relationship_parameter_values,
-    import_relationship_parameters,
-    import_relationships,
-    import_scenario_alternatives,
-    import_scenarios,
-)
-from spinedb_api.filters.scenario_filter import (
-    scenario_filter_config,
-    scenario_filter_config_to_shorthand,
-    scenario_filter_from_dict,
-    scenario_filter_shorthand_to_config,
-    scenario_name_from_dict,
-)
-
-
-class TestScenarioFilter(unittest.TestCase):
-    _db_url = None
-    _temp_dir = None
-
-    @classmethod
-    def setUpClass(cls):
-        cls._temp_dir = TemporaryDirectory()
-        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_scenario_filter_mapping.sqlite").as_posix())
-
-    def setUp(self):
-        create_new_spine_database(self._db_url)
-        self._out_map = DiffDatabaseMapping(self._db_url)
-        self._db_map = DatabaseMapping(self._db_url)
-        self._diff_db_map = DiffDatabaseMapping(self._db_url)
-
-    def tearDown(self):
-        self._out_map.connection.close()
-        self._db_map.connection.close()
-        self._diff_db_map.connection.close()
-
-    def _build_data_with_single_scenario(self):
-        import_alternatives(self._out_map, ["alternative"])
-        import_object_classes(self._out_map, ["object_class"])
-        import_objects(self._out_map, [("object_class", "object")])
-        import_object_parameters(self._out_map, [("object_class", "parameter")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", -1.0)])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 23.0, "alternative")])
-        import_scenarios(self._out_map, [("scenario", True)])
-        import_scenario_alternatives(self._out_map, [("scenario", "alternative")])
-
-    def test_scenario_filter(self):
-        _build_data_with_single_scenario(self._out_map)
-        for db_map in [self._db_map, self._diff_db_map]:
-            apply_scenario_filter_to_subqueries(db_map, "scenario")
-            parameters = db_map.query(db_map.parameter_value_sq).all()
-            self.assertEqual(len(parameters), 1)
-            self.assertEqual(parameters[0].value, b"23.0")
-            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
-            self.assertEqual(alternatives, [{"name": "alternative", "description": None, "id": 2, "commit_id": 2}])
-            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
-            self.assertEqual(
-                scenarios,
-                [
-                    {
-                        "name": "scenario",
-                        "description": None,
-                        "active": True,
-                        "alternative_name_list": "alternative",
-                        "alternative_id_list": "2",
-                        "id": 1,
-                        "commit_id": 2,
-                    }
-                ],
-            )
-
-    def test_scenario_filter_uncommitted_data(self):
-        _build_data_with_single_scenario(self._out_map, commit=False)
-        apply_scenario_filter_to_subqueries(self._out_map, "scenario")
-        parameters = self._out_map.query(self._out_map.parameter_value_sq).all()
-        self.assertEqual(len(parameters), 1)
-        self.assertEqual(parameters[0].value, b"23.0")
-        alternatives = [a._asdict() for a in self._out_map.query(self._out_map.alternative_sq)]
-        self.assertEqual(alternatives, [{"name": "alternative", "description": None, "id": 2, "commit_id": None}])
-        scenarios = [s._asdict() for s in self._out_map.query(self._out_map.wide_scenario_sq).all()]
-        self.assertEqual(
-            scenarios,
-            [
-                {
-                    "name": "scenario",
-                    "description": None,
-                    "active": True,
-                    "alternative_name_list": "alternative",
-                    "alternative_id_list": "2",
-                    "id": 1,
-                    "commit_id": None,
-                }
-            ],
-        )
-        self._out_map.rollback_session()
-
-    def test_scenario_filter_works_for_object_parameter_value_sq(self):
-        _build_data_with_single_scenario(self._out_map)
-        for db_map in [self._db_map, self._diff_db_map]:
-            apply_scenario_filter_to_subqueries(db_map, "scenario")
-            parameters = db_map.query(db_map.object_parameter_value_sq).all()
-            self.assertEqual(len(parameters), 1)
-            self.assertEqual(parameters[0].value, b"23.0")
-            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
-            self.assertEqual(alternatives, [{"name": "alternative", "description": None, "id": 2, "commit_id": 2}])
-            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
-            self.assertEqual(
-                scenarios,
-                [
-                    {
-                        "name": "scenario",
-                        "description": None,
-                        "active": True,
-                        "alternative_name_list": "alternative",
-                        "alternative_id_list": "2",
-                        "id": 1,
-                        "commit_id": 2,
-                    }
-                ],
-            )
-
-    def test_scenario_filter_works_for_relationship_parameter_value_sq(self):
-        _build_data_with_single_scenario(self._out_map)
-        import_relationship_classes(self._out_map, [("relationship_class", ["object_class"])])
-        import_relationship_parameters(self._out_map, [("relationship_class", "relationship_parameter")])
-        import_relationships(self._out_map, [("relationship_class", ["object"])])
-        import_relationship_parameter_values(
-            self._out_map, [("relationship_class", ["object"], "relationship_parameter", -1)]
-        )
-        import_relationship_parameter_values(
-            self._out_map, [("relationship_class", ["object"], "relationship_parameter", 23.0, "alternative")]
-        )
-        self._out_map.commit_session("Add test data")
-        for db_map in [self._db_map, self._diff_db_map]:
-            apply_scenario_filter_to_subqueries(db_map, "scenario")
-            parameters = db_map.query(db_map.relationship_parameter_value_sq).all()
-            self.assertEqual(len(parameters), 1)
-            self.assertEqual(parameters[0].value, b"23.0")
-            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
-            self.assertEqual(alternatives, [{"name": "alternative", "description": None, "id": 2, "commit_id": 2}])
-            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
-            self.assertEqual(
-                scenarios,
-                [
-                    {
-                        "name": "scenario",
-                        "description": None,
-                        "active": True,
-                        "alternative_name_list": "alternative",
-                        "alternative_id_list": "2",
-                        "id": 1,
-                        "commit_id": 2,
-                    }
-                ],
-            )
-
-    def test_scenario_filter_selects_highest_ranked_alternative(self):
-        import_alternatives(self._out_map, ["alternative3"])
-        import_alternatives(self._out_map, ["alternative1"])
-        import_alternatives(self._out_map, ["alternative2"])
-        import_object_classes(self._out_map, ["object_class"])
-        import_objects(self._out_map, [("object_class", "object")])
-        import_object_parameters(self._out_map, [("object_class", "parameter")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", -1.0)])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 10.0, "alternative1")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 2000.0, "alternative2")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 300.0, "alternative3")])
-        import_scenarios(self._out_map, [("scenario", True)])
-        import_scenario_alternatives(
-            self._out_map,
-            [
-                ("scenario", "alternative2"),
-                ("scenario", "alternative3", "alternative2"),
-                ("scenario", "alternative1", "alternative3"),
-            ],
-        )
-        self._out_map.commit_session("Add test data")
-        for db_map in [self._db_map, self._diff_db_map]:
-            apply_scenario_filter_to_subqueries(db_map, "scenario")
-            parameters = db_map.query(db_map.parameter_value_sq).all()
-            self.assertEqual(len(parameters), 1)
-            self.assertEqual(parameters[0].value, b"2000.0")
-            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
-            self.assertEqual(
-                alternatives,
-                [
-                    {"name": "alternative3", "description": None, "id": 2, "commit_id": 2},
-                    {"name": "alternative1", "description": None, "id": 3, "commit_id": 2},
-                    {"name": "alternative2", "description": None, "id": 4, "commit_id": 2},
-                ],
-            )
-            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
-            self.assertEqual(
-                scenarios,
-                [
-                    {
-                        "name": "scenario",
-                        "description": None,
-                        "active": True,
-                        "alternative_name_list": "alternative1,alternative3,alternative2",
-                        "alternative_id_list": "3,2,4",
-                        "id": 1,
-                        "commit_id": 2,
-                    }
-                ],
-            )
-
-    def test_scenario_filter_selects_highest_ranked_alternative_of_active_scenario(self):
-        import_alternatives(self._out_map, ["alternative3"])
-        import_alternatives(self._out_map, ["alternative1"])
-        import_alternatives(self._out_map, ["alternative2"])
-        import_alternatives(self._out_map, ["non_active_alternative"])
-        import_object_classes(self._out_map, ["object_class"])
-        import_objects(self._out_map, [("object_class", "object")])
-        import_object_parameters(self._out_map, [("object_class", "parameter")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", -1.0)])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 10.0, "alternative1")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 2000.0, "alternative2")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 300.0, "alternative3")])
-        import_scenarios(self._out_map, [("scenario", True)])
-        import_scenarios(self._out_map, [("non_active_scenario", False)])
-        result = import_scenario_alternatives(
-            self._out_map,
-            [
-                ("scenario", "alternative2"),
-                ("scenario", "alternative3", "alternative2"),
-                ("scenario", "alternative1", "alternative3"),
-            ],
-        )
-        self.assertEqual(result, (7, []))
-        result = import_scenario_alternatives(
-            self._out_map,
-            [
-                ("non_active_scenario", "non_active_alternative"),
-                ("scenario", "alternative2", "non_active_alternative"),
-                ("scenario", "alternative3", "alternative2"),
-                ("scenario", "alternative1", "alternative3"),
-            ],
-        )
-        self.assertEqual(result, (11, []))
-        self._out_map.commit_session("Add test data")
-        for db_map in [self._db_map, self._diff_db_map]:
-            apply_scenario_filter_to_subqueries(db_map, "scenario")
-            parameters = db_map.query(db_map.parameter_value_sq).all()
-            self.assertEqual(len(parameters), 1)
-            self.assertEqual(parameters[0].value, b"2000.0")
-            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
-            self.assertEqual(
-                alternatives,
-                [
-                    {"name": "alternative3", "description": None, "id": 2, "commit_id": 2},
-                    {"name": "alternative1", "description": None, "id": 3, "commit_id": 2},
-                    {"name": "alternative2", "description": None, "id": 4, "commit_id": 2},
-                    {"name": "non_active_alternative", "description": None, "id": 5, "commit_id": 2},
-                ],
-            )
-            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
-            self.assertEqual(
-                scenarios,
-                [
-                    {
-                        "name": "scenario",
-                        "description": None,
-                        "active": True,
-                        "alternative_name_list": "alternative1,alternative3,alternative2,non_active_alternative",
-                        "alternative_id_list": "3,2,4,5",
-                        "id": 1,
-                        "commit_id": 2,
-                    }
-                ],
-            )
-
-    def test_scenario_filter_for_multiple_objects_and_parameters(self):
-        import_alternatives(self._out_map, ["alternative"])
-        import_object_classes(self._out_map, ["object_class"])
-        import_objects(self._out_map, [("object_class", "object1")])
-        import_objects(self._out_map, [("object_class", "object2")])
-        import_object_parameters(self._out_map, [("object_class", "parameter1")])
-        import_object_parameters(self._out_map, [("object_class", "parameter2")])
-        import_object_parameter_values(self._out_map, [("object_class", "object1", "parameter1", -1.0)])
-        import_object_parameter_values(self._out_map, [("object_class", "object1", "parameter1", 10.0, "alternative")])
-        import_object_parameter_values(self._out_map, [("object_class", "object1", "parameter2", -1.0)])
-        import_object_parameter_values(self._out_map, [("object_class", "object1", "parameter2", 11.0, "alternative")])
-        import_object_parameter_values(self._out_map, [("object_class", "object2", "parameter1", -2.0)])
-        import_object_parameter_values(self._out_map, [("object_class", "object2", "parameter1", 20.0, "alternative")])
-        import_object_parameter_values(self._out_map, [("object_class", "object2", "parameter2", -2.0)])
-        import_object_parameter_values(self._out_map, [("object_class", "object2", "parameter2", 22.0, "alternative")])
-        import_scenarios(self._out_map, [("scenario", True)])
-        import_scenario_alternatives(self._out_map, [("scenario", "alternative")])
-        self._out_map.commit_session("Add test data")
-        for db_map in [self._db_map, self._diff_db_map]:
-            apply_scenario_filter_to_subqueries(db_map, "scenario")
-            parameters = db_map.query(db_map.parameter_value_sq).all()
-            self.assertEqual(len(parameters), 4)
-            object_names = {o.id: o.name for o in db_map.query(db_map.object_sq).all()}
-            alternative_names = {a.id: a.name for a in db_map.query(db_map.alternative_sq).all()}
-            parameter_names = {d.id: d.name for d in db_map.query(db_map.parameter_definition_sq).all()}
-            datamined_values = dict()
-            for parameter in parameters:
-                self.assertEqual(alternative_names[parameter.alternative_id], "alternative")
-                parameter_values = datamined_values.setdefault(object_names[parameter.object_id], dict())
-                parameter_values[parameter_names[parameter.parameter_definition_id]] = parameter.value
-            self.assertEqual(
-                datamined_values,
-                {
-                    "object1": {"parameter1": b"10.0", "parameter2": b"11.0"},
-                    "object2": {"parameter1": b"20.0", "parameter2": b"22.0"},
-                },
-            )
-            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
-            self.assertEqual(alternatives, [{"name": "alternative", "description": None, "id": 2, "commit_id": 2}])
-            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
-            self.assertEqual(
-                scenarios,
-                [
-                    {
-                        "name": "scenario",
-                        "description": None,
-                        "active": True,
-                        "alternative_name_list": "alternative",
-                        "alternative_id_list": "2",
-                        "id": 1,
-                        "commit_id": 2,
-                    }
-                ],
-            )
-
-    def test_filters_scenarios_and_alternatives(self):
-        import_scenarios(self._out_map, ("scenario1", "scenario2"))
-        import_alternatives(self._out_map, ("alternative1", "alternative2", "alternative3"))
-        import_scenario_alternatives(
-            self._out_map,
-            (
-                ("scenario1", "alternative2"),
-                ("scenario1", "alternative1", "alternative2"),
-                ("scenario2", "alternative3"),
-                ("scenario2", "alternative2", "alternative3"),
-            ),
-        )
-        self._out_map.commit_session("Add test data.")
-        for db_map in (self._db_map, self._diff_db_map):
-            apply_scenario_filter_to_subqueries(db_map, "scenario2")
-            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
-            self.assertEqual(
-                alternatives,
-                [
-                    {"name": "alternative2", "description": None, "id": 3, "commit_id": 2},
-                    {"name": "alternative3", "description": None, "id": 4, "commit_id": 2},
-                ],
-            )
-            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
-            self.assertEqual(
-                scenarios,
-                [
-                    {
-                        "name": "scenario2",
-                        "description": None,
-                        "active": False,
-                        "alternative_name_list": "alternative2,alternative3",
-                        "alternative_id_list": "3,4",
-                        "id": 2,
-                        "commit_id": 2,
-                    }
-                ],
-            )
-
-
-class TestScenarioFilterUtils(unittest.TestCase):
-    def test_scenario_filter_config(self):
-        config = scenario_filter_config("scenario name")
-        self.assertEqual(config, {"type": "scenario_filter", "scenario": "scenario name"})
-
-    def test_scenario_filter_from_dict(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        _build_data_with_single_scenario(db_map)
-        config = scenario_filter_config("scenario")
-        scenario_filter_from_dict(db_map, config)
-        parameters = db_map.query(db_map.parameter_value_sq).all()
-        self.assertEqual(len(parameters), 1)
-        self.assertEqual(parameters[0].value, b"23.0")
-
-    def test_scenario_name_from_dict(self):
-        config = scenario_filter_config("scenario name")
-        self.assertEqual(scenario_name_from_dict(config), "scenario name")
-
-    def test_scenario_filter_config_to_shorthand(self):
-        config = scenario_filter_config("scenario name")
-        shorthand = scenario_filter_config_to_shorthand(config)
-        self.assertEqual(shorthand, "scenario:scenario name")
-
-    def test_scenario_filter_shorthand_to_config(self):
-        config = scenario_filter_shorthand_to_config("scenario:scenario name")
-        self.assertEqual(config, {"type": "scenario_filter", "scenario": "scenario name"})
-
-
-def _build_data_with_single_scenario(db_map, commit=True):
-    import_alternatives(db_map, ["alternative"])
-    import_object_classes(db_map, ["object_class"])
-    import_objects(db_map, [("object_class", "object")])
-    import_object_parameters(db_map, [("object_class", "parameter")])
-    import_object_parameter_values(db_map, [("object_class", "object", "parameter", -1.0)])
-    import_object_parameter_values(db_map, [("object_class", "object", "parameter", 23.0, "alternative")])
-    import_scenarios(db_map, [("scenario", True)])
-    import_scenario_alternatives(db_map, [("scenario", "alternative")])
-    if commit:
-        db_map.commit_session("Add test data.")
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for ``alternative_value_filter`` module.
+
+"""
+from pathlib import Path
+from tempfile import TemporaryDirectory
+import unittest
+from sqlalchemy.engine.url import URL
+from spinedb_api import (
+    apply_scenario_filter_to_subqueries,
+    create_new_spine_database,
+    DatabaseMapping,
+    DiffDatabaseMapping,
+    import_alternatives,
+    import_object_classes,
+    import_object_parameter_values,
+    import_object_parameters,
+    import_objects,
+    import_relationship_classes,
+    import_relationship_parameter_values,
+    import_relationship_parameters,
+    import_relationships,
+    import_scenario_alternatives,
+    import_scenarios,
+)
+from spinedb_api.filters.scenario_filter import (
+    scenario_filter_config,
+    scenario_filter_config_to_shorthand,
+    scenario_filter_from_dict,
+    scenario_filter_shorthand_to_config,
+    scenario_name_from_dict,
+)
+
+
+class TestScenarioFilter(unittest.TestCase):
+    _db_url = None
+    _temp_dir = None
+
+    @classmethod
+    def setUpClass(cls):
+        cls._temp_dir = TemporaryDirectory()
+        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_scenario_filter_mapping.sqlite").as_posix())
+
+    def setUp(self):
+        create_new_spine_database(self._db_url)
+        self._out_map = DiffDatabaseMapping(self._db_url)
+        self._db_map = DatabaseMapping(self._db_url)
+        self._diff_db_map = DiffDatabaseMapping(self._db_url)
+
+    def tearDown(self):
+        self._out_map.connection.close()
+        self._db_map.connection.close()
+        self._diff_db_map.connection.close()
+
+    def _build_data_with_single_scenario(self):
+        import_alternatives(self._out_map, ["alternative"])
+        import_object_classes(self._out_map, ["object_class"])
+        import_objects(self._out_map, [("object_class", "object")])
+        import_object_parameters(self._out_map, [("object_class", "parameter")])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", -1.0)])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 23.0, "alternative")])
+        import_scenarios(self._out_map, [("scenario", True)])
+        import_scenario_alternatives(self._out_map, [("scenario", "alternative")])
+
+    def test_scenario_filter(self):
+        _build_data_with_single_scenario(self._out_map)
+        for db_map in [self._db_map, self._diff_db_map]:
+            apply_scenario_filter_to_subqueries(db_map, "scenario")
+            parameters = db_map.query(db_map.parameter_value_sq).all()
+            self.assertEqual(len(parameters), 1)
+            self.assertEqual(parameters[0].value, b"23.0")
+            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
+            self.assertEqual(alternatives, [{"name": "alternative", "description": None, "id": 2, "commit_id": 2}])
+            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
+            self.assertEqual(
+                scenarios,
+                [
+                    {
+                        "name": "scenario",
+                        "description": None,
+                        "active": True,
+                        "alternative_name_list": "alternative",
+                        "alternative_id_list": "2",
+                        "id": 1,
+                        "commit_id": 2,
+                    }
+                ],
+            )
+
+    def test_scenario_filter_uncommitted_data(self):
+        _build_data_with_single_scenario(self._out_map, commit=False)
+        apply_scenario_filter_to_subqueries(self._out_map, "scenario")
+        parameters = self._out_map.query(self._out_map.parameter_value_sq).all()
+        self.assertEqual(len(parameters), 1)
+        self.assertEqual(parameters[0].value, b"23.0")
+        alternatives = [a._asdict() for a in self._out_map.query(self._out_map.alternative_sq)]
+        self.assertEqual(alternatives, [{"name": "alternative", "description": None, "id": 2, "commit_id": None}])
+        scenarios = [s._asdict() for s in self._out_map.query(self._out_map.wide_scenario_sq).all()]
+        self.assertEqual(
+            scenarios,
+            [
+                {
+                    "name": "scenario",
+                    "description": None,
+                    "active": True,
+                    "alternative_name_list": "alternative",
+                    "alternative_id_list": "2",
+                    "id": 1,
+                    "commit_id": None,
+                }
+            ],
+        )
+        self._out_map.rollback_session()
+
+    def test_scenario_filter_works_for_object_parameter_value_sq(self):
+        _build_data_with_single_scenario(self._out_map)
+        for db_map in [self._db_map, self._diff_db_map]:
+            apply_scenario_filter_to_subqueries(db_map, "scenario")
+            parameters = db_map.query(db_map.object_parameter_value_sq).all()
+            self.assertEqual(len(parameters), 1)
+            self.assertEqual(parameters[0].value, b"23.0")
+            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
+            self.assertEqual(alternatives, [{"name": "alternative", "description": None, "id": 2, "commit_id": 2}])
+            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
+            self.assertEqual(
+                scenarios,
+                [
+                    {
+                        "name": "scenario",
+                        "description": None,
+                        "active": True,
+                        "alternative_name_list": "alternative",
+                        "alternative_id_list": "2",
+                        "id": 1,
+                        "commit_id": 2,
+                    }
+                ],
+            )
+
+    def test_scenario_filter_works_for_relationship_parameter_value_sq(self):
+        _build_data_with_single_scenario(self._out_map)
+        import_relationship_classes(self._out_map, [("relationship_class", ["object_class"])])
+        import_relationship_parameters(self._out_map, [("relationship_class", "relationship_parameter")])
+        import_relationships(self._out_map, [("relationship_class", ["object"])])
+        import_relationship_parameter_values(
+            self._out_map, [("relationship_class", ["object"], "relationship_parameter", -1)]
+        )
+        import_relationship_parameter_values(
+            self._out_map, [("relationship_class", ["object"], "relationship_parameter", 23.0, "alternative")]
+        )
+        self._out_map.commit_session("Add test data")
+        for db_map in [self._db_map, self._diff_db_map]:
+            apply_scenario_filter_to_subqueries(db_map, "scenario")
+            parameters = db_map.query(db_map.relationship_parameter_value_sq).all()
+            self.assertEqual(len(parameters), 1)
+            self.assertEqual(parameters[0].value, b"23.0")
+            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
+            self.assertEqual(alternatives, [{"name": "alternative", "description": None, "id": 2, "commit_id": 2}])
+            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
+            self.assertEqual(
+                scenarios,
+                [
+                    {
+                        "name": "scenario",
+                        "description": None,
+                        "active": True,
+                        "alternative_name_list": "alternative",
+                        "alternative_id_list": "2",
+                        "id": 1,
+                        "commit_id": 2,
+                    }
+                ],
+            )
+
+    def test_scenario_filter_selects_highest_ranked_alternative(self):
+        import_alternatives(self._out_map, ["alternative3"])
+        import_alternatives(self._out_map, ["alternative1"])
+        import_alternatives(self._out_map, ["alternative2"])
+        import_object_classes(self._out_map, ["object_class"])
+        import_objects(self._out_map, [("object_class", "object")])
+        import_object_parameters(self._out_map, [("object_class", "parameter")])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", -1.0)])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 10.0, "alternative1")])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 2000.0, "alternative2")])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 300.0, "alternative3")])
+        import_scenarios(self._out_map, [("scenario", True)])
+        import_scenario_alternatives(
+            self._out_map,
+            [
+                ("scenario", "alternative2"),
+                ("scenario", "alternative3", "alternative2"),
+                ("scenario", "alternative1", "alternative3"),
+            ],
+        )
+        self._out_map.commit_session("Add test data")
+        for db_map in [self._db_map, self._diff_db_map]:
+            apply_scenario_filter_to_subqueries(db_map, "scenario")
+            parameters = db_map.query(db_map.parameter_value_sq).all()
+            self.assertEqual(len(parameters), 1)
+            self.assertEqual(parameters[0].value, b"2000.0")
+            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
+            self.assertEqual(
+                alternatives,
+                [
+                    {"name": "alternative3", "description": None, "id": 2, "commit_id": 2},
+                    {"name": "alternative1", "description": None, "id": 3, "commit_id": 2},
+                    {"name": "alternative2", "description": None, "id": 4, "commit_id": 2},
+                ],
+            )
+            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
+            self.assertEqual(
+                scenarios,
+                [
+                    {
+                        "name": "scenario",
+                        "description": None,
+                        "active": True,
+                        "alternative_name_list": "alternative1,alternative3,alternative2",
+                        "alternative_id_list": "3,2,4",
+                        "id": 1,
+                        "commit_id": 2,
+                    }
+                ],
+            )
+
+    def test_scenario_filter_selects_highest_ranked_alternative_of_active_scenario(self):
+        import_alternatives(self._out_map, ["alternative3"])
+        import_alternatives(self._out_map, ["alternative1"])
+        import_alternatives(self._out_map, ["alternative2"])
+        import_alternatives(self._out_map, ["non_active_alternative"])
+        import_object_classes(self._out_map, ["object_class"])
+        import_objects(self._out_map, [("object_class", "object")])
+        import_object_parameters(self._out_map, [("object_class", "parameter")])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", -1.0)])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 10.0, "alternative1")])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 2000.0, "alternative2")])
+        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 300.0, "alternative3")])
+        import_scenarios(self._out_map, [("scenario", True)])
+        import_scenarios(self._out_map, [("non_active_scenario", False)])
+        result = import_scenario_alternatives(
+            self._out_map,
+            [
+                ("scenario", "alternative2"),
+                ("scenario", "alternative3", "alternative2"),
+                ("scenario", "alternative1", "alternative3"),
+            ],
+        )
+        self.assertEqual(result, (7, []))
+        result = import_scenario_alternatives(
+            self._out_map,
+            [
+                ("non_active_scenario", "non_active_alternative"),
+                ("scenario", "alternative2", "non_active_alternative"),
+                ("scenario", "alternative3", "alternative2"),
+                ("scenario", "alternative1", "alternative3"),
+            ],
+        )
+        self.assertEqual(result, (11, []))
+        self._out_map.commit_session("Add test data")
+        for db_map in [self._db_map, self._diff_db_map]:
+            apply_scenario_filter_to_subqueries(db_map, "scenario")
+            parameters = db_map.query(db_map.parameter_value_sq).all()
+            self.assertEqual(len(parameters), 1)
+            self.assertEqual(parameters[0].value, b"2000.0")
+            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
+            self.assertEqual(
+                alternatives,
+                [
+                    {"name": "alternative3", "description": None, "id": 2, "commit_id": 2},
+                    {"name": "alternative1", "description": None, "id": 3, "commit_id": 2},
+                    {"name": "alternative2", "description": None, "id": 4, "commit_id": 2},
+                    {"name": "non_active_alternative", "description": None, "id": 5, "commit_id": 2},
+                ],
+            )
+            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
+            self.assertEqual(
+                scenarios,
+                [
+                    {
+                        "name": "scenario",
+                        "description": None,
+                        "active": True,
+                        "alternative_name_list": "alternative1,alternative3,alternative2,non_active_alternative",
+                        "alternative_id_list": "3,2,4,5",
+                        "id": 1,
+                        "commit_id": 2,
+                    }
+                ],
+            )
+
+    def test_scenario_filter_for_multiple_objects_and_parameters(self):
+        import_alternatives(self._out_map, ["alternative"])
+        import_object_classes(self._out_map, ["object_class"])
+        import_objects(self._out_map, [("object_class", "object1")])
+        import_objects(self._out_map, [("object_class", "object2")])
+        import_object_parameters(self._out_map, [("object_class", "parameter1")])
+        import_object_parameters(self._out_map, [("object_class", "parameter2")])
+        import_object_parameter_values(self._out_map, [("object_class", "object1", "parameter1", -1.0)])
+        import_object_parameter_values(self._out_map, [("object_class", "object1", "parameter1", 10.0, "alternative")])
+        import_object_parameter_values(self._out_map, [("object_class", "object1", "parameter2", -1.0)])
+        import_object_parameter_values(self._out_map, [("object_class", "object1", "parameter2", 11.0, "alternative")])
+        import_object_parameter_values(self._out_map, [("object_class", "object2", "parameter1", -2.0)])
+        import_object_parameter_values(self._out_map, [("object_class", "object2", "parameter1", 20.0, "alternative")])
+        import_object_parameter_values(self._out_map, [("object_class", "object2", "parameter2", -2.0)])
+        import_object_parameter_values(self._out_map, [("object_class", "object2", "parameter2", 22.0, "alternative")])
+        import_scenarios(self._out_map, [("scenario", True)])
+        import_scenario_alternatives(self._out_map, [("scenario", "alternative")])
+        self._out_map.commit_session("Add test data")
+        for db_map in [self._db_map, self._diff_db_map]:
+            apply_scenario_filter_to_subqueries(db_map, "scenario")
+            parameters = db_map.query(db_map.parameter_value_sq).all()
+            self.assertEqual(len(parameters), 4)
+            object_names = {o.id: o.name for o in db_map.query(db_map.object_sq).all()}
+            alternative_names = {a.id: a.name for a in db_map.query(db_map.alternative_sq).all()}
+            parameter_names = {d.id: d.name for d in db_map.query(db_map.parameter_definition_sq).all()}
+            datamined_values = dict()
+            for parameter in parameters:
+                self.assertEqual(alternative_names[parameter.alternative_id], "alternative")
+                parameter_values = datamined_values.setdefault(object_names[parameter.object_id], dict())
+                parameter_values[parameter_names[parameter.parameter_definition_id]] = parameter.value
+            self.assertEqual(
+                datamined_values,
+                {
+                    "object1": {"parameter1": b"10.0", "parameter2": b"11.0"},
+                    "object2": {"parameter1": b"20.0", "parameter2": b"22.0"},
+                },
+            )
+            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
+            self.assertEqual(alternatives, [{"name": "alternative", "description": None, "id": 2, "commit_id": 2}])
+            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
+            self.assertEqual(
+                scenarios,
+                [
+                    {
+                        "name": "scenario",
+                        "description": None,
+                        "active": True,
+                        "alternative_name_list": "alternative",
+                        "alternative_id_list": "2",
+                        "id": 1,
+                        "commit_id": 2,
+                    }
+                ],
+            )
+
+    def test_filters_scenarios_and_alternatives(self):
+        import_scenarios(self._out_map, ("scenario1", "scenario2"))
+        import_alternatives(self._out_map, ("alternative1", "alternative2", "alternative3"))
+        import_scenario_alternatives(
+            self._out_map,
+            (
+                ("scenario1", "alternative2"),
+                ("scenario1", "alternative1", "alternative2"),
+                ("scenario2", "alternative3"),
+                ("scenario2", "alternative2", "alternative3"),
+            ),
+        )
+        self._out_map.commit_session("Add test data.")
+        for db_map in (self._db_map, self._diff_db_map):
+            apply_scenario_filter_to_subqueries(db_map, "scenario2")
+            alternatives = [a._asdict() for a in db_map.query(db_map.alternative_sq)]
+            self.assertEqual(
+                alternatives,
+                [
+                    {"name": "alternative2", "description": None, "id": 3, "commit_id": 2},
+                    {"name": "alternative3", "description": None, "id": 4, "commit_id": 2},
+                ],
+            )
+            scenarios = [s._asdict() for s in db_map.query(db_map.wide_scenario_sq).all()]
+            self.assertEqual(
+                scenarios,
+                [
+                    {
+                        "name": "scenario2",
+                        "description": None,
+                        "active": False,
+                        "alternative_name_list": "alternative2,alternative3",
+                        "alternative_id_list": "3,4",
+                        "id": 2,
+                        "commit_id": 2,
+                    }
+                ],
+            )
+
+
+class TestScenarioFilterUtils(unittest.TestCase):
+    def test_scenario_filter_config(self):
+        config = scenario_filter_config("scenario name")
+        self.assertEqual(config, {"type": "scenario_filter", "scenario": "scenario name"})
+
+    def test_scenario_filter_from_dict(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        _build_data_with_single_scenario(db_map)
+        config = scenario_filter_config("scenario")
+        scenario_filter_from_dict(db_map, config)
+        parameters = db_map.query(db_map.parameter_value_sq).all()
+        self.assertEqual(len(parameters), 1)
+        self.assertEqual(parameters[0].value, b"23.0")
+
+    def test_scenario_name_from_dict(self):
+        config = scenario_filter_config("scenario name")
+        self.assertEqual(scenario_name_from_dict(config), "scenario name")
+
+    def test_scenario_filter_config_to_shorthand(self):
+        config = scenario_filter_config("scenario name")
+        shorthand = scenario_filter_config_to_shorthand(config)
+        self.assertEqual(shorthand, "scenario:scenario name")
+
+    def test_scenario_filter_shorthand_to_config(self):
+        config = scenario_filter_shorthand_to_config("scenario:scenario name")
+        self.assertEqual(config, {"type": "scenario_filter", "scenario": "scenario name"})
+
+
+def _build_data_with_single_scenario(db_map, commit=True):
+    import_alternatives(db_map, ["alternative"])
+    import_object_classes(db_map, ["object_class"])
+    import_objects(db_map, [("object_class", "object")])
+    import_object_parameters(db_map, [("object_class", "parameter")])
+    import_object_parameter_values(db_map, [("object_class", "object", "parameter", -1.0)])
+    import_object_parameter_values(db_map, [("object_class", "object", "parameter", 23.0, "alternative")])
+    import_scenarios(db_map, [("scenario", True)])
+    import_scenario_alternatives(db_map, [("scenario", "alternative")])
+    if commit:
+        db_map.commit_session("Add test data.")
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/filters/test_tool_filter.py` & `spinedb_api-0.30.4/tests/filters/test_tool_filter.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,230 +1,230 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for ``tool_entity_filter`` module.
-
-"""
-from pathlib import Path
-from tempfile import TemporaryDirectory
-import unittest
-from sqlalchemy.engine.url import URL
-from spinedb_api import (
-    apply_tool_filter_to_entity_sq,
-    create_new_spine_database,
-    DiffDatabaseMapping,
-    import_object_classes,
-    import_relationship_classes,
-    import_object_parameter_values,
-    import_object_parameters,
-    import_objects,
-    import_relationships,
-    import_relationship_parameter_values,
-    import_relationship_parameters,
-    import_parameter_value_lists,
-    import_tools,
-    import_features,
-    import_tool_features,
-    import_tool_feature_methods,
-    SpineDBAPIError,
-)
-from spinedb_api.filters.tool_filter import (
-    tool_filter_config,
-    tool_filter_config_to_shorthand,
-    tool_filter_from_dict,
-    tool_filter_shorthand_to_config,
-)
-
-
-class TestToolEntityFilter(unittest.TestCase):
-    _db_url = None
-    _temp_dir = None
-
-    @classmethod
-    def setUpClass(cls):
-        cls._temp_dir = TemporaryDirectory()
-        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_tool_filter_mapping.sqlite").as_posix())
-
-    def setUp(self):
-        create_new_spine_database(self._db_url)
-        self._db_map = DiffDatabaseMapping(self._db_url)
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def _build_data_with_tools(self):
-        import_object_classes(self._db_map, ["object_class"])
-        import_objects(
-            self._db_map,
-            [
-                ("object_class", "object1"),
-                ("object_class", "object2"),
-                ("object_class", "object3"),
-                ("object_class", "object4"),
-            ],
-        )
-        import_parameter_value_lists(
-            self._db_map, [("methods", "methodA"), ("methods", "methodB"), ("methods", "methodC")]
-        )
-        import_object_parameters(
-            self._db_map,
-            [
-                ("object_class", "parameter1", "methodA", "methods"),
-                ("object_class", "parameter2", "methodC", "methods"),
-            ],
-        )
-        import_object_parameter_values(
-            self._db_map,
-            [
-                ("object_class", "object1", "parameter1", "methodA"),
-                ("object_class", "object2", "parameter1", "methodB"),
-                ("object_class", "object3", "parameter1", "methodC"),
-                ("object_class", "object4", "parameter1", "methodB"),
-                ("object_class", "object2", "parameter2", "methodA"),
-                ("object_class", "object3", "parameter2", "methodC"),
-            ],
-        )
-        import_tools(self._db_map, ["tool1", "tool2"])
-        import_features(self._db_map, [("object_class", "parameter1"), ("object_class", "parameter2")])
-        import_tool_features(
-            self._db_map,
-            [("tool1", "object_class", "parameter1", False), ("tool2", "object_class", "parameter1", False)],
-        )
-
-    def test_non_existing_tool_filter_raises(self):
-        self._build_data_with_tools()
-        self._db_map.commit_session("Add test data")
-        self.assertRaises(SpineDBAPIError, apply_tool_filter_to_entity_sq, self._db_map, "notool")
-
-    def test_tool_feature_no_filter(self):
-        self._build_data_with_tools()
-        self._db_map.commit_session("Add test data")
-        apply_tool_filter_to_entity_sq(self._db_map, "tool1")
-        entities = self._db_map.query(self._db_map.entity_sq).all()
-        self.assertEqual(len(entities), 4)
-        names = [x.name for x in entities]
-        self.assertIn("object1", names)
-        self.assertIn("object2", names)
-        self.assertIn("object3", names)
-        self.assertIn("object4", names)
-
-    def test_tool_feature_required(self):
-        self._build_data_with_tools()
-        import_tool_features(self._db_map, [("tool1", "object_class", "parameter2", True)])
-        self._db_map.commit_session("Add test data")
-        apply_tool_filter_to_entity_sq(self._db_map, "tool1")
-        entities = self._db_map.query(self._db_map.entity_sq).all()
-        self.assertEqual(len(entities), 2)
-        names = [x.name for x in entities]
-        self.assertIn("object2", names)
-        self.assertIn("object3", names)
-
-    def test_tool_feature_method(self):
-        self._build_data_with_tools()
-        import_tool_feature_methods(
-            self._db_map,
-            [("tool1", "object_class", "parameter1", "methodB"), ("tool2", "object_class", "parameter1", "methodC")],
-        )
-        self._db_map.commit_session("Add test data")
-        apply_tool_filter_to_entity_sq(self._db_map, "tool1")
-        entities = self._db_map.query(self._db_map.entity_sq).all()
-        self.assertEqual(len(entities), 2)
-        names = [x.name for x in entities]
-        self.assertIn("object2", names)
-        self.assertIn("object4", names)
-
-    def test_tool_feature_required_and_method(self):
-        self._build_data_with_tools()
-        import_tool_features(self._db_map, [("tool1", "object_class", "parameter2", True)])
-        import_tool_feature_methods(
-            self._db_map,
-            [("tool1", "object_class", "parameter1", "methodB"), ("tool2", "object_class", "parameter1", "methodC")],
-        )
-        self._db_map.commit_session("Add test data")
-        apply_tool_filter_to_entity_sq(self._db_map, "tool1")
-        entities = self._db_map.query(self._db_map.entity_sq).all()
-        self.assertEqual(len(entities), 1)
-        self.assertEqual(entities[0].name, "object2")
-
-    def test_tool_filter_config(self):
-        config = tool_filter_config("tool name")
-        self.assertEqual(config, {"type": "tool_filter", "tool": "tool name"})
-
-    def test_tool_filter_from_dict(self):
-        self._build_data_with_tools()
-        import_tool_features(self._db_map, [("tool1", "object_class", "parameter2", True)])
-        self._db_map.commit_session("Add test data")
-        config = tool_filter_config("tool1")
-        tool_filter_from_dict(self._db_map, config)
-        entities = self._db_map.query(self._db_map.entity_sq).all()
-        self.assertEqual(len(entities), 2)
-        names = [x.name for x in entities]
-        self.assertIn("object2", names)
-        self.assertIn("object3", names)
-
-    def test_tool_filter_config_to_shorthand(self):
-        config = tool_filter_config("tool name")
-        shorthand = tool_filter_config_to_shorthand(config)
-        self.assertEqual(shorthand, "tool:tool name")
-
-    def test_tool_filter_shorthand_to_config(self):
-        config = tool_filter_shorthand_to_config("tool:tool name")
-        self.assertEqual(config, {"type": "tool_filter", "tool": "tool name"})
-
-    def test_object_activity_control_filter(self):
-        import_object_classes(self._db_map, ["node", "unit"])
-        import_relationship_classes(self._db_map, [["node__unit", ["node", "unit"]]])
-        import_objects(self._db_map, [("node", "node1"), ("node", "node2"), ("unit", "unita"), ("unit", "unitb")])
-        import_relationships(
-            self._db_map,
-            [
-                ["node__unit", ["node1", "unita"]],
-                ["node__unit", ["node1", "unitb"]],
-                ["node__unit", ["node2", "unita"]],
-            ],
-        )
-        import_parameter_value_lists(self._db_map, [("boolean", True), ("boolean", False)])
-        import_object_parameters(self._db_map, [("node", "is_active", True, "boolean")])
-        import_relationship_parameters(self._db_map, [("node__unit", "x")])
-        import_object_parameter_values(self._db_map, [("node", "node1", "is_active", False)])
-        import_relationship_parameter_values(
-            self._db_map,
-            [
-                ["node__unit", ["node1", "unita"], "x", 5],
-                ["node__unit", ["node1", "unitb"], "x", 7],
-                ["node__unit", ["node2", "unita"], "x", 11],
-            ],
-        )
-        import_tools(self._db_map, ["obj_act_ctrl"])
-        import_features(self._db_map, [("node", "is_active")])
-        import_tool_features(self._db_map, [("obj_act_ctrl", "node", "is_active", False)])
-        import_tool_feature_methods(self._db_map, [("obj_act_ctrl", "node", "is_active", True)])
-        self._db_map.commit_session("Add obj act ctrl filter")
-        apply_tool_filter_to_entity_sq(self._db_map, "obj_act_ctrl")
-        objects = self._db_map.query(self._db_map.object_sq).all()
-        self.assertEqual(len(objects), 3)
-        object_names = [x.name for x in objects]
-        self.assertTrue("node1" not in object_names)
-        self.assertTrue("node2" in object_names)
-        self.assertTrue("unita" in object_names)
-        self.assertTrue("unitb" in object_names)
-        relationships = self._db_map.query(self._db_map.wide_relationship_sq).all()
-        self.assertEqual(len(relationships), 1)
-        relationship_object_names = relationships[0].object_name_list.split(",")
-        self.assertTrue("node1" not in relationship_object_names)
-        ent_pvals = self._db_map.query(self._db_map.entity_parameter_value_sq).all()
-        self.assertEqual(len(ent_pvals), 1)
-        pval_object_names = ent_pvals[0].object_name_list.split(",")
-        self.assertTrue("node1" not in pval_object_names)
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for ``tool_entity_filter`` module.
+
+"""
+from pathlib import Path
+from tempfile import TemporaryDirectory
+import unittest
+from sqlalchemy.engine.url import URL
+from spinedb_api import (
+    apply_tool_filter_to_entity_sq,
+    create_new_spine_database,
+    DiffDatabaseMapping,
+    import_object_classes,
+    import_relationship_classes,
+    import_object_parameter_values,
+    import_object_parameters,
+    import_objects,
+    import_relationships,
+    import_relationship_parameter_values,
+    import_relationship_parameters,
+    import_parameter_value_lists,
+    import_tools,
+    import_features,
+    import_tool_features,
+    import_tool_feature_methods,
+    SpineDBAPIError,
+)
+from spinedb_api.filters.tool_filter import (
+    tool_filter_config,
+    tool_filter_config_to_shorthand,
+    tool_filter_from_dict,
+    tool_filter_shorthand_to_config,
+)
+
+
+class TestToolEntityFilter(unittest.TestCase):
+    _db_url = None
+    _temp_dir = None
+
+    @classmethod
+    def setUpClass(cls):
+        cls._temp_dir = TemporaryDirectory()
+        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_tool_filter_mapping.sqlite").as_posix())
+
+    def setUp(self):
+        create_new_spine_database(self._db_url)
+        self._db_map = DiffDatabaseMapping(self._db_url)
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def _build_data_with_tools(self):
+        import_object_classes(self._db_map, ["object_class"])
+        import_objects(
+            self._db_map,
+            [
+                ("object_class", "object1"),
+                ("object_class", "object2"),
+                ("object_class", "object3"),
+                ("object_class", "object4"),
+            ],
+        )
+        import_parameter_value_lists(
+            self._db_map, [("methods", "methodA"), ("methods", "methodB"), ("methods", "methodC")]
+        )
+        import_object_parameters(
+            self._db_map,
+            [
+                ("object_class", "parameter1", "methodA", "methods"),
+                ("object_class", "parameter2", "methodC", "methods"),
+            ],
+        )
+        import_object_parameter_values(
+            self._db_map,
+            [
+                ("object_class", "object1", "parameter1", "methodA"),
+                ("object_class", "object2", "parameter1", "methodB"),
+                ("object_class", "object3", "parameter1", "methodC"),
+                ("object_class", "object4", "parameter1", "methodB"),
+                ("object_class", "object2", "parameter2", "methodA"),
+                ("object_class", "object3", "parameter2", "methodC"),
+            ],
+        )
+        import_tools(self._db_map, ["tool1", "tool2"])
+        import_features(self._db_map, [("object_class", "parameter1"), ("object_class", "parameter2")])
+        import_tool_features(
+            self._db_map,
+            [("tool1", "object_class", "parameter1", False), ("tool2", "object_class", "parameter1", False)],
+        )
+
+    def test_non_existing_tool_filter_raises(self):
+        self._build_data_with_tools()
+        self._db_map.commit_session("Add test data")
+        self.assertRaises(SpineDBAPIError, apply_tool_filter_to_entity_sq, self._db_map, "notool")
+
+    def test_tool_feature_no_filter(self):
+        self._build_data_with_tools()
+        self._db_map.commit_session("Add test data")
+        apply_tool_filter_to_entity_sq(self._db_map, "tool1")
+        entities = self._db_map.query(self._db_map.entity_sq).all()
+        self.assertEqual(len(entities), 4)
+        names = [x.name for x in entities]
+        self.assertIn("object1", names)
+        self.assertIn("object2", names)
+        self.assertIn("object3", names)
+        self.assertIn("object4", names)
+
+    def test_tool_feature_required(self):
+        self._build_data_with_tools()
+        import_tool_features(self._db_map, [("tool1", "object_class", "parameter2", True)])
+        self._db_map.commit_session("Add test data")
+        apply_tool_filter_to_entity_sq(self._db_map, "tool1")
+        entities = self._db_map.query(self._db_map.entity_sq).all()
+        self.assertEqual(len(entities), 2)
+        names = [x.name for x in entities]
+        self.assertIn("object2", names)
+        self.assertIn("object3", names)
+
+    def test_tool_feature_method(self):
+        self._build_data_with_tools()
+        import_tool_feature_methods(
+            self._db_map,
+            [("tool1", "object_class", "parameter1", "methodB"), ("tool2", "object_class", "parameter1", "methodC")],
+        )
+        self._db_map.commit_session("Add test data")
+        apply_tool_filter_to_entity_sq(self._db_map, "tool1")
+        entities = self._db_map.query(self._db_map.entity_sq).all()
+        self.assertEqual(len(entities), 2)
+        names = [x.name for x in entities]
+        self.assertIn("object2", names)
+        self.assertIn("object4", names)
+
+    def test_tool_feature_required_and_method(self):
+        self._build_data_with_tools()
+        import_tool_features(self._db_map, [("tool1", "object_class", "parameter2", True)])
+        import_tool_feature_methods(
+            self._db_map,
+            [("tool1", "object_class", "parameter1", "methodB"), ("tool2", "object_class", "parameter1", "methodC")],
+        )
+        self._db_map.commit_session("Add test data")
+        apply_tool_filter_to_entity_sq(self._db_map, "tool1")
+        entities = self._db_map.query(self._db_map.entity_sq).all()
+        self.assertEqual(len(entities), 1)
+        self.assertEqual(entities[0].name, "object2")
+
+    def test_tool_filter_config(self):
+        config = tool_filter_config("tool name")
+        self.assertEqual(config, {"type": "tool_filter", "tool": "tool name"})
+
+    def test_tool_filter_from_dict(self):
+        self._build_data_with_tools()
+        import_tool_features(self._db_map, [("tool1", "object_class", "parameter2", True)])
+        self._db_map.commit_session("Add test data")
+        config = tool_filter_config("tool1")
+        tool_filter_from_dict(self._db_map, config)
+        entities = self._db_map.query(self._db_map.entity_sq).all()
+        self.assertEqual(len(entities), 2)
+        names = [x.name for x in entities]
+        self.assertIn("object2", names)
+        self.assertIn("object3", names)
+
+    def test_tool_filter_config_to_shorthand(self):
+        config = tool_filter_config("tool name")
+        shorthand = tool_filter_config_to_shorthand(config)
+        self.assertEqual(shorthand, "tool:tool name")
+
+    def test_tool_filter_shorthand_to_config(self):
+        config = tool_filter_shorthand_to_config("tool:tool name")
+        self.assertEqual(config, {"type": "tool_filter", "tool": "tool name"})
+
+    def test_object_activity_control_filter(self):
+        import_object_classes(self._db_map, ["node", "unit"])
+        import_relationship_classes(self._db_map, [["node__unit", ["node", "unit"]]])
+        import_objects(self._db_map, [("node", "node1"), ("node", "node2"), ("unit", "unita"), ("unit", "unitb")])
+        import_relationships(
+            self._db_map,
+            [
+                ["node__unit", ["node1", "unita"]],
+                ["node__unit", ["node1", "unitb"]],
+                ["node__unit", ["node2", "unita"]],
+            ],
+        )
+        import_parameter_value_lists(self._db_map, [("boolean", True), ("boolean", False)])
+        import_object_parameters(self._db_map, [("node", "is_active", True, "boolean")])
+        import_relationship_parameters(self._db_map, [("node__unit", "x")])
+        import_object_parameter_values(self._db_map, [("node", "node1", "is_active", False)])
+        import_relationship_parameter_values(
+            self._db_map,
+            [
+                ["node__unit", ["node1", "unita"], "x", 5],
+                ["node__unit", ["node1", "unitb"], "x", 7],
+                ["node__unit", ["node2", "unita"], "x", 11],
+            ],
+        )
+        import_tools(self._db_map, ["obj_act_ctrl"])
+        import_features(self._db_map, [("node", "is_active")])
+        import_tool_features(self._db_map, [("obj_act_ctrl", "node", "is_active", False)])
+        import_tool_feature_methods(self._db_map, [("obj_act_ctrl", "node", "is_active", True)])
+        self._db_map.commit_session("Add obj act ctrl filter")
+        apply_tool_filter_to_entity_sq(self._db_map, "obj_act_ctrl")
+        objects = self._db_map.query(self._db_map.object_sq).all()
+        self.assertEqual(len(objects), 3)
+        object_names = [x.name for x in objects]
+        self.assertTrue("node1" not in object_names)
+        self.assertTrue("node2" in object_names)
+        self.assertTrue("unita" in object_names)
+        self.assertTrue("unitb" in object_names)
+        relationships = self._db_map.query(self._db_map.wide_relationship_sq).all()
+        self.assertEqual(len(relationships), 1)
+        relationship_object_names = relationships[0].object_name_list.split(",")
+        self.assertTrue("node1" not in relationship_object_names)
+        ent_pvals = self._db_map.query(self._db_map.entity_parameter_value_sq).all()
+        self.assertEqual(len(ent_pvals), 1)
+        pval_object_names = ent_pvals[0].object_name_list.split(",")
+        self.assertTrue("node1" not in pval_object_names)
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/filters/test_tools.py` & `spinedb_api-0.30.4/tests/filters/test_tools.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,318 +1,318 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Unit tests for the ``filters.tools`` module.
-
-"""
-import os.path
-from tempfile import TemporaryDirectory
-import unittest
-from sqlalchemy.engine.url import URL
-from spinedb_api import (
-    append_filter_config,
-    clear_filter_configs,
-    DatabaseMapping,
-    DiffDatabaseMapping,
-    export_object_classes,
-    import_object_classes,
-    pop_filter_configs,
-)
-from spinedb_api.filters.tools import (
-    apply_filter_stack,
-    ensure_filtering,
-    filter_configs,
-    filter_config,
-    load_filters,
-    name_from_dict,
-    store_filter,
-)
-from spinedb_api.filters.alternative_filter import alternative_filter_config, alternative_names_from_dict
-from spinedb_api.filters.renamer import entity_class_renamer_config
-from spinedb_api.filters.scenario_filter import scenario_filter_config, scenario_name_from_dict
-
-
-class TestLoadFilters(unittest.TestCase):
-    _dir = None
-
-    @classmethod
-    def setUpClass(cls):
-        cls._dir = TemporaryDirectory()
-
-    def test_no_config_files(self):
-        stack = load_filters([])
-        self.assertEqual(stack, [])
-
-    def test_single_config(self):
-        path = os.path.join(self._dir.name, "config.json")
-        with open(path, "w") as out_file:
-            store_filter({}, out_file)
-        stack = load_filters([path])
-        self.assertEqual(stack, [{}])
-
-    def test_config_ordering(self):
-        path1 = os.path.join(self._dir.name, "config1.json")
-        with open(path1, "w") as out_file:
-            store_filter({"first": 1}, out_file)
-        path2 = os.path.join(self._dir.name, "config2.json")
-        with open(path2, "w") as out_file:
-            store_filter({"second": 2}, out_file)
-        stack = load_filters([path1, path2])
-        self.assertEqual(stack, [{"first": 1}, {"second": 2}])
-
-    def test_config_dict_passes_through(self):
-        filters = [entity_class_renamer_config(object_class="renamed")]
-        stack = load_filters(filters)
-        self.assertEqual(stack, [entity_class_renamer_config(object_class="renamed")])
-
-    def test_mixture_of_files_and_shorthands(self):
-        path1 = os.path.join(self._dir.name, "config1.json")
-        with open(path1, "w") as out_file:
-            store_filter({"first": 1}, out_file)
-        path2 = os.path.join(self._dir.name, "config2.json")
-        with open(path2, "w") as out_file:
-            store_filter({"second": 2}, out_file)
-        stack = load_filters([path1, {"middle": -2}, path2])
-        self.assertEqual(stack, [{"first": 1}, {"middle": -2}, {"second": 2}])
-
-
-class TestApplyFilterStack(unittest.TestCase):
-    _db_url = URL("sqlite")
-    _dir = None
-
-    @classmethod
-    def setUpClass(cls):
-        cls._dir = TemporaryDirectory()
-        cls._db_url.database = os.path.join(cls._dir.name, ".json")
-        db_map = DiffDatabaseMapping(cls._db_url, create=True)
-        import_object_classes(db_map, ("object_class",))
-        db_map.commit_session("Add test data.")
-        db_map.connection.close()
-
-    def test_empty_stack(self):
-        db_map = DatabaseMapping(self._db_url)
-        try:
-            apply_filter_stack(db_map, [])
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("object_class", None, None)])
-        finally:
-            db_map.connection.close()
-
-    def test_single_renaming_filter(self):
-        db_map = DatabaseMapping(self._db_url)
-        try:
-            stack = [entity_class_renamer_config(object_class="renamed_once")]
-            apply_filter_stack(db_map, stack)
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("renamed_once", None, None)])
-        finally:
-            db_map.connection.close()
-
-    def test_two_renaming_filters(self):
-        db_map = DatabaseMapping(self._db_url)
-        try:
-            stack = [
-                entity_class_renamer_config(object_class="renamed_once"),
-                entity_class_renamer_config(renamed_once="renamed_twice"),
-            ]
-            apply_filter_stack(db_map, stack)
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("renamed_twice", None, None)])
-        finally:
-            db_map.connection.close()
-
-
-class TestFilteredDatabaseMap(unittest.TestCase):
-    _db_url = URL("sqlite")
-    _dir = None
-    _engine = None
-
-    @classmethod
-    def setUpClass(cls):
-        cls._dir = TemporaryDirectory()
-        cls._db_url.database = os.path.join(cls._dir.name, "TestFilteredDatabaseMap.json")
-        db_map = DiffDatabaseMapping(cls._db_url, create=True)
-        import_object_classes(db_map, ("object_class",))
-        db_map.commit_session("Add test data.")
-        db_map.connection.close()
-
-    def test_without_filters(self):
-        db_map = DatabaseMapping(self._db_url, self._engine)
-        try:
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("object_class", None, None)])
-        finally:
-            db_map.connection.close()
-
-    def test_single_renaming_filter(self):
-        path = os.path.join(self._dir.name, "config.json")
-        with open(path, "w") as out_file:
-            store_filter(entity_class_renamer_config(object_class="renamed_once"), out_file)
-        url = append_filter_config(str(self._db_url), path)
-        db_map = DatabaseMapping(url, self._engine)
-        try:
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("renamed_once", None, None)])
-        finally:
-            db_map.connection.close()
-
-    def test_two_renaming_filters(self):
-        path1 = os.path.join(self._dir.name, "config1.json")
-        with open(path1, "w") as out_file:
-            store_filter(entity_class_renamer_config(object_class="renamed_once"), out_file)
-        url = append_filter_config(str(self._db_url), path1)
-        path2 = os.path.join(self._dir.name, "config2.json")
-        with open(path2, "w") as out_file:
-            store_filter(entity_class_renamer_config(renamed_once="renamed_twice"), out_file)
-        url = append_filter_config(url, path2)
-        db_map = DatabaseMapping(url, self._engine)
-        try:
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("renamed_twice", None, None)])
-        finally:
-            db_map.connection.close()
-
-    def test_config_embedded_to_url(self):
-        config = entity_class_renamer_config(object_class="renamed_once")
-        url = append_filter_config(str(self._db_url), config)
-        db_map = DatabaseMapping(url, self._engine)
-        try:
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("renamed_once", None, None)])
-        finally:
-            db_map.connection.close()
-
-
-class TestAppendFilterConfig(unittest.TestCase):
-    def test_append_to_simple_url(self):
-        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
-        self.assertEqual(url, r"sqlite:///C:\dbs\database.sqlite?spinedbfilter=F%3A%5Cfltr%5Ca.json")
-
-    def test_append_to_existing_filters(self):
-        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
-        url = append_filter_config(url, r"F:\fltr\b.json")
-        self.assertEqual(
-            url,
-            r"sqlite:///C:\dbs\database.sqlite?spinedbfilter=F%3A%5Cfltr%5Ca.json&spinedbfilter=F%3A%5Cfltr%5Cb.json",
-        )
-
-    def test_append_to_remote_database_url(self):
-        url = append_filter_config(r"mysql+pymysql://username:password@remote.fi/database_name", r"F:\fltr\a.json")
-        self.assertEqual(
-            url, r"mysql+pymysql://username:password@remote.fi/database_name?spinedbfilter=F%3A%5Cfltr%5Ca.json"
-        )
-
-
-class TestFilterConfigs(unittest.TestCase):
-    def test_empty_query(self):
-        url = r"sqlite:///C:\dbs\database.sqlite"
-        filters = filter_configs(url)
-        self.assertEqual(filters, list())
-
-    def test_single_query(self):
-        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
-        filters = filter_configs(url)
-        self.assertEqual(filters, [r"F:\fltr\a.json"])
-
-    def test_filter_list_is_ordered(self):
-        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
-        url = append_filter_config(url, r"F:\fltr\b.json")
-        filters = filter_configs(url)
-        self.assertEqual(filters, [r"F:\fltr\a.json", r"F:\fltr\b.json"])
-
-
-class TestPopFilterConfigs(unittest.TestCase):
-    def test_pop_from_empty_query(self):
-        url = r"sqlite:///C:\dbs\database.sqlite"
-        filters, popped = pop_filter_configs(url)
-        self.assertEqual(filters, list())
-        self.assertEqual(popped, url)
-
-    def test_pop_single_query(self):
-        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
-        filters, popped = pop_filter_configs(url)
-        self.assertEqual(filters, [r"F:\fltr\a.json"])
-        self.assertEqual(popped, r"sqlite:///C:\dbs\database.sqlite")
-
-    def test_filter_list_is_ordered(self):
-        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
-        url = append_filter_config(url, r"F:\fltr\b.json")
-        filters, popped = pop_filter_configs(url)
-        self.assertEqual(filters, [r"F:\fltr\a.json", r"F:\fltr\b.json"])
-        self.assertEqual(popped, r"sqlite:///C:\dbs\database.sqlite")
-
-    def test_pop_from_remote_url(self):
-        url = append_filter_config(r"mysql+pymysql://username:password@remote.fi/database_name", r"F:\fltr\a.json")
-        filters, popped = pop_filter_configs(url)
-        self.assertEqual(filters, [r"F:\fltr\a.json"])
-        self.assertEqual(popped, r"mysql+pymysql://username:password@remote.fi/database_name")
-
-
-class TestClearFilterConfigs(unittest.TestCase):
-    def test_without_queries(self):
-        url = r"sqlite:///C:\dbs\database.sqlite"
-        cleared = clear_filter_configs(url)
-        self.assertEqual(cleared, url)
-
-    def test_pop_single_query(self):
-        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
-        cleared = clear_filter_configs(url)
-        self.assertEqual(cleared, r"sqlite:///C:\dbs\database.sqlite")
-
-
-class TestEnsureFiltering(unittest.TestCase):
-    def test_ensure_filtering_adds_fallback_alternative(self):
-        filtered = ensure_filtering("sqlite:///home/unittest/db.sqlite", fallback_alternative="fallback")
-        config = filter_configs(filtered)
-        self.assertEqual(len(config), 1)
-        self.assertEqual(alternative_names_from_dict(config[0]), ["fallback"])
-
-    def test_ensure_filtering_returns_original_url_if_alternative_exists(self):
-        url = append_filter_config("sqlite:///home/unittest/db.sqlite", alternative_filter_config(["alternative"]))
-        filtered = ensure_filtering(url, fallback_alternative="fallback")
-        config = filter_configs(filtered)
-        self.assertEqual(len(config), 1)
-        self.assertEqual(alternative_names_from_dict(config[0]), ["alternative"])
-
-    def test_ensure_filtering_returns_original_url_if_scenario_exists(self):
-        url = append_filter_config("sqlite:///home/unittest/db.sqlite", scenario_filter_config("scenario"))
-        filtered = ensure_filtering(url, fallback_alternative="fallback")
-        config = filter_configs(filtered)
-        self.assertEqual(len(config), 1)
-        self.assertEqual(scenario_name_from_dict(config[0]), "scenario")
-
-    def test_works_with_configs_stored_on_disk(self):
-        with TemporaryDirectory() as temp_dir:
-            path = os.path.join(temp_dir, "filter.json")
-            with open(path, "w") as out:
-                store_filter(scenario_filter_config("scenario"), out)
-            url = append_filter_config("sqlite:///home/unittest/db.sqlite", path)
-            filtered = ensure_filtering(url, fallback_alternative="fallback")
-            config = load_filters(filter_configs(filtered))
-            self.assertEqual(len(config), 1)
-            self.assertEqual(scenario_name_from_dict(config[0]), "scenario")
-
-
-class TestNameFromDict(unittest.TestCase):
-    def test_get_scenario_name(self):
-        config = filter_config("scenario_filter", "scenario_name")
-        self.assertEqual(name_from_dict(config), "scenario_name")
-
-    def test_get_tool_name(self):
-        config = filter_config("tool_filter", "tool_name")
-        self.assertEqual(name_from_dict(config), "tool_name")
-
-    def test_returns_none_if_name_not_found(self):
-        config = entity_class_renamer_config(name="rename")
-        self.assertIsNone(name_from_dict(config))
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Unit tests for the ``filters.tools`` module.
+
+"""
+import os.path
+from tempfile import TemporaryDirectory
+import unittest
+from sqlalchemy.engine.url import URL
+from spinedb_api import (
+    append_filter_config,
+    clear_filter_configs,
+    DatabaseMapping,
+    DiffDatabaseMapping,
+    export_object_classes,
+    import_object_classes,
+    pop_filter_configs,
+)
+from spinedb_api.filters.tools import (
+    apply_filter_stack,
+    ensure_filtering,
+    filter_configs,
+    filter_config,
+    load_filters,
+    name_from_dict,
+    store_filter,
+)
+from spinedb_api.filters.alternative_filter import alternative_filter_config, alternative_names_from_dict
+from spinedb_api.filters.renamer import entity_class_renamer_config
+from spinedb_api.filters.scenario_filter import scenario_filter_config, scenario_name_from_dict
+
+
+class TestLoadFilters(unittest.TestCase):
+    _dir = None
+
+    @classmethod
+    def setUpClass(cls):
+        cls._dir = TemporaryDirectory()
+
+    def test_no_config_files(self):
+        stack = load_filters([])
+        self.assertEqual(stack, [])
+
+    def test_single_config(self):
+        path = os.path.join(self._dir.name, "config.json")
+        with open(path, "w") as out_file:
+            store_filter({}, out_file)
+        stack = load_filters([path])
+        self.assertEqual(stack, [{}])
+
+    def test_config_ordering(self):
+        path1 = os.path.join(self._dir.name, "config1.json")
+        with open(path1, "w") as out_file:
+            store_filter({"first": 1}, out_file)
+        path2 = os.path.join(self._dir.name, "config2.json")
+        with open(path2, "w") as out_file:
+            store_filter({"second": 2}, out_file)
+        stack = load_filters([path1, path2])
+        self.assertEqual(stack, [{"first": 1}, {"second": 2}])
+
+    def test_config_dict_passes_through(self):
+        filters = [entity_class_renamer_config(object_class="renamed")]
+        stack = load_filters(filters)
+        self.assertEqual(stack, [entity_class_renamer_config(object_class="renamed")])
+
+    def test_mixture_of_files_and_shorthands(self):
+        path1 = os.path.join(self._dir.name, "config1.json")
+        with open(path1, "w") as out_file:
+            store_filter({"first": 1}, out_file)
+        path2 = os.path.join(self._dir.name, "config2.json")
+        with open(path2, "w") as out_file:
+            store_filter({"second": 2}, out_file)
+        stack = load_filters([path1, {"middle": -2}, path2])
+        self.assertEqual(stack, [{"first": 1}, {"middle": -2}, {"second": 2}])
+
+
+class TestApplyFilterStack(unittest.TestCase):
+    _db_url = URL("sqlite")
+    _dir = None
+
+    @classmethod
+    def setUpClass(cls):
+        cls._dir = TemporaryDirectory()
+        cls._db_url.database = os.path.join(cls._dir.name, ".json")
+        db_map = DiffDatabaseMapping(cls._db_url, create=True)
+        import_object_classes(db_map, ("object_class",))
+        db_map.commit_session("Add test data.")
+        db_map.connection.close()
+
+    def test_empty_stack(self):
+        db_map = DatabaseMapping(self._db_url)
+        try:
+            apply_filter_stack(db_map, [])
+            object_classes = export_object_classes(db_map)
+            self.assertEqual(object_classes, [("object_class", None, None)])
+        finally:
+            db_map.connection.close()
+
+    def test_single_renaming_filter(self):
+        db_map = DatabaseMapping(self._db_url)
+        try:
+            stack = [entity_class_renamer_config(object_class="renamed_once")]
+            apply_filter_stack(db_map, stack)
+            object_classes = export_object_classes(db_map)
+            self.assertEqual(object_classes, [("renamed_once", None, None)])
+        finally:
+            db_map.connection.close()
+
+    def test_two_renaming_filters(self):
+        db_map = DatabaseMapping(self._db_url)
+        try:
+            stack = [
+                entity_class_renamer_config(object_class="renamed_once"),
+                entity_class_renamer_config(renamed_once="renamed_twice"),
+            ]
+            apply_filter_stack(db_map, stack)
+            object_classes = export_object_classes(db_map)
+            self.assertEqual(object_classes, [("renamed_twice", None, None)])
+        finally:
+            db_map.connection.close()
+
+
+class TestFilteredDatabaseMap(unittest.TestCase):
+    _db_url = URL("sqlite")
+    _dir = None
+    _engine = None
+
+    @classmethod
+    def setUpClass(cls):
+        cls._dir = TemporaryDirectory()
+        cls._db_url.database = os.path.join(cls._dir.name, "TestFilteredDatabaseMap.json")
+        db_map = DiffDatabaseMapping(cls._db_url, create=True)
+        import_object_classes(db_map, ("object_class",))
+        db_map.commit_session("Add test data.")
+        db_map.connection.close()
+
+    def test_without_filters(self):
+        db_map = DatabaseMapping(self._db_url, self._engine)
+        try:
+            object_classes = export_object_classes(db_map)
+            self.assertEqual(object_classes, [("object_class", None, None)])
+        finally:
+            db_map.connection.close()
+
+    def test_single_renaming_filter(self):
+        path = os.path.join(self._dir.name, "config.json")
+        with open(path, "w") as out_file:
+            store_filter(entity_class_renamer_config(object_class="renamed_once"), out_file)
+        url = append_filter_config(str(self._db_url), path)
+        db_map = DatabaseMapping(url, self._engine)
+        try:
+            object_classes = export_object_classes(db_map)
+            self.assertEqual(object_classes, [("renamed_once", None, None)])
+        finally:
+            db_map.connection.close()
+
+    def test_two_renaming_filters(self):
+        path1 = os.path.join(self._dir.name, "config1.json")
+        with open(path1, "w") as out_file:
+            store_filter(entity_class_renamer_config(object_class="renamed_once"), out_file)
+        url = append_filter_config(str(self._db_url), path1)
+        path2 = os.path.join(self._dir.name, "config2.json")
+        with open(path2, "w") as out_file:
+            store_filter(entity_class_renamer_config(renamed_once="renamed_twice"), out_file)
+        url = append_filter_config(url, path2)
+        db_map = DatabaseMapping(url, self._engine)
+        try:
+            object_classes = export_object_classes(db_map)
+            self.assertEqual(object_classes, [("renamed_twice", None, None)])
+        finally:
+            db_map.connection.close()
+
+    def test_config_embedded_to_url(self):
+        config = entity_class_renamer_config(object_class="renamed_once")
+        url = append_filter_config(str(self._db_url), config)
+        db_map = DatabaseMapping(url, self._engine)
+        try:
+            object_classes = export_object_classes(db_map)
+            self.assertEqual(object_classes, [("renamed_once", None, None)])
+        finally:
+            db_map.connection.close()
+
+
+class TestAppendFilterConfig(unittest.TestCase):
+    def test_append_to_simple_url(self):
+        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
+        self.assertEqual(url, r"sqlite:///C:\dbs\database.sqlite?spinedbfilter=F%3A%5Cfltr%5Ca.json")
+
+    def test_append_to_existing_filters(self):
+        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
+        url = append_filter_config(url, r"F:\fltr\b.json")
+        self.assertEqual(
+            url,
+            r"sqlite:///C:\dbs\database.sqlite?spinedbfilter=F%3A%5Cfltr%5Ca.json&spinedbfilter=F%3A%5Cfltr%5Cb.json",
+        )
+
+    def test_append_to_remote_database_url(self):
+        url = append_filter_config(r"mysql+pymysql://username:password@remote.fi/database_name", r"F:\fltr\a.json")
+        self.assertEqual(
+            url, r"mysql+pymysql://username:password@remote.fi/database_name?spinedbfilter=F%3A%5Cfltr%5Ca.json"
+        )
+
+
+class TestFilterConfigs(unittest.TestCase):
+    def test_empty_query(self):
+        url = r"sqlite:///C:\dbs\database.sqlite"
+        filters = filter_configs(url)
+        self.assertEqual(filters, list())
+
+    def test_single_query(self):
+        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
+        filters = filter_configs(url)
+        self.assertEqual(filters, [r"F:\fltr\a.json"])
+
+    def test_filter_list_is_ordered(self):
+        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
+        url = append_filter_config(url, r"F:\fltr\b.json")
+        filters = filter_configs(url)
+        self.assertEqual(filters, [r"F:\fltr\a.json", r"F:\fltr\b.json"])
+
+
+class TestPopFilterConfigs(unittest.TestCase):
+    def test_pop_from_empty_query(self):
+        url = r"sqlite:///C:\dbs\database.sqlite"
+        filters, popped = pop_filter_configs(url)
+        self.assertEqual(filters, list())
+        self.assertEqual(popped, url)
+
+    def test_pop_single_query(self):
+        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
+        filters, popped = pop_filter_configs(url)
+        self.assertEqual(filters, [r"F:\fltr\a.json"])
+        self.assertEqual(popped, r"sqlite:///C:\dbs\database.sqlite")
+
+    def test_filter_list_is_ordered(self):
+        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
+        url = append_filter_config(url, r"F:\fltr\b.json")
+        filters, popped = pop_filter_configs(url)
+        self.assertEqual(filters, [r"F:\fltr\a.json", r"F:\fltr\b.json"])
+        self.assertEqual(popped, r"sqlite:///C:\dbs\database.sqlite")
+
+    def test_pop_from_remote_url(self):
+        url = append_filter_config(r"mysql+pymysql://username:password@remote.fi/database_name", r"F:\fltr\a.json")
+        filters, popped = pop_filter_configs(url)
+        self.assertEqual(filters, [r"F:\fltr\a.json"])
+        self.assertEqual(popped, r"mysql+pymysql://username:password@remote.fi/database_name")
+
+
+class TestClearFilterConfigs(unittest.TestCase):
+    def test_without_queries(self):
+        url = r"sqlite:///C:\dbs\database.sqlite"
+        cleared = clear_filter_configs(url)
+        self.assertEqual(cleared, url)
+
+    def test_pop_single_query(self):
+        url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
+        cleared = clear_filter_configs(url)
+        self.assertEqual(cleared, r"sqlite:///C:\dbs\database.sqlite")
+
+
+class TestEnsureFiltering(unittest.TestCase):
+    def test_ensure_filtering_adds_fallback_alternative(self):
+        filtered = ensure_filtering("sqlite:///home/unittest/db.sqlite", fallback_alternative="fallback")
+        config = filter_configs(filtered)
+        self.assertEqual(len(config), 1)
+        self.assertEqual(alternative_names_from_dict(config[0]), ["fallback"])
+
+    def test_ensure_filtering_returns_original_url_if_alternative_exists(self):
+        url = append_filter_config("sqlite:///home/unittest/db.sqlite", alternative_filter_config(["alternative"]))
+        filtered = ensure_filtering(url, fallback_alternative="fallback")
+        config = filter_configs(filtered)
+        self.assertEqual(len(config), 1)
+        self.assertEqual(alternative_names_from_dict(config[0]), ["alternative"])
+
+    def test_ensure_filtering_returns_original_url_if_scenario_exists(self):
+        url = append_filter_config("sqlite:///home/unittest/db.sqlite", scenario_filter_config("scenario"))
+        filtered = ensure_filtering(url, fallback_alternative="fallback")
+        config = filter_configs(filtered)
+        self.assertEqual(len(config), 1)
+        self.assertEqual(scenario_name_from_dict(config[0]), "scenario")
+
+    def test_works_with_configs_stored_on_disk(self):
+        with TemporaryDirectory() as temp_dir:
+            path = os.path.join(temp_dir, "filter.json")
+            with open(path, "w") as out:
+                store_filter(scenario_filter_config("scenario"), out)
+            url = append_filter_config("sqlite:///home/unittest/db.sqlite", path)
+            filtered = ensure_filtering(url, fallback_alternative="fallback")
+            config = load_filters(filter_configs(filtered))
+            self.assertEqual(len(config), 1)
+            self.assertEqual(scenario_name_from_dict(config[0]), "scenario")
+
+
+class TestNameFromDict(unittest.TestCase):
+    def test_get_scenario_name(self):
+        config = filter_config("scenario_filter", "scenario_name")
+        self.assertEqual(name_from_dict(config), "scenario_name")
+
+    def test_get_tool_name(self):
+        config = filter_config("tool_filter", "tool_name")
+        self.assertEqual(name_from_dict(config), "tool_name")
+
+    def test_returns_none_if_name_not_found(self):
+        config = entity_class_renamer_config(name="rename")
+        self.assertIsNone(name_from_dict(config))
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/filters/test_value_transformer.py` & `spinedb_api-0.30.4/tests/filters/test_value_transformer.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,218 +1,218 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for ``value_transformer`` module.
-
-"""
-from pathlib import Path
-import unittest
-from tempfile import TemporaryDirectory
-from sqlalchemy.engine.url import URL
-
-from spinedb_api import (
-    DatabaseMapping,
-    DiffDatabaseMapping,
-    import_object_classes,
-    import_object_parameter_values,
-    import_object_parameters,
-    import_objects,
-    append_filter_config,
-    create_new_spine_database,
-    from_database,
-    Map,
-    TimeSeriesFixedResolution,
-)
-from spinedb_api.filters.value_transformer import (
-    value_transformer_config,
-    value_transformer_config_to_shorthand,
-    value_transformer_shorthand_to_config,
-)
-
-
-class TestValueTransformerFunctions(unittest.TestCase):
-    def test_config(self):
-        instructions = {
-            "class1": {"parameter1": [{"operation": "negate"}], "parameter2": [{"operation": "reciprocal"}]},
-            "class2": {"parameter2": [{"operation": "enchant"}], "parameter3": [{"operation": "integrate"}]},
-        }
-        config = value_transformer_config(instructions)
-        expected = {"type": "value_transformer", "instructions": instructions}
-        self.assertEqual(config, expected)
-
-    def test_config_to_shorthand(self):
-        instructions = {
-            "class1": {"parameter1": [{"operation": "negate"}], "parameter2": [{"operation": "reciprocal"}]},
-            "class2": {"parameter2": [{"operation": "enchant"}], "parameter3": [{"operation": "integrate"}]},
-        }
-        config = value_transformer_config(instructions)
-        shorthand = value_transformer_config_to_shorthand(config)
-        expected = (
-            "value_transform:class1:parameter1:negate:class1:parameter2:reciprocal"
-            + ":class2:parameter2:enchant:class2:parameter3:integrate"
-        )
-        self.assertEqual(shorthand, expected)
-
-    def test_config_to_shorthand_multiple_instructions_for_single_parameter(self):
-        instructions = {"class": {"parameter": [{"operation": "negate"}, {"operation": "reciprocal"}]}}
-        config = value_transformer_config(instructions)
-        shorthand = value_transformer_config_to_shorthand(config)
-        expected = "value_transform:class:parameter:negate:class:parameter:reciprocal"
-        self.assertEqual(shorthand, expected)
-
-    def test_shorthand_to_config(self):
-        shorthand = (
-            "value_transform:class1:parameter1:negate:class1:parameter2:reciprocal"
-            + ":class2:parameter2:enchant:class2:parameter3:integrate"
-        )
-        config = value_transformer_shorthand_to_config(shorthand)
-        expected = {
-            "type": "value_transformer",
-            "instructions": {
-                "class1": {"parameter1": [{"operation": "negate"}], "parameter2": [{"operation": "reciprocal"}]},
-                "class2": {"parameter2": [{"operation": "enchant"}], "parameter3": [{"operation": "integrate"}]},
-            },
-        }
-        self.assertEqual(config, expected)
-
-    def test_shorthand_to_config_with_multiple_instructions_for_single_parameter(self):
-        shorthand = "value_transform:class:parameter:negate:class:parameter:reciprocal"
-        config = value_transformer_shorthand_to_config(shorthand)
-        expected = {
-            "type": "value_transformer",
-            "instructions": {"class": {"parameter": [{"operation": "negate"}, {"operation": "reciprocal"}]}},
-        }
-        self.assertEqual(config, expected)
-
-
-class TestValueTransformerUsingDatabase(unittest.TestCase):
-    _db_url = None
-    _temp_dir = None
-
-    @classmethod
-    def setUpClass(cls):
-        cls._temp_dir = TemporaryDirectory()
-        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_value_transformer.sqlite").as_posix())
-
-    @classmethod
-    def tearDownClass(cls):
-        cls._temp_dir.cleanup()
-
-    def setUp(self):
-        create_new_spine_database(self._db_url)
-        self._out_map = DiffDatabaseMapping(self._db_url)
-
-    def tearDown(self):
-        self._out_map.connection.close()
-
-    def test_negate_manipulator(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
-        self._out_map.commit_session("Add test data.")
-        instructions = {"class": {"parameter": [{"operation": "negate"}]}}
-        config = value_transformer_config(instructions)
-        url = append_filter_config(str(self._db_url), config)
-        db_map = DatabaseMapping(url)
-        try:
-            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
-            self.assertEqual(values, [2.3])
-        finally:
-            db_map.connection.close()
-
-    def test_negate_manipulator_with_nested_map(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
-        value = Map(["A"], [Map(["1"], [2.3])])
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", value),))
-        self._out_map.commit_session("Add test data.")
-        instructions = {"class": {"parameter": [{"operation": "negate"}]}}
-        config = value_transformer_config(instructions)
-        url = append_filter_config(str(self._db_url), config)
-        db_map = DatabaseMapping(url)
-        try:
-            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
-            expected = Map(["A"], [Map(["1"], [-2.3])])
-            self.assertEqual(values, [expected])
-        finally:
-            db_map.connection.close()
-
-    def test_multiply_manipulator(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
-        self._out_map.commit_session("Add test data.")
-        instructions = {"class": {"parameter": [{"operation": "multiply", "rhs": 10.0}]}}
-        config = value_transformer_config(instructions)
-        url = append_filter_config(str(self._db_url), config)
-        db_map = DatabaseMapping(url)
-        try:
-            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
-            self.assertEqual(values, [-23.0])
-        finally:
-            db_map.connection.close()
-
-    def test_invert_manipulator(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
-        self._out_map.commit_session("Add test data.")
-        instructions = {"class": {"parameter": [{"operation": "invert"}]}}
-        config = value_transformer_config(instructions)
-        url = append_filter_config(str(self._db_url), config)
-        db_map = DatabaseMapping(url)
-        try:
-            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
-            self.assertEqual(values, [-1.0 / 2.3])
-        finally:
-            db_map.connection.close()
-
-    def test_multiple_instructions(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
-        self._out_map.commit_session("Add test data.")
-        instructions = {"class": {"parameter": [{"operation": "invert"}, {"operation": "negate"}]}}
-        config = value_transformer_config(instructions)
-        url = append_filter_config(str(self._db_url), config)
-        db_map = DatabaseMapping(url)
-        try:
-            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
-            self.assertEqual(values, [1.0 / 2.3])
-        finally:
-            db_map.connection.close()
-
-    def test_index_generator_on_time_series(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
-        value = TimeSeriesFixedResolution("2021-06-07T08:00", "1D", [-5.0, -2.3], False, False)
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", value),))
-        self._out_map.commit_session("Add test data.")
-        instructions = {"class": {"parameter": [{"operation": "generate_index", "expression": "float(i)"}]}}
-        config = value_transformer_config(instructions)
-        url = append_filter_config(str(self._db_url), config)
-        db_map = DatabaseMapping(url)
-        try:
-            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
-            expected = Map([1.0, 2.0], [-5.0, -2.3])
-            self.assertEqual(values, [expected])
-        finally:
-            db_map.connection.close()
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for ``value_transformer`` module.
+
+"""
+from pathlib import Path
+import unittest
+from tempfile import TemporaryDirectory
+from sqlalchemy.engine.url import URL
+
+from spinedb_api import (
+    DatabaseMapping,
+    DiffDatabaseMapping,
+    import_object_classes,
+    import_object_parameter_values,
+    import_object_parameters,
+    import_objects,
+    append_filter_config,
+    create_new_spine_database,
+    from_database,
+    Map,
+    TimeSeriesFixedResolution,
+)
+from spinedb_api.filters.value_transformer import (
+    value_transformer_config,
+    value_transformer_config_to_shorthand,
+    value_transformer_shorthand_to_config,
+)
+
+
+class TestValueTransformerFunctions(unittest.TestCase):
+    def test_config(self):
+        instructions = {
+            "class1": {"parameter1": [{"operation": "negate"}], "parameter2": [{"operation": "reciprocal"}]},
+            "class2": {"parameter2": [{"operation": "enchant"}], "parameter3": [{"operation": "integrate"}]},
+        }
+        config = value_transformer_config(instructions)
+        expected = {"type": "value_transformer", "instructions": instructions}
+        self.assertEqual(config, expected)
+
+    def test_config_to_shorthand(self):
+        instructions = {
+            "class1": {"parameter1": [{"operation": "negate"}], "parameter2": [{"operation": "reciprocal"}]},
+            "class2": {"parameter2": [{"operation": "enchant"}], "parameter3": [{"operation": "integrate"}]},
+        }
+        config = value_transformer_config(instructions)
+        shorthand = value_transformer_config_to_shorthand(config)
+        expected = (
+            "value_transform:class1:parameter1:negate:class1:parameter2:reciprocal"
+            + ":class2:parameter2:enchant:class2:parameter3:integrate"
+        )
+        self.assertEqual(shorthand, expected)
+
+    def test_config_to_shorthand_multiple_instructions_for_single_parameter(self):
+        instructions = {"class": {"parameter": [{"operation": "negate"}, {"operation": "reciprocal"}]}}
+        config = value_transformer_config(instructions)
+        shorthand = value_transformer_config_to_shorthand(config)
+        expected = "value_transform:class:parameter:negate:class:parameter:reciprocal"
+        self.assertEqual(shorthand, expected)
+
+    def test_shorthand_to_config(self):
+        shorthand = (
+            "value_transform:class1:parameter1:negate:class1:parameter2:reciprocal"
+            + ":class2:parameter2:enchant:class2:parameter3:integrate"
+        )
+        config = value_transformer_shorthand_to_config(shorthand)
+        expected = {
+            "type": "value_transformer",
+            "instructions": {
+                "class1": {"parameter1": [{"operation": "negate"}], "parameter2": [{"operation": "reciprocal"}]},
+                "class2": {"parameter2": [{"operation": "enchant"}], "parameter3": [{"operation": "integrate"}]},
+            },
+        }
+        self.assertEqual(config, expected)
+
+    def test_shorthand_to_config_with_multiple_instructions_for_single_parameter(self):
+        shorthand = "value_transform:class:parameter:negate:class:parameter:reciprocal"
+        config = value_transformer_shorthand_to_config(shorthand)
+        expected = {
+            "type": "value_transformer",
+            "instructions": {"class": {"parameter": [{"operation": "negate"}, {"operation": "reciprocal"}]}},
+        }
+        self.assertEqual(config, expected)
+
+
+class TestValueTransformerUsingDatabase(unittest.TestCase):
+    _db_url = None
+    _temp_dir = None
+
+    @classmethod
+    def setUpClass(cls):
+        cls._temp_dir = TemporaryDirectory()
+        cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_value_transformer.sqlite").as_posix())
+
+    @classmethod
+    def tearDownClass(cls):
+        cls._temp_dir.cleanup()
+
+    def setUp(self):
+        create_new_spine_database(self._db_url)
+        self._out_map = DiffDatabaseMapping(self._db_url)
+
+    def tearDown(self):
+        self._out_map.connection.close()
+
+    def test_negate_manipulator(self):
+        import_object_classes(self._out_map, ("class",))
+        import_object_parameters(self._out_map, (("class", "parameter"),))
+        import_objects(self._out_map, (("class", "object"),))
+        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
+        self._out_map.commit_session("Add test data.")
+        instructions = {"class": {"parameter": [{"operation": "negate"}]}}
+        config = value_transformer_config(instructions)
+        url = append_filter_config(str(self._db_url), config)
+        db_map = DatabaseMapping(url)
+        try:
+            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
+            self.assertEqual(values, [2.3])
+        finally:
+            db_map.connection.close()
+
+    def test_negate_manipulator_with_nested_map(self):
+        import_object_classes(self._out_map, ("class",))
+        import_object_parameters(self._out_map, (("class", "parameter"),))
+        import_objects(self._out_map, (("class", "object"),))
+        value = Map(["A"], [Map(["1"], [2.3])])
+        import_object_parameter_values(self._out_map, (("class", "object", "parameter", value),))
+        self._out_map.commit_session("Add test data.")
+        instructions = {"class": {"parameter": [{"operation": "negate"}]}}
+        config = value_transformer_config(instructions)
+        url = append_filter_config(str(self._db_url), config)
+        db_map = DatabaseMapping(url)
+        try:
+            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
+            expected = Map(["A"], [Map(["1"], [-2.3])])
+            self.assertEqual(values, [expected])
+        finally:
+            db_map.connection.close()
+
+    def test_multiply_manipulator(self):
+        import_object_classes(self._out_map, ("class",))
+        import_object_parameters(self._out_map, (("class", "parameter"),))
+        import_objects(self._out_map, (("class", "object"),))
+        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
+        self._out_map.commit_session("Add test data.")
+        instructions = {"class": {"parameter": [{"operation": "multiply", "rhs": 10.0}]}}
+        config = value_transformer_config(instructions)
+        url = append_filter_config(str(self._db_url), config)
+        db_map = DatabaseMapping(url)
+        try:
+            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
+            self.assertEqual(values, [-23.0])
+        finally:
+            db_map.connection.close()
+
+    def test_invert_manipulator(self):
+        import_object_classes(self._out_map, ("class",))
+        import_object_parameters(self._out_map, (("class", "parameter"),))
+        import_objects(self._out_map, (("class", "object"),))
+        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
+        self._out_map.commit_session("Add test data.")
+        instructions = {"class": {"parameter": [{"operation": "invert"}]}}
+        config = value_transformer_config(instructions)
+        url = append_filter_config(str(self._db_url), config)
+        db_map = DatabaseMapping(url)
+        try:
+            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
+            self.assertEqual(values, [-1.0 / 2.3])
+        finally:
+            db_map.connection.close()
+
+    def test_multiple_instructions(self):
+        import_object_classes(self._out_map, ("class",))
+        import_object_parameters(self._out_map, (("class", "parameter"),))
+        import_objects(self._out_map, (("class", "object"),))
+        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
+        self._out_map.commit_session("Add test data.")
+        instructions = {"class": {"parameter": [{"operation": "invert"}, {"operation": "negate"}]}}
+        config = value_transformer_config(instructions)
+        url = append_filter_config(str(self._db_url), config)
+        db_map = DatabaseMapping(url)
+        try:
+            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
+            self.assertEqual(values, [1.0 / 2.3])
+        finally:
+            db_map.connection.close()
+
+    def test_index_generator_on_time_series(self):
+        import_object_classes(self._out_map, ("class",))
+        import_object_parameters(self._out_map, (("class", "parameter"),))
+        import_objects(self._out_map, (("class", "object"),))
+        value = TimeSeriesFixedResolution("2021-06-07T08:00", "1D", [-5.0, -2.3], False, False)
+        import_object_parameter_values(self._out_map, (("class", "object", "parameter", value),))
+        self._out_map.commit_session("Add test data.")
+        instructions = {"class": {"parameter": [{"operation": "generate_index", "expression": "float(i)"}]}}
+        config = value_transformer_config(instructions)
+        url = append_filter_config(str(self._db_url), config)
+        db_map = DatabaseMapping(url)
+        try:
+            values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
+            expected = Map([1.0, 2.0], [-5.0, -2.3])
+            self.assertEqual(values, [expected])
+        finally:
+            db_map.connection.close()
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/import_mapping/__init__.py` & `spinedb_api-0.30.4/spinedb_api/import_mapping/__init__.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
```

### Comparing `spinedb_api-0.30.3/tests/import_mapping/test_generator.py` & `spinedb_api-0.30.4/tests/import_mapping/test_generator.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,440 +1,440 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains unit tests for the generator module.
-
-"""
-import unittest
-
-from spinedb_api import Array, Map
-from spinedb_api.import_mapping.generator import get_mapped_data
-from spinedb_api.import_mapping.type_conversion import value_to_convert_spec
-
-
-class TestGetMappedData(unittest.TestCase):
-    def test_does_not_give_traceback_when_pivoted_mapping_encounters_empty_data(self):
-        data_source = iter([])
-        mappings = [
-            [
-                {"map_type": "RelationshipClass", "position": "hidden", "value": "unit__sourceNode"},
-                {"map_type": "RelationshipClassObjectClass", "position": "hidden", "value": "unit"},
-                {"map_type": "RelationshipClassObjectClass", "position": "hidden", "value": "node"},
-                {"map_type": "Relationship", "position": "hidden", "value": "relationship"},
-                {"map_type": "RelationshipObject", "position": 1},
-                {"map_type": "RelationshipObject", "position": 2},
-                {"map_type": "RelationshipMetadata", "position": 3},
-                {"map_type": "ParameterDefinition", "position": -2},
-                {"map_type": "Alternative", "position": 0},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-                {"map_type": "IndexName", "position": "hidden", "value": "constraint"},
-                {"map_type": "ParameterValueIndex", "position": 4},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        mapped_data, errors = get_mapped_data(data_source, mappings)
-        self.assertEqual(errors, [])
-        self.assertEqual(mapped_data, {})
-
-    def test_returns_appropriate_error_if_last_row_is_empty(self):
-        data_source = iter([["", "T1", "T2"], ["Parameter", "5.0", "99.0"], [" "]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "hidden", "value": "Object"},
-                {"map_type": "Object", "position": "hidden", "value": "data"},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": 0},
-                {"map_type": "Alternative", "position": "hidden", "value": "Base"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-                {"map_type": "IndexName", "position": "hidden"},
-                {"map_type": "ParameterValueIndex", "position": -1},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        convert_function_specs = {0: "string", 1: "float", 2: "float"}
-        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
-
-        mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
-        self.assertEqual(errors, ["Could not process incomplete row 2"])
-        self.assertEqual(
-            mapped_data,
-            {
-                'alternatives': {'Base'},
-                'object_classes': {'Object'},
-                'object_parameter_values': [['Object', 'data', 'Parameter', Map(["T1", "T2"], [5.0, 99.0]), 'Base']],
-                'object_parameters': [('Object', 'Parameter')],
-                'objects': {('Object', 'data')},
-            },
-        )
-
-    def test_convert_functions_get_expanded_over_last_defined_column_in_pivoted_data(self):
-        data_source = iter([["", "T1", "T2"], ["Parameter", "5.0", "99.0"]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "hidden", "value": "Object"},
-                {"map_type": "Object", "position": "hidden", "value": "data"},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": 0},
-                {"map_type": "Alternative", "position": "hidden", "value": "Base"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-                {"map_type": "IndexName", "position": "hidden"},
-                {"map_type": "ParameterValueIndex", "position": -1},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        convert_function_specs = {0: "string", 1: "float"}
-        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
-
-        mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
-        self.assertEqual(errors, [])
-        self.assertEqual(
-            mapped_data,
-            {
-                'alternatives': {'Base'},
-                'object_classes': {'Object'},
-                'object_parameter_values': [['Object', 'data', 'Parameter', Map(["T1", "T2"], [5.0, 99.0]), 'Base']],
-                'object_parameters': [('Object', 'Parameter')],
-                'objects': {('Object', 'data')},
-            },
-        )
-
-    def test_read_start_row_skips_rows_in_pivoted_data(self):
-        data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "hidden", "value": "klass", "read_start_row": 2},
-                {"map_type": "Object", "position": "hidden", "value": "kloss"},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": 0},
-                {"map_type": "Alternative", "position": "hidden"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-                {"map_type": "IndexName", "position": "hidden"},
-                {"map_type": "ParameterValueIndex", "position": -1},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        convert_function_specs = {0: "string", 1: "float", 2: "float"}
-        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
-
-        mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
-        self.assertEqual(errors, [])
-        self.assertEqual(
-            mapped_data,
-            {
-                'object_classes': {'klass'},
-                'object_parameter_values': [['klass', 'kloss', 'Parameter_2', Map(["T1", "T2"], [2.3, 23.0])]],
-                'object_parameters': [('klass', 'Parameter_2')],
-                'objects': {('klass', 'kloss')},
-            },
-        )
-
-    def test_empty_pivoted_data_is_skipped(self):
-        data_header = ["period", "time"]
-        data_source = iter([["p2020", "t0"], ["p2020", "t1"]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "hidden", "value": "unit"},
-                {"map_type": "Object", "position": "header"},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": "hidden", "value": "price"},
-                {"map_type": "Alternative", "position": "hidden"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-                {"map_type": "IndexName", "position": "hidden", "value": "period"},
-                {"map_type": "ParameterValueIndex", "position": 0},
-                {"map_type": "IndexName", "position": "hidden", "value": "time"},
-                {"map_type": "ParameterValueIndex", "position": 1},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        mapped_data, errors = get_mapped_data(data_source, mappings, data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(mapped_data, {})
-
-    def test_map_without_values_is_ignored_and_not_interpreted_as_null(self):
-        data_source = iter([["map index", "parameter_name"], ["t1", None]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "hidden", "value": "o"},
-                {"map_type": "Object", "position": "hidden", "value": "o1"},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": -1},
-                {"map_type": "Alternative", "position": "hidden", "value": "base"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-                {"map_type": "IndexName", "position": "hidden"},
-                {"map_type": "ParameterValueIndex", "position": 0},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        convert_function_specs = {0: "string", 1: "float"}
-        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
-        mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
-        self.assertEqual(errors, [])
-        self.assertEqual(
-            mapped_data,
-            {
-                "alternatives": {"base"},
-                "object_classes": {"o"},
-                "object_parameters": [("o", "parameter_name")],
-                "object_parameter_values": [],
-                "objects": {("o", "o1")},
-            },
-        )
-
-    def test_import_object_works_with_multiple_relationship_object_imports(self):
-        header = ["time", "relationship 1", "relationship 2", "relationship 3"]
-        data_source = iter([[None, "o1", "o2", "o1"], [None, "q1", "q2", "q2"], ["t1", 11, 33, 55], ["t2", 22, 44, 66]])
-        mappings = [
-            [
-                {"map_type": "RelationshipClass", "position": "hidden", "value": "o_to_q"},
-                {"map_type": "RelationshipClassObjectClass", "position": "hidden", "value": "o"},
-                {"map_type": "RelationshipClassObjectClass", "position": "hidden", "value": "q"},
-                {"map_type": "Relationship", "position": "hidden", "value": "relationship"},
-                {"map_type": "RelationshipObject", "position": -1, "import_objects": True},
-                {"map_type": "RelationshipObject", "position": -2, "import_objects": True},
-                {"map_type": "RelationshipMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": "hidden", "value": "param"},
-                {"map_type": "Alternative", "position": "hidden", "value": "base"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-                {"map_type": "IndexName", "position": "hidden", "value": "time"},
-                {"map_type": "ParameterValueIndex", "position": 0},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        convert_function_specs = {0: "string", 1: "float", 2: "float", 3: "float"}
-        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
-        mapped_data, errors = get_mapped_data(data_source, mappings, header, column_convert_fns=convert_functions)
-        self.assertEqual(errors, [])
-        self.assertEqual(
-            mapped_data,
-            {
-                "alternatives": {"base"},
-                "object_classes": {"o", "q"},
-                "objects": {("o", "o1"), ("o", "o2"), ("q", "q1"), ("q", "q2")},
-                "relationship_classes": [("o_to_q", ["o", "q"])],
-                "relationships": {("o_to_q", ("o1", "q1")), ("o_to_q", ("o1", "q2")), ("o_to_q", ("o2", "q2"))},
-                "relationship_parameters": [("o_to_q", "param")],
-                "relationship_parameter_values": [
-                    ["o_to_q", ("o1", "q1"), "param", Map(["t1", "t2"], [11, 22], index_name="time"), "base"],
-                    ["o_to_q", ("o2", "q2"), "param", Map(["t1", "t2"], [33, 44], index_name="time"), "base"],
-                    ["o_to_q", ("o1", "q2"), "param", Map(["t1", "t2"], [55, 66], index_name="time"), "base"],
-                ],
-            },
-        )
-
-    def test_default_convert_function_in_column_convert_functions(self):
-        data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "hidden", "value": "klass", "read_start_row": 2},
-                {"map_type": "Object", "position": "hidden", "value": "kloss"},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": 0},
-                {"map_type": "Alternative", "position": "hidden"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-                {"map_type": "IndexName", "position": "hidden"},
-                {"map_type": "ParameterValueIndex", "position": -1},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        convert_function_specs = {0: "string"}
-        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
-        mapped_data, errors = get_mapped_data(
-            data_source, mappings, column_convert_fns=convert_functions, default_column_convert_fn=float
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(
-            mapped_data,
-            {
-                "object_classes": {"klass"},
-                "object_parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], [2.3, 23.0])]],
-                "object_parameters": [("klass", "Parameter_2")],
-                "objects": {("klass", "kloss")},
-            },
-        )
-
-    def test_identity_function_is_used_as_convert_function_when_no_convert_functions_given(self):
-        data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "hidden", "value": "klass", "read_start_row": 2},
-                {"map_type": "Object", "position": "hidden", "value": "kloss"},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": 0},
-                {"map_type": "Alternative", "position": "hidden"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-                {"map_type": "IndexName", "position": "hidden"},
-                {"map_type": "ParameterValueIndex", "position": -1},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        mapped_data, errors = get_mapped_data(data_source, mappings)
-        self.assertEqual(errors, [])
-        self.assertEqual(
-            mapped_data,
-            {
-                "object_classes": {"klass"},
-                "object_parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], ["2.3", "23.0"])]],
-                "object_parameters": [("klass", "Parameter_2")],
-                "objects": {("klass", "kloss")},
-            },
-        )
-
-    def test_last_convert_function_gets_used_as_default_convert_function_when_no_default_is_set(self):
-        data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "hidden", "value": "klass", "read_start_row": 2},
-                {"map_type": "Object", "position": "hidden", "value": "kloss"},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": 0},
-                {"map_type": "Alternative", "position": "hidden"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-                {"map_type": "IndexName", "position": "hidden"},
-                {"map_type": "ParameterValueIndex", "position": -1},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        convert_function_specs = {0: "string", 1: "float"}
-        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
-        mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
-        self.assertEqual(errors, [])
-        self.assertEqual(
-            mapped_data,
-            {
-                "object_classes": {"klass"},
-                "object_parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], [2.3, 23.0])]],
-                "object_parameters": [("klass", "Parameter_2")],
-                "objects": {("klass", "kloss")},
-            },
-        )
-
-    def test_array_parameters_get_imported_correctly_when_objects_are_in_header(self):
-        header = ["object_1", "object_2"]
-        data_source = iter([[-1.1, 2.3], [1.1, -2.3]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "hidden", "value": "class"},
-                {"map_type": "Object", "position": "header"},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": "hidden", "value": "param"},
-                {"map_type": "Alternative", "position": "hidden", "value": "Base"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "array"},
-                {"map_type": "IndexName", "position": "hidden"},
-                {"map_type": "ParameterValueIndex", "position": "hidden"},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        convert_function_specs = {0: "float", 1: "float"}
-        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
-
-        mapped_data, errors = get_mapped_data(data_source, mappings, header, column_convert_fns=convert_functions)
-        self.assertEqual(errors, [])
-        self.assertEqual(
-            mapped_data,
-            {
-                "alternatives": {"Base"},
-                "object_classes": {"class"},
-                "object_parameter_values": [
-                    ["class", "object_1", "param", Array([-1.1, 1.1]), "Base"],
-                    ["class", "object_2", "param", Array([2.3, -2.3]), "Base"],
-                ],
-                "object_parameters": [("class", "param")],
-                "objects": {("class", "object_1"), ("class", "object_2")},
-            },
-        )
-
-    def test_arrays_get_imported_correctly_when_objects_are_in_header_and_alternatives_in_first_row(self):
-        header = ["object_1", "object_2"]
-        data_source = iter([["Base", "Base"], [-1.1, 2.3], [1.1, -2.3]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "hidden", "value": "Gadget"},
-                {"map_type": "Object", "position": "header"},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": "hidden", "value": "data"},
-                {"map_type": "Alternative", "position": -1},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValueType", "position": "hidden", "value": "array"},
-                {"map_type": "IndexName", "position": "hidden"},
-                {"map_type": "ParameterValueIndex", "position": "hidden"},
-                {"map_type": "ExpandedValue", "position": "hidden"},
-            ]
-        ]
-        convert_function_specs = {0: "float", 1: "float"}
-        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
-
-        mapped_data, errors = get_mapped_data(data_source, mappings, header, column_convert_fns=convert_functions)
-        self.assertEqual(errors, [])
-        self.assertEqual(
-            mapped_data,
-            {
-                "alternatives": {"Base"},
-                "object_classes": {"Gadget"},
-                "object_parameter_values": [
-                    ["Gadget", "object_1", "data", Array([-1.1, 1.1]), "Base"],
-                    ["Gadget", "object_2", "data", Array([2.3, -2.3]), "Base"],
-                ],
-                "object_parameters": [("Gadget", "data")],
-                "objects": {("Gadget", "object_1"), ("Gadget", "object_2")},
-            },
-        )
-
-    def test_header_position_is_ignored_in_last_mapping_if_other_mappings_are_in_header(self):
-        header = ["Dimension", "parameter1", "parameter2"]
-        data_source = iter([["d1", 1.1, -2.3], ["d2", -1.1, 2.3]])
-        mappings = [
-            [
-                {"map_type": "ObjectClass", "position": "table_name"},
-                {"map_type": "Object", "position": 0},
-                {"map_type": "ObjectMetadata", "position": "hidden"},
-                {"map_type": "ParameterDefinition", "position": "header"},
-                {"map_type": "Alternative", "position": "hidden", "value": "Base"},
-                {"map_type": "ParameterValueMetadata", "position": "hidden"},
-                {"map_type": "ParameterValue", "position": "header"},
-            ]
-        ]
-        convert_function_specs = {0: "string", 1: "float", 2: "float"}
-        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
-
-        mapped_data, errors = get_mapped_data(
-            data_source, mappings, header, table_name="Data", column_convert_fns=convert_functions
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(
-            mapped_data,
-            {
-                "alternatives": {"Base"},
-                "object_classes": {"Data"},
-                "object_parameter_values": [
-                    ["Data", "d1", "parameter1", 1.1, "Base"],
-                    ["Data", "d1", "parameter2", -2.3, "Base"],
-                    ["Data", "d2", "parameter1", -1.1, "Base"],
-                    ["Data", "d2", "parameter2", 2.3, "Base"],
-                ],
-                "object_parameters": [("Data", "parameter1"), ("Data", "parameter2")],
-                "objects": {("Data", "d1"), ("Data", "d2")},
-            },
-        )
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains unit tests for the generator module.
+
+"""
+import unittest
+
+from spinedb_api import Array, Map
+from spinedb_api.import_mapping.generator import get_mapped_data
+from spinedb_api.import_mapping.type_conversion import value_to_convert_spec
+
+
+class TestGetMappedData(unittest.TestCase):
+    def test_does_not_give_traceback_when_pivoted_mapping_encounters_empty_data(self):
+        data_source = iter([])
+        mappings = [
+            [
+                {"map_type": "RelationshipClass", "position": "hidden", "value": "unit__sourceNode"},
+                {"map_type": "RelationshipClassObjectClass", "position": "hidden", "value": "unit"},
+                {"map_type": "RelationshipClassObjectClass", "position": "hidden", "value": "node"},
+                {"map_type": "Relationship", "position": "hidden", "value": "relationship"},
+                {"map_type": "RelationshipObject", "position": 1},
+                {"map_type": "RelationshipObject", "position": 2},
+                {"map_type": "RelationshipMetadata", "position": 3},
+                {"map_type": "ParameterDefinition", "position": -2},
+                {"map_type": "Alternative", "position": 0},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+                {"map_type": "IndexName", "position": "hidden", "value": "constraint"},
+                {"map_type": "ParameterValueIndex", "position": 4},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        mapped_data, errors = get_mapped_data(data_source, mappings)
+        self.assertEqual(errors, [])
+        self.assertEqual(mapped_data, {})
+
+    def test_returns_appropriate_error_if_last_row_is_empty(self):
+        data_source = iter([["", "T1", "T2"], ["Parameter", "5.0", "99.0"], [" "]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "hidden", "value": "Object"},
+                {"map_type": "Object", "position": "hidden", "value": "data"},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": 0},
+                {"map_type": "Alternative", "position": "hidden", "value": "Base"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+                {"map_type": "IndexName", "position": "hidden"},
+                {"map_type": "ParameterValueIndex", "position": -1},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        convert_function_specs = {0: "string", 1: "float", 2: "float"}
+        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
+
+        mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
+        self.assertEqual(errors, ["Could not process incomplete row 2"])
+        self.assertEqual(
+            mapped_data,
+            {
+                'alternatives': {'Base'},
+                'object_classes': {'Object'},
+                'object_parameter_values': [['Object', 'data', 'Parameter', Map(["T1", "T2"], [5.0, 99.0]), 'Base']],
+                'object_parameters': [('Object', 'Parameter')],
+                'objects': {('Object', 'data')},
+            },
+        )
+
+    def test_convert_functions_get_expanded_over_last_defined_column_in_pivoted_data(self):
+        data_source = iter([["", "T1", "T2"], ["Parameter", "5.0", "99.0"]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "hidden", "value": "Object"},
+                {"map_type": "Object", "position": "hidden", "value": "data"},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": 0},
+                {"map_type": "Alternative", "position": "hidden", "value": "Base"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+                {"map_type": "IndexName", "position": "hidden"},
+                {"map_type": "ParameterValueIndex", "position": -1},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        convert_function_specs = {0: "string", 1: "float"}
+        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
+
+        mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
+        self.assertEqual(errors, [])
+        self.assertEqual(
+            mapped_data,
+            {
+                'alternatives': {'Base'},
+                'object_classes': {'Object'},
+                'object_parameter_values': [['Object', 'data', 'Parameter', Map(["T1", "T2"], [5.0, 99.0]), 'Base']],
+                'object_parameters': [('Object', 'Parameter')],
+                'objects': {('Object', 'data')},
+            },
+        )
+
+    def test_read_start_row_skips_rows_in_pivoted_data(self):
+        data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "hidden", "value": "klass", "read_start_row": 2},
+                {"map_type": "Object", "position": "hidden", "value": "kloss"},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": 0},
+                {"map_type": "Alternative", "position": "hidden"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+                {"map_type": "IndexName", "position": "hidden"},
+                {"map_type": "ParameterValueIndex", "position": -1},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        convert_function_specs = {0: "string", 1: "float", 2: "float"}
+        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
+
+        mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
+        self.assertEqual(errors, [])
+        self.assertEqual(
+            mapped_data,
+            {
+                'object_classes': {'klass'},
+                'object_parameter_values': [['klass', 'kloss', 'Parameter_2', Map(["T1", "T2"], [2.3, 23.0])]],
+                'object_parameters': [('klass', 'Parameter_2')],
+                'objects': {('klass', 'kloss')},
+            },
+        )
+
+    def test_empty_pivoted_data_is_skipped(self):
+        data_header = ["period", "time"]
+        data_source = iter([["p2020", "t0"], ["p2020", "t1"]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "hidden", "value": "unit"},
+                {"map_type": "Object", "position": "header"},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": "hidden", "value": "price"},
+                {"map_type": "Alternative", "position": "hidden"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+                {"map_type": "IndexName", "position": "hidden", "value": "period"},
+                {"map_type": "ParameterValueIndex", "position": 0},
+                {"map_type": "IndexName", "position": "hidden", "value": "time"},
+                {"map_type": "ParameterValueIndex", "position": 1},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        mapped_data, errors = get_mapped_data(data_source, mappings, data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(mapped_data, {})
+
+    def test_map_without_values_is_ignored_and_not_interpreted_as_null(self):
+        data_source = iter([["map index", "parameter_name"], ["t1", None]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "hidden", "value": "o"},
+                {"map_type": "Object", "position": "hidden", "value": "o1"},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": -1},
+                {"map_type": "Alternative", "position": "hidden", "value": "base"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+                {"map_type": "IndexName", "position": "hidden"},
+                {"map_type": "ParameterValueIndex", "position": 0},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        convert_function_specs = {0: "string", 1: "float"}
+        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
+        mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
+        self.assertEqual(errors, [])
+        self.assertEqual(
+            mapped_data,
+            {
+                "alternatives": {"base"},
+                "object_classes": {"o"},
+                "object_parameters": [("o", "parameter_name")],
+                "object_parameter_values": [],
+                "objects": {("o", "o1")},
+            },
+        )
+
+    def test_import_object_works_with_multiple_relationship_object_imports(self):
+        header = ["time", "relationship 1", "relationship 2", "relationship 3"]
+        data_source = iter([[None, "o1", "o2", "o1"], [None, "q1", "q2", "q2"], ["t1", 11, 33, 55], ["t2", 22, 44, 66]])
+        mappings = [
+            [
+                {"map_type": "RelationshipClass", "position": "hidden", "value": "o_to_q"},
+                {"map_type": "RelationshipClassObjectClass", "position": "hidden", "value": "o"},
+                {"map_type": "RelationshipClassObjectClass", "position": "hidden", "value": "q"},
+                {"map_type": "Relationship", "position": "hidden", "value": "relationship"},
+                {"map_type": "RelationshipObject", "position": -1, "import_objects": True},
+                {"map_type": "RelationshipObject", "position": -2, "import_objects": True},
+                {"map_type": "RelationshipMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": "hidden", "value": "param"},
+                {"map_type": "Alternative", "position": "hidden", "value": "base"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+                {"map_type": "IndexName", "position": "hidden", "value": "time"},
+                {"map_type": "ParameterValueIndex", "position": 0},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        convert_function_specs = {0: "string", 1: "float", 2: "float", 3: "float"}
+        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
+        mapped_data, errors = get_mapped_data(data_source, mappings, header, column_convert_fns=convert_functions)
+        self.assertEqual(errors, [])
+        self.assertEqual(
+            mapped_data,
+            {
+                "alternatives": {"base"},
+                "object_classes": {"o", "q"},
+                "objects": {("o", "o1"), ("o", "o2"), ("q", "q1"), ("q", "q2")},
+                "relationship_classes": [("o_to_q", ["o", "q"])],
+                "relationships": {("o_to_q", ("o1", "q1")), ("o_to_q", ("o1", "q2")), ("o_to_q", ("o2", "q2"))},
+                "relationship_parameters": [("o_to_q", "param")],
+                "relationship_parameter_values": [
+                    ["o_to_q", ("o1", "q1"), "param", Map(["t1", "t2"], [11, 22], index_name="time"), "base"],
+                    ["o_to_q", ("o2", "q2"), "param", Map(["t1", "t2"], [33, 44], index_name="time"), "base"],
+                    ["o_to_q", ("o1", "q2"), "param", Map(["t1", "t2"], [55, 66], index_name="time"), "base"],
+                ],
+            },
+        )
+
+    def test_default_convert_function_in_column_convert_functions(self):
+        data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "hidden", "value": "klass", "read_start_row": 2},
+                {"map_type": "Object", "position": "hidden", "value": "kloss"},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": 0},
+                {"map_type": "Alternative", "position": "hidden"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+                {"map_type": "IndexName", "position": "hidden"},
+                {"map_type": "ParameterValueIndex", "position": -1},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        convert_function_specs = {0: "string"}
+        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
+        mapped_data, errors = get_mapped_data(
+            data_source, mappings, column_convert_fns=convert_functions, default_column_convert_fn=float
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(
+            mapped_data,
+            {
+                "object_classes": {"klass"},
+                "object_parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], [2.3, 23.0])]],
+                "object_parameters": [("klass", "Parameter_2")],
+                "objects": {("klass", "kloss")},
+            },
+        )
+
+    def test_identity_function_is_used_as_convert_function_when_no_convert_functions_given(self):
+        data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "hidden", "value": "klass", "read_start_row": 2},
+                {"map_type": "Object", "position": "hidden", "value": "kloss"},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": 0},
+                {"map_type": "Alternative", "position": "hidden"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+                {"map_type": "IndexName", "position": "hidden"},
+                {"map_type": "ParameterValueIndex", "position": -1},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        mapped_data, errors = get_mapped_data(data_source, mappings)
+        self.assertEqual(errors, [])
+        self.assertEqual(
+            mapped_data,
+            {
+                "object_classes": {"klass"},
+                "object_parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], ["2.3", "23.0"])]],
+                "object_parameters": [("klass", "Parameter_2")],
+                "objects": {("klass", "kloss")},
+            },
+        )
+
+    def test_last_convert_function_gets_used_as_default_convert_function_when_no_default_is_set(self):
+        data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "hidden", "value": "klass", "read_start_row": 2},
+                {"map_type": "Object", "position": "hidden", "value": "kloss"},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": 0},
+                {"map_type": "Alternative", "position": "hidden"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+                {"map_type": "IndexName", "position": "hidden"},
+                {"map_type": "ParameterValueIndex", "position": -1},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        convert_function_specs = {0: "string", 1: "float"}
+        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
+        mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
+        self.assertEqual(errors, [])
+        self.assertEqual(
+            mapped_data,
+            {
+                "object_classes": {"klass"},
+                "object_parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], [2.3, 23.0])]],
+                "object_parameters": [("klass", "Parameter_2")],
+                "objects": {("klass", "kloss")},
+            },
+        )
+
+    def test_array_parameters_get_imported_correctly_when_objects_are_in_header(self):
+        header = ["object_1", "object_2"]
+        data_source = iter([[-1.1, 2.3], [1.1, -2.3]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "hidden", "value": "class"},
+                {"map_type": "Object", "position": "header"},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": "hidden", "value": "param"},
+                {"map_type": "Alternative", "position": "hidden", "value": "Base"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "array"},
+                {"map_type": "IndexName", "position": "hidden"},
+                {"map_type": "ParameterValueIndex", "position": "hidden"},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        convert_function_specs = {0: "float", 1: "float"}
+        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
+
+        mapped_data, errors = get_mapped_data(data_source, mappings, header, column_convert_fns=convert_functions)
+        self.assertEqual(errors, [])
+        self.assertEqual(
+            mapped_data,
+            {
+                "alternatives": {"Base"},
+                "object_classes": {"class"},
+                "object_parameter_values": [
+                    ["class", "object_1", "param", Array([-1.1, 1.1]), "Base"],
+                    ["class", "object_2", "param", Array([2.3, -2.3]), "Base"],
+                ],
+                "object_parameters": [("class", "param")],
+                "objects": {("class", "object_1"), ("class", "object_2")},
+            },
+        )
+
+    def test_arrays_get_imported_correctly_when_objects_are_in_header_and_alternatives_in_first_row(self):
+        header = ["object_1", "object_2"]
+        data_source = iter([["Base", "Base"], [-1.1, 2.3], [1.1, -2.3]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "hidden", "value": "Gadget"},
+                {"map_type": "Object", "position": "header"},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": "hidden", "value": "data"},
+                {"map_type": "Alternative", "position": -1},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValueType", "position": "hidden", "value": "array"},
+                {"map_type": "IndexName", "position": "hidden"},
+                {"map_type": "ParameterValueIndex", "position": "hidden"},
+                {"map_type": "ExpandedValue", "position": "hidden"},
+            ]
+        ]
+        convert_function_specs = {0: "float", 1: "float"}
+        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
+
+        mapped_data, errors = get_mapped_data(data_source, mappings, header, column_convert_fns=convert_functions)
+        self.assertEqual(errors, [])
+        self.assertEqual(
+            mapped_data,
+            {
+                "alternatives": {"Base"},
+                "object_classes": {"Gadget"},
+                "object_parameter_values": [
+                    ["Gadget", "object_1", "data", Array([-1.1, 1.1]), "Base"],
+                    ["Gadget", "object_2", "data", Array([2.3, -2.3]), "Base"],
+                ],
+                "object_parameters": [("Gadget", "data")],
+                "objects": {("Gadget", "object_1"), ("Gadget", "object_2")},
+            },
+        )
+
+    def test_header_position_is_ignored_in_last_mapping_if_other_mappings_are_in_header(self):
+        header = ["Dimension", "parameter1", "parameter2"]
+        data_source = iter([["d1", 1.1, -2.3], ["d2", -1.1, 2.3]])
+        mappings = [
+            [
+                {"map_type": "ObjectClass", "position": "table_name"},
+                {"map_type": "Object", "position": 0},
+                {"map_type": "ObjectMetadata", "position": "hidden"},
+                {"map_type": "ParameterDefinition", "position": "header"},
+                {"map_type": "Alternative", "position": "hidden", "value": "Base"},
+                {"map_type": "ParameterValueMetadata", "position": "hidden"},
+                {"map_type": "ParameterValue", "position": "header"},
+            ]
+        ]
+        convert_function_specs = {0: "string", 1: "float", 2: "float"}
+        convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
+
+        mapped_data, errors = get_mapped_data(
+            data_source, mappings, header, table_name="Data", column_convert_fns=convert_functions
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(
+            mapped_data,
+            {
+                "alternatives": {"Base"},
+                "object_classes": {"Data"},
+                "object_parameter_values": [
+                    ["Data", "d1", "parameter1", 1.1, "Base"],
+                    ["Data", "d1", "parameter2", -2.3, "Base"],
+                    ["Data", "d2", "parameter1", -1.1, "Base"],
+                    ["Data", "d2", "parameter2", 2.3, "Base"],
+                ],
+                "object_parameters": [("Data", "parameter1"), ("Data", "parameter2")],
+                "objects": {("Data", "d1"), ("Data", "d2")},
+            },
+        )
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/import_mapping/test_import_mapping.py` & `spinedb_api-0.30.4/tests/import_mapping/test_import_mapping.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,2119 +1,2119 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for import Mappings.
-
-"""
-import unittest
-from unittest.mock import Mock
-from spinedb_api.exception import InvalidMapping
-from spinedb_api.mapping import Position, to_dict as mapping_to_dict, unflatten
-from spinedb_api.import_mapping.import_mapping import (
-    ImportMapping,
-    check_validity,
-    ParameterDefinitionMapping,
-    ObjectClassMapping,
-    ObjectMapping,
-    IndexNameMapping,
-    ParameterValueIndexMapping,
-    ExpandedParameterValueMapping,
-    ParameterValueMapping,
-    ParameterValueTypeMapping,
-    ParameterDefaultValueTypeMapping,
-    DefaultValueIndexNameMapping,
-    ParameterDefaultValueIndexMapping,
-    ExpandedParameterDefaultValueMapping,
-    AlternativeMapping,
-)
-from spinedb_api.import_mapping.import_mapping_compat import (
-    import_mapping_from_dict,
-    parameter_mapping_from_dict,
-    parameter_value_mapping_from_dict,
-)
-from spinedb_api.import_mapping.type_conversion import BooleanConvertSpec, StringConvertSpec, FloatConvertSpec
-from spinedb_api.import_mapping.generator import get_mapped_data
-from spinedb_api.parameter_value import Array, DateTime, TimeSeriesVariableResolution, Map
-
-
-class TestConvertFunctions(unittest.TestCase):
-    def test_convert_functions_float(self):
-        data = [["a", "1.2"]]
-        column_convert_fns = {0: str, 1: FloatConvertSpec()}
-        mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        mapping.position = 0
-        mapping.child.value = "obj"
-        mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
-            {"map_type": "ParameterDefinition"}
-        )
-        param_def_mapping.value = "param"
-        param_def_mapping.flatten()[-1].position = 1
-        mapped_data, _ = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
-        expected = {'object_classes': {'a'}, 'objects': {('a', 'obj')}, 'object_parameters': [('a', 'param', 1.2)]}
-        self.assertEqual(mapped_data, expected)
-
-    def test_convert_functions_str(self):
-        data = [["a", '"1111.2222"']]
-        column_convert_fns = {0: str, 1: StringConvertSpec()}
-        mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        mapping.position = 0
-        mapping.child.value = "obj"
-        mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
-            {"map_type": "ParameterDefinition"}
-        )
-        param_def_mapping.value = "param"
-        param_def_mapping.flatten()[-1].position = 1
-        mapped_data, _ = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
-        expected = {
-            'object_classes': {'a'},
-            'objects': {('a', 'obj')},
-            'object_parameters': [('a', 'param', '1111.2222')],
-        }
-        self.assertEqual(mapped_data, expected)
-
-    def test_convert_functions_bool(self):
-        data = [["a", "false"]]
-        column_convert_fns = {0: str, 1: BooleanConvertSpec()}
-        mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        mapping.position = 0
-        mapping.child.value = "obj"
-        mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
-            {"map_type": "ParameterDefinition"}
-        )
-        param_def_mapping.value = "param"
-        param_def_mapping.flatten()[-1].position = 1
-        mapped_data, _ = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
-        expected = {'object_classes': {'a'}, 'objects': {('a', 'obj')}, 'object_parameters': [('a', 'param', False)]}
-        self.assertEqual(mapped_data, expected)
-
-    def test_convert_functions_with_error(self):
-        data = [["a", "not a float"]]
-        column_convert_fns = {0: str, 1: FloatConvertSpec()}
-        mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        mapping.position = 0
-        mapping.child.value = "obj"
-        mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
-            {"map_type": "ParameterDefinition"}
-        )
-        param_def_mapping.value = "param"
-        param_def_mapping.flatten()[-1].position = 1
-        _, errors = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
-        self.assertEqual(len(errors), 1)
-
-
-class TestPolishImportMapping(unittest.TestCase):
-    def test_polish_null_mapping(self):
-        mapping = ImportMapping(Position.hidden, value=None)
-        table_name = "tablename"
-        header = ["A", "B", "C"]
-        mapping.polish(table_name, header)
-        self.assertEqual(mapping.position, Position.hidden)
-        self.assertIsNone(mapping.value)
-
-    def test_polish_column_mapping(self):
-        mapping = ImportMapping("B", value=None)
-        table_name = "tablename"
-        header = ["A", "B", "C"]
-        mapping.polish(table_name, header)
-        self.assertEqual(mapping.position, 1)
-        self.assertIsNone(mapping.value)
-
-    def test_polish_column_header_mapping(self):
-        mapping = ImportMapping(Position.header, value=2)
-        table_name = "tablename"
-        header = ["A", "B", "C"]
-        mapping.polish(table_name, header)
-        self.assertEqual(mapping.position, Position.header)
-        self.assertEqual(mapping.value, "C")
-
-    def test_polish_column_header_mapping_str(self):
-        mapping = ImportMapping(Position.header, value="2")
-        table_name = "tablename"
-        header = ["A", "B", "C"]
-        mapping.polish(table_name, header)
-        self.assertEqual(mapping.position, Position.header)
-        self.assertEqual(mapping.value, "C")
-
-    def test_polish_column_header_mapping_duplicates(self):
-        mapping = ImportMapping(Position.header, value=3)
-        table_name = "tablename"
-        header = ["A", "B", "C", "A"]
-        mapping.polish(table_name, header, for_preview=True)
-        self.assertEqual(mapping.position, Position.header)
-        self.assertEqual(mapping.value, 3)
-
-    def test_polish_column_header_mapping_invalid_header(self):
-        mapping = ImportMapping(Position.header, value="D")
-        table_name = "tablename"
-        header = ["A", "B", "C"]
-        with self.assertRaises(InvalidMapping):
-            mapping.polish(table_name, header)
-
-    def test_polish_column_header_mapping_invalid_index(self):
-        mapping = ImportMapping(Position.header, value=4)
-        table_name = "tablename"
-        header = ["A", "B", "C"]
-        with self.assertRaises(InvalidMapping):
-            mapping.polish(table_name, header)
-
-    def test_polish_table_name_mapping(self):
-        mapping = ImportMapping(Position.table_name)
-        table_name = "tablename"
-        header = ["A", "B", "C"]
-        mapping.polish(table_name, header)
-        self.assertEqual(mapping.position, Position.table_name)
-        self.assertEqual(mapping.value, "tablename")
-
-    def test_polish_row_header_mapping(self):
-        mapping = ImportMapping(Position.header, value=None)
-        table_name = "tablename"
-        header = ["A", "B", "C"]
-        mapping.polish(table_name, header)
-        self.assertEqual(mapping.position, Position.header)
-        self.assertIsNone(mapping.value)
-
-
-class TestImportMappingIO(unittest.TestCase):
-    def test_object_class_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = ['ObjectClass', 'Object', 'ObjectMetadata']
-        self.assertEqual(types, expected)
-
-    def test_relationship_class_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = [
-            'RelationshipClass',
-            'RelationshipClassObjectClass',
-            'Relationship',
-            'RelationshipObject',
-            'RelationshipMetadata',
-        ]
-        self.assertEqual(types, expected)
-
-    def test_object_group_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "ObjectGroup"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = ['ObjectClass', 'Object', 'ObjectGroup']
-        self.assertEqual(types, expected)
-
-    def test_alternative_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "Alternative"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = ['Alternative']
-        self.assertEqual(types, expected)
-
-    def test_scenario_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "Scenario"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = ['Scenario', 'ScenarioActiveFlag']
-        self.assertEqual(types, expected)
-
-    def test_scenario_alternative_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "ScenarioAlternative"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = ['Scenario', 'ScenarioAlternative', 'ScenarioBeforeAlternative']
-        self.assertEqual(types, expected)
-
-    def test_tool_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "Tool"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = ['Tool']
-        self.assertEqual(types, expected)
-
-    def test_tool_feature_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "ToolFeature"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = ['Tool', 'ToolFeatureEntityClass', 'ToolFeatureParameterDefinition', 'ToolFeatureRequiredFlag']
-        self.assertEqual(types, expected)
-
-    def test_tool_feature_method_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "ToolFeatureMethod"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = [
-            'Tool',
-            'ToolFeatureMethodEntityClass',
-            'ToolFeatureMethodParameterDefinition',
-            'ToolFeatureMethodMethod',
-        ]
-        self.assertEqual(types, expected)
-
-    def test_parameter_value_list_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "ParameterValueList"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = ['ParameterValueList', 'ParameterValueListValue']
-        self.assertEqual(types, expected)
-
-
-class TestImportMappingLegacy(unittest.TestCase):
-    def test_ObjectClass_to_dict_from_dict(self):
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": 0,
-            "objects": 1,
-            "parameters": {"map_type": "parameter", "name": 2, "value": 3, "parameter_type": "single value"},
-        }
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'ObjectClass', 'position': 0},
-            {'map_type': 'Object', 'position': 1},
-            {'map_type': 'ObjectMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterDefinition', 'position': 2},
-            {'map_type': 'Alternative', 'position': 'hidden'},
-            {'map_type': 'ParameterValueMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterValue', 'position': 3},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_ObjectClass_object_from_dict_to_dict(self):
-        mapping = {"map_type": "ObjectClass", "name": 0, "objects": 1}
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'ObjectClass', 'position': 0},
-            {'map_type': 'Object', 'position': 1},
-            {'map_type': 'ObjectMetadata', 'position': 'hidden'},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_ObjectClass_object_from_dict_to_dict2(self):
-        mapping = {"map_type": "ObjectClass", "name": "cls", "objects": "obj"}
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'ObjectClass', 'position': 'hidden', 'value': 'cls'},
-            {'map_type': 'Object', 'position': 'hidden', 'value': 'obj'},
-            {'map_type': 'ObjectMetadata', 'position': 'hidden'},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_RelationshipClassMapping_from_dict_to_dict(self):
-        mapping = {
-            "map_type": "RelationshipClass",
-            "name": "unit__node",
-            "object_classes": [0, 1],
-            "objects": [0, 1],
-            "parameters": {"map_type": "parameter", "name": "pname", "value": 2},
-        }
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'RelationshipClass', 'position': 'hidden', 'value': 'unit__node'},
-            {'map_type': 'RelationshipClassObjectClass', 'position': 0},
-            {'map_type': 'RelationshipClassObjectClass', 'position': 1},
-            {'map_type': 'Relationship', 'position': 'hidden', 'value': 'relationship'},
-            {'map_type': 'RelationshipObject', 'position': 0},
-            {'map_type': 'RelationshipObject', 'position': 1},
-            {'map_type': 'RelationshipMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterDefinition', 'position': 'hidden', 'value': 'pname'},
-            {'map_type': 'Alternative', 'position': 'hidden'},
-            {'map_type': 'ParameterValueMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterValue', 'position': 2},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_RelationshipClassMapping_from_dict_to_dict2(self):
-        mapping = {
-            "map_type": "RelationshipClass",
-            "name": "unit__node",
-            "object_classes": ["cls", 0],
-            "objects": ["obj", 0],
-        }
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'RelationshipClass', 'position': 'hidden', 'value': 'unit__node'},
-            {'map_type': 'RelationshipClassObjectClass', 'position': 'hidden', 'value': 'cls'},
-            {'map_type': 'RelationshipClassObjectClass', 'position': 0},
-            {'map_type': 'Relationship', 'position': 'hidden', 'value': 'relationship'},
-            {'map_type': 'RelationshipObject', 'position': 'hidden', 'value': 'obj'},
-            {'map_type': 'RelationshipObject', 'position': 0},
-            {'map_type': 'RelationshipMetadata', 'position': 'hidden'},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_RelationshipClassMapping_from_dict_to_dict3(self):
-        mapping = {
-            "map_type": "RelationshipClass",
-            "name": "unit__node",
-            "parameters": {
-                "map_type": "parameter",
-                "name": "pname",
-                "value": 2,
-                "parameter_type": "array",
-                "extra_dimensions": ["dim"],
-            },
-        }
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'RelationshipClass', 'position': 'hidden', 'value': 'unit__node'},
-            {'map_type': 'RelationshipClassObjectClass', 'position': 'hidden'},
-            {'map_type': 'Relationship', 'position': 'hidden', 'value': 'relationship'},
-            {'map_type': 'RelationshipObject', 'position': 'hidden'},
-            {'map_type': 'RelationshipMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterDefinition', 'position': 'hidden', 'value': 'pname'},
-            {'map_type': 'Alternative', 'position': 'hidden'},
-            {'map_type': 'ParameterValueMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterValueType', 'position': 'hidden', 'value': 'array'},
-            {'map_type': 'IndexName', 'position': 'hidden'},
-            {'map_type': 'ParameterValueIndex', 'position': 'hidden', 'value': 'dim'},
-            {'map_type': 'ExpandedValue', 'position': 2},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_ObjectGroupMapping_to_dict_from_dict(self):
-        mapping = {
-            "map_type": "ObjectGroup",
-            "name": 0,
-            "groups": 1,
-            "members": 2,
-            "parameters": {
-                "map_type": "parameter",
-                "name": {"map_type": "constant", "reference": "pname"},
-                "parameter_value_metadata": {"map_type": "None"},
-                "parameter_type": "single value",
-                "value": {"reference": 2, "map_type": "column"},
-            },
-        }
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'ObjectClass', 'position': 0},
-            {'map_type': 'Object', 'position': 1},
-            {'map_type': 'ObjectGroup', 'position': 2},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_Alternative_to_dict_from_dict(self):
-        mapping = {"map_type": "Alternative", "name": 0}
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [{'map_type': 'Alternative', 'position': 0}]
-        self.assertEqual(out, expected)
-
-    def test_Scenario_to_dict_from_dict(self):
-        mapping = {"map_type": "Scenario", "name": 0}
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'Scenario', 'position': 0},
-            {'map_type': 'ScenarioActiveFlag', 'position': 'hidden', 'value': 'false'},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_ScenarioAlternative_to_dict_from_dict(self):
-        mapping = {
-            "map_type": "ScenarioAlternative",
-            "scenario_name": 0,
-            "alternative_name": 1,
-            "before_alternative_name": 2,
-        }
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'Scenario', 'position': 0},
-            {'map_type': 'ScenarioAlternative', 'position': 1},
-            {'map_type': 'ScenarioBeforeAlternative', 'position': 2},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_Tool_to_dict_from_dict(self):
-        mapping = {"map_type": "Tool", "name": 0}
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [{'map_type': 'Tool', 'position': 0}]
-        self.assertEqual(out, expected)
-
-    def test_Feature_to_dict_from_dict(self):
-        mapping = {"map_type": "Feature", "entity_class_name": 0, "parameter_definition_name": 1}
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'FeatureEntityClass', 'position': 0},
-            {'map_type': 'FeatureParameterDefinition', 'position': 1},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_ToolFeature_to_dict_from_dict(self):
-        mapping = {"map_type": "ToolFeature", "name": 0, "entity_class_name": 1, "parameter_definition_name": 2}
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'Tool', 'position': 0},
-            {'map_type': 'ToolFeatureEntityClass', 'position': 1},
-            {'map_type': 'ToolFeatureParameterDefinition', 'position': 2},
-            {'map_type': 'ToolFeatureRequiredFlag', 'position': 'hidden', 'value': 'false'},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_ToolFeatureMethod_to_dict_from_dict(self):
-        mapping = {
-            "map_type": "ToolFeatureMethod",
-            "name": 0,
-            "entity_class_name": 1,
-            "parameter_definition_name": 2,
-            "method": 3,
-        }
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'Tool', 'position': 0},
-            {'map_type': 'ToolFeatureMethodEntityClass', 'position': 1},
-            {'map_type': 'ToolFeatureMethodParameterDefinition', 'position': 2},
-            {'map_type': 'ToolFeatureMethodMethod', 'position': 3},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_MapValueMapping_from_dict_to_dict(self):
-        mapping_dict = {
-            "value_type": "map",
-            "main_value": {"reference": 23, "map_type": "row"},
-            "extra_dimensions": [{"reference": "fifth column", "map_type": "column"}],
-            "compress": True,
-        }
-        parameter_mapping = parameter_value_mapping_from_dict(mapping_dict)
-        out = mapping_to_dict(parameter_mapping)
-        expected = [
-            {'map_type': 'ParameterValueType', 'position': 'hidden', 'value': 'map', 'compress': True},
-            {'map_type': 'IndexName', 'position': 'hidden'},
-            {'map_type': 'ParameterValueIndex', 'position': 'fifth column'},
-            {'map_type': 'ExpandedValue', 'position': -24},
-        ]
-        self.assertEqual(out, expected)
-
-    def test_TimeSeriesValueMapping_from_dict_to_dict(self):
-        mapping_dict = {
-            "value_type": "time series",
-            "main_value": {"reference": 23, "map_type": "row"},
-            "extra_dimensions": [{"reference": "fifth column", "map_type": "column"}],
-            "options": {"repeat": True, "ignore_year": False, "fixed_resolution": False},
-        }
-        parameter_mapping = parameter_value_mapping_from_dict(mapping_dict)
-        out = mapping_to_dict(parameter_mapping)
-        expected = [
-            {
-                'map_type': 'ParameterValueType',
-                'position': 'hidden',
-                'value': 'time_series',
-                'options': {'repeat': True, 'ignore_year': False, 'fixed_resolution': False},
-            },
-            {'map_type': 'IndexName', 'position': 'hidden'},
-            {'map_type': 'ParameterValueIndex', 'position': 'fifth column'},
-            {'map_type': 'ExpandedValue', 'position': -24},
-        ]
-        self.assertEqual(out, expected)
-
-
-def _parent_with_pivot(is_pivoted):
-    parent = Mock()
-    parent.is_pivoted.return_value = is_pivoted
-    return parent
-
-
-def _pivoted_parent():
-    return _parent_with_pivot(True)
-
-
-def _unpivoted_parent():
-    return _parent_with_pivot(False)
-
-
-class TestMappingIsValid(unittest.TestCase):
-    def test_valid_object_class_mapping(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_invalid_object_default_value_mapping_missing_parameter_definition(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
-        default_value_mapping = cls_mapping.flatten()[-1]
-        cls_mapping.position = 0
-        default_value_mapping.position = 1
-        issues = check_validity(cls_mapping)
-        self.assertTrue(issues)
-
-    def test_valid_object_default_value_mapping_not_missing_parameter_definition(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
-            {"map_type": "ParameterDefinition"}
-        )
-        default_value_mapping = cls_mapping.flatten()[-1]
-        cls_mapping.position = 0
-        param_def_mapping.position = 1
-        default_value_mapping.position = 2
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_invalid_object_value_list_mapping_missing_parameter_definition(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
-        value_list_mapping = cls_mapping.flatten()[-2]
-        cls_mapping.position = 0
-        value_list_mapping.position = 1
-        issues = check_validity(cls_mapping)
-        self.assertTrue(issues)
-
-    def test_valid_object_value_list_mapping_not_missing_parameter_definition(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
-            {"map_type": "ParameterDefinition"}
-        )
-        value_list_mapping = cls_mapping.flatten()[-2]
-        cls_mapping.position = 0
-        param_def_mapping.position = 1
-        value_list_mapping.position = 2
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_valid_object_default_value_mapping_hidden(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
-        default_value_mapping = cls_mapping.flatten()[-1]
-        default_value_mapping.position = Position.hidden
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_invalid_object_parameter_value_mapping_missing_object(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
-        value_mapping = cls_mapping.flatten()[-1]
-        cls_mapping.position = 0
-        value_mapping.position = 1
-        issues = check_validity(cls_mapping)
-        self.assertTrue(issues)
-
-    def test_invalid_object_parameter_value_mapping_missing_parameter_definition(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        object_mapping = cls_mapping.flatten()[-2]
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
-        value_mapping = cls_mapping.flatten()[-1]
-        cls_mapping.position = 0
-        object_mapping.position = 1
-        value_mapping.position = 3
-        issues = check_validity(cls_mapping)
-        self.assertTrue(issues)
-
-    def test_valid_object_parameter_value_mapping(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        object_mapping = cls_mapping.flatten()[-2]
-        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
-            {"map_type": "ParameterValue"}
-        )
-        value_mapping = cls_mapping.flatten()[-1]
-        cls_mapping.position = 0
-        object_mapping.position = 1
-        param_def_mapping.position = 2
-        value_mapping.position = 3
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_valid_object_parameter_value_mapping_hidden(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
-        value_mapping = cls_mapping.flatten()[-1]
-        value_mapping.position = Position.hidden
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_valid_relationship_class_mapping(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_invalid_relationship_default_value_mapping_missing_parameter_definition(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
-        default_value_mapping = cls_mapping.flatten()[-1]
-        cls_mapping.position = 0
-        default_value_mapping.position = 1
-        issues = check_validity(cls_mapping)
-        self.assertTrue(issues)
-
-    def test_valid_relationship_default_value_mapping_not_missing_parameter_definition(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        obj_cls_mapping = cls_mapping.child
-        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
-            {"map_type": "ParameterDefinition"}
-        )
-        default_value_mapping = cls_mapping.flatten()[-1]
-        cls_mapping.position = 0
-        obj_cls_mapping.position = 1
-        param_def_mapping.position = 2
-        default_value_mapping.position = 3
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_invalid_relationship_value_list_mapping_missing_parameter_definition(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
-        value_list_mapping = cls_mapping.flatten()[-2]
-        cls_mapping.position = 0
-        value_list_mapping.position = 1
-        issues = check_validity(cls_mapping)
-        self.assertTrue(issues)
-
-    def test_valid_relationship_value_list_mapping_not_missing_parameter_definition(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        obj_cls_mapping = cls_mapping.child
-        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
-            {"map_type": "ParameterDefinition"}
-        )
-        value_list_mapping = cls_mapping.flatten()[-2]
-        cls_mapping.position = 0
-        obj_cls_mapping.position = 1
-        param_def_mapping.position = 2
-        value_list_mapping.position = 3
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_valid_relationship_default_value_mapping_hidden(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
-        default_value_mapping = cls_mapping.flatten()[-1]
-        default_value_mapping.position = Position.hidden
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_invalid_relationship_parameter_value_mapping_missing_objects(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
-        value_mapping = cls_mapping.flatten()[-1]
-        cls_mapping.position = 0
-        value_mapping.position = 1
-        issues = check_validity(cls_mapping)
-        self.assertTrue(issues)
-
-    def test_invalid_relationship_parameter_value_mapping_missing_parameter_definition(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        object_mapping = cls_mapping.flatten()[-2]
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
-        value_mapping = cls_mapping.flatten()[-1]
-        cls_mapping.position = 0
-        object_mapping.position = 1
-        value_mapping.position = 3
-        issues = check_validity(cls_mapping)
-        self.assertTrue(issues)
-
-    def test_valid_relationship_parameter_value_mapping(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        obj_cls_mapping = cls_mapping.child
-        object_mapping = cls_mapping.flatten()[-2]
-        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
-            {"map_type": "ParameterValue"}
-        )
-        value_mapping = cls_mapping.flatten()[-1]
-        cls_mapping.position = 0
-        obj_cls_mapping.position = 1
-        object_mapping.position = 2
-        param_def_mapping.position = 3
-        value_mapping.position = 4
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_valid_relationship_parameter_value_mapping_hidden(self):
-        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
-        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
-        value_mapping = cls_mapping.flatten()[-1]
-        value_mapping.position = Position.hidden
-        issues = check_validity(cls_mapping)
-        self.assertFalse(issues)
-
-    def test_valid_single_value_mapping(self):
-        value_mapping = parameter_value_mapping_from_dict({"value_type": "single_value"})
-        issues = check_validity(value_mapping)
-        self.assertFalse(issues)
-
-    def test_invalid_single_value_mapping_missing_parameter_definition(self):
-        value_mapping = parameter_value_mapping_from_dict({"value_type": "single_value"})
-        value_mapping.position = 0
-        issues = check_validity(value_mapping)
-        self.assertTrue(issues)
-
-    def test_valid_array_mapping(self):
-        value_mapping = parameter_value_mapping_from_dict({"value_type": "array"})
-        issues = check_validity(value_mapping)
-        self.assertFalse(issues)
-
-    def test_invalid_array_mapping_missing_parameter_definition(self):
-        value_mapping = parameter_value_mapping_from_dict({"value_type": "array"})
-        value_mapping.flatten()[-1].position = 0
-        issues = check_validity(value_mapping)
-        self.assertTrue(issues)
-
-    def test_valid_time_series_mapping(self):
-        value_mapping = parameter_value_mapping_from_dict({"value_type": "time_series"})
-        issues = check_validity(value_mapping)
-        self.assertFalse(issues)
-
-    def test_invalid_time_series_mapping_missing_parameter_definition(self):
-        value_mapping = parameter_value_mapping_from_dict({"value_type": "time_series"})
-        value_mapping.flatten()[-1].position = 0
-        issues = check_validity(value_mapping)
-        self.assertTrue(issues)
-
-
-class TestMappingIntegration(unittest.TestCase):
-    # just a placeholder test for different mapping testings
-
-    def test_bad_mapping_type(self):
-        """Tests that passing any other than a `dict` or a `mapping` to `get_mapped_data` raises `TypeError`."""
-        input_data = [["object_class"], ["oc1"]]
-        data = iter(input_data)
-        data_header = next(data)
-
-        with self.assertRaises(TypeError):
-            mapping = [1, 2, 3]
-            get_mapped_data(data, [mapping], data_header)
-
-        with self.assertRaises(TypeError):
-            mappings = [{"map_type": "ObjectClass", "name": 0}, [1, 2, 3]]
-            get_mapped_data(data, mappings, data_header)
-
-    def test_read_iterator_with_row_with_all_Nones(self):
-        input_data = [
-            ["object_class", "object", "parameter", "value"],
-            [None, None, None, None],
-            ["oc2", "obj2", "parameter_name2", 2],
-        ]
-        expected = {"object_classes": {"oc2"}}
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {"map_type": "ObjectClass", "name": 0}
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_iterator_with_None(self):
-        input_data = [["object_class", "object", "parameter", "value"], None, ["oc2", "obj2", "parameter_name2", 2]]
-        expected = {"object_classes": {"oc2"}}
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {"map_type": "ObjectClass", "name": 0}
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_flat_file(self):
-        input_data = [
-            ["object_class", "object", "parameter", "value"],
-            ["oc1", "obj1", "parameter_name1", 1],
-            ["oc2", "obj2", "parameter_name2", 2],
-        ]
-        expected = {
-            "object_classes": {"oc1", "oc2"},
-            "objects": {("oc1", "obj1"), ("oc2", "obj2")},
-            "object_parameters": [("oc1", "parameter_name1"), ("oc2", "parameter_name2")],
-            "object_parameter_values": [["oc1", "obj1", "parameter_name1", 1], ["oc2", "obj2", "parameter_name2", 2]],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": 0,
-            "objects": 1,
-            "parameters": {"map_type": "parameter", "name": 2, "value": 3},
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_flat_file_array(self):
-        input_data = [
-            ["object_class", "object", "parameter", "value"],
-            ["oc1", "obj1", "parameter_name1", 1],
-            ["oc1", "obj1", "parameter_name1", 2],
-        ]
-        expected = {
-            "object_classes": {"oc1"},
-            "objects": {("oc1", "obj1")},
-            "object_parameters": [("oc1", "parameter_name1")],
-            "object_parameter_values": [["oc1", "obj1", "parameter_name1", Array([1, 2])]],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": 0,
-            "objects": 1,
-            "parameters": {"map_type": "parameter", "name": "parameter_name1", "value": 3, "parameter_type": "array"},
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_flat_file_array_with_ed(self):
-        input_data = [
-            ["object_class", "object", "parameter", "value", "value_order"],
-            ["oc1", "obj1", "parameter_name1", 1, 0],
-            ["oc1", "obj1", "parameter_name1", 2, 1],
-        ]
-        expected = {
-            "object_classes": {"oc1"},
-            "objects": {("oc1", "obj1")},
-            "object_parameters": [("oc1", "parameter_name1")],
-            "object_parameter_values": [["oc1", "obj1", "parameter_name1", Array([1, 2])]],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": 0,
-            "objects": 1,
-            "parameters": {
-                "map_type": "parameter",
-                "name": "parameter_name1",
-                "value": 3,
-                "extra_dimension": [None],
-                "parameter_type": "array",
-            },
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_flat_file_with_column_name_reference(self):
-        input_data = [["object", "parameter", "value"], ["obj1", "parameter_name1", 1], ["obj2", "parameter_name2", 2]]
-        expected = {"object_classes": {"object"}, "objects": {("object", "obj1"), ("object", "obj2")}}
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {"map_type": "ObjectClass", "name": {"map_type": "column_name", "reference": 0}, "object": 0}
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_object_class_from_header_using_string_as_integral_index(self):
-        input_data = [["object_class"], ["obj1"], ["obj2"]]
-        expected = {"object_classes": {"object_class"}, "objects": {("object_class", "obj1"), ("object_class", "obj2")}}
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {"map_type": "ObjectClass", "name": {"map_type": "column_header", "reference": "0"}, "object": 0}
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_object_class_from_header_using_string_as_column_header_name(self):
-        input_data = [["object_class"], ["obj1"], ["obj2"]]
-        expected = {"object_classes": {"object_class"}, "objects": {("object_class", "obj1"), ("object_class", "obj2")}}
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": {"map_type": "column_header", "reference": "object_class"},
-            "object": 0,
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_with_list_of_mappings(self):
-        input_data = [["object", "parameter", "value"], ["obj1", "parameter_name1", 1], ["obj2", "parameter_name2", 2]]
-        expected = {"object_classes": {"object"}, "objects": {("object", "obj1"), ("object", "obj2")}}
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {"map_type": "ObjectClass", "name": {"map_type": "column_header", "reference": 0}, "object": 0}
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_pivoted_parameters_from_header(self):
-        input_data = [["object", "parameter_name1", "parameter_name2"], ["obj1", 0, 1], ["obj2", 2, 3]]
-        expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1"), ("object", "obj2")},
-            "object_parameters": [("object", "parameter_name1"), ("object", "parameter_name2")],
-            "object_parameter_values": [
-                ["object", "obj1", "parameter_name1", 0],
-                ["object", "obj1", "parameter_name2", 1],
-                ["object", "obj2", "parameter_name1", 2],
-                ["object", "obj2", "parameter_name2", 3],
-            ],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": {"map_type": "column_header", "reference": 0},
-            "object": 0,
-            "parameters": {"map_type": "parameter", "name": {"map_type": "row", "reference": -1}},
-        }  # -1 to read pivot from header
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_empty_pivot(self):
-        input_data = [["object", "parameter_name1", "parameter_name2"]]
-        expected = {}
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": {"map_type": "column_header", "reference": 0},
-            "object": 0,
-            "parameters": {"map_type": "parameter", "name": {"map_type": "row", "reference": -1}},
-        }  # -1 to read pivot from header
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_pivoted_parameters_from_data(self):
-        input_data = [["object", "parameter_name1", "parameter_name2"], ["obj1", 0, 1], ["obj2", 2, 3]]
-        expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1"), ("object", "obj2")},
-            "object_parameters": [("object", "parameter_name1"), ("object", "parameter_name2")],
-            "object_parameter_values": [
-                ["object", "obj1", "parameter_name1", 0],
-                ["object", "obj1", "parameter_name2", 1],
-                ["object", "obj2", "parameter_name1", 2],
-                ["object", "obj2", "parameter_name2", 3],
-            ],
-        }
-
-        data = iter(input_data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object",
-            "object": 0,
-            "parameters": {"map_type": "parameter", "name": {"map_type": "row", "reference": 0}},
-        }
-
-        out, errors = get_mapped_data(data, [mapping])
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_pivoted_value_has_actual_position(self):
-        """Pivoted mapping works even when last mapping has valid position in columns."""
-        input_data = [
-            ["object", "timestep", "value"],
-            ["obj1", "T1", 11.0],
-            ["obj1", "T2", 12.0],
-            ["obj2", "T1", 21.0],
-            ["obj2", "T2", 22.0],
-        ]
-        expected = {
-            "object_classes": {"timeline"},
-            "objects": {("timeline", "obj1"), ("timeline", "obj2")},
-            "object_parameters": [("timeline", "value")],
-            "alternatives": {"Base"},
-            "object_parameter_values": [
-                ["timeline", "obj1", "value", Map(["T1", "T2"], [11.0, 12.0], index_name="timestep"), "Base"],
-                ["timeline", "obj2", "value", Map(["T1", "T2"], [21.0, 22.0], index_name="timestep"), "Base"],
-            ],
-        }
-        data = iter(input_data)
-        mapping_dicts = [
-            {"map_type": "ObjectClass", "position": "hidden", "value": "timeline"},
-            {"map_type": "Object", "position": 0},
-            {"map_type": "ObjectMetadata", "position": "hidden"},
-            {"map_type": "ParameterDefinition", "position": -1},
-            {"map_type": "Alternative", "position": "hidden", "value": "Base"},
-            {"map_type": "ParameterValueMetadata", "position": "hidden"},
-            {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-            {"map_type": "IndexName", "position": "hidden", "value": "timestep"},
-            {"map_type": "ParameterValueIndex", "position": 1},
-            {"map_type": "ExpandedValue", "position": 2},  # This caused import to fail
-        ]
-        out, errors = get_mapped_data(data, [mapping_dicts])
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_import_objects_from_pivoted_data_when_they_lack_parameter_values(self):
-        """Pivoted mapping works even when last mapping has valid position in columns."""
-        input_data = [["object", "is_skilled", "has_powers"], ["obj1", "yes", "no"], ["obj2", None, None]]
-        expected = {
-            "object_classes": {"node"},
-            "objects": {("node", "obj1"), ("node", "obj2")},
-            "object_parameters": [("node", "is_skilled"), ("node", "has_powers")],
-            "alternatives": {"Base"},
-            "object_parameter_values": [
-                ["node", "obj1", "is_skilled", "yes", "Base"],
-                ["node", "obj1", "has_powers", "no", "Base"],
-            ],
-        }
-        data = iter(input_data)
-        mapping_dicts = [
-            {"map_type": "ObjectClass", "position": "hidden", "value": "node"},
-            {"map_type": "Object", "position": 0},
-            {"map_type": "ObjectMetadata", "position": "hidden"},
-            {"map_type": "ParameterDefinition", "position": -1},
-            {"map_type": "Alternative", "position": "hidden", "value": "Base"},
-            {"map_type": "ParameterValueMetadata", "position": "hidden"},
-            {"map_type": "ParameterValue", "position": "hidden"},
-        ]
-        out, errors = get_mapped_data(data, [mapping_dicts])
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_import_objects_from_pivoted_data_when_they_lack_map_type_parameter_values(self):
-        """Pivoted mapping works even when last mapping has valid position in columns."""
-        input_data = [
-            ["object", "my_index", "is_skilled", "has_powers"],
-            ["obj1", "yesterday", None, "no"],
-            ["obj1", "today", None, "yes"],
-        ]
-        expected = {
-            "object_classes": {"node"},
-            "objects": {("node", "obj1")},
-            "object_parameters": [("node", "is_skilled"), ("node", "has_powers")],
-            "alternatives": {"Base"},
-            "object_parameter_values": [
-                ["node", "obj1", "has_powers", Map(["yesterday", "today"], ["no", "yes"], index_name="period"), "Base"]
-            ],
-        }
-        data = iter(input_data)
-        mapping_dicts = [
-            {"map_type": "ObjectClass", "position": "hidden", "value": "node"},
-            {"map_type": "Object", "position": 0},
-            {"map_type": "ObjectMetadata", "position": "hidden"},
-            {"map_type": "ParameterDefinition", "position": -1},
-            {"map_type": "Alternative", "position": "hidden", "value": "Base"},
-            {"map_type": "ParameterValueMetadata", "position": "hidden"},
-            {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
-            {"map_type": "IndexName", "position": "hidden", "value": "period"},
-            {"map_type": "ParameterValueIndex", "position": 1},
-            {"map_type": "ExpandedValue", "position": "hidden"},
-        ]
-        out, errors = get_mapped_data(data, [mapping_dicts])
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_flat_file_with_extra_value_dimensions(self):
-        input_data = [["object", "time", "parameter_name1"], ["obj1", "2018-01-01", 1], ["obj1", "2018-01-02", 2]]
-
-        expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1")},
-            "object_parameters": [("object", "parameter_name1")],
-            "object_parameter_values": [
-                [
-                    "object",
-                    "obj1",
-                    "parameter_name1",
-                    TimeSeriesVariableResolution(["2018-01-01", "2018-01-02"], [1, 2], False, False),
-                ]
-            ],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object",
-            "object": 0,
-            "parameters": {
-                "map_type": "parameter",
-                "name": {"map_type": "column_header", "reference": 2},
-                "value": 2,
-                "parameter_type": "time series",
-                "extra_dimensions": [1],
-            },
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_flat_file_with_parameter_definition(self):
-        input_data = [["object", "time", "parameter_name1"], ["obj1", "2018-01-01", 1], ["obj1", "2018-01-02", 2]]
-
-        expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1")},
-            "object_parameters": [("object", "parameter_name1")],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object",
-            "object": 0,
-            "parameters": {
-                "map_type": "parameter",
-                "name": {"map_type": "column_header", "reference": 2},
-                "value": 2,
-                "parameter_type": "definition",
-            },
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_1dim_relationships(self):
-        input_data = [["unit", "node"], ["u1", "n1"], ["u1", "n2"]]
-        expected = {
-            "relationship_classes": [("node_group", ["node"])],
-            "relationships": {("node_group", ("n1",)), ("node_group", ("n2",))},
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "RelationshipClass",
-            "name": "node_group",
-            "object_classes": [{"map_type": "column_header", "reference": 1}],
-            "objects": [1],
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_relationships(self):
-        input_data = [["unit", "node"], ["u1", "n1"], ["u1", "n2"]]
-        expected = {
-            "relationship_classes": [("unit__node", ["unit", "node"])],
-            "relationships": {("unit__node", ("u1", "n1")), ("unit__node", ("u1", "n2"))},
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "RelationshipClass",
-            "name": "unit__node",
-            "object_classes": [
-                {"map_type": "column_header", "reference": 0},
-                {"map_type": "column_header", "reference": 1},
-            ],
-            "objects": [0, 1],
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_relationships_with_parameters(self):
-        input_data = [["unit", "node", "rel_parameter"], ["u1", "n1", 0], ["u1", "n2", 1]]
-        expected = {
-            "relationship_classes": [("unit__node", ["unit", "node"])],
-            "relationships": {("unit__node", ("u1", "n1")), ("unit__node", ("u1", "n2"))},
-            "relationship_parameters": [("unit__node", "rel_parameter")],
-            "relationship_parameter_values": [
-                ["unit__node", ["u1", "n1"], "rel_parameter", 0],
-                ["unit__node", ["u1", "n2"], "rel_parameter", 1],
-            ],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "RelationshipClass",
-            "name": "unit__node",
-            "object_classes": [
-                {"map_type": "column_header", "reference": 0},
-                {"map_type": "column_header", "reference": 1},
-            ],
-            "objects": [0, 1],
-            "parameters": {"map_type": "parameter", "name": {"map_type": "column_header", "reference": 2}, "value": 2},
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_relationships_with_parameters2(self):
-        input_data = [["nuts2", "Capacity", "Fueltype"], ["BE23", 268.0, "Bioenergy"], ["DE11", 14.0, "Bioenergy"]]
-        expected = {
-            "object_classes": {"nuts2", "fueltype"},
-            "objects": {("nuts2", "BE23"), ("fueltype", "Bioenergy"), ("nuts2", "DE11"), ("fueltype", "Bioenergy")},
-            "relationship_classes": [("nuts2__fueltype", ["nuts2", "fueltype"])],
-            "relationships": {("nuts2__fueltype", ("BE23", "Bioenergy")), ("nuts2__fueltype", ("DE11", "Bioenergy"))},
-            "relationship_parameters": [("nuts2__fueltype", "capacity")],
-            "relationship_parameter_values": [
-                ["nuts2__fueltype", ["BE23", "Bioenergy"], "capacity", 268.0],
-                ["nuts2__fueltype", ["DE11", "Bioenergy"], "capacity", 14.0],
-            ],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "RelationshipClass",
-            "name": {"map_type": "constant", "reference": "nuts2__fueltype"},
-            "parameters": {
-                "map_type": "parameter",
-                "name": {"map_type": "constant", "reference": "capacity"},
-                "parameter_type": "single value",
-                "value": {"map_type": "column", "reference": 1},
-            },
-            "skip_columns": [],
-            "read_start_row": 0,
-            "objects": [{"map_type": "column", "reference": 0}, {"map_type": "column", "reference": 2}],
-            "object_classes": [
-                {"map_type": "constant", "reference": "nuts2"},
-                {"map_type": "constant", "reference": "fueltype"},
-            ],
-            "import_objects": True,
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_parameter_header_with_only_one_parameter(self):
-        input_data = [["object", "parameter_name1"], ["obj1", 0], ["obj2", 2]]
-        expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1"), ("object", "obj2")},
-            "object_parameters": [("object", "parameter_name1")],
-            "object_parameter_values": [
-                ["object", "obj1", "parameter_name1", 0],
-                ["object", "obj2", "parameter_name1", 2],
-            ],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object",
-            "object": 0,
-            "parameters": {"map_type": "parameter", "name": {"map_type": "row", "reference": -1}},
-        }  # -1 to read pivot from header
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_pivoted_parameters_from_data_with_skipped_column(self):
-        input_data = [["object", "parameter_name1", "parameter_name2"], ["obj1", 0, 1], ["obj2", 2, 3]]
-        expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1"), ("object", "obj2")},
-            "object_parameters": [("object", "parameter_name1")],
-            "object_parameter_values": [
-                ["object", "obj1", "parameter_name1", 0],
-                ["object", "obj2", "parameter_name1", 2],
-            ],
-        }
-
-        data = iter(input_data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object",
-            "object": 0,
-            "skip_columns": [2],
-            "parameters": {"map_type": "parameter", "name": {"map_type": "row", "reference": 0}},
-        }  # -1 to read pivot from header
-
-        out, errors = get_mapped_data(data, [mapping])
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_relationships_and_import_objects(self):
-        input_data = [["unit", "node"], ["u1", "n1"], ["u2", "n2"]]
-        expected = {
-            "relationship_classes": [("unit__node", ["unit", "node"])],
-            "relationships": {("unit__node", ("u1", "n1")), ("unit__node", ("u2", "n2"))},
-            "object_classes": {"unit", "node"},
-            "objects": {("unit", "u1"), ("node", "n1"), ("unit", "u2"), ("node", "n2")},
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "RelationshipClass",
-            "name": "unit__node",
-            "object_classes": [
-                {"map_type": "column_header", "reference": 0},
-                {"map_type": "column_header", "reference": 1},
-            ],
-            "objects": [0, 1],
-            "import_objects": True,
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_relationships_parameter_values_with_extra_dimensions(self):
-        input_data = [["", "a", "b"], ["", "c", "d"], ["", "e", "f"], ["a", 2, 3], ["b", 4, 5]]
-
-        expected = {
-            "relationship_classes": [("unit__node", ["unit", "node"])],
-            "relationship_parameters": [("unit__node", "e"), ("unit__node", "f")],
-            "relationships": {("unit__node", ("a", "c")), ("unit__node", ("b", "d"))},
-            "relationship_parameter_values": [
-                ["unit__node", ("a", "c"), "e", Map(["a", "b"], [2, 4])],
-                ["unit__node", ("b", "d"), "f", Map(["a", "b"], [3, 5])],
-            ],
-        }
-
-        data = iter(input_data)
-        data_header = []
-
-        mapping = {
-            "map_type": "RelationshipClass",
-            "name": "unit__node",
-            "object_classes": ["unit", "node"],
-            "objects": [{"map_type": "row", "reference": i} for i in range(2)],
-            "parameters": {
-                "map_type": "parameter",
-                "parameter_type": "map",
-                "name": {"map_type": "row", "reference": 2},
-                "extra_dimensions": [0],
-            },
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_data_with_read_start_row(self):
-        input_data = [
-            ["object_class", "object", "parameter", "value"],
-            [" ", " ", " ", " "],
-            ["oc1", "obj1", "parameter_name1", 1],
-            ["oc2", "obj2", "parameter_name2", 2],
-        ]
-        expected = {
-            "object_classes": {"oc1", "oc2"},
-            "objects": {("oc1", "obj1"), ("oc2", "obj2")},
-            "object_parameters": [("oc1", "parameter_name1"), ("oc2", "parameter_name2")],
-            "object_parameter_values": [["oc1", "obj1", "parameter_name1", 1], ["oc2", "obj2", "parameter_name2", 2]],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": 0,
-            "object": 1,
-            "parameters": {"map_type": "parameter", "name": 2, "value": 3},
-            "read_start_row": 1,
-        }
-
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_data_with_two_mappings_with_different_read_start_row(self):
-        input_data = [
-            ["oc1", "oc2", "parameter_class1", "parameter_class2"],
-            [" ", " ", " ", " "],
-            ["oc1_obj1", "oc2_obj1", 1, 3],
-            ["oc1_obj2", "oc2_obj2", 2, 4],
-        ]
-        expected = {
-            "object_classes": {"oc1", "oc2"},
-            "objects": {("oc1", "oc1_obj1"), ("oc1", "oc1_obj2"), ("oc2", "oc2_obj2")},
-            "object_parameters": [("oc1", "parameter_class1"), ("oc2", "parameter_class2")],
-            "object_parameter_values": [
-                ["oc1", "oc1_obj1", "parameter_class1", 1],
-                ["oc1", "oc1_obj2", "parameter_class1", 2],
-                ["oc2", "oc2_obj2", "parameter_class2", 4],
-            ],
-        }
-
-        data = iter(input_data)
-        data_header = next(data)
-
-        mapping1 = {
-            "map_type": "ObjectClass",
-            "name": {"map_type": "column_header", "reference": 0},
-            "object": 0,
-            "parameters": {"map_type": "parameter", "name": {"map_type": "column_header", "reference": 2}, "value": 2},
-            "read_start_row": 1,
-        }
-        mapping2 = {
-            "map_type": "ObjectClass",
-            "name": {"map_type": "column_header", "reference": 1},
-            "object": 1,
-            "parameters": {"map_type": "parameter", "name": {"map_type": "column_header", "reference": 3}, "value": 3},
-            "read_start_row": 2,
-        }
-
-        out, errors = get_mapped_data(data, [mapping1, mapping2], data_header)
-        self.assertEqual(errors, [])
-        self.assertEqual(out, expected)
-
-    def test_read_object_class_with_table_name_as_class_name(self):
-        input_data = [["Object names"], ["object 1"], ["object 2"]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": {"map_type": "table_name", "reference": "class name"},
-            "object": 0,
-        }
-        out, errors = get_mapped_data(data, [mapping], data_header, "class name")
-        expected = {
-            "object_classes": {"class name"},
-            "objects": {("class name", "object 1"), ("class name", "object 2")},
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_flat_map_from_columns(self):
-        input_data = [["Index", "Value"], ["key1", -2], ["key2", -1]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object_class",
-            "object": "object",
-            "parameters": {
-                "name": "parameter",
-                "parameter_type": "map",
-                "value": 1,
-                "compress": False,
-                "extra_dimensions": [0],
-            },
-        }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected_map = Map(["key1", "key2"], [-2, -1])
-        expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_nested_map_from_columns(self):
-        input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object_class",
-            "object": "object",
-            "parameters": {
-                "name": "parameter",
-                "parameter_type": "map",
-                "value": 2,
-                "compress": False,
-                "extra_dimensions": [0, 1],
-            },
-        }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected_map = Map(["key11", "key21"], [Map(["key12"], [-2]), Map(["key22"], [-1])])
-        expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_uneven_nested_map_from_columns(self):
-        input_data = [
-            ["Index", "A", "B", "C"],
-            ["key1", "key11", -2, ""],
-            ["key1", "key12", -1, ""],
-            ["key2", -23, "", ""],
-            ["key3", -33, "", ""],
-            ["key4", "key31", "key311", 50],
-            ["key4", "key31", "key312", 51],
-            ["key4", "key32", 66, ""],
-        ]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object_class",
-            "object": "object",
-            "parameters": {
-                "name": "parameter",
-                "parameter_type": "map",
-                "value": 3,
-                "compress": False,
-                "extra_dimensions": [0, 1, 2],
-            },
-        }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected_map = Map(
-            ["key1", "key2", "key3", "key4"],
-            [
-                Map(["key11", "key12"], [-2, -1]),
-                -23,
-                -33,
-                Map(["key31", "key32"], [Map(["key311", "key312"], [50, 51]), 66]),
-            ],
-        )
-        expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_nested_map_with_compression(self):
-        input_data = [
-            ["Index 1", "Time stamp", "Value"],
-            ["key", DateTime("2020-09-10T08:00"), -2.0],
-            ["key", DateTime("2020-09-11T08:00"), -1.0],
-        ]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object_class",
-            "object": "object",
-            "parameters": {
-                "name": "parameter",
-                "parameter_type": "map",
-                "value": 2,
-                "compress": True,
-                "extra_dimensions": [0, 1],
-            },
-        }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected_map = Map(
-            ["key"],
-            [
-                TimeSeriesVariableResolution(
-                    ["2020-09-10T08:00", "2020-09-11T08:00"],
-                    [-2.0, -1.0],
-                    False,
-                    False,
-                    index_name=Map.DEFAULT_INDEX_NAME,
-                )
-            ],
-        )
-        expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_alternative(self):
-        input_data = [["Alternatives"], ["alternative1"], ["second_alternative"], ["last_one"]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {"map_type": "Alternative", "name": 0}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"alternatives": {"alternative1", "second_alternative", "last_one"}}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_scenario(self):
-        input_data = [["Scenarios"], ["scenario1"], ["second_scenario"], ["last_one"]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {"map_type": "Scenario", "name": 0}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"scenarios": {("scenario1", False), ("second_scenario", False), ("last_one", False)}}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_scenario_with_active_flags(self):
-        input_data = [["Scenarios", "Active"], ["scenario1", 1], ["second_scenario", "f"], ["last_one", "true"]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {"map_type": "Scenario", "name": 0, "active": 1}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"scenarios": {("scenario1", True), ("second_scenario", False), ("last_one", True)}}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_scenario_alternative(self):
-        input_data = [
-            ["Scenario", "Alternative", "Before alternative"],
-            ["scenario_A", "alternative1", "second_alternative"],
-            ["scenario_A", "second_alternative", "last_one"],
-            ["scenario_B", "last_one", ""],
-        ]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {
-            "map_type": "ScenarioAlternative",
-            "scenario_name": 0,
-            "alternative_name": 1,
-            "before_alternative_name": 2,
-        }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = dict()
-        expected["scenario_alternatives"] = [
-            ["scenario_A", "alternative1", "second_alternative"],
-            ["scenario_A", "second_alternative", "last_one"],
-            ["scenario_B", "last_one", ""],
-        ]
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_pivoted_scenario_alternative(self):
-        input_data = [["scenario_A", "scenario_B"], ["first_alternative", "Base"], ["second_alternative", ""]]
-        data = iter(input_data)
-        mappings = [{"map_type": "Scenario", "position": -1}, {"map_type": "ScenarioAlternative", "position": "hidden"}]
-        out, errors = get_mapped_data(data, [mappings])
-        expected = dict()
-        expected["scenario_alternatives"] = [
-            ["scenario_A", "first_alternative"],
-            ["scenario_A", "second_alternative"],
-            ["scenario_B", "Base"],
-        ]
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_tool(self):
-        input_data = [["Tools"], ["tool1"], ["second_tool"], ["last_one"]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {"map_type": "Tool", "name": 0}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"tools": {"tool1", "second_tool", "last_one"}}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_feature(self):
-        input_data = [["Class", "Parameter"], ["class1", "param1"], ["class2", "param2"]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {"map_type": "Feature", "entity_class_name": 0, "parameter_definition_name": 1}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"features": {("class1", "param1"), ("class2", "param2")}}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_tool_feature(self):
-        input_data = [["Tool", "Class", "Parameter"], ["tool1", "class1", "param1"], ["tool2", "class2", "param2"]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {"map_type": "ToolFeature", "name": 0, "entity_class_name": 1, "parameter_definition_name": 2}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"tool_features": [["tool1", "class1", "param1", False], ["tool2", "class2", "param2", False]]}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_tool_feature_with_required_flag(self):
-        input_data = [
-            ["Tool", "Class", "Parameter", "Required"],
-            ["tool1", "class1", "param1", "f"],
-            ["tool2", "class2", "param2", "true"],
-        ]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {
-            "map_type": "ToolFeature",
-            "name": 0,
-            "entity_class_name": 1,
-            "parameter_definition_name": 2,
-            "required": 3,
-        }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"tool_features": [["tool1", "class1", "param1", False], ["tool2", "class2", "param2", True]]}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_tool_feature_method(self):
-        input_data = [
-            ["Tool", "Class", "Parameter", "Method"],
-            ["tool1", "class1", "param1", "meth1"],
-            ["tool2", "class2", "param2", "meth2"],
-        ]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {
-            "map_type": "ToolFeatureMethod",
-            "name": 0,
-            "entity_class_name": 1,
-            "parameter_definition_name": 2,
-            "method": 3,
-        }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = dict()
-        expected["tool_feature_methods"] = [
-            ["tool1", "class1", "param1", "meth1"],
-            ["tool2", "class2", "param2", "meth2"],
-        ]
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_object_group_without_parameters(self):
-        input_data = [
-            ["Object Class", "Group", "Object"],
-            ["class_A", "group1", "object1"],
-            ["class_A", "group1", "object2"],
-            ["class_A", "group2", "object3"],
-        ]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {"map_type": "ObjectGroup", "name": 0, "groups": 1, "members": 2}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = dict()
-        expected["object_classes"] = {"class_A"}
-        expected["object_groups"] = {
-            ("class_A", "group1", "object1"),
-            ("class_A", "group1", "object2"),
-            ("class_A", "group2", "object3"),
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_object_group_and_import_objects(self):
-        input_data = [
-            ["Object Class", "Group", "Object"],
-            ["class_A", "group1", "object1"],
-            ["class_A", "group1", "object2"],
-            ["class_A", "group2", "object3"],
-        ]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {"map_type": "ObjectGroup", "name": 0, "groups": 1, "members": 2, "import_objects": True}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = dict()
-        expected["object_groups"] = {
-            ("class_A", "group1", "object1"),
-            ("class_A", "group1", "object2"),
-            ("class_A", "group2", "object3"),
-        }
-        expected["object_classes"] = {"class_A"}
-        expected["objects"] = {
-            ("class_A", "group1"),
-            ("class_A", "object1"),
-            ("class_A", "group1"),
-            ("class_A", "object2"),
-            ("class_A", "group2"),
-            ("class_A", "object3"),
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_parameter_definition_with_default_values_and_value_lists(self):
-        input_data = [
-            ["Class", "Parameter", "Default", "Value list"],
-            ["class_A", "param1", 23.0, "listA"],
-            ["class_A", "param2", 42.0, "listB"],
-            ["class_B", "param3", 5.0, "listA"],
-        ]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": 0,
-            "parameters": {
-                "name": 1,
-                "map_type": "ParameterDefinition",
-                "default_value": {"value_type": "single value", "main_value": 2},
-                "parameter_value_list_name": 3,
-            },
-        }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = dict()
-        expected["object_classes"] = {"class_A", "class_A", "class_B"}
-        expected["object_parameters"] = [
-            ("class_A", "param1", 23.0, "listA"),
-            ("class_A", "param2", 42.0, "listB"),
-            ("class_B", "param3", 5.0, "listA"),
-        ]
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_map_as_default_parameter_value(self):
-        input_data = [["key1", -2.3], ["key2", 5.5], ["key3", 3.2]]
-        data = iter(input_data)
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object_class",
-            "parameters": {
-                "name": "parameter",
-                "map_type": "ParameterDefinition",
-                "default_value": {"value_type": "map", "main_value": 1, "compress": False, "extra_dimensions": [0]},
-            },
-        }
-        out, errors = get_mapped_data(data, [mapping])
-        expected_map = Map(["key1", "key2", "key3"], [-2.3, 5.5, 3.2])
-        expected = {
-            "object_classes": {"object_class"},
-            "object_parameters": [("object_class", "parameter", expected_map)],
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_parameter_definition_with_nested_map_as_default_value(self):
-        input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping = {
-            "map_type": "ObjectClass",
-            "name": "object_class",
-            "parameters": {
-                "name": "parameter",
-                "map_type": "ParameterDefinition",
-                "default_value": {"value_type": "map", "main_value": 2, "compress": False, "extra_dimensions": [0, 1]},
-            },
-        }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected_map = Map(["key11", "key21"], [Map(["key12"], [-2]), Map(["key22"], [-1])])
-        expected = {
-            "object_classes": {"object_class"},
-            "object_parameters": [("object_class", "parameter", expected_map)],
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_map_index_names_from_columns(self):
-        input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping_root = unflatten(
-            [
-                ObjectClassMapping(Position.hidden, value="object_class"),
-                ParameterDefinitionMapping(Position.hidden, value="parameter"),
-                ObjectMapping(Position.hidden, value="object"),
-                ParameterValueTypeMapping(Position.hidden, value="map"),
-                IndexNameMapping(Position.header, value=0),
-                ParameterValueIndexMapping(0),
-                IndexNameMapping(Position.header, value=1),
-                ParameterValueIndexMapping(1),
-                ExpandedParameterValueMapping(2),
-            ]
-        )
-        out, errors = get_mapped_data(data, [mapping_root], data_header)
-        expected_map = Map(
-            ["key11", "key21"],
-            [Map(["key12"], [-2], index_name="Index 2"), Map(["key22"], [-1], index_name="Index 2")],
-            index_name="Index 1",
-        )
-        expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_missing_map_index_name(self):
-        input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping_root = unflatten(
-            [
-                ObjectClassMapping(Position.hidden, value="object_class"),
-                ParameterDefinitionMapping(Position.hidden, value="parameter"),
-                ObjectMapping(Position.hidden, value="object"),
-                ParameterValueTypeMapping(Position.hidden, value="map"),
-                IndexNameMapping(Position.hidden, value=None),
-                ParameterValueIndexMapping(0),
-                IndexNameMapping(Position.header, value=1),
-                ParameterValueIndexMapping(1),
-                ExpandedParameterValueMapping(2),
-            ]
-        )
-        out, errors = get_mapped_data(data, [mapping_root], data_header)
-        expected_map = Map(
-            ["key11", "key21"],
-            [Map(["key12"], [-2], index_name="Index 2"), Map(["key22"], [-1], index_name="Index 2")],
-            index_name="",
-        )
-        expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_read_default_value_index_names_from_columns(self):
-        input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
-        data = iter(input_data)
-        data_header = next(data)
-        mapping_root = unflatten(
-            [
-                ObjectClassMapping(Position.hidden, value="object_class"),
-                ParameterDefinitionMapping(Position.hidden, value="parameter"),
-                ParameterDefaultValueTypeMapping(Position.hidden, value="map"),
-                DefaultValueIndexNameMapping(Position.header, value=0),
-                ParameterDefaultValueIndexMapping(0),
-                DefaultValueIndexNameMapping(Position.header, value=1),
-                ParameterDefaultValueIndexMapping(1),
-                ExpandedParameterDefaultValueMapping(2),
-            ]
-        )
-        out, errors = get_mapped_data(data, [mapping_root], data_header)
-        expected_map = Map(
-            ["key11", "key21"],
-            [Map(["key12"], [-2], index_name="Index 2"), Map(["key22"], [-1], index_name="Index 2")],
-            index_name="Index 1",
-        )
-        expected = {
-            "object_classes": {"object_class"},
-            "object_parameters": [("object_class", "parameter", expected_map)],
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_filter_regular_expression_in_root_mapping(self):
-        input_data = [["A", "p"], ["A", "q"], ["B", "r"]]
-        data = iter(input_data)
-        mapping_root = unflatten([ObjectClassMapping(0, filter_re="B"), ObjectMapping(1)])
-        out, errors = get_mapped_data(data, [mapping_root])
-        expected = {"object_classes": {"B"}, "objects": {("B", "r")}}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_filter_regular_expression_in_child_mapping(self):
-        input_data = [["A", "p"], ["A", "q"], ["B", "r"]]
-        data = iter(input_data)
-        mapping_root = unflatten([ObjectClassMapping(0), ObjectMapping(1, filter_re="q|r")])
-        out, errors = get_mapped_data(data, [mapping_root])
-        expected = {"object_classes": {"A", "B"}, "objects": {("A", "q"), ("B", "r")}}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_filter_regular_expression_in_child_mapping_filters_parent_mappings_too(self):
-        input_data = [["A", "p"], ["A", "q"], ["B", "r"]]
-        data = iter(input_data)
-        mapping_root = unflatten([ObjectClassMapping(0), ObjectMapping(1, filter_re="q")])
-        out, errors = get_mapped_data(data, [mapping_root])
-        expected = {"object_classes": {"A"}, "objects": {("A", "q")}}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-    def test_arrays_get_imported_to_correct_alternatives(self):
-        input_data = [["Base", "y", "p1"], ["alternative", "y", "p1"]]
-        data = iter(input_data)
-        mapping_root = unflatten(
-            [
-                ObjectClassMapping(Position.hidden, value="class"),
-                ObjectMapping(1),
-                ParameterDefinitionMapping(Position.hidden, value="parameter"),
-                AlternativeMapping(0),
-                ParameterValueTypeMapping(Position.hidden, value="array"),
-                ExpandedParameterValueMapping(2),
-            ]
-        )
-        out, errors = get_mapped_data(data, [mapping_root])
-        expected = {
-            "object_classes": {"class"},
-            "objects": {("class", "y")},
-            "object_parameters": [("class", "parameter")],
-            "alternatives": {"Base", "alternative"},
-            "object_parameter_values": [
-                ["class", "y", "parameter", Array(["p1"]), "Base"],
-                ["class", "y", "parameter", Array(["p1"]), "alternative"],
-            ],
-        }
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
-
-
-class TestHasFilter(unittest.TestCase):
-    def test_mapping_without_filter_doesnt_have_filter(self):
-        mapping = ObjectClassMapping(0)
-        self.assertFalse(mapping.has_filter())
-
-    def test_hidden_mapping_without_value_doesnt_have_filter(self):
-        mapping = ObjectClassMapping(Position.hidden, filter_re="a")
-        self.assertFalse(mapping.has_filter())
-
-    def test_hidden_mapping_with_value_has_filter(self):
-        mapping = ObjectClassMapping(0, value="a", filter_re="b")
-        self.assertTrue(mapping.has_filter())
-
-    def test_mapping_without_value_has_filter(self):
-        mapping = ObjectClassMapping(Position.hidden, value="a", filter_re="b")
-        self.assertTrue(mapping.has_filter())
-
-    def test_mapping_with_value_but_without_filter_doesnt_have_filter(self):
-        mapping = ObjectClassMapping(0, value="a")
-        self.assertFalse(mapping.has_filter())
-
-    def test_child_mapping_with_filter_has_filter(self):
-        mapping = ObjectClassMapping(0)
-        mapping.child = ObjectMapping(1, filter_re="a")
-        self.assertTrue(mapping.has_filter())
-
-    def test_child_mapping_without_filter_doesnt_have_filter(self):
-        mapping = ObjectClassMapping(0)
-        mapping.child = ObjectMapping(1)
-        self.assertFalse(mapping.has_filter())
-
-
-class TestIsPivoted(unittest.TestCase):
-    def test_pivoted_position_returns_true(self):
-        mapping = AlternativeMapping(-1)
-        self.assertTrue(mapping.is_pivoted())
-
-    def test_recursively_returns_false_when_all_mappings_are_non_pivoted(self):
-        mapping = unflatten([AlternativeMapping(0), ParameterValueMapping(1)])
-        self.assertFalse(mapping.is_pivoted())
-
-    def test_returns_true_when_position_is_header_and_has_child(self):
-        mapping = unflatten([AlternativeMapping(Position.header), ParameterValueMapping(0)])
-        self.assertTrue(mapping.is_pivoted())
-
-    def test_returns_false_when_position_is_header_and_is_leaf(self):
-        mapping = unflatten([AlternativeMapping(0), ParameterValueMapping(Position.header)])
-        self.assertFalse(mapping.is_pivoted())
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for import Mappings.
+
+"""
+import unittest
+from unittest.mock import Mock
+from spinedb_api.exception import InvalidMapping
+from spinedb_api.mapping import Position, to_dict as mapping_to_dict, unflatten
+from spinedb_api.import_mapping.import_mapping import (
+    ImportMapping,
+    check_validity,
+    ParameterDefinitionMapping,
+    ObjectClassMapping,
+    ObjectMapping,
+    IndexNameMapping,
+    ParameterValueIndexMapping,
+    ExpandedParameterValueMapping,
+    ParameterValueMapping,
+    ParameterValueTypeMapping,
+    ParameterDefaultValueTypeMapping,
+    DefaultValueIndexNameMapping,
+    ParameterDefaultValueIndexMapping,
+    ExpandedParameterDefaultValueMapping,
+    AlternativeMapping,
+)
+from spinedb_api.import_mapping.import_mapping_compat import (
+    import_mapping_from_dict,
+    parameter_mapping_from_dict,
+    parameter_value_mapping_from_dict,
+)
+from spinedb_api.import_mapping.type_conversion import BooleanConvertSpec, StringConvertSpec, FloatConvertSpec
+from spinedb_api.import_mapping.generator import get_mapped_data
+from spinedb_api.parameter_value import Array, DateTime, TimeSeriesVariableResolution, Map
+
+
+class TestConvertFunctions(unittest.TestCase):
+    def test_convert_functions_float(self):
+        data = [["a", "1.2"]]
+        column_convert_fns = {0: str, 1: FloatConvertSpec()}
+        mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        mapping.position = 0
+        mapping.child.value = "obj"
+        mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
+            {"map_type": "ParameterDefinition"}
+        )
+        param_def_mapping.value = "param"
+        param_def_mapping.flatten()[-1].position = 1
+        mapped_data, _ = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
+        expected = {'object_classes': {'a'}, 'objects': {('a', 'obj')}, 'object_parameters': [('a', 'param', 1.2)]}
+        self.assertEqual(mapped_data, expected)
+
+    def test_convert_functions_str(self):
+        data = [["a", '"1111.2222"']]
+        column_convert_fns = {0: str, 1: StringConvertSpec()}
+        mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        mapping.position = 0
+        mapping.child.value = "obj"
+        mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
+            {"map_type": "ParameterDefinition"}
+        )
+        param_def_mapping.value = "param"
+        param_def_mapping.flatten()[-1].position = 1
+        mapped_data, _ = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
+        expected = {
+            'object_classes': {'a'},
+            'objects': {('a', 'obj')},
+            'object_parameters': [('a', 'param', '1111.2222')],
+        }
+        self.assertEqual(mapped_data, expected)
+
+    def test_convert_functions_bool(self):
+        data = [["a", "false"]]
+        column_convert_fns = {0: str, 1: BooleanConvertSpec()}
+        mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        mapping.position = 0
+        mapping.child.value = "obj"
+        mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
+            {"map_type": "ParameterDefinition"}
+        )
+        param_def_mapping.value = "param"
+        param_def_mapping.flatten()[-1].position = 1
+        mapped_data, _ = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
+        expected = {'object_classes': {'a'}, 'objects': {('a', 'obj')}, 'object_parameters': [('a', 'param', False)]}
+        self.assertEqual(mapped_data, expected)
+
+    def test_convert_functions_with_error(self):
+        data = [["a", "not a float"]]
+        column_convert_fns = {0: str, 1: FloatConvertSpec()}
+        mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        mapping.position = 0
+        mapping.child.value = "obj"
+        mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
+            {"map_type": "ParameterDefinition"}
+        )
+        param_def_mapping.value = "param"
+        param_def_mapping.flatten()[-1].position = 1
+        _, errors = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
+        self.assertEqual(len(errors), 1)
+
+
+class TestPolishImportMapping(unittest.TestCase):
+    def test_polish_null_mapping(self):
+        mapping = ImportMapping(Position.hidden, value=None)
+        table_name = "tablename"
+        header = ["A", "B", "C"]
+        mapping.polish(table_name, header)
+        self.assertEqual(mapping.position, Position.hidden)
+        self.assertIsNone(mapping.value)
+
+    def test_polish_column_mapping(self):
+        mapping = ImportMapping("B", value=None)
+        table_name = "tablename"
+        header = ["A", "B", "C"]
+        mapping.polish(table_name, header)
+        self.assertEqual(mapping.position, 1)
+        self.assertIsNone(mapping.value)
+
+    def test_polish_column_header_mapping(self):
+        mapping = ImportMapping(Position.header, value=2)
+        table_name = "tablename"
+        header = ["A", "B", "C"]
+        mapping.polish(table_name, header)
+        self.assertEqual(mapping.position, Position.header)
+        self.assertEqual(mapping.value, "C")
+
+    def test_polish_column_header_mapping_str(self):
+        mapping = ImportMapping(Position.header, value="2")
+        table_name = "tablename"
+        header = ["A", "B", "C"]
+        mapping.polish(table_name, header)
+        self.assertEqual(mapping.position, Position.header)
+        self.assertEqual(mapping.value, "C")
+
+    def test_polish_column_header_mapping_duplicates(self):
+        mapping = ImportMapping(Position.header, value=3)
+        table_name = "tablename"
+        header = ["A", "B", "C", "A"]
+        mapping.polish(table_name, header, for_preview=True)
+        self.assertEqual(mapping.position, Position.header)
+        self.assertEqual(mapping.value, 3)
+
+    def test_polish_column_header_mapping_invalid_header(self):
+        mapping = ImportMapping(Position.header, value="D")
+        table_name = "tablename"
+        header = ["A", "B", "C"]
+        with self.assertRaises(InvalidMapping):
+            mapping.polish(table_name, header)
+
+    def test_polish_column_header_mapping_invalid_index(self):
+        mapping = ImportMapping(Position.header, value=4)
+        table_name = "tablename"
+        header = ["A", "B", "C"]
+        with self.assertRaises(InvalidMapping):
+            mapping.polish(table_name, header)
+
+    def test_polish_table_name_mapping(self):
+        mapping = ImportMapping(Position.table_name)
+        table_name = "tablename"
+        header = ["A", "B", "C"]
+        mapping.polish(table_name, header)
+        self.assertEqual(mapping.position, Position.table_name)
+        self.assertEqual(mapping.value, "tablename")
+
+    def test_polish_row_header_mapping(self):
+        mapping = ImportMapping(Position.header, value=None)
+        table_name = "tablename"
+        header = ["A", "B", "C"]
+        mapping.polish(table_name, header)
+        self.assertEqual(mapping.position, Position.header)
+        self.assertIsNone(mapping.value)
+
+
+class TestImportMappingIO(unittest.TestCase):
+    def test_object_class_mapping(self):
+        mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        d = mapping_to_dict(mapping)
+        types = [m["map_type"] for m in d]
+        expected = ['ObjectClass', 'Object', 'ObjectMetadata']
+        self.assertEqual(types, expected)
+
+    def test_relationship_class_mapping(self):
+        mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        d = mapping_to_dict(mapping)
+        types = [m["map_type"] for m in d]
+        expected = [
+            'RelationshipClass',
+            'RelationshipClassObjectClass',
+            'Relationship',
+            'RelationshipObject',
+            'RelationshipMetadata',
+        ]
+        self.assertEqual(types, expected)
+
+    def test_object_group_mapping(self):
+        mapping = import_mapping_from_dict({"map_type": "ObjectGroup"})
+        d = mapping_to_dict(mapping)
+        types = [m["map_type"] for m in d]
+        expected = ['ObjectClass', 'Object', 'ObjectGroup']
+        self.assertEqual(types, expected)
+
+    def test_alternative_mapping(self):
+        mapping = import_mapping_from_dict({"map_type": "Alternative"})
+        d = mapping_to_dict(mapping)
+        types = [m["map_type"] for m in d]
+        expected = ['Alternative']
+        self.assertEqual(types, expected)
+
+    def test_scenario_mapping(self):
+        mapping = import_mapping_from_dict({"map_type": "Scenario"})
+        d = mapping_to_dict(mapping)
+        types = [m["map_type"] for m in d]
+        expected = ['Scenario', 'ScenarioActiveFlag']
+        self.assertEqual(types, expected)
+
+    def test_scenario_alternative_mapping(self):
+        mapping = import_mapping_from_dict({"map_type": "ScenarioAlternative"})
+        d = mapping_to_dict(mapping)
+        types = [m["map_type"] for m in d]
+        expected = ['Scenario', 'ScenarioAlternative', 'ScenarioBeforeAlternative']
+        self.assertEqual(types, expected)
+
+    def test_tool_mapping(self):
+        mapping = import_mapping_from_dict({"map_type": "Tool"})
+        d = mapping_to_dict(mapping)
+        types = [m["map_type"] for m in d]
+        expected = ['Tool']
+        self.assertEqual(types, expected)
+
+    def test_tool_feature_mapping(self):
+        mapping = import_mapping_from_dict({"map_type": "ToolFeature"})
+        d = mapping_to_dict(mapping)
+        types = [m["map_type"] for m in d]
+        expected = ['Tool', 'ToolFeatureEntityClass', 'ToolFeatureParameterDefinition', 'ToolFeatureRequiredFlag']
+        self.assertEqual(types, expected)
+
+    def test_tool_feature_method_mapping(self):
+        mapping = import_mapping_from_dict({"map_type": "ToolFeatureMethod"})
+        d = mapping_to_dict(mapping)
+        types = [m["map_type"] for m in d]
+        expected = [
+            'Tool',
+            'ToolFeatureMethodEntityClass',
+            'ToolFeatureMethodParameterDefinition',
+            'ToolFeatureMethodMethod',
+        ]
+        self.assertEqual(types, expected)
+
+    def test_parameter_value_list_mapping(self):
+        mapping = import_mapping_from_dict({"map_type": "ParameterValueList"})
+        d = mapping_to_dict(mapping)
+        types = [m["map_type"] for m in d]
+        expected = ['ParameterValueList', 'ParameterValueListValue']
+        self.assertEqual(types, expected)
+
+
+class TestImportMappingLegacy(unittest.TestCase):
+    def test_ObjectClass_to_dict_from_dict(self):
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": 0,
+            "objects": 1,
+            "parameters": {"map_type": "parameter", "name": 2, "value": 3, "parameter_type": "single value"},
+        }
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'ObjectClass', 'position': 0},
+            {'map_type': 'Object', 'position': 1},
+            {'map_type': 'ObjectMetadata', 'position': 'hidden'},
+            {'map_type': 'ParameterDefinition', 'position': 2},
+            {'map_type': 'Alternative', 'position': 'hidden'},
+            {'map_type': 'ParameterValueMetadata', 'position': 'hidden'},
+            {'map_type': 'ParameterValue', 'position': 3},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_ObjectClass_object_from_dict_to_dict(self):
+        mapping = {"map_type": "ObjectClass", "name": 0, "objects": 1}
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'ObjectClass', 'position': 0},
+            {'map_type': 'Object', 'position': 1},
+            {'map_type': 'ObjectMetadata', 'position': 'hidden'},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_ObjectClass_object_from_dict_to_dict2(self):
+        mapping = {"map_type": "ObjectClass", "name": "cls", "objects": "obj"}
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'ObjectClass', 'position': 'hidden', 'value': 'cls'},
+            {'map_type': 'Object', 'position': 'hidden', 'value': 'obj'},
+            {'map_type': 'ObjectMetadata', 'position': 'hidden'},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_RelationshipClassMapping_from_dict_to_dict(self):
+        mapping = {
+            "map_type": "RelationshipClass",
+            "name": "unit__node",
+            "object_classes": [0, 1],
+            "objects": [0, 1],
+            "parameters": {"map_type": "parameter", "name": "pname", "value": 2},
+        }
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'RelationshipClass', 'position': 'hidden', 'value': 'unit__node'},
+            {'map_type': 'RelationshipClassObjectClass', 'position': 0},
+            {'map_type': 'RelationshipClassObjectClass', 'position': 1},
+            {'map_type': 'Relationship', 'position': 'hidden', 'value': 'relationship'},
+            {'map_type': 'RelationshipObject', 'position': 0},
+            {'map_type': 'RelationshipObject', 'position': 1},
+            {'map_type': 'RelationshipMetadata', 'position': 'hidden'},
+            {'map_type': 'ParameterDefinition', 'position': 'hidden', 'value': 'pname'},
+            {'map_type': 'Alternative', 'position': 'hidden'},
+            {'map_type': 'ParameterValueMetadata', 'position': 'hidden'},
+            {'map_type': 'ParameterValue', 'position': 2},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_RelationshipClassMapping_from_dict_to_dict2(self):
+        mapping = {
+            "map_type": "RelationshipClass",
+            "name": "unit__node",
+            "object_classes": ["cls", 0],
+            "objects": ["obj", 0],
+        }
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'RelationshipClass', 'position': 'hidden', 'value': 'unit__node'},
+            {'map_type': 'RelationshipClassObjectClass', 'position': 'hidden', 'value': 'cls'},
+            {'map_type': 'RelationshipClassObjectClass', 'position': 0},
+            {'map_type': 'Relationship', 'position': 'hidden', 'value': 'relationship'},
+            {'map_type': 'RelationshipObject', 'position': 'hidden', 'value': 'obj'},
+            {'map_type': 'RelationshipObject', 'position': 0},
+            {'map_type': 'RelationshipMetadata', 'position': 'hidden'},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_RelationshipClassMapping_from_dict_to_dict3(self):
+        mapping = {
+            "map_type": "RelationshipClass",
+            "name": "unit__node",
+            "parameters": {
+                "map_type": "parameter",
+                "name": "pname",
+                "value": 2,
+                "parameter_type": "array",
+                "extra_dimensions": ["dim"],
+            },
+        }
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'RelationshipClass', 'position': 'hidden', 'value': 'unit__node'},
+            {'map_type': 'RelationshipClassObjectClass', 'position': 'hidden'},
+            {'map_type': 'Relationship', 'position': 'hidden', 'value': 'relationship'},
+            {'map_type': 'RelationshipObject', 'position': 'hidden'},
+            {'map_type': 'RelationshipMetadata', 'position': 'hidden'},
+            {'map_type': 'ParameterDefinition', 'position': 'hidden', 'value': 'pname'},
+            {'map_type': 'Alternative', 'position': 'hidden'},
+            {'map_type': 'ParameterValueMetadata', 'position': 'hidden'},
+            {'map_type': 'ParameterValueType', 'position': 'hidden', 'value': 'array'},
+            {'map_type': 'IndexName', 'position': 'hidden'},
+            {'map_type': 'ParameterValueIndex', 'position': 'hidden', 'value': 'dim'},
+            {'map_type': 'ExpandedValue', 'position': 2},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_ObjectGroupMapping_to_dict_from_dict(self):
+        mapping = {
+            "map_type": "ObjectGroup",
+            "name": 0,
+            "groups": 1,
+            "members": 2,
+            "parameters": {
+                "map_type": "parameter",
+                "name": {"map_type": "constant", "reference": "pname"},
+                "parameter_value_metadata": {"map_type": "None"},
+                "parameter_type": "single value",
+                "value": {"reference": 2, "map_type": "column"},
+            },
+        }
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'ObjectClass', 'position': 0},
+            {'map_type': 'Object', 'position': 1},
+            {'map_type': 'ObjectGroup', 'position': 2},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_Alternative_to_dict_from_dict(self):
+        mapping = {"map_type": "Alternative", "name": 0}
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [{'map_type': 'Alternative', 'position': 0}]
+        self.assertEqual(out, expected)
+
+    def test_Scenario_to_dict_from_dict(self):
+        mapping = {"map_type": "Scenario", "name": 0}
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'Scenario', 'position': 0},
+            {'map_type': 'ScenarioActiveFlag', 'position': 'hidden', 'value': 'false'},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_ScenarioAlternative_to_dict_from_dict(self):
+        mapping = {
+            "map_type": "ScenarioAlternative",
+            "scenario_name": 0,
+            "alternative_name": 1,
+            "before_alternative_name": 2,
+        }
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'Scenario', 'position': 0},
+            {'map_type': 'ScenarioAlternative', 'position': 1},
+            {'map_type': 'ScenarioBeforeAlternative', 'position': 2},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_Tool_to_dict_from_dict(self):
+        mapping = {"map_type": "Tool", "name": 0}
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [{'map_type': 'Tool', 'position': 0}]
+        self.assertEqual(out, expected)
+
+    def test_Feature_to_dict_from_dict(self):
+        mapping = {"map_type": "Feature", "entity_class_name": 0, "parameter_definition_name": 1}
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'FeatureEntityClass', 'position': 0},
+            {'map_type': 'FeatureParameterDefinition', 'position': 1},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_ToolFeature_to_dict_from_dict(self):
+        mapping = {"map_type": "ToolFeature", "name": 0, "entity_class_name": 1, "parameter_definition_name": 2}
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'Tool', 'position': 0},
+            {'map_type': 'ToolFeatureEntityClass', 'position': 1},
+            {'map_type': 'ToolFeatureParameterDefinition', 'position': 2},
+            {'map_type': 'ToolFeatureRequiredFlag', 'position': 'hidden', 'value': 'false'},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_ToolFeatureMethod_to_dict_from_dict(self):
+        mapping = {
+            "map_type": "ToolFeatureMethod",
+            "name": 0,
+            "entity_class_name": 1,
+            "parameter_definition_name": 2,
+            "method": 3,
+        }
+        mapping = import_mapping_from_dict(mapping)
+        out = mapping_to_dict(mapping)
+        expected = [
+            {'map_type': 'Tool', 'position': 0},
+            {'map_type': 'ToolFeatureMethodEntityClass', 'position': 1},
+            {'map_type': 'ToolFeatureMethodParameterDefinition', 'position': 2},
+            {'map_type': 'ToolFeatureMethodMethod', 'position': 3},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_MapValueMapping_from_dict_to_dict(self):
+        mapping_dict = {
+            "value_type": "map",
+            "main_value": {"reference": 23, "map_type": "row"},
+            "extra_dimensions": [{"reference": "fifth column", "map_type": "column"}],
+            "compress": True,
+        }
+        parameter_mapping = parameter_value_mapping_from_dict(mapping_dict)
+        out = mapping_to_dict(parameter_mapping)
+        expected = [
+            {'map_type': 'ParameterValueType', 'position': 'hidden', 'value': 'map', 'compress': True},
+            {'map_type': 'IndexName', 'position': 'hidden'},
+            {'map_type': 'ParameterValueIndex', 'position': 'fifth column'},
+            {'map_type': 'ExpandedValue', 'position': -24},
+        ]
+        self.assertEqual(out, expected)
+
+    def test_TimeSeriesValueMapping_from_dict_to_dict(self):
+        mapping_dict = {
+            "value_type": "time series",
+            "main_value": {"reference": 23, "map_type": "row"},
+            "extra_dimensions": [{"reference": "fifth column", "map_type": "column"}],
+            "options": {"repeat": True, "ignore_year": False, "fixed_resolution": False},
+        }
+        parameter_mapping = parameter_value_mapping_from_dict(mapping_dict)
+        out = mapping_to_dict(parameter_mapping)
+        expected = [
+            {
+                'map_type': 'ParameterValueType',
+                'position': 'hidden',
+                'value': 'time_series',
+                'options': {'repeat': True, 'ignore_year': False, 'fixed_resolution': False},
+            },
+            {'map_type': 'IndexName', 'position': 'hidden'},
+            {'map_type': 'ParameterValueIndex', 'position': 'fifth column'},
+            {'map_type': 'ExpandedValue', 'position': -24},
+        ]
+        self.assertEqual(out, expected)
+
+
+def _parent_with_pivot(is_pivoted):
+    parent = Mock()
+    parent.is_pivoted.return_value = is_pivoted
+    return parent
+
+
+def _pivoted_parent():
+    return _parent_with_pivot(True)
+
+
+def _unpivoted_parent():
+    return _parent_with_pivot(False)
+
+
+class TestMappingIsValid(unittest.TestCase):
+    def test_valid_object_class_mapping(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_invalid_object_default_value_mapping_missing_parameter_definition(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
+        default_value_mapping = cls_mapping.flatten()[-1]
+        cls_mapping.position = 0
+        default_value_mapping.position = 1
+        issues = check_validity(cls_mapping)
+        self.assertTrue(issues)
+
+    def test_valid_object_default_value_mapping_not_missing_parameter_definition(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
+            {"map_type": "ParameterDefinition"}
+        )
+        default_value_mapping = cls_mapping.flatten()[-1]
+        cls_mapping.position = 0
+        param_def_mapping.position = 1
+        default_value_mapping.position = 2
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_invalid_object_value_list_mapping_missing_parameter_definition(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
+        value_list_mapping = cls_mapping.flatten()[-2]
+        cls_mapping.position = 0
+        value_list_mapping.position = 1
+        issues = check_validity(cls_mapping)
+        self.assertTrue(issues)
+
+    def test_valid_object_value_list_mapping_not_missing_parameter_definition(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
+            {"map_type": "ParameterDefinition"}
+        )
+        value_list_mapping = cls_mapping.flatten()[-2]
+        cls_mapping.position = 0
+        param_def_mapping.position = 1
+        value_list_mapping.position = 2
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_valid_object_default_value_mapping_hidden(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
+        default_value_mapping = cls_mapping.flatten()[-1]
+        default_value_mapping.position = Position.hidden
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_invalid_object_parameter_value_mapping_missing_object(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
+        value_mapping = cls_mapping.flatten()[-1]
+        cls_mapping.position = 0
+        value_mapping.position = 1
+        issues = check_validity(cls_mapping)
+        self.assertTrue(issues)
+
+    def test_invalid_object_parameter_value_mapping_missing_parameter_definition(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        object_mapping = cls_mapping.flatten()[-2]
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
+        value_mapping = cls_mapping.flatten()[-1]
+        cls_mapping.position = 0
+        object_mapping.position = 1
+        value_mapping.position = 3
+        issues = check_validity(cls_mapping)
+        self.assertTrue(issues)
+
+    def test_valid_object_parameter_value_mapping(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        object_mapping = cls_mapping.flatten()[-2]
+        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
+            {"map_type": "ParameterValue"}
+        )
+        value_mapping = cls_mapping.flatten()[-1]
+        cls_mapping.position = 0
+        object_mapping.position = 1
+        param_def_mapping.position = 2
+        value_mapping.position = 3
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_valid_object_parameter_value_mapping_hidden(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
+        value_mapping = cls_mapping.flatten()[-1]
+        value_mapping.position = Position.hidden
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_valid_relationship_class_mapping(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_invalid_relationship_default_value_mapping_missing_parameter_definition(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
+        default_value_mapping = cls_mapping.flatten()[-1]
+        cls_mapping.position = 0
+        default_value_mapping.position = 1
+        issues = check_validity(cls_mapping)
+        self.assertTrue(issues)
+
+    def test_valid_relationship_default_value_mapping_not_missing_parameter_definition(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        obj_cls_mapping = cls_mapping.child
+        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
+            {"map_type": "ParameterDefinition"}
+        )
+        default_value_mapping = cls_mapping.flatten()[-1]
+        cls_mapping.position = 0
+        obj_cls_mapping.position = 1
+        param_def_mapping.position = 2
+        default_value_mapping.position = 3
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_invalid_relationship_value_list_mapping_missing_parameter_definition(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
+        value_list_mapping = cls_mapping.flatten()[-2]
+        cls_mapping.position = 0
+        value_list_mapping.position = 1
+        issues = check_validity(cls_mapping)
+        self.assertTrue(issues)
+
+    def test_valid_relationship_value_list_mapping_not_missing_parameter_definition(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        obj_cls_mapping = cls_mapping.child
+        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
+            {"map_type": "ParameterDefinition"}
+        )
+        value_list_mapping = cls_mapping.flatten()[-2]
+        cls_mapping.position = 0
+        obj_cls_mapping.position = 1
+        param_def_mapping.position = 2
+        value_list_mapping.position = 3
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_valid_relationship_default_value_mapping_hidden(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterDefinition"})
+        default_value_mapping = cls_mapping.flatten()[-1]
+        default_value_mapping.position = Position.hidden
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_invalid_relationship_parameter_value_mapping_missing_objects(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
+        value_mapping = cls_mapping.flatten()[-1]
+        cls_mapping.position = 0
+        value_mapping.position = 1
+        issues = check_validity(cls_mapping)
+        self.assertTrue(issues)
+
+    def test_invalid_relationship_parameter_value_mapping_missing_parameter_definition(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        object_mapping = cls_mapping.flatten()[-2]
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
+        value_mapping = cls_mapping.flatten()[-1]
+        cls_mapping.position = 0
+        object_mapping.position = 1
+        value_mapping.position = 3
+        issues = check_validity(cls_mapping)
+        self.assertTrue(issues)
+
+    def test_valid_relationship_parameter_value_mapping(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        obj_cls_mapping = cls_mapping.child
+        object_mapping = cls_mapping.flatten()[-2]
+        cls_mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
+            {"map_type": "ParameterValue"}
+        )
+        value_mapping = cls_mapping.flatten()[-1]
+        cls_mapping.position = 0
+        obj_cls_mapping.position = 1
+        object_mapping.position = 2
+        param_def_mapping.position = 3
+        value_mapping.position = 4
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_valid_relationship_parameter_value_mapping_hidden(self):
+        cls_mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
+        cls_mapping.flatten()[-1].child = parameter_mapping_from_dict({"map_type": "ParameterValue"})
+        value_mapping = cls_mapping.flatten()[-1]
+        value_mapping.position = Position.hidden
+        issues = check_validity(cls_mapping)
+        self.assertFalse(issues)
+
+    def test_valid_single_value_mapping(self):
+        value_mapping = parameter_value_mapping_from_dict({"value_type": "single_value"})
+        issues = check_validity(value_mapping)
+        self.assertFalse(issues)
+
+    def test_invalid_single_value_mapping_missing_parameter_definition(self):
+        value_mapping = parameter_value_mapping_from_dict({"value_type": "single_value"})
+        value_mapping.position = 0
+        issues = check_validity(value_mapping)
+        self.assertTrue(issues)
+
+    def test_valid_array_mapping(self):
+        value_mapping = parameter_value_mapping_from_dict({"value_type": "array"})
+        issues = check_validity(value_mapping)
+        self.assertFalse(issues)
+
+    def test_invalid_array_mapping_missing_parameter_definition(self):
+        value_mapping = parameter_value_mapping_from_dict({"value_type": "array"})
+        value_mapping.flatten()[-1].position = 0
+        issues = check_validity(value_mapping)
+        self.assertTrue(issues)
+
+    def test_valid_time_series_mapping(self):
+        value_mapping = parameter_value_mapping_from_dict({"value_type": "time_series"})
+        issues = check_validity(value_mapping)
+        self.assertFalse(issues)
+
+    def test_invalid_time_series_mapping_missing_parameter_definition(self):
+        value_mapping = parameter_value_mapping_from_dict({"value_type": "time_series"})
+        value_mapping.flatten()[-1].position = 0
+        issues = check_validity(value_mapping)
+        self.assertTrue(issues)
+
+
+class TestMappingIntegration(unittest.TestCase):
+    # just a placeholder test for different mapping testings
+
+    def test_bad_mapping_type(self):
+        """Tests that passing any other than a `dict` or a `mapping` to `get_mapped_data` raises `TypeError`."""
+        input_data = [["object_class"], ["oc1"]]
+        data = iter(input_data)
+        data_header = next(data)
+
+        with self.assertRaises(TypeError):
+            mapping = [1, 2, 3]
+            get_mapped_data(data, [mapping], data_header)
+
+        with self.assertRaises(TypeError):
+            mappings = [{"map_type": "ObjectClass", "name": 0}, [1, 2, 3]]
+            get_mapped_data(data, mappings, data_header)
+
+    def test_read_iterator_with_row_with_all_Nones(self):
+        input_data = [
+            ["object_class", "object", "parameter", "value"],
+            [None, None, None, None],
+            ["oc2", "obj2", "parameter_name2", 2],
+        ]
+        expected = {"object_classes": {"oc2"}}
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {"map_type": "ObjectClass", "name": 0}
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_iterator_with_None(self):
+        input_data = [["object_class", "object", "parameter", "value"], None, ["oc2", "obj2", "parameter_name2", 2]]
+        expected = {"object_classes": {"oc2"}}
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {"map_type": "ObjectClass", "name": 0}
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_flat_file(self):
+        input_data = [
+            ["object_class", "object", "parameter", "value"],
+            ["oc1", "obj1", "parameter_name1", 1],
+            ["oc2", "obj2", "parameter_name2", 2],
+        ]
+        expected = {
+            "object_classes": {"oc1", "oc2"},
+            "objects": {("oc1", "obj1"), ("oc2", "obj2")},
+            "object_parameters": [("oc1", "parameter_name1"), ("oc2", "parameter_name2")],
+            "object_parameter_values": [["oc1", "obj1", "parameter_name1", 1], ["oc2", "obj2", "parameter_name2", 2]],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": 0,
+            "objects": 1,
+            "parameters": {"map_type": "parameter", "name": 2, "value": 3},
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_flat_file_array(self):
+        input_data = [
+            ["object_class", "object", "parameter", "value"],
+            ["oc1", "obj1", "parameter_name1", 1],
+            ["oc1", "obj1", "parameter_name1", 2],
+        ]
+        expected = {
+            "object_classes": {"oc1"},
+            "objects": {("oc1", "obj1")},
+            "object_parameters": [("oc1", "parameter_name1")],
+            "object_parameter_values": [["oc1", "obj1", "parameter_name1", Array([1, 2])]],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": 0,
+            "objects": 1,
+            "parameters": {"map_type": "parameter", "name": "parameter_name1", "value": 3, "parameter_type": "array"},
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_flat_file_array_with_ed(self):
+        input_data = [
+            ["object_class", "object", "parameter", "value", "value_order"],
+            ["oc1", "obj1", "parameter_name1", 1, 0],
+            ["oc1", "obj1", "parameter_name1", 2, 1],
+        ]
+        expected = {
+            "object_classes": {"oc1"},
+            "objects": {("oc1", "obj1")},
+            "object_parameters": [("oc1", "parameter_name1")],
+            "object_parameter_values": [["oc1", "obj1", "parameter_name1", Array([1, 2])]],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": 0,
+            "objects": 1,
+            "parameters": {
+                "map_type": "parameter",
+                "name": "parameter_name1",
+                "value": 3,
+                "extra_dimension": [None],
+                "parameter_type": "array",
+            },
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_flat_file_with_column_name_reference(self):
+        input_data = [["object", "parameter", "value"], ["obj1", "parameter_name1", 1], ["obj2", "parameter_name2", 2]]
+        expected = {"object_classes": {"object"}, "objects": {("object", "obj1"), ("object", "obj2")}}
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {"map_type": "ObjectClass", "name": {"map_type": "column_name", "reference": 0}, "object": 0}
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_object_class_from_header_using_string_as_integral_index(self):
+        input_data = [["object_class"], ["obj1"], ["obj2"]]
+        expected = {"object_classes": {"object_class"}, "objects": {("object_class", "obj1"), ("object_class", "obj2")}}
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {"map_type": "ObjectClass", "name": {"map_type": "column_header", "reference": "0"}, "object": 0}
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_object_class_from_header_using_string_as_column_header_name(self):
+        input_data = [["object_class"], ["obj1"], ["obj2"]]
+        expected = {"object_classes": {"object_class"}, "objects": {("object_class", "obj1"), ("object_class", "obj2")}}
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": {"map_type": "column_header", "reference": "object_class"},
+            "object": 0,
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_with_list_of_mappings(self):
+        input_data = [["object", "parameter", "value"], ["obj1", "parameter_name1", 1], ["obj2", "parameter_name2", 2]]
+        expected = {"object_classes": {"object"}, "objects": {("object", "obj1"), ("object", "obj2")}}
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {"map_type": "ObjectClass", "name": {"map_type": "column_header", "reference": 0}, "object": 0}
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_pivoted_parameters_from_header(self):
+        input_data = [["object", "parameter_name1", "parameter_name2"], ["obj1", 0, 1], ["obj2", 2, 3]]
+        expected = {
+            "object_classes": {"object"},
+            "objects": {("object", "obj1"), ("object", "obj2")},
+            "object_parameters": [("object", "parameter_name1"), ("object", "parameter_name2")],
+            "object_parameter_values": [
+                ["object", "obj1", "parameter_name1", 0],
+                ["object", "obj1", "parameter_name2", 1],
+                ["object", "obj2", "parameter_name1", 2],
+                ["object", "obj2", "parameter_name2", 3],
+            ],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": {"map_type": "column_header", "reference": 0},
+            "object": 0,
+            "parameters": {"map_type": "parameter", "name": {"map_type": "row", "reference": -1}},
+        }  # -1 to read pivot from header
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_empty_pivot(self):
+        input_data = [["object", "parameter_name1", "parameter_name2"]]
+        expected = {}
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": {"map_type": "column_header", "reference": 0},
+            "object": 0,
+            "parameters": {"map_type": "parameter", "name": {"map_type": "row", "reference": -1}},
+        }  # -1 to read pivot from header
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_pivoted_parameters_from_data(self):
+        input_data = [["object", "parameter_name1", "parameter_name2"], ["obj1", 0, 1], ["obj2", 2, 3]]
+        expected = {
+            "object_classes": {"object"},
+            "objects": {("object", "obj1"), ("object", "obj2")},
+            "object_parameters": [("object", "parameter_name1"), ("object", "parameter_name2")],
+            "object_parameter_values": [
+                ["object", "obj1", "parameter_name1", 0],
+                ["object", "obj1", "parameter_name2", 1],
+                ["object", "obj2", "parameter_name1", 2],
+                ["object", "obj2", "parameter_name2", 3],
+            ],
+        }
+
+        data = iter(input_data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object",
+            "object": 0,
+            "parameters": {"map_type": "parameter", "name": {"map_type": "row", "reference": 0}},
+        }
+
+        out, errors = get_mapped_data(data, [mapping])
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_pivoted_value_has_actual_position(self):
+        """Pivoted mapping works even when last mapping has valid position in columns."""
+        input_data = [
+            ["object", "timestep", "value"],
+            ["obj1", "T1", 11.0],
+            ["obj1", "T2", 12.0],
+            ["obj2", "T1", 21.0],
+            ["obj2", "T2", 22.0],
+        ]
+        expected = {
+            "object_classes": {"timeline"},
+            "objects": {("timeline", "obj1"), ("timeline", "obj2")},
+            "object_parameters": [("timeline", "value")],
+            "alternatives": {"Base"},
+            "object_parameter_values": [
+                ["timeline", "obj1", "value", Map(["T1", "T2"], [11.0, 12.0], index_name="timestep"), "Base"],
+                ["timeline", "obj2", "value", Map(["T1", "T2"], [21.0, 22.0], index_name="timestep"), "Base"],
+            ],
+        }
+        data = iter(input_data)
+        mapping_dicts = [
+            {"map_type": "ObjectClass", "position": "hidden", "value": "timeline"},
+            {"map_type": "Object", "position": 0},
+            {"map_type": "ObjectMetadata", "position": "hidden"},
+            {"map_type": "ParameterDefinition", "position": -1},
+            {"map_type": "Alternative", "position": "hidden", "value": "Base"},
+            {"map_type": "ParameterValueMetadata", "position": "hidden"},
+            {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+            {"map_type": "IndexName", "position": "hidden", "value": "timestep"},
+            {"map_type": "ParameterValueIndex", "position": 1},
+            {"map_type": "ExpandedValue", "position": 2},  # This caused import to fail
+        ]
+        out, errors = get_mapped_data(data, [mapping_dicts])
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_import_objects_from_pivoted_data_when_they_lack_parameter_values(self):
+        """Pivoted mapping works even when last mapping has valid position in columns."""
+        input_data = [["object", "is_skilled", "has_powers"], ["obj1", "yes", "no"], ["obj2", None, None]]
+        expected = {
+            "object_classes": {"node"},
+            "objects": {("node", "obj1"), ("node", "obj2")},
+            "object_parameters": [("node", "is_skilled"), ("node", "has_powers")],
+            "alternatives": {"Base"},
+            "object_parameter_values": [
+                ["node", "obj1", "is_skilled", "yes", "Base"],
+                ["node", "obj1", "has_powers", "no", "Base"],
+            ],
+        }
+        data = iter(input_data)
+        mapping_dicts = [
+            {"map_type": "ObjectClass", "position": "hidden", "value": "node"},
+            {"map_type": "Object", "position": 0},
+            {"map_type": "ObjectMetadata", "position": "hidden"},
+            {"map_type": "ParameterDefinition", "position": -1},
+            {"map_type": "Alternative", "position": "hidden", "value": "Base"},
+            {"map_type": "ParameterValueMetadata", "position": "hidden"},
+            {"map_type": "ParameterValue", "position": "hidden"},
+        ]
+        out, errors = get_mapped_data(data, [mapping_dicts])
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_import_objects_from_pivoted_data_when_they_lack_map_type_parameter_values(self):
+        """Pivoted mapping works even when last mapping has valid position in columns."""
+        input_data = [
+            ["object", "my_index", "is_skilled", "has_powers"],
+            ["obj1", "yesterday", None, "no"],
+            ["obj1", "today", None, "yes"],
+        ]
+        expected = {
+            "object_classes": {"node"},
+            "objects": {("node", "obj1")},
+            "object_parameters": [("node", "is_skilled"), ("node", "has_powers")],
+            "alternatives": {"Base"},
+            "object_parameter_values": [
+                ["node", "obj1", "has_powers", Map(["yesterday", "today"], ["no", "yes"], index_name="period"), "Base"]
+            ],
+        }
+        data = iter(input_data)
+        mapping_dicts = [
+            {"map_type": "ObjectClass", "position": "hidden", "value": "node"},
+            {"map_type": "Object", "position": 0},
+            {"map_type": "ObjectMetadata", "position": "hidden"},
+            {"map_type": "ParameterDefinition", "position": -1},
+            {"map_type": "Alternative", "position": "hidden", "value": "Base"},
+            {"map_type": "ParameterValueMetadata", "position": "hidden"},
+            {"map_type": "ParameterValueType", "position": "hidden", "value": "map"},
+            {"map_type": "IndexName", "position": "hidden", "value": "period"},
+            {"map_type": "ParameterValueIndex", "position": 1},
+            {"map_type": "ExpandedValue", "position": "hidden"},
+        ]
+        out, errors = get_mapped_data(data, [mapping_dicts])
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_flat_file_with_extra_value_dimensions(self):
+        input_data = [["object", "time", "parameter_name1"], ["obj1", "2018-01-01", 1], ["obj1", "2018-01-02", 2]]
+
+        expected = {
+            "object_classes": {"object"},
+            "objects": {("object", "obj1")},
+            "object_parameters": [("object", "parameter_name1")],
+            "object_parameter_values": [
+                [
+                    "object",
+                    "obj1",
+                    "parameter_name1",
+                    TimeSeriesVariableResolution(["2018-01-01", "2018-01-02"], [1, 2], False, False),
+                ]
+            ],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object",
+            "object": 0,
+            "parameters": {
+                "map_type": "parameter",
+                "name": {"map_type": "column_header", "reference": 2},
+                "value": 2,
+                "parameter_type": "time series",
+                "extra_dimensions": [1],
+            },
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_flat_file_with_parameter_definition(self):
+        input_data = [["object", "time", "parameter_name1"], ["obj1", "2018-01-01", 1], ["obj1", "2018-01-02", 2]]
+
+        expected = {
+            "object_classes": {"object"},
+            "objects": {("object", "obj1")},
+            "object_parameters": [("object", "parameter_name1")],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object",
+            "object": 0,
+            "parameters": {
+                "map_type": "parameter",
+                "name": {"map_type": "column_header", "reference": 2},
+                "value": 2,
+                "parameter_type": "definition",
+            },
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_1dim_relationships(self):
+        input_data = [["unit", "node"], ["u1", "n1"], ["u1", "n2"]]
+        expected = {
+            "relationship_classes": [("node_group", ["node"])],
+            "relationships": {("node_group", ("n1",)), ("node_group", ("n2",))},
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "RelationshipClass",
+            "name": "node_group",
+            "object_classes": [{"map_type": "column_header", "reference": 1}],
+            "objects": [1],
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_relationships(self):
+        input_data = [["unit", "node"], ["u1", "n1"], ["u1", "n2"]]
+        expected = {
+            "relationship_classes": [("unit__node", ["unit", "node"])],
+            "relationships": {("unit__node", ("u1", "n1")), ("unit__node", ("u1", "n2"))},
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "RelationshipClass",
+            "name": "unit__node",
+            "object_classes": [
+                {"map_type": "column_header", "reference": 0},
+                {"map_type": "column_header", "reference": 1},
+            ],
+            "objects": [0, 1],
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_relationships_with_parameters(self):
+        input_data = [["unit", "node", "rel_parameter"], ["u1", "n1", 0], ["u1", "n2", 1]]
+        expected = {
+            "relationship_classes": [("unit__node", ["unit", "node"])],
+            "relationships": {("unit__node", ("u1", "n1")), ("unit__node", ("u1", "n2"))},
+            "relationship_parameters": [("unit__node", "rel_parameter")],
+            "relationship_parameter_values": [
+                ["unit__node", ["u1", "n1"], "rel_parameter", 0],
+                ["unit__node", ["u1", "n2"], "rel_parameter", 1],
+            ],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "RelationshipClass",
+            "name": "unit__node",
+            "object_classes": [
+                {"map_type": "column_header", "reference": 0},
+                {"map_type": "column_header", "reference": 1},
+            ],
+            "objects": [0, 1],
+            "parameters": {"map_type": "parameter", "name": {"map_type": "column_header", "reference": 2}, "value": 2},
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_relationships_with_parameters2(self):
+        input_data = [["nuts2", "Capacity", "Fueltype"], ["BE23", 268.0, "Bioenergy"], ["DE11", 14.0, "Bioenergy"]]
+        expected = {
+            "object_classes": {"nuts2", "fueltype"},
+            "objects": {("nuts2", "BE23"), ("fueltype", "Bioenergy"), ("nuts2", "DE11"), ("fueltype", "Bioenergy")},
+            "relationship_classes": [("nuts2__fueltype", ["nuts2", "fueltype"])],
+            "relationships": {("nuts2__fueltype", ("BE23", "Bioenergy")), ("nuts2__fueltype", ("DE11", "Bioenergy"))},
+            "relationship_parameters": [("nuts2__fueltype", "capacity")],
+            "relationship_parameter_values": [
+                ["nuts2__fueltype", ["BE23", "Bioenergy"], "capacity", 268.0],
+                ["nuts2__fueltype", ["DE11", "Bioenergy"], "capacity", 14.0],
+            ],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "RelationshipClass",
+            "name": {"map_type": "constant", "reference": "nuts2__fueltype"},
+            "parameters": {
+                "map_type": "parameter",
+                "name": {"map_type": "constant", "reference": "capacity"},
+                "parameter_type": "single value",
+                "value": {"map_type": "column", "reference": 1},
+            },
+            "skip_columns": [],
+            "read_start_row": 0,
+            "objects": [{"map_type": "column", "reference": 0}, {"map_type": "column", "reference": 2}],
+            "object_classes": [
+                {"map_type": "constant", "reference": "nuts2"},
+                {"map_type": "constant", "reference": "fueltype"},
+            ],
+            "import_objects": True,
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_parameter_header_with_only_one_parameter(self):
+        input_data = [["object", "parameter_name1"], ["obj1", 0], ["obj2", 2]]
+        expected = {
+            "object_classes": {"object"},
+            "objects": {("object", "obj1"), ("object", "obj2")},
+            "object_parameters": [("object", "parameter_name1")],
+            "object_parameter_values": [
+                ["object", "obj1", "parameter_name1", 0],
+                ["object", "obj2", "parameter_name1", 2],
+            ],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object",
+            "object": 0,
+            "parameters": {"map_type": "parameter", "name": {"map_type": "row", "reference": -1}},
+        }  # -1 to read pivot from header
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_pivoted_parameters_from_data_with_skipped_column(self):
+        input_data = [["object", "parameter_name1", "parameter_name2"], ["obj1", 0, 1], ["obj2", 2, 3]]
+        expected = {
+            "object_classes": {"object"},
+            "objects": {("object", "obj1"), ("object", "obj2")},
+            "object_parameters": [("object", "parameter_name1")],
+            "object_parameter_values": [
+                ["object", "obj1", "parameter_name1", 0],
+                ["object", "obj2", "parameter_name1", 2],
+            ],
+        }
+
+        data = iter(input_data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object",
+            "object": 0,
+            "skip_columns": [2],
+            "parameters": {"map_type": "parameter", "name": {"map_type": "row", "reference": 0}},
+        }  # -1 to read pivot from header
+
+        out, errors = get_mapped_data(data, [mapping])
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_relationships_and_import_objects(self):
+        input_data = [["unit", "node"], ["u1", "n1"], ["u2", "n2"]]
+        expected = {
+            "relationship_classes": [("unit__node", ["unit", "node"])],
+            "relationships": {("unit__node", ("u1", "n1")), ("unit__node", ("u2", "n2"))},
+            "object_classes": {"unit", "node"},
+            "objects": {("unit", "u1"), ("node", "n1"), ("unit", "u2"), ("node", "n2")},
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "RelationshipClass",
+            "name": "unit__node",
+            "object_classes": [
+                {"map_type": "column_header", "reference": 0},
+                {"map_type": "column_header", "reference": 1},
+            ],
+            "objects": [0, 1],
+            "import_objects": True,
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_relationships_parameter_values_with_extra_dimensions(self):
+        input_data = [["", "a", "b"], ["", "c", "d"], ["", "e", "f"], ["a", 2, 3], ["b", 4, 5]]
+
+        expected = {
+            "relationship_classes": [("unit__node", ["unit", "node"])],
+            "relationship_parameters": [("unit__node", "e"), ("unit__node", "f")],
+            "relationships": {("unit__node", ("a", "c")), ("unit__node", ("b", "d"))},
+            "relationship_parameter_values": [
+                ["unit__node", ("a", "c"), "e", Map(["a", "b"], [2, 4])],
+                ["unit__node", ("b", "d"), "f", Map(["a", "b"], [3, 5])],
+            ],
+        }
+
+        data = iter(input_data)
+        data_header = []
+
+        mapping = {
+            "map_type": "RelationshipClass",
+            "name": "unit__node",
+            "object_classes": ["unit", "node"],
+            "objects": [{"map_type": "row", "reference": i} for i in range(2)],
+            "parameters": {
+                "map_type": "parameter",
+                "parameter_type": "map",
+                "name": {"map_type": "row", "reference": 2},
+                "extra_dimensions": [0],
+            },
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_data_with_read_start_row(self):
+        input_data = [
+            ["object_class", "object", "parameter", "value"],
+            [" ", " ", " ", " "],
+            ["oc1", "obj1", "parameter_name1", 1],
+            ["oc2", "obj2", "parameter_name2", 2],
+        ]
+        expected = {
+            "object_classes": {"oc1", "oc2"},
+            "objects": {("oc1", "obj1"), ("oc2", "obj2")},
+            "object_parameters": [("oc1", "parameter_name1"), ("oc2", "parameter_name2")],
+            "object_parameter_values": [["oc1", "obj1", "parameter_name1", 1], ["oc2", "obj2", "parameter_name2", 2]],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": 0,
+            "object": 1,
+            "parameters": {"map_type": "parameter", "name": 2, "value": 3},
+            "read_start_row": 1,
+        }
+
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_data_with_two_mappings_with_different_read_start_row(self):
+        input_data = [
+            ["oc1", "oc2", "parameter_class1", "parameter_class2"],
+            [" ", " ", " ", " "],
+            ["oc1_obj1", "oc2_obj1", 1, 3],
+            ["oc1_obj2", "oc2_obj2", 2, 4],
+        ]
+        expected = {
+            "object_classes": {"oc1", "oc2"},
+            "objects": {("oc1", "oc1_obj1"), ("oc1", "oc1_obj2"), ("oc2", "oc2_obj2")},
+            "object_parameters": [("oc1", "parameter_class1"), ("oc2", "parameter_class2")],
+            "object_parameter_values": [
+                ["oc1", "oc1_obj1", "parameter_class1", 1],
+                ["oc1", "oc1_obj2", "parameter_class1", 2],
+                ["oc2", "oc2_obj2", "parameter_class2", 4],
+            ],
+        }
+
+        data = iter(input_data)
+        data_header = next(data)
+
+        mapping1 = {
+            "map_type": "ObjectClass",
+            "name": {"map_type": "column_header", "reference": 0},
+            "object": 0,
+            "parameters": {"map_type": "parameter", "name": {"map_type": "column_header", "reference": 2}, "value": 2},
+            "read_start_row": 1,
+        }
+        mapping2 = {
+            "map_type": "ObjectClass",
+            "name": {"map_type": "column_header", "reference": 1},
+            "object": 1,
+            "parameters": {"map_type": "parameter", "name": {"map_type": "column_header", "reference": 3}, "value": 3},
+            "read_start_row": 2,
+        }
+
+        out, errors = get_mapped_data(data, [mapping1, mapping2], data_header)
+        self.assertEqual(errors, [])
+        self.assertEqual(out, expected)
+
+    def test_read_object_class_with_table_name_as_class_name(self):
+        input_data = [["Object names"], ["object 1"], ["object 2"]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": {"map_type": "table_name", "reference": "class name"},
+            "object": 0,
+        }
+        out, errors = get_mapped_data(data, [mapping], data_header, "class name")
+        expected = {
+            "object_classes": {"class name"},
+            "objects": {("class name", "object 1"), ("class name", "object 2")},
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_flat_map_from_columns(self):
+        input_data = [["Index", "Value"], ["key1", -2], ["key2", -1]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object_class",
+            "object": "object",
+            "parameters": {
+                "name": "parameter",
+                "parameter_type": "map",
+                "value": 1,
+                "compress": False,
+                "extra_dimensions": [0],
+            },
+        }
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected_map = Map(["key1", "key2"], [-2, -1])
+        expected = {
+            "object_classes": {"object_class"},
+            "objects": {("object_class", "object")},
+            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "object_parameters": [("object_class", "parameter")],
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_nested_map_from_columns(self):
+        input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object_class",
+            "object": "object",
+            "parameters": {
+                "name": "parameter",
+                "parameter_type": "map",
+                "value": 2,
+                "compress": False,
+                "extra_dimensions": [0, 1],
+            },
+        }
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected_map = Map(["key11", "key21"], [Map(["key12"], [-2]), Map(["key22"], [-1])])
+        expected = {
+            "object_classes": {"object_class"},
+            "objects": {("object_class", "object")},
+            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "object_parameters": [("object_class", "parameter")],
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_uneven_nested_map_from_columns(self):
+        input_data = [
+            ["Index", "A", "B", "C"],
+            ["key1", "key11", -2, ""],
+            ["key1", "key12", -1, ""],
+            ["key2", -23, "", ""],
+            ["key3", -33, "", ""],
+            ["key4", "key31", "key311", 50],
+            ["key4", "key31", "key312", 51],
+            ["key4", "key32", 66, ""],
+        ]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object_class",
+            "object": "object",
+            "parameters": {
+                "name": "parameter",
+                "parameter_type": "map",
+                "value": 3,
+                "compress": False,
+                "extra_dimensions": [0, 1, 2],
+            },
+        }
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected_map = Map(
+            ["key1", "key2", "key3", "key4"],
+            [
+                Map(["key11", "key12"], [-2, -1]),
+                -23,
+                -33,
+                Map(["key31", "key32"], [Map(["key311", "key312"], [50, 51]), 66]),
+            ],
+        )
+        expected = {
+            "object_classes": {"object_class"},
+            "objects": {("object_class", "object")},
+            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "object_parameters": [("object_class", "parameter")],
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_nested_map_with_compression(self):
+        input_data = [
+            ["Index 1", "Time stamp", "Value"],
+            ["key", DateTime("2020-09-10T08:00"), -2.0],
+            ["key", DateTime("2020-09-11T08:00"), -1.0],
+        ]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object_class",
+            "object": "object",
+            "parameters": {
+                "name": "parameter",
+                "parameter_type": "map",
+                "value": 2,
+                "compress": True,
+                "extra_dimensions": [0, 1],
+            },
+        }
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected_map = Map(
+            ["key"],
+            [
+                TimeSeriesVariableResolution(
+                    ["2020-09-10T08:00", "2020-09-11T08:00"],
+                    [-2.0, -1.0],
+                    False,
+                    False,
+                    index_name=Map.DEFAULT_INDEX_NAME,
+                )
+            ],
+        )
+        expected = {
+            "object_classes": {"object_class"},
+            "objects": {("object_class", "object")},
+            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "object_parameters": [("object_class", "parameter")],
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_alternative(self):
+        input_data = [["Alternatives"], ["alternative1"], ["second_alternative"], ["last_one"]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {"map_type": "Alternative", "name": 0}
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = {"alternatives": {"alternative1", "second_alternative", "last_one"}}
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_scenario(self):
+        input_data = [["Scenarios"], ["scenario1"], ["second_scenario"], ["last_one"]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {"map_type": "Scenario", "name": 0}
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = {"scenarios": {("scenario1", False), ("second_scenario", False), ("last_one", False)}}
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_scenario_with_active_flags(self):
+        input_data = [["Scenarios", "Active"], ["scenario1", 1], ["second_scenario", "f"], ["last_one", "true"]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {"map_type": "Scenario", "name": 0, "active": 1}
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = {"scenarios": {("scenario1", True), ("second_scenario", False), ("last_one", True)}}
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_scenario_alternative(self):
+        input_data = [
+            ["Scenario", "Alternative", "Before alternative"],
+            ["scenario_A", "alternative1", "second_alternative"],
+            ["scenario_A", "second_alternative", "last_one"],
+            ["scenario_B", "last_one", ""],
+        ]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {
+            "map_type": "ScenarioAlternative",
+            "scenario_name": 0,
+            "alternative_name": 1,
+            "before_alternative_name": 2,
+        }
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = dict()
+        expected["scenario_alternatives"] = [
+            ["scenario_A", "alternative1", "second_alternative"],
+            ["scenario_A", "second_alternative", "last_one"],
+            ["scenario_B", "last_one", ""],
+        ]
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_pivoted_scenario_alternative(self):
+        input_data = [["scenario_A", "scenario_B"], ["first_alternative", "Base"], ["second_alternative", ""]]
+        data = iter(input_data)
+        mappings = [{"map_type": "Scenario", "position": -1}, {"map_type": "ScenarioAlternative", "position": "hidden"}]
+        out, errors = get_mapped_data(data, [mappings])
+        expected = dict()
+        expected["scenario_alternatives"] = [
+            ["scenario_A", "first_alternative"],
+            ["scenario_A", "second_alternative"],
+            ["scenario_B", "Base"],
+        ]
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_tool(self):
+        input_data = [["Tools"], ["tool1"], ["second_tool"], ["last_one"]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {"map_type": "Tool", "name": 0}
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = {"tools": {"tool1", "second_tool", "last_one"}}
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_feature(self):
+        input_data = [["Class", "Parameter"], ["class1", "param1"], ["class2", "param2"]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {"map_type": "Feature", "entity_class_name": 0, "parameter_definition_name": 1}
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = {"features": {("class1", "param1"), ("class2", "param2")}}
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_tool_feature(self):
+        input_data = [["Tool", "Class", "Parameter"], ["tool1", "class1", "param1"], ["tool2", "class2", "param2"]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {"map_type": "ToolFeature", "name": 0, "entity_class_name": 1, "parameter_definition_name": 2}
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = {"tool_features": [["tool1", "class1", "param1", False], ["tool2", "class2", "param2", False]]}
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_tool_feature_with_required_flag(self):
+        input_data = [
+            ["Tool", "Class", "Parameter", "Required"],
+            ["tool1", "class1", "param1", "f"],
+            ["tool2", "class2", "param2", "true"],
+        ]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {
+            "map_type": "ToolFeature",
+            "name": 0,
+            "entity_class_name": 1,
+            "parameter_definition_name": 2,
+            "required": 3,
+        }
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = {"tool_features": [["tool1", "class1", "param1", False], ["tool2", "class2", "param2", True]]}
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_tool_feature_method(self):
+        input_data = [
+            ["Tool", "Class", "Parameter", "Method"],
+            ["tool1", "class1", "param1", "meth1"],
+            ["tool2", "class2", "param2", "meth2"],
+        ]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {
+            "map_type": "ToolFeatureMethod",
+            "name": 0,
+            "entity_class_name": 1,
+            "parameter_definition_name": 2,
+            "method": 3,
+        }
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = dict()
+        expected["tool_feature_methods"] = [
+            ["tool1", "class1", "param1", "meth1"],
+            ["tool2", "class2", "param2", "meth2"],
+        ]
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_object_group_without_parameters(self):
+        input_data = [
+            ["Object Class", "Group", "Object"],
+            ["class_A", "group1", "object1"],
+            ["class_A", "group1", "object2"],
+            ["class_A", "group2", "object3"],
+        ]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {"map_type": "ObjectGroup", "name": 0, "groups": 1, "members": 2}
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = dict()
+        expected["object_classes"] = {"class_A"}
+        expected["object_groups"] = {
+            ("class_A", "group1", "object1"),
+            ("class_A", "group1", "object2"),
+            ("class_A", "group2", "object3"),
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_object_group_and_import_objects(self):
+        input_data = [
+            ["Object Class", "Group", "Object"],
+            ["class_A", "group1", "object1"],
+            ["class_A", "group1", "object2"],
+            ["class_A", "group2", "object3"],
+        ]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {"map_type": "ObjectGroup", "name": 0, "groups": 1, "members": 2, "import_objects": True}
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = dict()
+        expected["object_groups"] = {
+            ("class_A", "group1", "object1"),
+            ("class_A", "group1", "object2"),
+            ("class_A", "group2", "object3"),
+        }
+        expected["object_classes"] = {"class_A"}
+        expected["objects"] = {
+            ("class_A", "group1"),
+            ("class_A", "object1"),
+            ("class_A", "group1"),
+            ("class_A", "object2"),
+            ("class_A", "group2"),
+            ("class_A", "object3"),
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_parameter_definition_with_default_values_and_value_lists(self):
+        input_data = [
+            ["Class", "Parameter", "Default", "Value list"],
+            ["class_A", "param1", 23.0, "listA"],
+            ["class_A", "param2", 42.0, "listB"],
+            ["class_B", "param3", 5.0, "listA"],
+        ]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": 0,
+            "parameters": {
+                "name": 1,
+                "map_type": "ParameterDefinition",
+                "default_value": {"value_type": "single value", "main_value": 2},
+                "parameter_value_list_name": 3,
+            },
+        }
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected = dict()
+        expected["object_classes"] = {"class_A", "class_A", "class_B"}
+        expected["object_parameters"] = [
+            ("class_A", "param1", 23.0, "listA"),
+            ("class_A", "param2", 42.0, "listB"),
+            ("class_B", "param3", 5.0, "listA"),
+        ]
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_map_as_default_parameter_value(self):
+        input_data = [["key1", -2.3], ["key2", 5.5], ["key3", 3.2]]
+        data = iter(input_data)
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object_class",
+            "parameters": {
+                "name": "parameter",
+                "map_type": "ParameterDefinition",
+                "default_value": {"value_type": "map", "main_value": 1, "compress": False, "extra_dimensions": [0]},
+            },
+        }
+        out, errors = get_mapped_data(data, [mapping])
+        expected_map = Map(["key1", "key2", "key3"], [-2.3, 5.5, 3.2])
+        expected = {
+            "object_classes": {"object_class"},
+            "object_parameters": [("object_class", "parameter", expected_map)],
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_parameter_definition_with_nested_map_as_default_value(self):
+        input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping = {
+            "map_type": "ObjectClass",
+            "name": "object_class",
+            "parameters": {
+                "name": "parameter",
+                "map_type": "ParameterDefinition",
+                "default_value": {"value_type": "map", "main_value": 2, "compress": False, "extra_dimensions": [0, 1]},
+            },
+        }
+        out, errors = get_mapped_data(data, [mapping], data_header)
+        expected_map = Map(["key11", "key21"], [Map(["key12"], [-2]), Map(["key22"], [-1])])
+        expected = {
+            "object_classes": {"object_class"},
+            "object_parameters": [("object_class", "parameter", expected_map)],
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_map_index_names_from_columns(self):
+        input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping_root = unflatten(
+            [
+                ObjectClassMapping(Position.hidden, value="object_class"),
+                ParameterDefinitionMapping(Position.hidden, value="parameter"),
+                ObjectMapping(Position.hidden, value="object"),
+                ParameterValueTypeMapping(Position.hidden, value="map"),
+                IndexNameMapping(Position.header, value=0),
+                ParameterValueIndexMapping(0),
+                IndexNameMapping(Position.header, value=1),
+                ParameterValueIndexMapping(1),
+                ExpandedParameterValueMapping(2),
+            ]
+        )
+        out, errors = get_mapped_data(data, [mapping_root], data_header)
+        expected_map = Map(
+            ["key11", "key21"],
+            [Map(["key12"], [-2], index_name="Index 2"), Map(["key22"], [-1], index_name="Index 2")],
+            index_name="Index 1",
+        )
+        expected = {
+            "object_classes": {"object_class"},
+            "objects": {("object_class", "object")},
+            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "object_parameters": [("object_class", "parameter")],
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_missing_map_index_name(self):
+        input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping_root = unflatten(
+            [
+                ObjectClassMapping(Position.hidden, value="object_class"),
+                ParameterDefinitionMapping(Position.hidden, value="parameter"),
+                ObjectMapping(Position.hidden, value="object"),
+                ParameterValueTypeMapping(Position.hidden, value="map"),
+                IndexNameMapping(Position.hidden, value=None),
+                ParameterValueIndexMapping(0),
+                IndexNameMapping(Position.header, value=1),
+                ParameterValueIndexMapping(1),
+                ExpandedParameterValueMapping(2),
+            ]
+        )
+        out, errors = get_mapped_data(data, [mapping_root], data_header)
+        expected_map = Map(
+            ["key11", "key21"],
+            [Map(["key12"], [-2], index_name="Index 2"), Map(["key22"], [-1], index_name="Index 2")],
+            index_name="",
+        )
+        expected = {
+            "object_classes": {"object_class"},
+            "objects": {("object_class", "object")},
+            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "object_parameters": [("object_class", "parameter")],
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_read_default_value_index_names_from_columns(self):
+        input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
+        data = iter(input_data)
+        data_header = next(data)
+        mapping_root = unflatten(
+            [
+                ObjectClassMapping(Position.hidden, value="object_class"),
+                ParameterDefinitionMapping(Position.hidden, value="parameter"),
+                ParameterDefaultValueTypeMapping(Position.hidden, value="map"),
+                DefaultValueIndexNameMapping(Position.header, value=0),
+                ParameterDefaultValueIndexMapping(0),
+                DefaultValueIndexNameMapping(Position.header, value=1),
+                ParameterDefaultValueIndexMapping(1),
+                ExpandedParameterDefaultValueMapping(2),
+            ]
+        )
+        out, errors = get_mapped_data(data, [mapping_root], data_header)
+        expected_map = Map(
+            ["key11", "key21"],
+            [Map(["key12"], [-2], index_name="Index 2"), Map(["key22"], [-1], index_name="Index 2")],
+            index_name="Index 1",
+        )
+        expected = {
+            "object_classes": {"object_class"},
+            "object_parameters": [("object_class", "parameter", expected_map)],
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_filter_regular_expression_in_root_mapping(self):
+        input_data = [["A", "p"], ["A", "q"], ["B", "r"]]
+        data = iter(input_data)
+        mapping_root = unflatten([ObjectClassMapping(0, filter_re="B"), ObjectMapping(1)])
+        out, errors = get_mapped_data(data, [mapping_root])
+        expected = {"object_classes": {"B"}, "objects": {("B", "r")}}
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_filter_regular_expression_in_child_mapping(self):
+        input_data = [["A", "p"], ["A", "q"], ["B", "r"]]
+        data = iter(input_data)
+        mapping_root = unflatten([ObjectClassMapping(0), ObjectMapping(1, filter_re="q|r")])
+        out, errors = get_mapped_data(data, [mapping_root])
+        expected = {"object_classes": {"A", "B"}, "objects": {("A", "q"), ("B", "r")}}
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_filter_regular_expression_in_child_mapping_filters_parent_mappings_too(self):
+        input_data = [["A", "p"], ["A", "q"], ["B", "r"]]
+        data = iter(input_data)
+        mapping_root = unflatten([ObjectClassMapping(0), ObjectMapping(1, filter_re="q")])
+        out, errors = get_mapped_data(data, [mapping_root])
+        expected = {"object_classes": {"A"}, "objects": {("A", "q")}}
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+    def test_arrays_get_imported_to_correct_alternatives(self):
+        input_data = [["Base", "y", "p1"], ["alternative", "y", "p1"]]
+        data = iter(input_data)
+        mapping_root = unflatten(
+            [
+                ObjectClassMapping(Position.hidden, value="class"),
+                ObjectMapping(1),
+                ParameterDefinitionMapping(Position.hidden, value="parameter"),
+                AlternativeMapping(0),
+                ParameterValueTypeMapping(Position.hidden, value="array"),
+                ExpandedParameterValueMapping(2),
+            ]
+        )
+        out, errors = get_mapped_data(data, [mapping_root])
+        expected = {
+            "object_classes": {"class"},
+            "objects": {("class", "y")},
+            "object_parameters": [("class", "parameter")],
+            "alternatives": {"Base", "alternative"},
+            "object_parameter_values": [
+                ["class", "y", "parameter", Array(["p1"]), "Base"],
+                ["class", "y", "parameter", Array(["p1"]), "alternative"],
+            ],
+        }
+        self.assertFalse(errors)
+        self.assertEqual(out, expected)
+
+
+class TestHasFilter(unittest.TestCase):
+    def test_mapping_without_filter_doesnt_have_filter(self):
+        mapping = ObjectClassMapping(0)
+        self.assertFalse(mapping.has_filter())
+
+    def test_hidden_mapping_without_value_doesnt_have_filter(self):
+        mapping = ObjectClassMapping(Position.hidden, filter_re="a")
+        self.assertFalse(mapping.has_filter())
+
+    def test_hidden_mapping_with_value_has_filter(self):
+        mapping = ObjectClassMapping(0, value="a", filter_re="b")
+        self.assertTrue(mapping.has_filter())
+
+    def test_mapping_without_value_has_filter(self):
+        mapping = ObjectClassMapping(Position.hidden, value="a", filter_re="b")
+        self.assertTrue(mapping.has_filter())
+
+    def test_mapping_with_value_but_without_filter_doesnt_have_filter(self):
+        mapping = ObjectClassMapping(0, value="a")
+        self.assertFalse(mapping.has_filter())
+
+    def test_child_mapping_with_filter_has_filter(self):
+        mapping = ObjectClassMapping(0)
+        mapping.child = ObjectMapping(1, filter_re="a")
+        self.assertTrue(mapping.has_filter())
+
+    def test_child_mapping_without_filter_doesnt_have_filter(self):
+        mapping = ObjectClassMapping(0)
+        mapping.child = ObjectMapping(1)
+        self.assertFalse(mapping.has_filter())
+
+
+class TestIsPivoted(unittest.TestCase):
+    def test_pivoted_position_returns_true(self):
+        mapping = AlternativeMapping(-1)
+        self.assertTrue(mapping.is_pivoted())
+
+    def test_recursively_returns_false_when_all_mappings_are_non_pivoted(self):
+        mapping = unflatten([AlternativeMapping(0), ParameterValueMapping(1)])
+        self.assertFalse(mapping.is_pivoted())
+
+    def test_returns_true_when_position_is_header_and_has_child(self):
+        mapping = unflatten([AlternativeMapping(Position.header), ParameterValueMapping(0)])
+        self.assertTrue(mapping.is_pivoted())
+
+    def test_returns_false_when_position_is_header_and_is_leaf(self):
+        mapping = unflatten([AlternativeMapping(0), ParameterValueMapping(Position.header)])
+        self.assertFalse(mapping.is_pivoted())
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/import_mapping/test_type_conversion.py` & `spinedb_api-0.30.4/tests/import_mapping/test_type_conversion.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,108 +1,108 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-import unittest
-
-from spinedb_api import DateTime, Duration
-
-from spinedb_api.import_mapping.type_conversion import (
-    value_to_convert_spec,
-    StringConvertSpec,
-    FloatConvertSpec,
-    DateTimeConvertSpec,
-    DurationConvertSpec,
-    IntegerSequenceDateTimeConvertSpec,
-)
-
-
-class TestValueToConvertSpec(unittest.TestCase):
-    def test_string(self):
-        self.assertIsInstance(value_to_convert_spec("string"), StringConvertSpec)
-
-    def test_float(self):
-        self.assertIsInstance(value_to_convert_spec("float"), FloatConvertSpec)
-
-    def test_DateTime(self):
-        self.assertIsInstance(value_to_convert_spec("datetime"), DateTimeConvertSpec)
-
-    def test_Duration(self):
-        self.assertIsInstance(value_to_convert_spec("duration"), DurationConvertSpec)
-
-    def test_interger_sequence_datetime(self):
-        self.assertIsInstance(
-            value_to_convert_spec({"start_datetime": "2019-01-01T00:00", "start_int": 0, "duration": "1h"}),
-            IntegerSequenceDateTimeConvertSpec,
-        )
-
-
-class TestConvertSpec(unittest.TestCase):
-    def test_string(self):
-        self.assertEqual(StringConvertSpec()(1), "1")
-
-    def test_float(self):
-        self.assertEqual(FloatConvertSpec()("1"), 1.0)
-
-    def test_DateTime(self):
-        self.assertEqual(DateTimeConvertSpec()("2019-01-01T00:00"), DateTime("2019-01-01T00:00"))
-
-    def test_Duration(self):
-        self.assertEqual(DurationConvertSpec()("1h"), Duration("1h"))
-
-    def test_interger_sequence_datetime(self):
-        converter = IntegerSequenceDateTimeConvertSpec("2019-01-01T00:00", 0, "1h")
-        self.assertEqual(converter("t00000"), DateTime("2019-01-01T00:00"))
-        self.assertEqual(converter("t00002"), DateTime("2019-01-01T02:00"))
-
-    def test_interger_sequence_datetime_shifted_start_int(self):
-        converter = IntegerSequenceDateTimeConvertSpec("2019-01-01T00:00", 1, "1h")
-        self.assertEqual(converter("t00000"), DateTime("2018-12-31T23:00"))
-        self.assertEqual(converter("t00002"), DateTime("2019-01-01T01:00"))
-
-    def test_interger_sequence_datetime_different_duration(self):
-        converter = IntegerSequenceDateTimeConvertSpec("2019-01-01T00:00", 0, "2h")
-        self.assertEqual(converter("t00000"), DateTime("2019-01-01T00:00"))
-        self.assertEqual(converter("t00002"), DateTime("2019-01-01T04:00"))
-
-    def test_interger_sequence_datetime_non_int_string(self):
-        converter = IntegerSequenceDateTimeConvertSpec("2019-01-01T00:00", 0, "2h")
-        with self.assertRaises(ValueError) as cm:
-            converter("not a sequence")
-
-
-class TestConvertSpecToJsonValue(unittest.TestCase):
-    def test_string(self):
-        self.assertEqual(StringConvertSpec().to_json_value(), "string")
-
-    def test_float(self):
-        self.assertEqual(FloatConvertSpec().to_json_value(), "float")
-
-    def test_DateTime(self):
-        self.assertEqual(DateTimeConvertSpec().to_json_value(), "datetime")
-
-    def test_Duration(self):
-        self.assertEqual(DurationConvertSpec().to_json_value(), "duration")
-
-    def test_interger_sequence_datetime(self):
-        converter = IntegerSequenceDateTimeConvertSpec("2019-01-01T00:00", 0, "1h")
-        self.assertEqual(
-            converter.to_json_value(),
-            {
-                "name": "integer sequence datetime",
-                "start_datetime": "2019-01-01T00:00:00",
-                "start_int": 0,
-                "duration": "1h",
-            },
-        )
-
-
-class TestFloatConvertSpec(unittest.TestCase):
-    def test_empty_string_conversion_gives_none(self):
-        convert_spec = FloatConvertSpec()
-        self.assertIsNone(convert_spec(""))
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+import unittest
+
+from spinedb_api import DateTime, Duration
+
+from spinedb_api.import_mapping.type_conversion import (
+    value_to_convert_spec,
+    StringConvertSpec,
+    FloatConvertSpec,
+    DateTimeConvertSpec,
+    DurationConvertSpec,
+    IntegerSequenceDateTimeConvertSpec,
+)
+
+
+class TestValueToConvertSpec(unittest.TestCase):
+    def test_string(self):
+        self.assertIsInstance(value_to_convert_spec("string"), StringConvertSpec)
+
+    def test_float(self):
+        self.assertIsInstance(value_to_convert_spec("float"), FloatConvertSpec)
+
+    def test_DateTime(self):
+        self.assertIsInstance(value_to_convert_spec("datetime"), DateTimeConvertSpec)
+
+    def test_Duration(self):
+        self.assertIsInstance(value_to_convert_spec("duration"), DurationConvertSpec)
+
+    def test_interger_sequence_datetime(self):
+        self.assertIsInstance(
+            value_to_convert_spec({"start_datetime": "2019-01-01T00:00", "start_int": 0, "duration": "1h"}),
+            IntegerSequenceDateTimeConvertSpec,
+        )
+
+
+class TestConvertSpec(unittest.TestCase):
+    def test_string(self):
+        self.assertEqual(StringConvertSpec()(1), "1")
+
+    def test_float(self):
+        self.assertEqual(FloatConvertSpec()("1"), 1.0)
+
+    def test_DateTime(self):
+        self.assertEqual(DateTimeConvertSpec()("2019-01-01T00:00"), DateTime("2019-01-01T00:00"))
+
+    def test_Duration(self):
+        self.assertEqual(DurationConvertSpec()("1h"), Duration("1h"))
+
+    def test_interger_sequence_datetime(self):
+        converter = IntegerSequenceDateTimeConvertSpec("2019-01-01T00:00", 0, "1h")
+        self.assertEqual(converter("t00000"), DateTime("2019-01-01T00:00"))
+        self.assertEqual(converter("t00002"), DateTime("2019-01-01T02:00"))
+
+    def test_interger_sequence_datetime_shifted_start_int(self):
+        converter = IntegerSequenceDateTimeConvertSpec("2019-01-01T00:00", 1, "1h")
+        self.assertEqual(converter("t00000"), DateTime("2018-12-31T23:00"))
+        self.assertEqual(converter("t00002"), DateTime("2019-01-01T01:00"))
+
+    def test_interger_sequence_datetime_different_duration(self):
+        converter = IntegerSequenceDateTimeConvertSpec("2019-01-01T00:00", 0, "2h")
+        self.assertEqual(converter("t00000"), DateTime("2019-01-01T00:00"))
+        self.assertEqual(converter("t00002"), DateTime("2019-01-01T04:00"))
+
+    def test_interger_sequence_datetime_non_int_string(self):
+        converter = IntegerSequenceDateTimeConvertSpec("2019-01-01T00:00", 0, "2h")
+        with self.assertRaises(ValueError) as cm:
+            converter("not a sequence")
+
+
+class TestConvertSpecToJsonValue(unittest.TestCase):
+    def test_string(self):
+        self.assertEqual(StringConvertSpec().to_json_value(), "string")
+
+    def test_float(self):
+        self.assertEqual(FloatConvertSpec().to_json_value(), "float")
+
+    def test_DateTime(self):
+        self.assertEqual(DateTimeConvertSpec().to_json_value(), "datetime")
+
+    def test_Duration(self):
+        self.assertEqual(DurationConvertSpec().to_json_value(), "duration")
+
+    def test_interger_sequence_datetime(self):
+        converter = IntegerSequenceDateTimeConvertSpec("2019-01-01T00:00", 0, "1h")
+        self.assertEqual(
+            converter.to_json_value(),
+            {
+                "name": "integer sequence datetime",
+                "start_datetime": "2019-01-01T00:00:00",
+                "start_int": 0,
+                "duration": "1h",
+            },
+        )
+
+
+class TestFloatConvertSpec(unittest.TestCase):
+    def test_empty_string_conversion_gives_none(self):
+        convert_spec = FloatConvertSpec()
+        self.assertIsNone(convert_spec(""))
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/__init__.py` & `spinedb_api-0.30.4/tests/spine_io/importers/test_excel_reader.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,15 +1,29 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Init file for tests.spine_io package. Intentionally empty.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains unit tests for ExcelConnector class.
+
+"""
+import pickle
+import unittest
+from spinedb_api.spine_io.importers.excel_reader import ExcelConnector
+
+
+class TestExcelConnector(unittest.TestCase):
+    def test_connector_is_picklable(self):
+        reader = ExcelConnector(None)
+        pickled = pickle.dumps(reader)
+        self.assertTrue(pickled)
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/exporters/__init__.py` & `spinedb_api-0.30.4/tests/export_mapping/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,15 +1,10 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Init file for tests.spine_io.exporters package. Intentionally empty.
-
-"""
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/exporters/test_csv_writer.py` & `spinedb_api-0.30.4/tests/spine_io/exporters/test_csv_writer.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,98 +1,98 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Unit tests for csv writer.
-
-"""
-from pathlib import Path
-from tempfile import TemporaryDirectory
-import unittest
-from spinedb_api import DiffDatabaseMapping, import_object_classes, import_objects
-from spinedb_api.mapping import Position
-from spinedb_api.export_mapping import object_export
-from spinedb_api.spine_io.exporters.writer import write
-from spinedb_api.spine_io.exporters.csv_writer import CsvWriter
-
-
-class TestCsvWriter(unittest.TestCase):
-    def setUp(self):
-        self._temp_dir = TemporaryDirectory()
-
-    def tearDown(self):
-        self._temp_dir.cleanup()
-
-    def test_write_empty_database(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root_mapping = object_export(0, 1)
-        out_path = Path(self._temp_dir.name, "out.csv")
-        writer = CsvWriter(out_path.parent, out_path.name)
-        write(db_map, writer, root_mapping)
-        self.assertTrue(out_path.exists())
-        with open(out_path) as out_file:
-            self.assertEqual(out_file.readlines(), [])
-        db_map.connection.close()
-
-    def test_write_single_object_class_and_object(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_export(0, 1)
-        out_path = Path(self._temp_dir.name, "out.csv")
-        writer = CsvWriter(out_path.parent, out_path.name)
-        write(db_map, writer, root_mapping)
-        self.assertTrue(out_path.exists())
-        with open(out_path) as out_file:
-            self.assertEqual(out_file.readlines(), ["oc,o1\n"])
-        db_map.connection.close()
-
-    def test_tables_are_written_to_separate_files(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_objects(db_map, (("oc1", "o1"), ("oc2", "o2")))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_export(Position.table_name, 0)
-        out_path = Path(self._temp_dir.name, "out.csv")
-        writer = CsvWriter(out_path.parent, out_path.name)
-        write(db_map, writer, root_mapping)
-        self.assertFalse(out_path.exists())
-        out_files = list()
-        for real_out_path in Path(self._temp_dir.name).iterdir():
-            out_files.append(real_out_path.name)
-            expected = None
-            if real_out_path.name == "oc1.csv":
-                expected = ["o1\n"]
-            elif real_out_path.name == "oc2.csv":
-                expected = ["o2\n"]
-            with open(real_out_path) as out_file:
-                self.assertEqual(out_file.readlines(), expected)
-        self.assertEqual(len(out_files), 2)
-        self.assertEqual(set(out_files), {"oc1.csv", "oc2.csv"})
-        db_map.connection.close()
-
-    def test_append_to_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        root_mapping1 = object_export(0, 1)
-        root_mapping2 = object_export(0, 1)
-        out_path = Path(self._temp_dir.name, "out.csv")
-        writer = CsvWriter(out_path.parent, out_path.name)
-        write(db_map, writer, root_mapping1, root_mapping2)
-        self.assertTrue(out_path.exists())
-        with open(out_path) as out_file:
-            self.assertEqual(out_file.readlines(), ["oc,o1\n", "oc,o1\n"])
-        db_map.connection.close()
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Unit tests for csv writer.
+
+"""
+from pathlib import Path
+from tempfile import TemporaryDirectory
+import unittest
+from spinedb_api import DiffDatabaseMapping, import_object_classes, import_objects
+from spinedb_api.mapping import Position
+from spinedb_api.export_mapping import object_export
+from spinedb_api.spine_io.exporters.writer import write
+from spinedb_api.spine_io.exporters.csv_writer import CsvWriter
+
+
+class TestCsvWriter(unittest.TestCase):
+    def setUp(self):
+        self._temp_dir = TemporaryDirectory()
+
+    def tearDown(self):
+        self._temp_dir.cleanup()
+
+    def test_write_empty_database(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        root_mapping = object_export(0, 1)
+        out_path = Path(self._temp_dir.name, "out.csv")
+        writer = CsvWriter(out_path.parent, out_path.name)
+        write(db_map, writer, root_mapping)
+        self.assertTrue(out_path.exists())
+        with open(out_path) as out_file:
+            self.assertEqual(out_file.readlines(), [])
+        db_map.connection.close()
+
+    def test_write_single_object_class_and_object(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_export(0, 1)
+        out_path = Path(self._temp_dir.name, "out.csv")
+        writer = CsvWriter(out_path.parent, out_path.name)
+        write(db_map, writer, root_mapping)
+        self.assertTrue(out_path.exists())
+        with open(out_path) as out_file:
+            self.assertEqual(out_file.readlines(), ["oc,o1\n"])
+        db_map.connection.close()
+
+    def test_tables_are_written_to_separate_files(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_objects(db_map, (("oc1", "o1"), ("oc2", "o2")))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_export(Position.table_name, 0)
+        out_path = Path(self._temp_dir.name, "out.csv")
+        writer = CsvWriter(out_path.parent, out_path.name)
+        write(db_map, writer, root_mapping)
+        self.assertFalse(out_path.exists())
+        out_files = list()
+        for real_out_path in Path(self._temp_dir.name).iterdir():
+            out_files.append(real_out_path.name)
+            expected = None
+            if real_out_path.name == "oc1.csv":
+                expected = ["o1\n"]
+            elif real_out_path.name == "oc2.csv":
+                expected = ["o2\n"]
+            with open(real_out_path) as out_file:
+                self.assertEqual(out_file.readlines(), expected)
+        self.assertEqual(len(out_files), 2)
+        self.assertEqual(set(out_files), {"oc1.csv", "oc2.csv"})
+        db_map.connection.close()
+
+    def test_append_to_table(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        root_mapping1 = object_export(0, 1)
+        root_mapping2 = object_export(0, 1)
+        out_path = Path(self._temp_dir.name, "out.csv")
+        writer = CsvWriter(out_path.parent, out_path.name)
+        write(db_map, writer, root_mapping1, root_mapping2)
+        self.assertTrue(out_path.exists())
+        with open(out_path) as out_file:
+            self.assertEqual(out_file.readlines(), ["oc,o1\n", "oc,o1\n"])
+        db_map.connection.close()
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/exporters/test_excel_writer.py` & `spinedb_api-0.30.4/tests/spine_io/exporters/test_excel_writer.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,144 +1,144 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Unit tests for Excel writer.
-
-"""
-import os.path
-from tempfile import TemporaryDirectory
-import unittest
-from openpyxl import load_workbook
-from spinedb_api import DiffDatabaseMapping, import_object_classes, import_objects
-from spinedb_api.mapping import Position
-from spinedb_api.export_mapping import object_export
-from spinedb_api.spine_io.exporters.writer import write
-from spinedb_api.spine_io.exporters.excel_writer import ExcelWriter
-
-
-class TestExcelWriter(unittest.TestCase):
-    def setUp(self):
-        self._temp_dir = TemporaryDirectory()
-
-    def tearDown(self):
-        self._temp_dir.cleanup()
-
-    def test_write_empty_database(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root_mapping = object_export(0, 1)
-        path = os.path.join(self._temp_dir.name, "test.xlsx")
-        writer = ExcelWriter(path)
-        write(db_map, writer, root_mapping)
-        workbook = load_workbook(path, read_only=True)
-        self.assertEqual(workbook.sheetnames, ["Sheet1"])
-        sheet = workbook["Sheet1"]
-        self.assertEqual(sheet.calculate_dimension(), "A1:A1")
-        workbook.close()
-        db_map.connection.close()
-
-    def test_write_single_object_class_and_object(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_export(0, 1)
-        path = os.path.join(self._temp_dir.name, "test.xlsx")
-        writer = ExcelWriter(path)
-        write(db_map, writer, root_mapping)
-        workbook = load_workbook(path, read_only=True)
-        self.assertEqual(workbook.sheetnames, ["Sheet1"])
-        expected = [["oc", "o1"]]
-        self.check_sheet(workbook, "Sheet1", expected)
-        workbook.close()
-        db_map.connection.close()
-
-    def test_write_to_existing_sheet(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("Sheet1",))
-        import_objects(db_map, (("Sheet1", "o1"), ("Sheet1", "o2")))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_export(Position.table_name, 0)
-        path = os.path.join(self._temp_dir.name, "test.xlsx")
-        writer = ExcelWriter(path)
-        write(db_map, writer, root_mapping)
-        workbook = load_workbook(path, read_only=True)
-        self.assertEqual(workbook.sheetnames, ["Sheet1"])
-        expected = [["o1"], ["o2"]]
-        self.check_sheet(workbook, "Sheet1", expected)
-        workbook.close()
-        db_map.connection.close()
-
-    def test_write_to_named_sheets(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", ("oc2")))
-        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_export(Position.table_name, 1)
-        path = os.path.join(self._temp_dir.name, "test.xlsx")
-        writer = ExcelWriter(path)
-        write(db_map, writer, root_mapping)
-        workbook = load_workbook(path, read_only=True)
-        self.assertEqual(workbook.sheetnames, ["oc1", "oc2"])
-        expected = [[None, "o11"], [None, "o12"]]
-        self.check_sheet(workbook, "oc1", expected)
-        expected = [[None, "o21"]]
-        self.check_sheet(workbook, "oc2", expected)
-        workbook.close()
-        db_map.connection.close()
-
-    def test_append_to_anonymous_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        root_mapping1 = object_export(0, 1)
-        root_mapping2 = object_export(0, 1)
-        path = os.path.join(self._temp_dir.name, "test.xlsx")
-        writer = ExcelWriter(path)
-        write(db_map, writer, root_mapping1, root_mapping2)
-        workbook = load_workbook(path, read_only=True)
-        self.assertEqual(workbook.sheetnames, ["Sheet1"])
-        expected = [["oc", "o1"], ["oc", "o1"]]
-        self.check_sheet(workbook, "Sheet1", expected)
-        workbook.close()
-        db_map.connection.close()
-
-    def test_append_to_named_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        root_mapping1 = object_export(Position.table_name, 0)
-        root_mapping2 = object_export(Position.table_name, 0)
-        path = os.path.join(self._temp_dir.name, "test.xlsx")
-        writer = ExcelWriter(path)
-        write(db_map, writer, root_mapping1, root_mapping2)
-        workbook = load_workbook(path, read_only=True)
-        self.assertEqual(workbook.sheetnames, ["oc"])
-        expected = [["o1"], ["o1"]]
-        self.check_sheet(workbook, "oc", expected)
-        workbook.close()
-        db_map.connection.close()
-
-    def check_sheet(self, workbook, sheet_name, expected):
-        """
-        Args:
-            workbook (Workbook): a workbook to check
-            sheet_name (str): sheet name
-            expected (list): expected rows
-        """
-        sheet = workbook[sheet_name]
-        for row, expected_row in zip(sheet.iter_rows(), expected):
-            values = [cell.value for cell in row]
-            self.assertEqual(values, expected_row)
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Unit tests for Excel writer.
+
+"""
+import os.path
+from tempfile import TemporaryDirectory
+import unittest
+from openpyxl import load_workbook
+from spinedb_api import DiffDatabaseMapping, import_object_classes, import_objects
+from spinedb_api.mapping import Position
+from spinedb_api.export_mapping import object_export
+from spinedb_api.spine_io.exporters.writer import write
+from spinedb_api.spine_io.exporters.excel_writer import ExcelWriter
+
+
+class TestExcelWriter(unittest.TestCase):
+    def setUp(self):
+        self._temp_dir = TemporaryDirectory()
+
+    def tearDown(self):
+        self._temp_dir.cleanup()
+
+    def test_write_empty_database(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        root_mapping = object_export(0, 1)
+        path = os.path.join(self._temp_dir.name, "test.xlsx")
+        writer = ExcelWriter(path)
+        write(db_map, writer, root_mapping)
+        workbook = load_workbook(path, read_only=True)
+        self.assertEqual(workbook.sheetnames, ["Sheet1"])
+        sheet = workbook["Sheet1"]
+        self.assertEqual(sheet.calculate_dimension(), "A1:A1")
+        workbook.close()
+        db_map.connection.close()
+
+    def test_write_single_object_class_and_object(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_export(0, 1)
+        path = os.path.join(self._temp_dir.name, "test.xlsx")
+        writer = ExcelWriter(path)
+        write(db_map, writer, root_mapping)
+        workbook = load_workbook(path, read_only=True)
+        self.assertEqual(workbook.sheetnames, ["Sheet1"])
+        expected = [["oc", "o1"]]
+        self.check_sheet(workbook, "Sheet1", expected)
+        workbook.close()
+        db_map.connection.close()
+
+    def test_write_to_existing_sheet(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("Sheet1",))
+        import_objects(db_map, (("Sheet1", "o1"), ("Sheet1", "o2")))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_export(Position.table_name, 0)
+        path = os.path.join(self._temp_dir.name, "test.xlsx")
+        writer = ExcelWriter(path)
+        write(db_map, writer, root_mapping)
+        workbook = load_workbook(path, read_only=True)
+        self.assertEqual(workbook.sheetnames, ["Sheet1"])
+        expected = [["o1"], ["o2"]]
+        self.check_sheet(workbook, "Sheet1", expected)
+        workbook.close()
+        db_map.connection.close()
+
+    def test_write_to_named_sheets(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", ("oc2")))
+        import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_export(Position.table_name, 1)
+        path = os.path.join(self._temp_dir.name, "test.xlsx")
+        writer = ExcelWriter(path)
+        write(db_map, writer, root_mapping)
+        workbook = load_workbook(path, read_only=True)
+        self.assertEqual(workbook.sheetnames, ["oc1", "oc2"])
+        expected = [[None, "o11"], [None, "o12"]]
+        self.check_sheet(workbook, "oc1", expected)
+        expected = [[None, "o21"]]
+        self.check_sheet(workbook, "oc2", expected)
+        workbook.close()
+        db_map.connection.close()
+
+    def test_append_to_anonymous_table(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        root_mapping1 = object_export(0, 1)
+        root_mapping2 = object_export(0, 1)
+        path = os.path.join(self._temp_dir.name, "test.xlsx")
+        writer = ExcelWriter(path)
+        write(db_map, writer, root_mapping1, root_mapping2)
+        workbook = load_workbook(path, read_only=True)
+        self.assertEqual(workbook.sheetnames, ["Sheet1"])
+        expected = [["oc", "o1"], ["oc", "o1"]]
+        self.check_sheet(workbook, "Sheet1", expected)
+        workbook.close()
+        db_map.connection.close()
+
+    def test_append_to_named_table(self):
+        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        root_mapping1 = object_export(Position.table_name, 0)
+        root_mapping2 = object_export(Position.table_name, 0)
+        path = os.path.join(self._temp_dir.name, "test.xlsx")
+        writer = ExcelWriter(path)
+        write(db_map, writer, root_mapping1, root_mapping2)
+        workbook = load_workbook(path, read_only=True)
+        self.assertEqual(workbook.sheetnames, ["oc"])
+        expected = [["o1"], ["o1"]]
+        self.check_sheet(workbook, "oc", expected)
+        workbook.close()
+        db_map.connection.close()
+
+    def check_sheet(self, workbook, sheet_name, expected):
+        """
+        Args:
+            workbook (Workbook): a workbook to check
+            sheet_name (str): sheet name
+            expected (list): expected rows
+        """
+        sheet = workbook[sheet_name]
+        for row, expected_row in zip(sheet.iter_rows(), expected):
+            values = [cell.value for cell in row]
+            self.assertEqual(values, expected_row)
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/exporters/test_gdx_writer.py` & `spinedb_api-0.30.4/tests/spine_io/exporters/test_gdx_writer.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,302 +1,302 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Unit tests for gdx writer.
-
-"""
-import math
-from pathlib import Path
-import sys
-from tempfile import TemporaryDirectory
-import unittest
-from gdx2py import GdxFile, GAMSParameter
-
-from spinedb_api.spine_io.gdx_utils import find_gams_directory
-from spinedb_api.spine_io.exporters.gdx_writer import GdxWriter
-from spinedb_api.spine_io.exporters.writer import write, WriterException
-from spinedb_api import (
-    DatabaseMapping,
-    import_object_classes,
-    import_object_parameters,
-    import_object_parameter_values,
-    import_objects,
-    import_relationship_classes,
-    import_relationships,
-    Map,
-)
-from spinedb_api.mapping import Position, unflatten
-from spinedb_api.export_mapping import object_export, object_parameter_export, relationship_export
-from spinedb_api.export_mapping.export_mapping import FixedValueMapping
-
-
-class TestGdxWriter(unittest.TestCase):
-    _gams_dir = find_gams_directory()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_write_empty_database(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        root_mapping = object_export(class_position=Position.table_name, object_position=0)
-        root_mapping.child.header = "*"
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_write_empty_database.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            write(db_map, writer, root_mapping)
-            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
-                self.assertEqual(len(gdx_file), 0)
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_write_single_object_class_and_object(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_export(Position.table_name, 0)
-        root_mapping.child.header = "*"
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_write_single_object_class_and_object.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            write(db_map, writer, root_mapping)
-            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
-                self.assertEqual(len(gdx_file), 1)
-                gams_set = gdx_file["oc"]
-                self.assertIsNone(gams_set.domain)
-                self.assertEqual(gams_set.elements, ["o1"])
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_write_2D_relationship(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_objects(db_map, (("oc1", "o1"), ("oc2", "o2")))
-        import_relationship_classes(db_map, (("rel", ("oc1", "oc2")),))
-        import_relationships(db_map, (("rel", ("o1", "o2")),))
-        db_map.commit_session("Add test data.")
-        root_mapping = relationship_export(
-            Position.table_name, Position.hidden, [Position.header, Position.header], [0, 1]
-        )
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_write_2D_relationship.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            write(db_map, writer, root_mapping)
-            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
-                self.assertEqual(len(gdx_file), 1)
-                gams_set = gdx_file["rel"]
-                self.assertEqual(gams_set.domain, ["oc1", "oc2"])
-                self.assertEqual(gams_set.elements, [("o1", "o2")])
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_write_parameters(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"),))
-        import_object_parameter_values(db_map, (("oc", "o1", "p", 2.3),))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(class_position=Position.table_name, object_position=0, value_position=1)
-        mappings = root_mapping.flatten()
-        mappings[3].header = "*"
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_write_parameters.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            write(db_map, writer, root_mapping)
-            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
-                self.assertEqual(len(gdx_file), 1)
-                gams_parameter = gdx_file["oc"]
-                self.assertEqual(len(gams_parameter), 1)
-                self.assertEqual(gams_parameter["o1"], 2.3)
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_non_numerical_parameter_value_raises_writer_expection(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"),))
-        import_object_parameter_values(db_map, (("oc", "o1", "p", "text"),))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(class_position=Position.table_name, object_position=0, value_position=1)
-        mappings = root_mapping.flatten()
-        mappings[3].header = "*"
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_write_parameters.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            self.assertRaises(WriterException, write, db_map, writer, root_mapping)
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_empty_parameter(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"),))
-        import_object_parameter_values(db_map, (("oc", "o1", "p", Map([], [], str)),))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(class_position=Position.table_name, object_position=0, value_position=1)
-        mappings = root_mapping.flatten()
-        mappings[3].header = "*"
-        mappings[-1].filter_re = "single_value"
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_write_parameters.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            write(db_map, writer, root_mapping)
-            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
-                self.assertEqual(len(gdx_file), 1)
-                gams_parameter = gdx_file["oc"]
-                self.assertIsInstance(gams_parameter, GAMSParameter)
-                self.assertEqual(len(gams_parameter), 0)
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_write_scalars(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"),))
-        import_object_parameter_values(db_map, (("oc", "o1", "p", 2.3),))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(class_position=Position.table_name, value_position=0)
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_write_scalars.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            write(db_map, writer, root_mapping)
-            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
-                self.assertEqual(len(gdx_file), 1)
-                gams_scalar = gdx_file["oc"]
-                self.assertEqual(float(gams_scalar), 2.3)
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_two_tables(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_objects(db_map, (("oc1", "o"), ("oc2", "p")))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_export(class_position=Position.table_name, object_position=0)
-        root_mapping.child.header = "*"
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_two_tables.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            write(db_map, writer, root_mapping)
-            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
-                self.assertEqual(len(gdx_file), 2)
-                gams_set = gdx_file["oc1"]
-                self.assertIsNone(gams_set.domain)
-                self.assertEqual(gams_set.elements, ["o"])
-                gams_set = gdx_file["oc2"]
-                self.assertIsNone(gams_set.domain)
-                self.assertEqual(gams_set.elements, ["p"])
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_append_to_table(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_objects(db_map, (("oc1", "o"), ("oc2", "p")))
-        db_map.commit_session("Add test data.")
-        root_mapping1 = unflatten(
-            [FixedValueMapping(Position.table_name, value="set_X")] + object_export(object_position=0).flatten()
-        )
-        root_mapping1.child.filter_re = "oc1"
-        root_mapping1.child.child.header = "*"
-        root_mapping2 = unflatten(
-            [FixedValueMapping(Position.table_name, value="set_X")] + object_export(object_position=0).flatten()
-        )
-        root_mapping2.child.filter_re = "oc2"
-        root_mapping2.child.child.header = "*"
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_two_tables.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            write(db_map, writer, root_mapping1, root_mapping2)
-            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
-                self.assertEqual(len(gdx_file), 1)
-                gams_set = gdx_file["set_X"]
-                self.assertIsNone(gams_set.domain)
-                self.assertEqual(gams_set.elements, ["o", "p"])
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_parameter_value_non_convertible_to_float_raises_WriterException(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "param"),))
-        import_objects(db_map, (("oc", "o"), ("oc", "p")))
-        import_object_parameter_values(db_map, (("oc", "o", "param", "text"), ("oc", "p", "param", 2.3)))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(
-            class_position=Position.hidden, definition_position=Position.table_name, object_position=0, value_position=1
-        )
-        root_mapping.child.child.child.header = "*"
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_two_tables.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            self.assertRaises(WriterException, write, db_map, writer, root_mapping)
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_non_string_set_element_raises_WriterException(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "param"),))
-        import_objects(db_map, (("oc", "o"), ("oc", "p")))
-        import_object_parameter_values(db_map, (("oc", "o", "param", 2.3), ("oc", "p", "param", "text")))
-        db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(
-            class_position=Position.hidden, definition_position=Position.table_name, object_position=0, value_position=1
-        )
-        root_mapping.child.child.child.header = "*"
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_two_tables.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            self.assertRaises(WriterException, write, db_map, writer, root_mapping)
-        db_map.connection.close()
-
-    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
-    def test_special_value_conversions(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(
-            db_map, (("oc", "epsilon"), ("oc", "infinity"), ("oc", "negative_infinity"), ("oc", "nan"))
-        )
-        import_objects(db_map, (("oc", "o1"),))
-        import_object_parameter_values(
-            db_map,
-            (
-                ("oc", "o1", "epsilon", sys.float_info.min),
-                ("oc", "o1", "infinity", math.inf),
-                ("oc", "o1", "negative_infinity", -math.inf),
-                ("oc", "o1", "nan", math.nan),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(
-            class_position=Position.table_name, object_position=0, definition_position=1, value_position=2
-        )
-        mappings = root_mapping.flatten()
-        mappings[1].header = mappings[3].header = "*"
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir, "test_special_value_conversions.gdx")
-            writer = GdxWriter(str(file_path), self._gams_dir)
-            write(db_map, writer, root_mapping)
-            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
-                self.assertEqual(len(gdx_file), 1)
-                gams_parameter = gdx_file["oc"]
-                self.assertEqual(len(gams_parameter), 4)
-                self.assertEqual(gams_parameter[("o1", "epsilon")], sys.float_info.min)
-                self.assertEqual(gams_parameter[("o1", "infinity")], math.inf)
-                self.assertEqual(gams_parameter[("o1", "negative_infinity")], -math.inf)
-                self.assertTrue(math.isnan(gams_parameter[("o1", "nan")]))
-        db_map.connection.close()
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Unit tests for gdx writer.
+
+"""
+import math
+from pathlib import Path
+import sys
+from tempfile import TemporaryDirectory
+import unittest
+from gdx2py import GdxFile, GAMSParameter
+
+from spinedb_api.spine_io.gdx_utils import find_gams_directory
+from spinedb_api.spine_io.exporters.gdx_writer import GdxWriter
+from spinedb_api.spine_io.exporters.writer import write, WriterException
+from spinedb_api import (
+    DatabaseMapping,
+    import_object_classes,
+    import_object_parameters,
+    import_object_parameter_values,
+    import_objects,
+    import_relationship_classes,
+    import_relationships,
+    Map,
+)
+from spinedb_api.mapping import Position, unflatten
+from spinedb_api.export_mapping import object_export, object_parameter_export, relationship_export
+from spinedb_api.export_mapping.export_mapping import FixedValueMapping
+
+
+class TestGdxWriter(unittest.TestCase):
+    _gams_dir = find_gams_directory()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_write_empty_database(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        root_mapping = object_export(class_position=Position.table_name, object_position=0)
+        root_mapping.child.header = "*"
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_write_empty_database.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            write(db_map, writer, root_mapping)
+            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
+                self.assertEqual(len(gdx_file), 0)
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_write_single_object_class_and_object(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_export(Position.table_name, 0)
+        root_mapping.child.header = "*"
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_write_single_object_class_and_object.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            write(db_map, writer, root_mapping)
+            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
+                self.assertEqual(len(gdx_file), 1)
+                gams_set = gdx_file["oc"]
+                self.assertIsNone(gams_set.domain)
+                self.assertEqual(gams_set.elements, ["o1"])
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_write_2D_relationship(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_objects(db_map, (("oc1", "o1"), ("oc2", "o2")))
+        import_relationship_classes(db_map, (("rel", ("oc1", "oc2")),))
+        import_relationships(db_map, (("rel", ("o1", "o2")),))
+        db_map.commit_session("Add test data.")
+        root_mapping = relationship_export(
+            Position.table_name, Position.hidden, [Position.header, Position.header], [0, 1]
+        )
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_write_2D_relationship.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            write(db_map, writer, root_mapping)
+            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
+                self.assertEqual(len(gdx_file), 1)
+                gams_set = gdx_file["rel"]
+                self.assertEqual(gams_set.domain, ["oc1", "oc2"])
+                self.assertEqual(gams_set.elements, [("o1", "o2")])
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_write_parameters(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"),))
+        import_object_parameter_values(db_map, (("oc", "o1", "p", 2.3),))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_parameter_export(class_position=Position.table_name, object_position=0, value_position=1)
+        mappings = root_mapping.flatten()
+        mappings[3].header = "*"
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_write_parameters.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            write(db_map, writer, root_mapping)
+            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
+                self.assertEqual(len(gdx_file), 1)
+                gams_parameter = gdx_file["oc"]
+                self.assertEqual(len(gams_parameter), 1)
+                self.assertEqual(gams_parameter["o1"], 2.3)
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_non_numerical_parameter_value_raises_writer_expection(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"),))
+        import_object_parameter_values(db_map, (("oc", "o1", "p", "text"),))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_parameter_export(class_position=Position.table_name, object_position=0, value_position=1)
+        mappings = root_mapping.flatten()
+        mappings[3].header = "*"
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_write_parameters.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            self.assertRaises(WriterException, write, db_map, writer, root_mapping)
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_empty_parameter(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"),))
+        import_object_parameter_values(db_map, (("oc", "o1", "p", Map([], [], str)),))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_parameter_export(class_position=Position.table_name, object_position=0, value_position=1)
+        mappings = root_mapping.flatten()
+        mappings[3].header = "*"
+        mappings[-1].filter_re = "single_value"
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_write_parameters.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            write(db_map, writer, root_mapping)
+            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
+                self.assertEqual(len(gdx_file), 1)
+                gams_parameter = gdx_file["oc"]
+                self.assertIsInstance(gams_parameter, GAMSParameter)
+                self.assertEqual(len(gams_parameter), 0)
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_write_scalars(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"),))
+        import_object_parameter_values(db_map, (("oc", "o1", "p", 2.3),))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_parameter_export(class_position=Position.table_name, value_position=0)
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_write_scalars.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            write(db_map, writer, root_mapping)
+            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
+                self.assertEqual(len(gdx_file), 1)
+                gams_scalar = gdx_file["oc"]
+                self.assertEqual(float(gams_scalar), 2.3)
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_two_tables(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_objects(db_map, (("oc1", "o"), ("oc2", "p")))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_export(class_position=Position.table_name, object_position=0)
+        root_mapping.child.header = "*"
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_two_tables.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            write(db_map, writer, root_mapping)
+            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
+                self.assertEqual(len(gdx_file), 2)
+                gams_set = gdx_file["oc1"]
+                self.assertIsNone(gams_set.domain)
+                self.assertEqual(gams_set.elements, ["o"])
+                gams_set = gdx_file["oc2"]
+                self.assertIsNone(gams_set.domain)
+                self.assertEqual(gams_set.elements, ["p"])
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_append_to_table(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc1", "oc2"))
+        import_objects(db_map, (("oc1", "o"), ("oc2", "p")))
+        db_map.commit_session("Add test data.")
+        root_mapping1 = unflatten(
+            [FixedValueMapping(Position.table_name, value="set_X")] + object_export(object_position=0).flatten()
+        )
+        root_mapping1.child.filter_re = "oc1"
+        root_mapping1.child.child.header = "*"
+        root_mapping2 = unflatten(
+            [FixedValueMapping(Position.table_name, value="set_X")] + object_export(object_position=0).flatten()
+        )
+        root_mapping2.child.filter_re = "oc2"
+        root_mapping2.child.child.header = "*"
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_two_tables.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            write(db_map, writer, root_mapping1, root_mapping2)
+            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
+                self.assertEqual(len(gdx_file), 1)
+                gams_set = gdx_file["set_X"]
+                self.assertIsNone(gams_set.domain)
+                self.assertEqual(gams_set.elements, ["o", "p"])
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_parameter_value_non_convertible_to_float_raises_WriterException(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "param"),))
+        import_objects(db_map, (("oc", "o"), ("oc", "p")))
+        import_object_parameter_values(db_map, (("oc", "o", "param", "text"), ("oc", "p", "param", 2.3)))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_parameter_export(
+            class_position=Position.hidden, definition_position=Position.table_name, object_position=0, value_position=1
+        )
+        root_mapping.child.child.child.header = "*"
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_two_tables.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            self.assertRaises(WriterException, write, db_map, writer, root_mapping)
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_non_string_set_element_raises_WriterException(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "param"),))
+        import_objects(db_map, (("oc", "o"), ("oc", "p")))
+        import_object_parameter_values(db_map, (("oc", "o", "param", 2.3), ("oc", "p", "param", "text")))
+        db_map.commit_session("Add test data.")
+        root_mapping = object_parameter_export(
+            class_position=Position.hidden, definition_position=Position.table_name, object_position=0, value_position=1
+        )
+        root_mapping.child.child.child.header = "*"
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_two_tables.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            self.assertRaises(WriterException, write, db_map, writer, root_mapping)
+        db_map.connection.close()
+
+    @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
+    def test_special_value_conversions(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(
+            db_map, (("oc", "epsilon"), ("oc", "infinity"), ("oc", "negative_infinity"), ("oc", "nan"))
+        )
+        import_objects(db_map, (("oc", "o1"),))
+        import_object_parameter_values(
+            db_map,
+            (
+                ("oc", "o1", "epsilon", sys.float_info.min),
+                ("oc", "o1", "infinity", math.inf),
+                ("oc", "o1", "negative_infinity", -math.inf),
+                ("oc", "o1", "nan", math.nan),
+            ),
+        )
+        db_map.commit_session("Add test data.")
+        root_mapping = object_parameter_export(
+            class_position=Position.table_name, object_position=0, definition_position=1, value_position=2
+        )
+        mappings = root_mapping.flatten()
+        mappings[1].header = mappings[3].header = "*"
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir, "test_special_value_conversions.gdx")
+            writer = GdxWriter(str(file_path), self._gams_dir)
+            write(db_map, writer, root_mapping)
+            with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
+                self.assertEqual(len(gdx_file), 1)
+                gams_parameter = gdx_file["oc"]
+                self.assertEqual(len(gams_parameter), 4)
+                self.assertEqual(gams_parameter[("o1", "epsilon")], sys.float_info.min)
+                self.assertEqual(gams_parameter[("o1", "infinity")], math.inf)
+                self.assertEqual(gams_parameter[("o1", "negative_infinity")], -math.inf)
+                self.assertTrue(math.isnan(gams_parameter[("o1", "nan")]))
+        db_map.connection.close()
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/exporters/test_sql_writer.py` & `spinedb_api-0.30.4/tests/spine_io/exporters/test_sql_writer.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,282 +1,282 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Unit tests for SQL writer.
-
-"""
-from pathlib import Path
-from tempfile import TemporaryDirectory
-import unittest
-from sqlalchemy import Column, create_engine, MetaData, String, Table
-from sqlalchemy.orm import Session
-from spinedb_api import (
-    DateTime,
-    DatabaseMapping,
-    Duration,
-    import_object_classes,
-    import_objects,
-    import_object_parameters,
-    import_object_parameter_values,
-)
-from spinedb_api.mapping import Position, unflatten
-from spinedb_api.export_mapping import object_export
-from spinedb_api.export_mapping.export_mapping import (
-    AlternativeMapping,
-    FixedValueMapping,
-    ObjectClassMapping,
-    ObjectMapping,
-    ParameterDefinitionMapping,
-    ParameterValueMapping,
-)
-from spinedb_api.spine_io.exporters.writer import write
-from spinedb_api.spine_io.exporters.sql_writer import SqlWriter
-
-
-class TestSqlWriter(unittest.TestCase):
-    def setUp(self):
-        self._temp_dir = TemporaryDirectory()
-
-    def tearDown(self):
-        self._temp_dir.cleanup()
-
-    def test_write_empty_database(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        settings = FixedValueMapping(Position.table_name, "table 1")
-        out_path = Path(self._temp_dir.name, "out.sqlite")
-        writer = SqlWriter(str(out_path), overwrite_existing=True)
-        write(db_map, writer, settings)
-        db_map.connection.close()
-        self.assertTrue(out_path.exists())
-
-    def test_write_header_only(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        db_map.commit_session("Add test data.")
-        root_mapping = unflatten(
-            [
-                FixedValueMapping(Position.table_name, "table 1"),
-                ObjectClassMapping(0, header="classes"),
-                ObjectMapping(1, header="objects"),
-            ]
-        )
-        out_path = Path(self._temp_dir.name, "out.sqlite")
-        writer = SqlWriter(str(out_path), overwrite_existing=True)
-        write(db_map, writer, root_mapping)
-        db_map.connection.close()
-        self.assertTrue(out_path.exists())
-        engine = create_engine("sqlite:///" + str(out_path))
-        connection = engine.connect()
-        try:
-            metadata = MetaData()
-            metadata.reflect(bind=engine)
-            session = Session(engine)
-            self.assertIn("table 1", metadata.tables)
-            table = metadata.tables["table 1"]
-            column_names = [str(c) for c in table.c]
-            self.assertEqual(column_names, ["table 1.classes", "table 1.objects"])
-            self.assertEqual(len(session.query(table).all()), 0)
-            session.close()
-        finally:
-            connection.close()
-
-    def test_write_single_object_class_and_object(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        root_mapping = unflatten(
-            [
-                FixedValueMapping(Position.table_name, "table 1"),
-                ObjectClassMapping(0, header="classes"),
-                ObjectMapping(1, header="objects"),
-            ]
-        )
-        out_path = Path(self._temp_dir.name, "out.sqlite")
-        writer = SqlWriter(str(out_path), overwrite_existing=True)
-        write(db_map, writer, root_mapping)
-        db_map.connection.close()
-        self.assertTrue(out_path.exists())
-        engine = create_engine("sqlite:///" + str(out_path))
-        connection = engine.connect()
-        try:
-            metadata = MetaData()
-            metadata.reflect(bind=engine)
-            session = Session(engine)
-            self.assertIn("table 1", metadata.tables)
-            table = metadata.tables["table 1"]
-            column_names = [str(c) for c in table.c]
-            self.assertEqual(column_names, ["table 1.classes", "table 1.objects"])
-            for class_ in session.query(table):
-                self.assertEqual(class_, ("oc", "o1"))
-            session.close()
-        finally:
-            connection.close()
-
-    def test_write_datetime_value(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"),))
-        dt = DateTime("2021-04-08T08:00")
-        import_object_parameter_values(db_map, (("oc", "o1", "p", DateTime("2021-04-08T08:00")),))
-        db_map.commit_session("Add test data.")
-        root_mapping = unflatten(
-            [
-                FixedValueMapping(Position.table_name, "table 1"),
-                ObjectClassMapping(0, header="classes"),
-                ObjectMapping(1, header="objects"),
-                ParameterDefinitionMapping(2, header="parameters"),
-                AlternativeMapping(Position.hidden),
-                ParameterValueMapping(3, header="values"),
-            ]
-        )
-        out_path = Path(self._temp_dir.name, "out.sqlite")
-        writer = SqlWriter(str(out_path), overwrite_existing=True)
-        write(db_map, writer, root_mapping)
-        db_map.connection.close()
-        self.assertTrue(out_path.exists())
-        engine = create_engine("sqlite:///" + str(out_path))
-        connection = engine.connect()
-        try:
-            metadata = MetaData()
-            metadata.reflect(bind=engine)
-            session = Session(engine)
-            self.assertIn("table 1", metadata.tables)
-            table = metadata.tables["table 1"]
-            column_names = [str(c) for c in table.c]
-            self.assertEqual(
-                column_names, ["table 1.classes", "table 1.objects", "table 1.parameters", "table 1.values"]
-            )
-            for class_ in session.query(table):
-                self.assertEqual(class_, ("oc", "o1", "p", dt.value))
-            session.close()
-        finally:
-            connection.close()
-
-    def test_write_duration_value(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o1"),))
-        import_object_parameter_values(db_map, (("oc", "o1", "p", Duration("3h")),))
-        db_map.commit_session("Add test data.")
-        root_mapping = unflatten(
-            [
-                FixedValueMapping(Position.table_name, "table 1"),
-                ObjectClassMapping(0, header="classes"),
-                ObjectMapping(1, header="objects"),
-                ParameterDefinitionMapping(2, header="parameters"),
-                AlternativeMapping(Position.hidden),
-                ParameterValueMapping(3, header="values"),
-            ]
-        )
-        out_path = Path(self._temp_dir.name, "out.sqlite")
-        writer = SqlWriter(str(out_path), overwrite_existing=True)
-        write(db_map, writer, root_mapping)
-        db_map.connection.close()
-        self.assertTrue(out_path.exists())
-        engine = create_engine("sqlite:///" + str(out_path))
-        connection = engine.connect()
-        try:
-            metadata = MetaData()
-            metadata.reflect(bind=engine)
-            session = Session(engine)
-            self.assertIn("table 1", metadata.tables)
-            table = metadata.tables["table 1"]
-            column_names = [str(c) for c in table.c]
-            self.assertEqual(
-                column_names, ["table 1.classes", "table 1.objects", "table 1.parameters", "table 1.values"]
-            )
-            for class_ in session.query(table):
-                self.assertEqual(class_, ("oc", "o1", "p", "3h"))
-            session.close()
-        finally:
-            connection.close()
-
-    def test_append_to_table(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"), ("oc", "q1")))
-        db_map.commit_session("Add test data.")
-        root_mapping1 = object_export(Position.table_name, 0)
-        root_mapping1.child.header = "objects"
-        root_mapping1.child.filter_re = "o1"
-        root_mapping2 = object_export(Position.table_name, 0)
-        root_mapping2.child.header = "objects"
-        root_mapping2.child.filter_re = "q1"
-        out_path = Path(self._temp_dir.name, "out.sqlite")
-        writer = SqlWriter(str(out_path), overwrite_existing=True)
-        write(db_map, writer, root_mapping1)
-        write(db_map, writer, root_mapping2)
-        db_map.connection.close()
-        self.assertTrue(out_path.exists())
-        engine = create_engine("sqlite:///" + str(out_path))
-        connection = engine.connect()
-        try:
-            metadata = MetaData()
-            metadata.reflect(bind=engine)
-            session = Session(engine)
-            self.assertIn("oc", metadata.tables)
-            table = metadata.tables["oc"]
-            column_names = [str(c) for c in table.c]
-            self.assertEqual(column_names, ["oc.objects"])
-            expected_rows = (("o1",), ("q1",))
-            rows = session.query(table).all()
-            self.assertEqual(len(rows), len(expected_rows))
-            for row, expected in zip(rows, expected_rows):
-                self.assertEqual(row, expected)
-            session.close()
-        finally:
-            connection.close()
-
-    def test_appending_to_table_in_existing_database(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_objects(db_map, (("oc", "o1"),))
-        db_map.commit_session("Add test data.")
-        out_path = Path(self._temp_dir.name, "out.sqlite")
-        out_engine = create_engine("sqlite:///" + str(out_path))
-        out_connection = out_engine.connect()
-        try:
-            metadata = MetaData()
-            object_table = Table("oc", metadata, Column("objects", String))
-            metadata.create_all(out_engine)
-            out_connection.execute(object_table.insert(), objects="initial_object")
-        finally:
-            out_connection.close()
-        root_mapping = object_export(Position.table_name, 0)
-        root_mapping.child.header = "objects"
-        writer = SqlWriter(str(out_path), overwrite_existing=False)
-        write(db_map, writer, root_mapping)
-        db_map.connection.close()
-        self.assertTrue(out_path.exists())
-        engine = create_engine("sqlite:///" + str(out_path))
-        connection = engine.connect()
-        try:
-            metadata = MetaData()
-            metadata.reflect(bind=engine)
-            session = Session(engine)
-            self.assertIn("oc", metadata.tables)
-            table = metadata.tables["oc"]
-            column_names = [str(c) for c in table.c]
-            self.assertEqual(column_names, ["oc.objects"])
-            expected_rows = (("initial_object",), ("o1",))
-            rows = session.query(table).all()
-            self.assertEqual(len(rows), len(expected_rows))
-            for row, expected in zip(rows, expected_rows):
-                self.assertEqual(row, expected)
-            session.close()
-        finally:
-            connection.close()
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Unit tests for SQL writer.
+
+"""
+from pathlib import Path
+from tempfile import TemporaryDirectory
+import unittest
+from sqlalchemy import Column, create_engine, MetaData, String, Table
+from sqlalchemy.orm import Session
+from spinedb_api import (
+    DateTime,
+    DatabaseMapping,
+    Duration,
+    import_object_classes,
+    import_objects,
+    import_object_parameters,
+    import_object_parameter_values,
+)
+from spinedb_api.mapping import Position, unflatten
+from spinedb_api.export_mapping import object_export
+from spinedb_api.export_mapping.export_mapping import (
+    AlternativeMapping,
+    FixedValueMapping,
+    ObjectClassMapping,
+    ObjectMapping,
+    ParameterDefinitionMapping,
+    ParameterValueMapping,
+)
+from spinedb_api.spine_io.exporters.writer import write
+from spinedb_api.spine_io.exporters.sql_writer import SqlWriter
+
+
+class TestSqlWriter(unittest.TestCase):
+    def setUp(self):
+        self._temp_dir = TemporaryDirectory()
+
+    def tearDown(self):
+        self._temp_dir.cleanup()
+
+    def test_write_empty_database(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        settings = FixedValueMapping(Position.table_name, "table 1")
+        out_path = Path(self._temp_dir.name, "out.sqlite")
+        writer = SqlWriter(str(out_path), overwrite_existing=True)
+        write(db_map, writer, settings)
+        db_map.connection.close()
+        self.assertTrue(out_path.exists())
+
+    def test_write_header_only(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        db_map.commit_session("Add test data.")
+        root_mapping = unflatten(
+            [
+                FixedValueMapping(Position.table_name, "table 1"),
+                ObjectClassMapping(0, header="classes"),
+                ObjectMapping(1, header="objects"),
+            ]
+        )
+        out_path = Path(self._temp_dir.name, "out.sqlite")
+        writer = SqlWriter(str(out_path), overwrite_existing=True)
+        write(db_map, writer, root_mapping)
+        db_map.connection.close()
+        self.assertTrue(out_path.exists())
+        engine = create_engine("sqlite:///" + str(out_path))
+        connection = engine.connect()
+        try:
+            metadata = MetaData()
+            metadata.reflect(bind=engine)
+            session = Session(engine)
+            self.assertIn("table 1", metadata.tables)
+            table = metadata.tables["table 1"]
+            column_names = [str(c) for c in table.c]
+            self.assertEqual(column_names, ["table 1.classes", "table 1.objects"])
+            self.assertEqual(len(session.query(table).all()), 0)
+            session.close()
+        finally:
+            connection.close()
+
+    def test_write_single_object_class_and_object(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        root_mapping = unflatten(
+            [
+                FixedValueMapping(Position.table_name, "table 1"),
+                ObjectClassMapping(0, header="classes"),
+                ObjectMapping(1, header="objects"),
+            ]
+        )
+        out_path = Path(self._temp_dir.name, "out.sqlite")
+        writer = SqlWriter(str(out_path), overwrite_existing=True)
+        write(db_map, writer, root_mapping)
+        db_map.connection.close()
+        self.assertTrue(out_path.exists())
+        engine = create_engine("sqlite:///" + str(out_path))
+        connection = engine.connect()
+        try:
+            metadata = MetaData()
+            metadata.reflect(bind=engine)
+            session = Session(engine)
+            self.assertIn("table 1", metadata.tables)
+            table = metadata.tables["table 1"]
+            column_names = [str(c) for c in table.c]
+            self.assertEqual(column_names, ["table 1.classes", "table 1.objects"])
+            for class_ in session.query(table):
+                self.assertEqual(class_, ("oc", "o1"))
+            session.close()
+        finally:
+            connection.close()
+
+    def test_write_datetime_value(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"),))
+        dt = DateTime("2021-04-08T08:00")
+        import_object_parameter_values(db_map, (("oc", "o1", "p", DateTime("2021-04-08T08:00")),))
+        db_map.commit_session("Add test data.")
+        root_mapping = unflatten(
+            [
+                FixedValueMapping(Position.table_name, "table 1"),
+                ObjectClassMapping(0, header="classes"),
+                ObjectMapping(1, header="objects"),
+                ParameterDefinitionMapping(2, header="parameters"),
+                AlternativeMapping(Position.hidden),
+                ParameterValueMapping(3, header="values"),
+            ]
+        )
+        out_path = Path(self._temp_dir.name, "out.sqlite")
+        writer = SqlWriter(str(out_path), overwrite_existing=True)
+        write(db_map, writer, root_mapping)
+        db_map.connection.close()
+        self.assertTrue(out_path.exists())
+        engine = create_engine("sqlite:///" + str(out_path))
+        connection = engine.connect()
+        try:
+            metadata = MetaData()
+            metadata.reflect(bind=engine)
+            session = Session(engine)
+            self.assertIn("table 1", metadata.tables)
+            table = metadata.tables["table 1"]
+            column_names = [str(c) for c in table.c]
+            self.assertEqual(
+                column_names, ["table 1.classes", "table 1.objects", "table 1.parameters", "table 1.values"]
+            )
+            for class_ in session.query(table):
+                self.assertEqual(class_, ("oc", "o1", "p", dt.value))
+            session.close()
+        finally:
+            connection.close()
+
+    def test_write_duration_value(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_object_parameters(db_map, (("oc", "p"),))
+        import_objects(db_map, (("oc", "o1"),))
+        import_object_parameter_values(db_map, (("oc", "o1", "p", Duration("3h")),))
+        db_map.commit_session("Add test data.")
+        root_mapping = unflatten(
+            [
+                FixedValueMapping(Position.table_name, "table 1"),
+                ObjectClassMapping(0, header="classes"),
+                ObjectMapping(1, header="objects"),
+                ParameterDefinitionMapping(2, header="parameters"),
+                AlternativeMapping(Position.hidden),
+                ParameterValueMapping(3, header="values"),
+            ]
+        )
+        out_path = Path(self._temp_dir.name, "out.sqlite")
+        writer = SqlWriter(str(out_path), overwrite_existing=True)
+        write(db_map, writer, root_mapping)
+        db_map.connection.close()
+        self.assertTrue(out_path.exists())
+        engine = create_engine("sqlite:///" + str(out_path))
+        connection = engine.connect()
+        try:
+            metadata = MetaData()
+            metadata.reflect(bind=engine)
+            session = Session(engine)
+            self.assertIn("table 1", metadata.tables)
+            table = metadata.tables["table 1"]
+            column_names = [str(c) for c in table.c]
+            self.assertEqual(
+                column_names, ["table 1.classes", "table 1.objects", "table 1.parameters", "table 1.values"]
+            )
+            for class_ in session.query(table):
+                self.assertEqual(class_, ("oc", "o1", "p", "3h"))
+            session.close()
+        finally:
+            connection.close()
+
+    def test_append_to_table(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"), ("oc", "q1")))
+        db_map.commit_session("Add test data.")
+        root_mapping1 = object_export(Position.table_name, 0)
+        root_mapping1.child.header = "objects"
+        root_mapping1.child.filter_re = "o1"
+        root_mapping2 = object_export(Position.table_name, 0)
+        root_mapping2.child.header = "objects"
+        root_mapping2.child.filter_re = "q1"
+        out_path = Path(self._temp_dir.name, "out.sqlite")
+        writer = SqlWriter(str(out_path), overwrite_existing=True)
+        write(db_map, writer, root_mapping1)
+        write(db_map, writer, root_mapping2)
+        db_map.connection.close()
+        self.assertTrue(out_path.exists())
+        engine = create_engine("sqlite:///" + str(out_path))
+        connection = engine.connect()
+        try:
+            metadata = MetaData()
+            metadata.reflect(bind=engine)
+            session = Session(engine)
+            self.assertIn("oc", metadata.tables)
+            table = metadata.tables["oc"]
+            column_names = [str(c) for c in table.c]
+            self.assertEqual(column_names, ["oc.objects"])
+            expected_rows = (("o1",), ("q1",))
+            rows = session.query(table).all()
+            self.assertEqual(len(rows), len(expected_rows))
+            for row, expected in zip(rows, expected_rows):
+                self.assertEqual(row, expected)
+            session.close()
+        finally:
+            connection.close()
+
+    def test_appending_to_table_in_existing_database(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_object_classes(db_map, ("oc",))
+        import_objects(db_map, (("oc", "o1"),))
+        db_map.commit_session("Add test data.")
+        out_path = Path(self._temp_dir.name, "out.sqlite")
+        out_engine = create_engine("sqlite:///" + str(out_path))
+        out_connection = out_engine.connect()
+        try:
+            metadata = MetaData()
+            object_table = Table("oc", metadata, Column("objects", String))
+            metadata.create_all(out_engine)
+            out_connection.execute(object_table.insert(), objects="initial_object")
+        finally:
+            out_connection.close()
+        root_mapping = object_export(Position.table_name, 0)
+        root_mapping.child.header = "objects"
+        writer = SqlWriter(str(out_path), overwrite_existing=False)
+        write(db_map, writer, root_mapping)
+        db_map.connection.close()
+        self.assertTrue(out_path.exists())
+        engine = create_engine("sqlite:///" + str(out_path))
+        connection = engine.connect()
+        try:
+            metadata = MetaData()
+            metadata.reflect(bind=engine)
+            session = Session(engine)
+            self.assertIn("oc", metadata.tables)
+            table = metadata.tables["oc"]
+            column_names = [str(c) for c in table.c]
+            self.assertEqual(column_names, ["oc.objects"])
+            expected_rows = (("initial_object",), ("o1",))
+            rows = session.query(table).all()
+            self.assertEqual(len(rows), len(expected_rows))
+            for row, expected in zip(rows, expected_rows):
+                self.assertEqual(row, expected)
+            session.close()
+        finally:
+            connection.close()
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/exporters/test_writer.py` & `spinedb_api-0.30.4/tests/spine_io/exporters/test_writer.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,90 +1,90 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-"""
-Unit tests for ``writer`` module.
-
-"""
-import unittest
-from spinedb_api import DatabaseMapping, import_object_classes, import_objects
-from spinedb_api.spine_io.exporters.writer import Writer, write
-from spinedb_api.export_mapping.settings import object_export
-
-
-class _TableWriter(Writer):
-    def __init__(self):
-        self._tables = dict()
-        self._current_table = None
-
-    def finish_table(self):
-        self._current_table = None
-
-    def start_table(self, table_name, title_key):
-        self._current_table = self._tables.setdefault(table_name, list())
-        return True
-
-    @property
-    def tables(self):
-        return self._tables
-
-    def write_row(self, row):
-        self._current_table.append(row)
-        return True
-
-
-class TestWrite(unittest.TestCase):
-    def setUp(self):
-        self._db_map = DatabaseMapping("sqlite://", create=True)
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_max_rows(self):
-        import_object_classes(self._db_map, ("class1", "class2"))
-        import_objects(
-            self._db_map,
-            (
-                ("class1", "obj1"),
-                ("class1", "obj2"),
-                ("class1", "obj3"),
-                ("class2", "obj4"),
-                ("class2", "obj5"),
-                ("class2", "obj6"),
-            ),
-        )
-        self._db_map.commit_session("Add test data.")
-        writer = _TableWriter()
-        root_mapping = object_export(0, 1)
-        write(self._db_map, writer, root_mapping, max_rows=2)
-        self.assertEqual(writer.tables, {None: [["class1", "obj1"], ["class1", "obj2"]]})
-
-    def test_max_rows_with_filter(self):
-        import_object_classes(self._db_map, ("class1", "class2"))
-        import_objects(
-            self._db_map,
-            (
-                ("class1", "obj1"),
-                ("class1", "obj2"),
-                ("class1", "obj3"),
-                ("class2", "obj4"),
-                ("class2", "obj5"),
-                ("class2", "obj6"),
-            ),
-        )
-        self._db_map.commit_session("Add test data.")
-        writer = _TableWriter()
-        root_mapping = object_export(0, 1)
-        root_mapping.child.filter_re = "obj6"
-        write(self._db_map, writer, root_mapping, max_rows=1)
-        self.assertEqual(writer.tables, {None: [["class2", "obj6"]]})
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+"""
+Unit tests for ``writer`` module.
+
+"""
+import unittest
+from spinedb_api import DatabaseMapping, import_object_classes, import_objects
+from spinedb_api.spine_io.exporters.writer import Writer, write
+from spinedb_api.export_mapping.settings import object_export
+
+
+class _TableWriter(Writer):
+    def __init__(self):
+        self._tables = dict()
+        self._current_table = None
+
+    def finish_table(self):
+        self._current_table = None
+
+    def start_table(self, table_name, title_key):
+        self._current_table = self._tables.setdefault(table_name, list())
+        return True
+
+    @property
+    def tables(self):
+        return self._tables
+
+    def write_row(self, row):
+        self._current_table.append(row)
+        return True
+
+
+class TestWrite(unittest.TestCase):
+    def setUp(self):
+        self._db_map = DatabaseMapping("sqlite://", create=True)
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_max_rows(self):
+        import_object_classes(self._db_map, ("class1", "class2"))
+        import_objects(
+            self._db_map,
+            (
+                ("class1", "obj1"),
+                ("class1", "obj2"),
+                ("class1", "obj3"),
+                ("class2", "obj4"),
+                ("class2", "obj5"),
+                ("class2", "obj6"),
+            ),
+        )
+        self._db_map.commit_session("Add test data.")
+        writer = _TableWriter()
+        root_mapping = object_export(0, 1)
+        write(self._db_map, writer, root_mapping, max_rows=2)
+        self.assertEqual(writer.tables, {None: [["class1", "obj1"], ["class1", "obj2"]]})
+
+    def test_max_rows_with_filter(self):
+        import_object_classes(self._db_map, ("class1", "class2"))
+        import_objects(
+            self._db_map,
+            (
+                ("class1", "obj1"),
+                ("class1", "obj2"),
+                ("class1", "obj3"),
+                ("class2", "obj4"),
+                ("class2", "obj5"),
+                ("class2", "obj6"),
+            ),
+        )
+        self._db_map.commit_session("Add test data.")
+        writer = _TableWriter()
+        root_mapping = object_export(0, 1)
+        root_mapping.child.filter_re = "obj6"
+        write(self._db_map, writer, root_mapping, max_rows=1)
+        self.assertEqual(writer.tables, {None: [["class2", "obj6"]]})
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/importers/test_CSVConnector.py` & `spinedb_api-0.30.4/tests/spine_io/importers/test_CSVConnector.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,81 +1,81 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains unit tests for CSVConnector.
-
-"""
-
-import csv
-import os.path
-import pickle
-from tempfile import TemporaryDirectory
-import unittest
-from spinedb_api.spine_io.importers.csv_reader import CSVConnector
-
-
-class TestCSVConnector(unittest.TestCase):
-    @staticmethod
-    def _write_basic_csv(file_name):
-        with open(file_name, "w", newline="") as csv_file:
-            writer = csv.writer(csv_file)
-            writer.writerow(["1a", "1b", "1c"])
-            writer.writerow(["2a", "2b", "2c"])
-
-    def test_get_tables(self):
-        with TemporaryDirectory() as data_directory:
-            file_name = os.path.join(data_directory, "test_get_tables.csv")
-            self._write_basic_csv(file_name)
-            connector = CSVConnector(None)
-            connector.connect_to_source(file_name)
-            tables = connector.get_tables()
-            self.assertEqual(len(tables), 1)
-            self.assertTrue("data" in tables)
-            options = tables["data"]["options"]
-            self.assertEqual(options["encoding"], "ascii")
-            self.assertEqual(options["delimiter"], ",")
-            self.assertEqual(options["quotechar"], '"')
-            self.assertEqual(options["skip"], 0)
-            self.assertTrue("has_header" not in options)
-
-    def test_get_data_iterator(self):
-        with TemporaryDirectory() as data_directory:
-            file_name = os.path.join(data_directory, "test_get_data_iterator.csv")
-            self._write_basic_csv(file_name)
-            connector = CSVConnector(None)
-            connector.connect_to_source(file_name)
-            tables = connector.get_tables()
-            options = tables["data"]["options"]
-            _, header = connector.get_data_iterator("", options)
-            self.assertTrue(not header)
-
-    def test_get_data(self):
-        with TemporaryDirectory() as data_directory:
-            file_name = os.path.join(data_directory, "test_get_data.csv")
-            self._write_basic_csv(file_name)
-            connector = CSVConnector(None)
-            connector.connect_to_source(file_name)
-            tables = connector.get_tables()
-            options = tables["data"]["options"]
-            data, header = connector.get_data("", options)
-            self.assertTrue(not header)
-            self.assertEqual(len(data), 2)
-            self.assertEqual(data[0], ["1a", "1b", "1c"])
-            self.assertEqual(data[1], ["2a", "2b", "2c"])
-
-    def test_connector_is_picklable(self):
-        reader = CSVConnector(None)
-        pickled = pickle.dumps(reader)
-        self.assertTrue(pickled)
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains unit tests for CSVConnector.
+
+"""
+
+import csv
+import os.path
+import pickle
+from tempfile import TemporaryDirectory
+import unittest
+from spinedb_api.spine_io.importers.csv_reader import CSVConnector
+
+
+class TestCSVConnector(unittest.TestCase):
+    @staticmethod
+    def _write_basic_csv(file_name):
+        with open(file_name, "w", newline="") as csv_file:
+            writer = csv.writer(csv_file)
+            writer.writerow(["1a", "1b", "1c"])
+            writer.writerow(["2a", "2b", "2c"])
+
+    def test_get_tables(self):
+        with TemporaryDirectory() as data_directory:
+            file_name = os.path.join(data_directory, "test_get_tables.csv")
+            self._write_basic_csv(file_name)
+            connector = CSVConnector(None)
+            connector.connect_to_source(file_name)
+            tables = connector.get_tables()
+            self.assertEqual(len(tables), 1)
+            self.assertTrue("data" in tables)
+            options = tables["data"]["options"]
+            self.assertEqual(options["encoding"], "ascii")
+            self.assertEqual(options["delimiter"], ",")
+            self.assertEqual(options["quotechar"], '"')
+            self.assertEqual(options["skip"], 0)
+            self.assertTrue("has_header" not in options)
+
+    def test_get_data_iterator(self):
+        with TemporaryDirectory() as data_directory:
+            file_name = os.path.join(data_directory, "test_get_data_iterator.csv")
+            self._write_basic_csv(file_name)
+            connector = CSVConnector(None)
+            connector.connect_to_source(file_name)
+            tables = connector.get_tables()
+            options = tables["data"]["options"]
+            _, header = connector.get_data_iterator("", options)
+            self.assertTrue(not header)
+
+    def test_get_data(self):
+        with TemporaryDirectory() as data_directory:
+            file_name = os.path.join(data_directory, "test_get_data.csv")
+            self._write_basic_csv(file_name)
+            connector = CSVConnector(None)
+            connector.connect_to_source(file_name)
+            tables = connector.get_tables()
+            options = tables["data"]["options"]
+            data, header = connector.get_data("", options)
+            self.assertTrue(not header)
+            self.assertEqual(len(data), 2)
+            self.assertEqual(data[0], ["1a", "1b", "1c"])
+            self.assertEqual(data[1], ["2a", "2b", "2c"])
+
+    def test_connector_is_picklable(self):
+        reader = CSVConnector(None)
+        pickled = pickle.dumps(reader)
+        self.assertTrue(pickled)
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/importers/test_GdxConnector.py` & `spinedb_api-0.30.4/tests/spine_io/importers/test_GdxConnector.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,232 +1,232 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for GDXConnector class.
-
-"""
-
-import os.path
-import pickle
-from tempfile import TemporaryDirectory
-import unittest
-from gdx2py import GdxFile
-from spinedb_api.spine_io.gdx_utils import find_gams_directory
-from spinedb_api.spine_io.importers.gdx_connector import GdxConnector, GAMSParameter, GAMSScalar, GAMSSet
-
-
-@unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
-class TestGdxConnector(unittest.TestCase):
-    def test_get_tables(self):
-        gams_directory = find_gams_directory()
-        connector_settings = {"gams_directory": gams_directory}
-        connector = GdxConnector(connector_settings)
-        with TemporaryDirectory() as temporary_dir:
-            path = os.path.join(temporary_dir, "test_get_tables.gdx")
-            with GdxFile(path, "w", gams_directory) as gdx_file:
-                domain = GAMSSet([("key1",)])
-                gdx_file["domain1"] = domain
-                domain = GAMSSet([("key2",)])
-                gdx_file["domain2"] = domain
-                gams_set = GAMSSet([("key1", "key2")], ["domain1", "domain2"])
-                gdx_file["set"] = gams_set
-                gams_parameter = GAMSParameter({("key1", "key2"): 3.14}, domain=["domain1", "domain2"])
-                gdx_file["parameter"] = gams_parameter
-                gams_scalar = GAMSScalar(2.3)
-                gdx_file["scalar"] = gams_scalar
-            connector.connect_to_source(path)
-            tables = connector.get_tables()
-            connector.disconnect()
-        self.assertEqual(len(tables), 5)
-        self.assertTrue("domain1" in tables)
-        self.assertTrue("domain2" in tables)
-        self.assertTrue("set" in tables)
-        self.assertTrue("parameter" in tables)
-        self.assertTrue("scalar" in tables)
-
-    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
-    def test_get_data_iterator_for_domains(self):
-        gams_directory = find_gams_directory()
-        connector_settings = {"gams_directory": gams_directory}
-        connector = GdxConnector(connector_settings)
-        with TemporaryDirectory() as temporary_dir:
-            path = os.path.join(temporary_dir, "test_get_data_iterator_for_domains.gdx")
-            with GdxFile(path, "w", gams_directory) as gdx_file:
-                domain = GAMSSet([("key1",), ("key2",)])
-                gdx_file["domain"] = domain
-            connector.connect_to_source(path)
-            data_iterator, header = connector.get_data_iterator("domain", {})
-            connector.disconnect()
-        self.assertEqual(header, ["dim0"])
-        self.assertEqual(next(data_iterator), ["key1"])
-        self.assertEqual(next(data_iterator), ["key2"])
-        with self.assertRaises(StopIteration):
-            next(data_iterator)
-
-    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
-    def test_get_data_iterator_for_multiple_universal_sets(self):
-        gams_directory = find_gams_directory()
-        connector_settings = {"gams_directory": gams_directory}
-        connector = GdxConnector(connector_settings)
-        with TemporaryDirectory() as temporary_dir:
-            path = os.path.join(temporary_dir, "test_get_data_iterator_for_domains.gdx")
-            with GdxFile(path, "w", gams_directory) as gdx_file:
-                set_ = GAMSSet([("i", "key1"), ("j", "key2")])
-                gdx_file["almost_domain"] = set_
-            connector.connect_to_source(path)
-            data_iterator, header = connector.get_data_iterator("almost_domain", {})
-            connector.disconnect()
-        self.assertEqual(header, ["dim0", "dim1"])
-        self.assertEqual(next(data_iterator), ["i", "key1"])
-        self.assertEqual(next(data_iterator), ["j", "key2"])
-        with self.assertRaises(StopIteration):
-            next(data_iterator)
-
-    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
-    def test_get_data_iterator_for_mixed_universal_and_named_sets(self):
-        gams_directory = find_gams_directory()
-        connector_settings = {"gams_directory": gams_directory}
-        connector = GdxConnector(connector_settings)
-        with TemporaryDirectory() as temporary_dir:
-            path = os.path.join(temporary_dir, "test_get_data_iterator_for_domains.gdx")
-            with GdxFile(path, "w", gams_directory) as gdx_file:
-                set_ = GAMSSet([("key1",), ("key2",)])
-                gdx_file["domain"] = set_
-                set_ = GAMSSet([("i", "key1"), ("j", "key2")], [None, "domain"])
-                gdx_file["almost_domain"] = set_
-            connector.connect_to_source(path)
-            data_iterator, header = connector.get_data_iterator("almost_domain", {})
-            connector.disconnect()
-        self.assertEqual(header, ["dim0", "domain"])
-        self.assertEqual(next(data_iterator), ["i", "key1"])
-        self.assertEqual(next(data_iterator), ["j", "key2"])
-        with self.assertRaises(StopIteration):
-            next(data_iterator)
-
-    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
-    def test_get_data_iterator_for_sets_with_single_indexing_domain(self):
-        gams_directory = find_gams_directory()
-        connector_settings = {"gams_directory": gams_directory}
-        connector = GdxConnector(connector_settings)
-        with TemporaryDirectory() as temporary_dir:
-            path = os.path.join(temporary_dir, "test_get_data_iterator_for_sets_with_single_indexing_domain.gdx")
-            with GdxFile(path, "w", gams_directory) as gdx_file:
-                domain = GAMSSet([("key1",), ("key2",)])
-                gdx_file["domain"] = domain
-                gams_set = GAMSSet([("key1",), ("key2",)], ["domain"])
-                gdx_file["set"] = gams_set
-            connector.connect_to_source(path)
-            data_iterator, header = connector.get_data_iterator("set", {})
-            connector.disconnect()
-        self.assertEqual(header, ["domain"])
-        self.assertEqual(next(data_iterator), ["key1"])
-        self.assertEqual(next(data_iterator), ["key2"])
-        with self.assertRaises(StopIteration):
-            next(data_iterator)
-
-    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
-    def test_get_data_iterator_for_sets_with_multiple_indexing_domains(self):
-        gams_directory = find_gams_directory()
-        connector_settings = {"gams_directory": gams_directory}
-        connector = GdxConnector(connector_settings)
-        with TemporaryDirectory() as temporary_dir:
-            path = os.path.join(temporary_dir, "test_get_data_iterator_for_sets_with_single_indexing_domain.gdx")
-            with GdxFile(path, "w", gams_directory) as gdx_file:
-                domain = GAMSSet([("key1",), ("key2",)])
-                gdx_file["domain1"] = domain
-                domain = GAMSSet([("keyA",), ("keyB",)])
-                gdx_file["domainA"] = domain
-                gams_set = GAMSSet([("key1", "keyA"), ("key2", "keyB")], ["domain1", "domainA"])
-                gdx_file["set"] = gams_set
-            connector.connect_to_source(path)
-            data_iterator, header = connector.get_data_iterator("set", {})
-            connector.disconnect()
-        self.assertEqual(header, ["domain1", "domainA"])
-        self.assertEqual(next(data_iterator), ["key1", "keyA"])
-        self.assertEqual(next(data_iterator), ["key2", "keyB"])
-        with self.assertRaises(StopIteration):
-            next(data_iterator)
-
-    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
-    def test_get_data_iterator_for_parameters_with_single_indexing_domain(self):
-        gams_directory = find_gams_directory()
-        connector_settings = {"gams_directory": gams_directory}
-        connector = GdxConnector(connector_settings)
-        with TemporaryDirectory() as temporary_dir:
-            path = os.path.join(temporary_dir, "test_get_data_iterator_for_parameters_with_single_indexing_domain.gdx")
-            with GdxFile(path, "w", gams_directory) as gdx_file:
-                domain = GAMSSet([("key1",), ("key2",)])
-                gdx_file["domain"] = domain
-                gams_parameter = GAMSParameter({("key1",): 3.14, ("key2",): -2.3}, domain=["domain"])
-                gdx_file["parameter"] = gams_parameter
-            connector.connect_to_source(path)
-            data_iterator, header = connector.get_data_iterator("parameter", {})
-            connector.disconnect()
-        self.assertEqual(header, ["domain", "Value"])
-        self.assertEqual(next(data_iterator), ["key1", 3.14])
-        self.assertEqual(next(data_iterator), ["key2", -2.3])
-        with self.assertRaises(StopIteration):
-            next(data_iterator)
-
-    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
-    def test_get_data_iterator_for_parameters_with_multiple_indexing_domains(self):
-        gams_directory = find_gams_directory()
-        connector_settings = {"gams_directory": gams_directory}
-        connector = GdxConnector(connector_settings)
-        with TemporaryDirectory() as temporary_dir:
-            path = os.path.join(
-                temporary_dir, "test_get_data_iterator_for_parameters_with_multiple_indexing_domains.gdx"
-            )
-            with GdxFile(path, "w", gams_directory) as gdx_file:
-                domain = GAMSSet([("key1",), ("key2",)])
-                gdx_file["domain"] = domain
-                domain = GAMSSet([("keyA",), ("keyB",)])
-                gdx_file["domainA"] = domain
-                gams_parameter = GAMSParameter(
-                    {("key1", "keyA"): 3.14, ("key2", "keyB"): -2.3}, domain=["domain1", "domainA"]
-                )
-                gdx_file["parameter"] = gams_parameter
-            connector.connect_to_source(path)
-            data_iterator, header = connector.get_data_iterator("parameter", {})
-            connector.disconnect()
-        self.assertEqual(header, ["domain1", "domainA", "Value"])
-        self.assertEqual(next(data_iterator), ["key1", "keyA", 3.14])
-        self.assertEqual(next(data_iterator), ["key2", "keyB", -2.3])
-        with self.assertRaises(StopIteration):
-            next(data_iterator)
-
-    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
-    def test_get_data_iterator_for_scalars(self):
-        gams_directory = find_gams_directory()
-        connector_settings = {"gams_directory": gams_directory}
-        connector = GdxConnector(connector_settings)
-        with TemporaryDirectory() as temporary_dir:
-            path = os.path.join(temporary_dir, "test_get_data_iterator_for_scalars.gdx")
-            with GdxFile(path, "w", gams_directory) as gdx_file:
-                gams_scalar = GAMSScalar(2.3)
-                gdx_file["scalar"] = gams_scalar
-            connector.connect_to_source(path)
-            data_iterator, header = connector.get_data_iterator("scalar", {})
-            connector.disconnect()
-        self.assertEqual(header, ["Value"])
-        self.assertEqual(next(data_iterator), [2.3])
-        with self.assertRaises(StopIteration):
-            next(data_iterator)
-
-    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
-    def test_connector_is_picklable(self):
-        reader = GdxConnector(None)
-        pickled = pickle.dumps(reader)
-        self.assertTrue(pickled)
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for GDXConnector class.
+
+"""
+
+import os.path
+import pickle
+from tempfile import TemporaryDirectory
+import unittest
+from gdx2py import GdxFile
+from spinedb_api.spine_io.gdx_utils import find_gams_directory
+from spinedb_api.spine_io.importers.gdx_connector import GdxConnector, GAMSParameter, GAMSScalar, GAMSSet
+
+
+@unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
+class TestGdxConnector(unittest.TestCase):
+    def test_get_tables(self):
+        gams_directory = find_gams_directory()
+        connector_settings = {"gams_directory": gams_directory}
+        connector = GdxConnector(connector_settings)
+        with TemporaryDirectory() as temporary_dir:
+            path = os.path.join(temporary_dir, "test_get_tables.gdx")
+            with GdxFile(path, "w", gams_directory) as gdx_file:
+                domain = GAMSSet([("key1",)])
+                gdx_file["domain1"] = domain
+                domain = GAMSSet([("key2",)])
+                gdx_file["domain2"] = domain
+                gams_set = GAMSSet([("key1", "key2")], ["domain1", "domain2"])
+                gdx_file["set"] = gams_set
+                gams_parameter = GAMSParameter({("key1", "key2"): 3.14}, domain=["domain1", "domain2"])
+                gdx_file["parameter"] = gams_parameter
+                gams_scalar = GAMSScalar(2.3)
+                gdx_file["scalar"] = gams_scalar
+            connector.connect_to_source(path)
+            tables = connector.get_tables()
+            connector.disconnect()
+        self.assertEqual(len(tables), 5)
+        self.assertTrue("domain1" in tables)
+        self.assertTrue("domain2" in tables)
+        self.assertTrue("set" in tables)
+        self.assertTrue("parameter" in tables)
+        self.assertTrue("scalar" in tables)
+
+    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
+    def test_get_data_iterator_for_domains(self):
+        gams_directory = find_gams_directory()
+        connector_settings = {"gams_directory": gams_directory}
+        connector = GdxConnector(connector_settings)
+        with TemporaryDirectory() as temporary_dir:
+            path = os.path.join(temporary_dir, "test_get_data_iterator_for_domains.gdx")
+            with GdxFile(path, "w", gams_directory) as gdx_file:
+                domain = GAMSSet([("key1",), ("key2",)])
+                gdx_file["domain"] = domain
+            connector.connect_to_source(path)
+            data_iterator, header = connector.get_data_iterator("domain", {})
+            connector.disconnect()
+        self.assertEqual(header, ["dim0"])
+        self.assertEqual(next(data_iterator), ["key1"])
+        self.assertEqual(next(data_iterator), ["key2"])
+        with self.assertRaises(StopIteration):
+            next(data_iterator)
+
+    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
+    def test_get_data_iterator_for_multiple_universal_sets(self):
+        gams_directory = find_gams_directory()
+        connector_settings = {"gams_directory": gams_directory}
+        connector = GdxConnector(connector_settings)
+        with TemporaryDirectory() as temporary_dir:
+            path = os.path.join(temporary_dir, "test_get_data_iterator_for_domains.gdx")
+            with GdxFile(path, "w", gams_directory) as gdx_file:
+                set_ = GAMSSet([("i", "key1"), ("j", "key2")])
+                gdx_file["almost_domain"] = set_
+            connector.connect_to_source(path)
+            data_iterator, header = connector.get_data_iterator("almost_domain", {})
+            connector.disconnect()
+        self.assertEqual(header, ["dim0", "dim1"])
+        self.assertEqual(next(data_iterator), ["i", "key1"])
+        self.assertEqual(next(data_iterator), ["j", "key2"])
+        with self.assertRaises(StopIteration):
+            next(data_iterator)
+
+    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
+    def test_get_data_iterator_for_mixed_universal_and_named_sets(self):
+        gams_directory = find_gams_directory()
+        connector_settings = {"gams_directory": gams_directory}
+        connector = GdxConnector(connector_settings)
+        with TemporaryDirectory() as temporary_dir:
+            path = os.path.join(temporary_dir, "test_get_data_iterator_for_domains.gdx")
+            with GdxFile(path, "w", gams_directory) as gdx_file:
+                set_ = GAMSSet([("key1",), ("key2",)])
+                gdx_file["domain"] = set_
+                set_ = GAMSSet([("i", "key1"), ("j", "key2")], [None, "domain"])
+                gdx_file["almost_domain"] = set_
+            connector.connect_to_source(path)
+            data_iterator, header = connector.get_data_iterator("almost_domain", {})
+            connector.disconnect()
+        self.assertEqual(header, ["dim0", "domain"])
+        self.assertEqual(next(data_iterator), ["i", "key1"])
+        self.assertEqual(next(data_iterator), ["j", "key2"])
+        with self.assertRaises(StopIteration):
+            next(data_iterator)
+
+    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
+    def test_get_data_iterator_for_sets_with_single_indexing_domain(self):
+        gams_directory = find_gams_directory()
+        connector_settings = {"gams_directory": gams_directory}
+        connector = GdxConnector(connector_settings)
+        with TemporaryDirectory() as temporary_dir:
+            path = os.path.join(temporary_dir, "test_get_data_iterator_for_sets_with_single_indexing_domain.gdx")
+            with GdxFile(path, "w", gams_directory) as gdx_file:
+                domain = GAMSSet([("key1",), ("key2",)])
+                gdx_file["domain"] = domain
+                gams_set = GAMSSet([("key1",), ("key2",)], ["domain"])
+                gdx_file["set"] = gams_set
+            connector.connect_to_source(path)
+            data_iterator, header = connector.get_data_iterator("set", {})
+            connector.disconnect()
+        self.assertEqual(header, ["domain"])
+        self.assertEqual(next(data_iterator), ["key1"])
+        self.assertEqual(next(data_iterator), ["key2"])
+        with self.assertRaises(StopIteration):
+            next(data_iterator)
+
+    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
+    def test_get_data_iterator_for_sets_with_multiple_indexing_domains(self):
+        gams_directory = find_gams_directory()
+        connector_settings = {"gams_directory": gams_directory}
+        connector = GdxConnector(connector_settings)
+        with TemporaryDirectory() as temporary_dir:
+            path = os.path.join(temporary_dir, "test_get_data_iterator_for_sets_with_single_indexing_domain.gdx")
+            with GdxFile(path, "w", gams_directory) as gdx_file:
+                domain = GAMSSet([("key1",), ("key2",)])
+                gdx_file["domain1"] = domain
+                domain = GAMSSet([("keyA",), ("keyB",)])
+                gdx_file["domainA"] = domain
+                gams_set = GAMSSet([("key1", "keyA"), ("key2", "keyB")], ["domain1", "domainA"])
+                gdx_file["set"] = gams_set
+            connector.connect_to_source(path)
+            data_iterator, header = connector.get_data_iterator("set", {})
+            connector.disconnect()
+        self.assertEqual(header, ["domain1", "domainA"])
+        self.assertEqual(next(data_iterator), ["key1", "keyA"])
+        self.assertEqual(next(data_iterator), ["key2", "keyB"])
+        with self.assertRaises(StopIteration):
+            next(data_iterator)
+
+    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
+    def test_get_data_iterator_for_parameters_with_single_indexing_domain(self):
+        gams_directory = find_gams_directory()
+        connector_settings = {"gams_directory": gams_directory}
+        connector = GdxConnector(connector_settings)
+        with TemporaryDirectory() as temporary_dir:
+            path = os.path.join(temporary_dir, "test_get_data_iterator_for_parameters_with_single_indexing_domain.gdx")
+            with GdxFile(path, "w", gams_directory) as gdx_file:
+                domain = GAMSSet([("key1",), ("key2",)])
+                gdx_file["domain"] = domain
+                gams_parameter = GAMSParameter({("key1",): 3.14, ("key2",): -2.3}, domain=["domain"])
+                gdx_file["parameter"] = gams_parameter
+            connector.connect_to_source(path)
+            data_iterator, header = connector.get_data_iterator("parameter", {})
+            connector.disconnect()
+        self.assertEqual(header, ["domain", "Value"])
+        self.assertEqual(next(data_iterator), ["key1", 3.14])
+        self.assertEqual(next(data_iterator), ["key2", -2.3])
+        with self.assertRaises(StopIteration):
+            next(data_iterator)
+
+    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
+    def test_get_data_iterator_for_parameters_with_multiple_indexing_domains(self):
+        gams_directory = find_gams_directory()
+        connector_settings = {"gams_directory": gams_directory}
+        connector = GdxConnector(connector_settings)
+        with TemporaryDirectory() as temporary_dir:
+            path = os.path.join(
+                temporary_dir, "test_get_data_iterator_for_parameters_with_multiple_indexing_domains.gdx"
+            )
+            with GdxFile(path, "w", gams_directory) as gdx_file:
+                domain = GAMSSet([("key1",), ("key2",)])
+                gdx_file["domain"] = domain
+                domain = GAMSSet([("keyA",), ("keyB",)])
+                gdx_file["domainA"] = domain
+                gams_parameter = GAMSParameter(
+                    {("key1", "keyA"): 3.14, ("key2", "keyB"): -2.3}, domain=["domain1", "domainA"]
+                )
+                gdx_file["parameter"] = gams_parameter
+            connector.connect_to_source(path)
+            data_iterator, header = connector.get_data_iterator("parameter", {})
+            connector.disconnect()
+        self.assertEqual(header, ["domain1", "domainA", "Value"])
+        self.assertEqual(next(data_iterator), ["key1", "keyA", 3.14])
+        self.assertEqual(next(data_iterator), ["key2", "keyB", -2.3])
+        with self.assertRaises(StopIteration):
+            next(data_iterator)
+
+    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
+    def test_get_data_iterator_for_scalars(self):
+        gams_directory = find_gams_directory()
+        connector_settings = {"gams_directory": gams_directory}
+        connector = GdxConnector(connector_settings)
+        with TemporaryDirectory() as temporary_dir:
+            path = os.path.join(temporary_dir, "test_get_data_iterator_for_scalars.gdx")
+            with GdxFile(path, "w", gams_directory) as gdx_file:
+                gams_scalar = GAMSScalar(2.3)
+                gdx_file["scalar"] = gams_scalar
+            connector.connect_to_source(path)
+            data_iterator, header = connector.get_data_iterator("scalar", {})
+            connector.disconnect()
+        self.assertEqual(header, ["Value"])
+        self.assertEqual(next(data_iterator), [2.3])
+        with self.assertRaises(StopIteration):
+            next(data_iterator)
+
+    @unittest.skipIf(find_gams_directory() is None, "No working GAMS installation found.")
+    def test_connector_is_picklable(self):
+        reader = GdxConnector(None)
+        pickled = pickle.dumps(reader)
+        self.assertTrue(pickled)
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/importers/test_datapackage_reader.py` & `spinedb_api-0.30.4/tests/spine_io/importers/test_datapackage_reader.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,75 +1,75 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains unit tests for DatapackageConnector.
-
-"""
-from contextlib import contextmanager
-import csv
-import unittest
-from pathlib import Path
-import pickle
-from tempfile import TemporaryDirectory
-from datapackage import Package
-from spinedb_api.spine_io.importers.datapackage_reader import DataPackageConnector
-
-
-class TestDatapackageConnector(unittest.TestCase):
-    def test_connector_is_picklable(self):
-        reader = DataPackageConnector(None)
-        pickled = pickle.dumps(reader)
-        self.assertTrue(pickled)
-
-    def test_header_on(self):
-        data = [["a", "b"], ["1.1", "2.2"]]
-        with test_datapackage(data) as package_path:
-            reader = DataPackageConnector(None)
-            reader.connect_to_source(str(package_path))
-            data_iterator, header = reader.get_data_iterator("test_data", {"has_header": True})
-            self.assertEqual(header, ["a", "b"])
-            self.assertEqual(list(data_iterator), [["1.1", "2.2"]])
-
-    def test_header_off(self):
-        data = [["a", "b"], ["1.1", "2.2"]]
-        with test_datapackage(data) as package_path:
-            reader = DataPackageConnector(None)
-            reader.connect_to_source(str(package_path))
-            data_iterator, header = reader.get_data_iterator("test_data", {"has_header": False})
-            self.assertIsNone(header)
-            self.assertEqual(list(data_iterator), data)
-
-    def test_header_off_does_not_append_numbers_to_duplicate_cells(self):
-        data = [["a", "a"]]
-        with test_datapackage(data) as package_path:
-            reader = DataPackageConnector(None)
-            reader.connect_to_source(str(package_path))
-            data_iterator, header = reader.get_data_iterator("test_data", {"has_header": False})
-            self.assertIsNone(header)
-            self.assertEqual(list(data_iterator), data)
-
-
-@contextmanager
-def test_datapackage(rows):
-    with TemporaryDirectory() as temp_dir:
-        csv_file_path = Path(temp_dir, "test_data.csv")
-        with open(csv_file_path, "w", newline="") as csv_file:
-            csv_writer = csv.writer(csv_file)
-            csv_writer.writerows(rows)
-        package = Package(base_path=temp_dir)
-        package.add_resource({"path": str(csv_file_path.relative_to(temp_dir))})
-        package_path = Path(temp_dir, "datapackage.json")
-        package.save(package_path)
-        yield package_path
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains unit tests for DatapackageConnector.
+
+"""
+from contextlib import contextmanager
+import csv
+import unittest
+from pathlib import Path
+import pickle
+from tempfile import TemporaryDirectory
+from datapackage import Package
+from spinedb_api.spine_io.importers.datapackage_reader import DataPackageConnector
+
+
+class TestDatapackageConnector(unittest.TestCase):
+    def test_connector_is_picklable(self):
+        reader = DataPackageConnector(None)
+        pickled = pickle.dumps(reader)
+        self.assertTrue(pickled)
+
+    def test_header_on(self):
+        data = [["a", "b"], ["1.1", "2.2"]]
+        with test_datapackage(data) as package_path:
+            reader = DataPackageConnector(None)
+            reader.connect_to_source(str(package_path))
+            data_iterator, header = reader.get_data_iterator("test_data", {"has_header": True})
+            self.assertEqual(header, ["a", "b"])
+            self.assertEqual(list(data_iterator), [["1.1", "2.2"]])
+
+    def test_header_off(self):
+        data = [["a", "b"], ["1.1", "2.2"]]
+        with test_datapackage(data) as package_path:
+            reader = DataPackageConnector(None)
+            reader.connect_to_source(str(package_path))
+            data_iterator, header = reader.get_data_iterator("test_data", {"has_header": False})
+            self.assertIsNone(header)
+            self.assertEqual(list(data_iterator), data)
+
+    def test_header_off_does_not_append_numbers_to_duplicate_cells(self):
+        data = [["a", "a"]]
+        with test_datapackage(data) as package_path:
+            reader = DataPackageConnector(None)
+            reader.connect_to_source(str(package_path))
+            data_iterator, header = reader.get_data_iterator("test_data", {"has_header": False})
+            self.assertIsNone(header)
+            self.assertEqual(list(data_iterator), data)
+
+
+@contextmanager
+def test_datapackage(rows):
+    with TemporaryDirectory() as temp_dir:
+        csv_file_path = Path(temp_dir, "test_data.csv")
+        with open(csv_file_path, "w", newline="") as csv_file:
+            csv_writer = csv.writer(csv_file)
+            csv_writer.writerows(rows)
+        package = Package(base_path=temp_dir)
+        package.add_resource({"path": str(csv_file_path.relative_to(temp_dir))})
+        package_path = Path(temp_dir, "datapackage.json")
+        package.save(package_path)
+        yield package_path
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/importers/test_json_reader.py` & `spinedb_api-0.30.4/tests/spine_io/importers/test_json_reader.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,44 +1,44 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains unit tests for JSONConnector.
-
-"""
-import json
-from pathlib import Path
-import pickle
-from tempfile import TemporaryDirectory
-import unittest
-from spinedb_api.spine_io.importers.json_reader import JSONConnector
-
-
-class TestJSONConnector(unittest.TestCase):
-    def test_connector_is_picklable(self):
-        reader = JSONConnector(None)
-        pickled = pickle.dumps(reader)
-        self.assertTrue(pickled)
-
-    def test_file_iterator_works_with_empty_options(self):
-        reader = JSONConnector(None)
-        data = {"a": 1, "b": {"c": 2}}
-        with TemporaryDirectory() as temp_dir:
-            file_path = Path(temp_dir) / "test file.json"
-            with open(file_path, "w") as out_file:
-                json.dump(data, out_file)
-            reader.connect_to_source(str(file_path))
-            rows = list(reader.file_iterator("data", {}))
-            reader.disconnect()
-        self.assertEqual(rows, [["a", 1], ["b", "c", 2]])
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains unit tests for JSONConnector.
+
+"""
+import json
+from pathlib import Path
+import pickle
+from tempfile import TemporaryDirectory
+import unittest
+from spinedb_api.spine_io.importers.json_reader import JSONConnector
+
+
+class TestJSONConnector(unittest.TestCase):
+    def test_connector_is_picklable(self):
+        reader = JSONConnector(None)
+        pickled = pickle.dumps(reader)
+        self.assertTrue(pickled)
+
+    def test_file_iterator_works_with_empty_options(self):
+        reader = JSONConnector(None)
+        data = {"a": 1, "b": {"c": 2}}
+        with TemporaryDirectory() as temp_dir:
+            file_path = Path(temp_dir) / "test file.json"
+            with open(file_path, "w") as out_file:
+                json.dump(data, out_file)
+            reader.connect_to_source(str(file_path))
+            rows = list(reader.file_iterator("data", {}))
+            reader.disconnect()
+        self.assertEqual(rows, [["a", 1], ["b", "c", 2]])
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/importers/test_sqlalchemy_connector.py` & `spinedb_api-0.30.4/tests/spine_io/importers/test_sqlalchemy_connector.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Contains unit tests for SqlAlchemyConnector.
-
-"""
-import pickle
-import unittest
-from spinedb_api.spine_io.importers.sqlalchemy_connector import SqlAlchemyConnector
-
-
-class TestSqlAlchemyConnector(unittest.TestCase):
-    def test_connector_is_picklable(self):
-        reader = SqlAlchemyConnector(None)
-        pickled = pickle.dumps(reader)
-        self.assertTrue(pickled)
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Contains unit tests for SqlAlchemyConnector.
+
+"""
+import pickle
+import unittest
+from spinedb_api.spine_io.importers.sqlalchemy_connector import SqlAlchemyConnector
+
+
+class TestSqlAlchemyConnector(unittest.TestCase):
+    def test_connector_is_picklable(self):
+        reader = SqlAlchemyConnector(None)
+        pickled = pickle.dumps(reader)
+        self.assertTrue(pickled)
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/spine_io/test_excel_integration.py` & `spinedb_api-0.30.4/tests/spine_io/test_excel_integration.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,141 +1,141 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Integration tests for Excel import and export.
-
-"""
-
-from pathlib import PurePath
-from tempfile import TemporaryDirectory
-import unittest
-import json
-from spinedb_api import DatabaseMapping, import_data, from_database
-from spinedb_api.spine_io.exporters.excel import export_spine_database_to_xlsx
-from spinedb_api.spine_io.importers.excel_reader import get_mapped_data_from_xlsx
-from ..test_import_functions import assert_import_equivalent
-
-_TEMP_EXCEL_FILENAME = "excel.xlsx"
-
-
-class TestExcelIntegration(unittest.TestCase):
-    def test_array(self):
-        array = b'{"type": "array", "data": [1, 2, 3]}'
-        array = from_database(array, value_type="array")
-        self._check_parameter_value(array)
-
-    def test_time_series(self):
-        ts = b'{"type": "time_series", "index": {"start": "1999-12-31 23:00:00", "resolution": "1h"}, "data": [0.1, 0.2]}'
-        ts = from_database(ts, value_type="time_series")
-        self._check_parameter_value(ts)
-
-    def test_map(self):
-        map_ = json.dumps(
-            {
-                "type": "map",
-                "index_type": "str",
-                "data": [
-                    [
-                        "realization",
-                        {
-                            "type": "map",
-                            "index_type": "date_time",
-                            "data": [
-                                [
-                                    "2000-01-01T00:00:00",
-                                    {
-                                        "type": "time_series",
-                                        "index": {"start": "2000-01-01T00:00:00", "resolution": "1h"},
-                                        "data": [0.732885319, 0.658604529],
-                                    },
-                                ]
-                            ],
-                        },
-                    ],
-                    [
-                        "forecast1",
-                        {
-                            "type": "map",
-                            "index_type": "date_time",
-                            "data": [
-                                [
-                                    "2000-01-01T00:00:00",
-                                    {
-                                        "type": "time_series",
-                                        "index": {"start": "2000-01-01T00:00:00", "resolution": "1h"},
-                                        "data": [0.65306041, 0.60853286],
-                                    },
-                                ]
-                            ],
-                        },
-                    ],
-                    [
-                        "forecast_tail",
-                        {
-                            "type": "map",
-                            "index_type": "date_time",
-                            "data": [
-                                [
-                                    "2000-01-01T00:00:00",
-                                    {
-                                        "type": "time_series",
-                                        "index": {"start": "2000-01-01T00:00:00", "resolution": "1h"},
-                                        "data": [0.680549132, 0.636555097],
-                                    },
-                                ]
-                            ],
-                        },
-                    ],
-                ],
-            }
-        ).encode("UTF8")
-        map_ = from_database(map_, value_type="map")
-        self._check_parameter_value(map_)
-
-    def _check_parameter_value(self, val):
-        input_data = {
-            "object_classes": ["dog"],
-            "objects": [("dog", "pluto")],
-            "object_parameters": [("dog", "bone")],
-            "object_parameter_values": [("dog", "pluto", "bone", val)],
-        }
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_data(db_map, **input_data)
-        db_map.commit_session("yeah")
-        with TemporaryDirectory() as directory:
-            path = str(PurePath(directory, _TEMP_EXCEL_FILENAME))
-            export_spine_database_to_xlsx(db_map, path)
-            output_data, errors = get_mapped_data_from_xlsx(path)
-        db_map.connection.close()
-        self.assertEqual([], errors)
-        input_obj_param_vals = input_data.pop("object_parameter_values")
-        output_obj_param_vals = output_data.pop("object_parameter_values")
-        self.assertEqual(1, len(output_obj_param_vals))
-        input_obj_param_val = input_obj_param_vals[0]
-        output_obj_param_val = output_obj_param_vals[0]
-        for input_, output in zip(input_obj_param_val[:3], output_obj_param_val[:3]):
-            self.assertEqual(input_, output)
-        input_val = input_obj_param_val[3]
-        output_val = output_obj_param_val[3]
-        self.assertEqual(set(indexed_values(output_val)), set(indexed_values(input_val)))
-        assert_import_equivalent(self, input_data, output_data, strict=False)
-
-
-def indexed_values(value, k=1, prefix=()):
-    try:
-        for index, new_value in zip(value.indexes, value.values):
-            yield from indexed_values(new_value, k=k + 1, prefix=(*prefix, str(index)))
-    except AttributeError:
-        yield str(prefix), value
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Integration tests for Excel import and export.
+
+"""
+
+from pathlib import PurePath
+from tempfile import TemporaryDirectory
+import unittest
+import json
+from spinedb_api import DatabaseMapping, import_data, from_database
+from spinedb_api.spine_io.exporters.excel import export_spine_database_to_xlsx
+from spinedb_api.spine_io.importers.excel_reader import get_mapped_data_from_xlsx
+from ..test_import_functions import assert_import_equivalent
+
+_TEMP_EXCEL_FILENAME = "excel.xlsx"
+
+
+class TestExcelIntegration(unittest.TestCase):
+    def test_array(self):
+        array = b'{"type": "array", "data": [1, 2, 3]}'
+        array = from_database(array, value_type="array")
+        self._check_parameter_value(array)
+
+    def test_time_series(self):
+        ts = b'{"type": "time_series", "index": {"start": "1999-12-31 23:00:00", "resolution": "1h"}, "data": [0.1, 0.2]}'
+        ts = from_database(ts, value_type="time_series")
+        self._check_parameter_value(ts)
+
+    def test_map(self):
+        map_ = json.dumps(
+            {
+                "type": "map",
+                "index_type": "str",
+                "data": [
+                    [
+                        "realization",
+                        {
+                            "type": "map",
+                            "index_type": "date_time",
+                            "data": [
+                                [
+                                    "2000-01-01T00:00:00",
+                                    {
+                                        "type": "time_series",
+                                        "index": {"start": "2000-01-01T00:00:00", "resolution": "1h"},
+                                        "data": [0.732885319, 0.658604529],
+                                    },
+                                ]
+                            ],
+                        },
+                    ],
+                    [
+                        "forecast1",
+                        {
+                            "type": "map",
+                            "index_type": "date_time",
+                            "data": [
+                                [
+                                    "2000-01-01T00:00:00",
+                                    {
+                                        "type": "time_series",
+                                        "index": {"start": "2000-01-01T00:00:00", "resolution": "1h"},
+                                        "data": [0.65306041, 0.60853286],
+                                    },
+                                ]
+                            ],
+                        },
+                    ],
+                    [
+                        "forecast_tail",
+                        {
+                            "type": "map",
+                            "index_type": "date_time",
+                            "data": [
+                                [
+                                    "2000-01-01T00:00:00",
+                                    {
+                                        "type": "time_series",
+                                        "index": {"start": "2000-01-01T00:00:00", "resolution": "1h"},
+                                        "data": [0.680549132, 0.636555097],
+                                    },
+                                ]
+                            ],
+                        },
+                    ],
+                ],
+            }
+        ).encode("UTF8")
+        map_ = from_database(map_, value_type="map")
+        self._check_parameter_value(map_)
+
+    def _check_parameter_value(self, val):
+        input_data = {
+            "object_classes": ["dog"],
+            "objects": [("dog", "pluto")],
+            "object_parameters": [("dog", "bone")],
+            "object_parameter_values": [("dog", "pluto", "bone", val)],
+        }
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_data(db_map, **input_data)
+        db_map.commit_session("yeah")
+        with TemporaryDirectory() as directory:
+            path = str(PurePath(directory, _TEMP_EXCEL_FILENAME))
+            export_spine_database_to_xlsx(db_map, path)
+            output_data, errors = get_mapped_data_from_xlsx(path)
+        db_map.connection.close()
+        self.assertEqual([], errors)
+        input_obj_param_vals = input_data.pop("object_parameter_values")
+        output_obj_param_vals = output_data.pop("object_parameter_values")
+        self.assertEqual(1, len(output_obj_param_vals))
+        input_obj_param_val = input_obj_param_vals[0]
+        output_obj_param_val = output_obj_param_vals[0]
+        for input_, output in zip(input_obj_param_val[:3], output_obj_param_val[:3]):
+            self.assertEqual(input_, output)
+        input_val = input_obj_param_val[3]
+        output_val = output_obj_param_val[3]
+        self.assertEqual(set(indexed_values(output_val)), set(indexed_values(input_val)))
+        assert_import_equivalent(self, input_data, output_data, strict=False)
+
+
+def indexed_values(value, k=1, prefix=()):
+    try:
+        for index, new_value in zip(value.indexes, value.values):
+            yield from indexed_values(new_value, k=k + 1, prefix=(*prefix, str(index)))
+    except AttributeError:
+        yield str(prefix), value
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/test_DatabaseMapping.py` & `spinedb_api-0.30.4/tests/test_DatabaseMapping.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,1014 +1,1014 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for DatabaseMapping class.
-
-"""
-import unittest
-from unittest.mock import patch
-from sqlalchemy.engine.url import URL
-from spinedb_api import (
-    DatabaseMapping,
-    to_database,
-    import_functions,
-    from_database,
-    SpineDBAPIError,
-    SpineIntegrityError,
-)
-
-IN_MEMORY_DB_URL = "sqlite://"
-
-
-class TestDatabaseMappingBase(unittest.TestCase):
-    _db_map = None
-
-    @classmethod
-    def setUpClass(cls):
-        cls._db_map = DatabaseMapping(IN_MEMORY_DB_URL, create=True)
-
-    @classmethod
-    def tearDownClass(cls):
-        cls._db_map.connection.close()
-
-    def test_construction_with_filters(self):
-        db_url = IN_MEMORY_DB_URL + "?spinedbfilter=fltr1&spinedbfilter=fltr2"
-        with patch("spinedb_api.db_mapping.apply_filter_stack") as mock_apply:
-            with patch(
-                "spinedb_api.db_mapping.load_filters", return_value=[{"fltr1": "config1", "fltr2": "config2"}]
-            ) as mock_load:
-                db_map = DatabaseMapping(db_url, create=True)
-                db_map.connection.close()
-                mock_load.assert_called_once_with(["fltr1", "fltr2"])
-                mock_apply.assert_called_once_with(db_map, [{"fltr1": "config1", "fltr2": "config2"}])
-
-    def test_construction_with_sqlalchemy_url_and_filters(self):
-        sa_url = URL("sqlite")
-        sa_url.query = {"spinedbfilter": ["fltr1", "fltr2"]}
-        with patch("spinedb_api.db_mapping.apply_filter_stack") as mock_apply:
-            with patch(
-                "spinedb_api.db_mapping.load_filters", return_value=[{"fltr1": "config1", "fltr2": "config2"}]
-            ) as mock_load:
-                db_map = DatabaseMapping(sa_url, create=True)
-                db_map.connection.close()
-                mock_load.assert_called_once_with(["fltr1", "fltr2"])
-                mock_apply.assert_called_once_with(db_map, [{"fltr1": "config1", "fltr2": "config2"}])
-
-    def test_entity_class_type_sq(self):
-        columns = ["id", "name", "commit_id"]
-        self.assertEqual(len(self._db_map.entity_class_type_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.entity_class_type_sq.c, column_name))
-
-    def test_entity_type_sq(self):
-        columns = ["id", "name", "commit_id"]
-        self.assertEqual(len(self._db_map.entity_type_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.entity_type_sq.c, column_name))
-
-    def test_entity_sq(self):
-        columns = ["id", "type_id", "class_id", "name", "description", "commit_id"]
-        self.assertEqual(len(self._db_map.entity_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.entity_sq.c, column_name))
-
-    def test_object_class_sq(self):
-        columns = ["id", "name", "description", "display_order", "display_icon", "hidden", "commit_id"]
-        self.assertEqual(len(self._db_map.object_class_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.object_class_sq.c, column_name))
-
-    def test_object_sq(self):
-        columns = ["id", "class_id", "name", "description", "commit_id"]
-        self.assertEqual(len(self._db_map.object_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.object_sq.c, column_name))
-
-    def test_relationship_class_sq(self):
-        columns = ["id", "dimension", "object_class_id", "name", "description", "display_icon", "hidden", "commit_id"]
-        self.assertEqual(len(self._db_map.relationship_class_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.relationship_class_sq.c, column_name))
-
-    def test_relationship_sq(self):
-        columns = ["id", "dimension", "object_id", "class_id", "name", "commit_id"]
-        self.assertEqual(len(self._db_map.relationship_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.relationship_sq.c, column_name))
-
-    def test_entity_group_sq(self):
-        columns = ["id", "entity_id", "entity_class_id", "member_id"]
-        self.assertEqual(len(self._db_map.entity_group_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.entity_group_sq.c, column_name))
-
-    def test_parameter_definition_sq(self):
-        columns = [
-            "id",
-            "name",
-            "description",
-            "entity_class_id",
-            "object_class_id",
-            "relationship_class_id",
-            "default_value",
-            "default_type",
-            "list_value_id",
-            "commit_id",
-            "parameter_value_list_id",
-        ]
-        self.assertEqual(len(self._db_map.parameter_definition_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.parameter_definition_sq.c, column_name))
-
-    def test_parameter_value_sq(self):
-        columns = [
-            "id",
-            "parameter_definition_id",
-            "entity_class_id",
-            "entity_id",
-            "object_class_id",
-            "relationship_class_id",
-            "object_id",
-            "relationship_id",
-            "value",
-            "type",
-            "list_value_id",
-            "commit_id",
-            "alternative_id",
-        ]
-        self.assertEqual(len(self._db_map.parameter_value_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.parameter_value_sq.c, column_name))
-
-    def test_parameter_value_list_sq(self):
-        columns = ["id", "name", "commit_id"]
-        self.assertEqual(len(self._db_map.parameter_value_list_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.parameter_value_list_sq.c, column_name))
-
-    def test_ext_object_sq(self):
-        columns = ["id", "class_id", "class_name", "name", "description", "group_id", "commit_id"]
-        self.assertEqual(len(self._db_map.ext_object_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.ext_object_sq.c, column_name))
-
-    def test_ext_relationship_class_sq(self):
-        columns = [
-            "id",
-            "name",
-            "description",
-            "display_icon",
-            "dimension",
-            "object_class_id",
-            "object_class_name",
-            "commit_id",
-        ]
-        self.assertEqual(len(self._db_map.ext_relationship_class_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.ext_relationship_class_sq.c, column_name))
-
-    def test_wide_relationship_class_sq(self):
-        columns = [
-            "id",
-            "name",
-            "description",
-            "display_icon",
-            "commit_id",
-            "object_class_id_list",
-            "object_class_name_list",
-        ]
-        self.assertEqual(len(self._db_map.wide_relationship_class_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.wide_relationship_class_sq.c, column_name))
-
-    def test_ext_relationship_sq(self):
-        columns = [
-            "id",
-            "name",
-            "class_id",
-            "class_name",
-            "dimension",
-            "object_id",
-            "object_name",
-            "object_class_id",
-            "object_class_name",
-            "commit_id",
-        ]
-        self.assertEqual(len(self._db_map.ext_relationship_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.ext_relationship_sq.c, column_name))
-
-    def test_wide_relationship_sq(self):
-        columns = [
-            "id",
-            "name",
-            "class_id",
-            "class_name",
-            "commit_id",
-            "object_id_list",
-            "object_name_list",
-            "object_class_id_list",
-            "object_class_name_list",
-        ]
-        self.assertEqual(len(self._db_map.wide_relationship_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.wide_relationship_sq.c, column_name))
-
-    def test_object_parameter_definition_sq(self):
-        columns = [
-            "id",
-            "entity_class_id",
-            "entity_class_name",
-            "object_class_id",
-            "object_class_name",
-            "parameter_name",
-            "value_list_id",
-            "value_list_name",
-            "default_value",
-            "default_type",
-            "description",
-        ]
-        self.assertEqual(len(self._db_map.object_parameter_definition_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.object_parameter_definition_sq.c, column_name))
-
-    def test_relationship_parameter_definition_sq(self):
-        columns = [
-            "id",
-            "entity_class_id",
-            "entity_class_name",
-            "relationship_class_id",
-            "relationship_class_name",
-            "object_class_id_list",
-            "object_class_name_list",
-            "parameter_name",
-            "value_list_id",
-            "value_list_name",
-            "default_value",
-            "default_type",
-            "description",
-        ]
-        self.assertEqual(len(self._db_map.relationship_parameter_definition_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.relationship_parameter_definition_sq.c, column_name))
-
-    def test_object_parameter_value_sq(self):
-        columns = [
-            "id",
-            "entity_class_id",
-            "object_class_id",
-            "object_class_name",
-            "entity_id",
-            "object_id",
-            "object_name",
-            "parameter_id",
-            "parameter_name",
-            "alternative_id",
-            "alternative_name",
-            "value",
-            "type",
-        ]
-        self.assertEqual(len(self._db_map.object_parameter_value_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.object_parameter_value_sq.c, column_name))
-
-    def test_relationship_parameter_value_sq(self):
-        columns = [
-            "id",
-            "entity_class_id",
-            "relationship_class_id",
-            "relationship_class_name",
-            "object_class_id_list",
-            "object_class_name_list",
-            "entity_id",
-            "relationship_id",
-            "object_id_list",
-            "object_name_list",
-            "parameter_id",
-            "parameter_name",
-            "alternative_id",
-            "alternative_name",
-            "value",
-            "type",
-        ]
-        self.assertEqual(len(self._db_map.relationship_parameter_value_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.relationship_parameter_value_sq.c, column_name))
-
-    def test_wide_parameter_value_list_sq(self):
-        columns = ["id", "name", "value_index_list", "value_id_list", "commit_id"]
-        self.assertEqual(len(self._db_map.wide_parameter_value_list_sq.c), len(columns))
-        for column_name in columns:
-            self.assertTrue(hasattr(self._db_map.wide_parameter_value_list_sq.c, column_name))
-
-
-class TestDatabaseMappingBaseQueries(unittest.TestCase):
-    def setUp(self):
-        self._db_map = DatabaseMapping(IN_MEMORY_DB_URL, create=True)
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def create_object_classes(self):
-        obj_classes = ['class1', 'class2']
-        import_functions.import_object_classes(self._db_map, obj_classes)
-        return obj_classes
-
-    def create_objects(self):
-        objects = [('class1', 'obj11'), ('class1', 'obj12'), ('class2', 'obj21')]
-        import_functions.import_objects(self._db_map, objects)
-        return objects
-
-    def create_relationship_classes(self):
-        relationship_classes = [('rel1', ['class1']), ('rel2', ['class1', 'class2'])]
-        import_functions.import_relationship_classes(self._db_map, relationship_classes)
-        return relationship_classes
-
-    def create_relationships(self):
-        relationships = [('rel1', ['obj11']), ('rel2', ['obj11', 'obj21'])]
-        import_functions.import_relationships(self._db_map, relationships)
-        return relationships
-
-    def test_commit_sq_hides_pending_commit(self):
-        commits = self._db_map.query(self._db_map.commit_sq).all()
-        self.assertEqual(len(commits), 1)
-
-    def test_alternative_sq(self):
-        import_functions.import_alternatives(self._db_map, (("alt1", "test alternative"),))
-        alternative_rows = self._db_map.query(self._db_map.alternative_sq).all()
-        expected_names_and_descriptions = {"Base": "Base alternative", "alt1": "test alternative"}
-        self.assertEqual(len(alternative_rows), len(expected_names_and_descriptions))
-        for row in alternative_rows:
-            self.assertTrue(row.name in expected_names_and_descriptions)
-            self.assertEqual(row.description, expected_names_and_descriptions[row.name])
-            expected_names_and_descriptions.pop(row.name)
-        self.assertEqual(expected_names_and_descriptions, {})
-
-    def test_scenario_sq(self):
-        import_functions.import_scenarios(self._db_map, (("scen1", True, "test scenario"),))
-        scenario_rows = self._db_map.query(self._db_map.scenario_sq).all()
-        self.assertEqual(len(scenario_rows), 1)
-        self.assertEqual(scenario_rows[0].name, "scen1")
-        self.assertEqual(scenario_rows[0].description, "test scenario")
-        self.assertTrue(scenario_rows[0].active)
-
-    def test_ext_linked_scenario_alternative_sq(self):
-        import_functions.import_scenarios(self._db_map, (("scen1", True),))
-        import_functions.import_alternatives(self._db_map, ("alt1", "alt2", "alt3"))
-        import_functions.import_scenario_alternatives(self._db_map, (("scen1", "alt2"),))
-        import_functions.import_scenario_alternatives(self._db_map, (("scen1", "alt3"),))
-        import_functions.import_scenario_alternatives(self._db_map, (("scen1", "alt1"),))
-        scenario_alternative_rows = self._db_map.query(self._db_map.ext_linked_scenario_alternative_sq).all()
-        self.assertEqual(len(scenario_alternative_rows), 3)
-        expected_befores = {"alt2": "alt3", "alt3": "alt1", "alt1": None}
-        expected_ranks = {"alt2": 1, "alt3": 2, "alt1": 3}
-        for row in scenario_alternative_rows:
-            self.assertEqual(row.scenario_name, "scen1")
-            self.assertIn(row.alternative_name, expected_befores)
-            self.assertEqual(row.rank, expected_ranks[row.alternative_name])
-            expected_before_alternative = expected_befores.pop(row.alternative_name)
-            self.assertEqual(row.before_alternative_name, expected_before_alternative)
-            if expected_before_alternative is not None:
-                self.assertIsNotNone(row.before_alternative_id)
-                self.assertEqual(row.before_rank, expected_ranks[row.before_alternative_name])
-            else:
-                self.assertIsNone(row.before_alternative_id)
-                self.assertIsNone(row.before_rank)
-        self.assertEqual(expected_befores, {})
-
-    def test_entity_class_sq(self):
-        obj_classes = self.create_object_classes()
-        relationship_classes = self.create_relationship_classes()
-        results = self._db_map.query(self._db_map.entity_class_sq).all()
-        # Check that number of results matches total entities
-        self.assertEqual(len(results), len(obj_classes) + len(relationship_classes))
-        # Check result values
-        for row, class_name in zip(results, obj_classes + [rel[0] for rel in relationship_classes]):
-            self.assertEqual(row.name, class_name)
-
-    def test_entity_sq(self):
-        self.create_object_classes()
-        objects = self.create_objects()
-        self.create_relationship_classes()
-        relationships = self.create_relationships()
-        entity_rows = self._db_map.query(self._db_map.entity_sq).all()
-        self.assertEqual(len(entity_rows), len(objects) + len(relationships))
-        object_names = [o[1] for o in objects]
-        relationship_names = [r[0] + "_" + "__".join(r[1]) for r in relationships]
-        for row, expected_name in zip(entity_rows, object_names + relationship_names):
-            self.assertEqual(row.name, expected_name)
-
-    def test_object_class_sq_picks_object_classes_only(self):
-        obj_classes = self.create_object_classes()
-        self.create_relationship_classes()
-        class_rows = self._db_map.query(self._db_map.object_class_sq).all()
-        self.assertEqual(len(class_rows), len(obj_classes))
-        for row, expected_name in zip(class_rows, obj_classes):
-            self.assertEqual(row.name, expected_name)
-
-    def test_object_sq_picks_objects_only(self):
-        self.create_object_classes()
-        objects = self.create_objects()
-        self.create_relationship_classes()
-        self.create_relationships()
-        object_rows = self._db_map.query(self._db_map.object_sq).all()
-        self.assertEqual(len(object_rows), len(objects))
-        for row, expected_object in zip(object_rows, objects):
-            self.assertEqual(row.name, expected_object[1])
-
-    def test_wide_relationship_class_sq(self):
-        self.create_object_classes()
-        relationship_classes = self.create_relationship_classes()
-        class_rows = self._db_map.query(self._db_map.wide_relationship_class_sq).all()
-        self.assertEqual(len(class_rows), 2)
-        for row, relationship_class in zip(class_rows, relationship_classes):
-            self.assertEqual(row.name, relationship_class[0])
-            self.assertEqual(row.object_class_name_list, ",".join(relationship_class[1]))
-
-    def test_wide_relationship_sq(self):
-        self.create_object_classes()
-        self.create_objects()
-        relationship_classes = self.create_relationship_classes()
-        object_classes = {rel_class[0]: rel_class[1] for rel_class in relationship_classes}
-        relationships = self.create_relationships()
-        relationship_rows = self._db_map.query(self._db_map.wide_relationship_sq).all()
-        self.assertEqual(len(relationship_rows), 2)
-        for row, relationship in zip(relationship_rows, relationships):
-            self.assertEqual(row.name, relationship[0] + "_" + "__".join(relationship[1]))
-            self.assertEqual(row.class_name, relationship[0])
-            self.assertEqual(row.object_class_name_list, ",".join(object_classes[relationship[0]]))
-            self.assertEqual(row.object_name_list, ",".join(relationship[1]))
-
-    def test_parameter_definition_sq_for_object_class(self):
-        self.create_object_classes()
-        import_functions.import_object_parameters(self._db_map, (("class1", "par1"),))
-        definition_rows = self._db_map.query(self._db_map.parameter_definition_sq).all()
-        self.assertEqual(len(definition_rows), 1)
-        self.assertEqual(definition_rows[0].name, "par1")
-        self.assertIsNotNone(definition_rows[0].object_class_id)
-        self.assertIsNone(definition_rows[0].relationship_class_id)
-
-    def test_parameter_definition_sq_for_relationship_class(self):
-        self.create_object_classes()
-        self.create_relationship_classes()
-        import_functions.import_relationship_parameters(self._db_map, (("rel1", "par1"),))
-        definition_rows = self._db_map.query(self._db_map.parameter_definition_sq).all()
-        self.assertEqual(len(definition_rows), 1)
-        self.assertEqual(definition_rows[0].name, "par1")
-        self.assertIsNone(definition_rows[0].object_class_id)
-        self.assertIsNotNone(definition_rows[0].relationship_class_id)
-
-    def test_entity_parameter_definition_sq_for_object_class(self):
-        self.create_object_classes()
-        self.create_relationship_classes()
-        import_functions.import_object_parameters(self._db_map, (("class1", "par1"),))
-        definition_rows = self._db_map.query(self._db_map.entity_parameter_definition_sq).all()
-        self.assertEqual(len(definition_rows), 1)
-        self.assertEqual(definition_rows[0].parameter_name, "par1")
-        self.assertEqual(definition_rows[0].entity_class_name, "class1")
-        self.assertEqual(definition_rows[0].object_class_name, "class1")
-        self.assertIsNone(definition_rows[0].relationship_class_id)
-        self.assertIsNone(definition_rows[0].relationship_class_name)
-        self.assertIsNone(definition_rows[0].object_class_id_list)
-        self.assertIsNone(definition_rows[0].object_class_name_list)
-
-    def test_entity_parameter_definition_sq_for_relationship_class(self):
-        object_classes = self.create_object_classes()
-        self.create_relationship_classes()
-        import_functions.import_relationship_parameters(self._db_map, (("rel2", "par1"),))
-        definition_rows = self._db_map.query(self._db_map.entity_parameter_definition_sq).all()
-        self.assertEqual(len(definition_rows), 1)
-        self.assertEqual(definition_rows[0].parameter_name, "par1")
-        self.assertEqual(definition_rows[0].entity_class_name, "rel2")
-        self.assertIsNotNone(definition_rows[0].relationship_class_id)
-        self.assertEqual(definition_rows[0].relationship_class_name, "rel2")
-        self.assertIsNotNone(definition_rows[0].object_class_id_list)
-        self.assertEqual(definition_rows[0].object_class_name_list, ",".join(object_classes))
-        self.assertIsNone(definition_rows[0].object_class_name)
-
-    def test_entity_parameter_definition_sq_with_multiple_relationship_classes_but_single_parameter(self):
-        self.create_object_classes()
-        self.create_relationship_classes()
-        obj_parameter_definitions = [('class1', 'par1a'), ('class1', 'par1b')]
-        rel_parameter_definitions = [('rel1', 'rpar1a')]
-        import_functions.import_object_parameters(self._db_map, obj_parameter_definitions)
-        import_functions.import_relationship_parameters(self._db_map, rel_parameter_definitions)
-        results = self._db_map.query(self._db_map.entity_parameter_definition_sq).all()
-        # Check that number of results matches total entities
-        self.assertEqual(len(results), len(obj_parameter_definitions) + len(rel_parameter_definitions))
-        # Check result values
-        for row, par_def in zip(results, obj_parameter_definitions + rel_parameter_definitions):
-            self.assertTupleEqual((row.entity_class_name, row.parameter_name), par_def)
-
-    def test_entity_parameter_values(self):
-        self.create_object_classes()
-        self.create_objects()
-        self.create_relationship_classes()
-        self.create_relationships()
-        obj_parameter_definitions = [('class1', 'par1a'), ('class1', 'par1b'), ('class2', 'par2a')]
-        rel_parameter_definitions = [('rel1', 'rpar1a'), ('rel2', 'rpar2a')]
-        import_functions.import_object_parameters(self._db_map, obj_parameter_definitions)
-        import_functions.import_relationship_parameters(self._db_map, rel_parameter_definitions)
-        object_parameter_values = [
-            ('class1', 'obj11', 'par1a', 123),
-            ('class1', 'obj11', 'par1b', 333),
-            ('class2', 'obj21', 'par2a', 'empty'),
-        ]
-        _, errors = import_functions.import_object_parameter_values(self._db_map, object_parameter_values)
-        self.assertFalse(errors)
-        relationship_parameter_values = [('rel1', ['obj11'], 'rpar1a', 1.1), ('rel2', ['obj11', 'obj21'], 'rpar2a', 42)]
-        _, errors = import_functions.import_relationship_parameter_values(self._db_map, relationship_parameter_values)
-        self.assertFalse(errors)
-        results = self._db_map.query(self._db_map.entity_parameter_value_sq).all()
-        # Check that number of results matches total entities
-        self.assertEqual(len(results), len(object_parameter_values) + len(relationship_parameter_values))
-        # Check result values
-        for row, par_val in zip(results, object_parameter_values + relationship_parameter_values):
-            self.assertEqual(row.entity_class_name, par_val[0])
-            if row.object_name:  # This is an object parameter
-                self.assertEqual(row.object_name, par_val[1])
-            else:  # This is a relationship parameter
-                self.assertEqual(row.object_name_list, ','.join(par_val[1]))
-            self.assertEqual(row.parameter_name, par_val[2])
-            self.assertEqual(from_database(row.value, row.type), par_val[3])
-
-    def test_wide_parameter_value_list_sq(self):
-        _, errors = import_functions.import_parameter_value_lists(
-            self._db_map, (("list1", "value1"), ("list1", "value2"), ("list2", "valueA"))
-        )
-        self.assertEqual(errors, [])
-        value_lists = self._db_map.query(self._db_map.wide_parameter_value_list_sq).all()
-        self.assertEqual(len(value_lists), 2)
-        self.assertEqual(value_lists[0].name, "list1")
-        self.assertEqual(value_lists[1].name, "list2")
-
-
-class TestDatabaseMappingUpdateMixin(unittest.TestCase):
-    def setUp(self):
-        self._db_map = DatabaseMapping(IN_MEMORY_DB_URL, create=True)
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_update_method_of_tool_feature_method(self):
-        import_functions.import_object_classes(self._db_map, ("object_class1", "object_class2"))
-        import_functions.import_parameter_value_lists(
-            self._db_map, (("value_list", "value1"), ("value_list", "value2"))
-        )
-        import_functions.import_object_parameters(
-            self._db_map, (("object_class1", "parameter1", "value1", "value_list"), ("object_class1", "parameter2"))
-        )
-        import_functions.import_features(self._db_map, (("object_class1", "parameter1"),))
-        import_functions.import_tools(self._db_map, ("tool1",))
-        import_functions.import_tool_features(self._db_map, (("tool1", "object_class1", "parameter1"),))
-        import_functions.import_tool_feature_methods(
-            self._db_map, (("tool1", "object_class1", "parameter1", "value2"),)
-        )
-        self._db_map.commit_session("Populate with initial data.")
-        updated_ids, errors = self._db_map.update_tool_feature_methods(
-            {"id": 1, "method_index": 0, "method": to_database("value1")[0]}
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(updated_ids, {1})
-        self._db_map.commit_session("Update data.")
-        tool_feature_methods = self._db_map.query(self._db_map.ext_tool_feature_method_sq).all()
-        self.assertEqual(len(tool_feature_methods), 1)
-        tool_feature_method = tool_feature_methods[0]
-        self.assertEqual(tool_feature_method.method, to_database("value1")[0])
-
-    def test_update_wide_relationship_class(self):
-        _ = import_functions.import_object_classes(self._db_map, ("object_class_1",))
-        _ = import_functions.import_relationship_classes(self._db_map, (("my_class", ("object_class_1",)),))
-        self._db_map.commit_session("Add test data")
-        updated_ids, errors = self._db_map.update_wide_relationship_classes({"id": 2, "name": "renamed"})
-        self.assertEqual(errors, [])
-        self.assertEqual(updated_ids, {2})
-        self._db_map.commit_session("Update data.")
-        classes = self._db_map.query(self._db_map.wide_relationship_class_sq).all()
-        self.assertEqual(len(classes), 1)
-        self.assertEqual(classes[0].name, "renamed")
-
-    def test_update_wide_relationship_class_does_not_update_member_class_id(self):
-        import_functions.import_object_classes(self._db_map, ("object_class_1", "object_class_2"))
-        import_functions.import_relationship_classes(self._db_map, (("my_class", ("object_class_1",)),))
-        self._db_map.commit_session("Add test data")
-        updated_ids, errors = self._db_map.update_wide_relationship_classes(
-            {"id": 3, "name": "renamed", "object_class_id_list": [2]}
-        )
-        self.assertEqual([str(err) for err in errors], ["Can't update fixed fields 'object_class_id_list'"])
-        self.assertEqual(updated_ids, {3})
-        self._db_map.commit_session("Update data.")
-        classes = self._db_map.query(self._db_map.wide_relationship_class_sq).all()
-        self.assertEqual(len(classes), 1)
-        self.assertEqual(classes[0].name, "renamed")
-        self.assertEqual(classes[0].object_class_name_list, "object_class_1")
-
-    def test_update_wide_relationship(self):
-        import_functions.import_object_classes(self._db_map, ("object_class_1", "object_class_2"))
-        import_functions.import_objects(
-            self._db_map,
-            (("object_class_1", "object_11"), ("object_class_1", "object_12"), ("object_class_2", "object_21")),
-        )
-        import_functions.import_relationship_classes(
-            self._db_map, (("my_class", ("object_class_1", "object_class_2")),)
-        )
-        import_functions.import_relationships(self._db_map, (("my_class", ("object_11", "object_21")),))
-        self._db_map.commit_session("Add test data")
-        updated_ids, errors = self._db_map.update_wide_relationships(
-            {"id": 4, "name": "renamed", "object_id_list": [2, 3]}
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(updated_ids, {4})
-        self._db_map.commit_session("Update data.")
-        relationships = self._db_map.query(self._db_map.wide_relationship_sq).all()
-        self.assertEqual(len(relationships), 1)
-        self.assertEqual(relationships[0].name, "renamed")
-        self.assertEqual(relationships[0].object_name_list, "object_12,object_21")
-
-    def test_update_parameter_value_by_id_only(self):
-        import_functions.import_object_classes(self._db_map, ("object_class1",))
-        import_functions.import_object_parameters(self._db_map, (("object_class1", "parameter1"),))
-        import_functions.import_objects(self._db_map, (("object_class1", "object1"),))
-        import_functions.import_object_parameter_values(
-            self._db_map, (("object_class1", "object1", "parameter1", "something"),)
-        )
-        self._db_map.commit_session("Populate with initial data.")
-        updated_ids, errors = self._db_map.update_parameter_values({"id": 1, "value": b"something else"})
-        self.assertEqual(errors, [])
-        self.assertEqual(updated_ids, {1})
-        self._db_map.commit_session("Update data.")
-        pvals = self._db_map.query(self._db_map.parameter_value_sq).all()
-        self.assertEqual(len(pvals), 1)
-        pval = pvals[0]
-        self.assertEqual(pval.value, b"something else")
-
-    def test_update_parameter_definition_by_id_only(self):
-        import_functions.import_object_classes(self._db_map, ("object_class1",))
-        import_functions.import_object_parameters(self._db_map, (("object_class1", "parameter1"),))
-        self._db_map.commit_session("Populate with initial data.")
-        updated_ids, errors = self._db_map.update_parameter_definitions({"id": 1, "name": "parameter2"})
-        self.assertEqual(errors, [])
-        self.assertEqual(updated_ids, {1})
-        self._db_map.commit_session("Update data.")
-        pdefs = self._db_map.query(self._db_map.parameter_definition_sq).all()
-        self.assertEqual(len(pdefs), 1)
-        self.assertEqual(pdefs[0].name, "parameter2")
-
-    def test_update_parameter_definition_value_list(self):
-        import_functions.import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
-        import_functions.import_object_classes(self._db_map, ("object_class",))
-        import_functions.import_object_parameters(self._db_map, (("object_class", "my_parameter"),))
-        self._db_map.commit_session("Populate with initial data.")
-        updated_ids, errors = self._db_map.update_parameter_definitions(
-            {"id": 1, "name": "my_parameter", "parameter_value_list_id": 1}
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(updated_ids, {1})
-        self._db_map.commit_session("Update data.")
-        pdefs = self._db_map.query(self._db_map.parameter_definition_sq).all()
-        self.assertEqual(len(pdefs), 1)
-        self.assertEqual(
-            pdefs[0]._asdict(),
-            {
-                "commit_id": 3,
-                "default_type": None,
-                "default_value": None,
-                "description": None,
-                "entity_class_id": 1,
-                "id": 1,
-                "list_value_id": None,
-                "name": "my_parameter",
-                "object_class_id": 1,
-                "parameter_value_list_id": 1,
-                "relationship_class_id": None,
-            },
-        )
-
-    def test_update_parameter_definition_value_list_when_values_exist_gives_error(self):
-        import_functions.import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
-        import_functions.import_object_classes(self._db_map, ("object_class",))
-        import_functions.import_objects(self._db_map, (("object_class", "my_object"),))
-        import_functions.import_object_parameters(self._db_map, (("object_class", "my_parameter"),))
-        import_functions.import_object_parameter_values(
-            self._db_map, (("object_class", "my_object", "my_parameter", 23.0),)
-        )
-        self._db_map.commit_session("Populate with initial data.")
-        updated_ids, errors = self._db_map.update_parameter_definitions(
-            {"id": 1, "name": "my_parameter", "parameter_value_list_id": 1}
-        )
-        self.assertEqual(
-            list(map(str, errors)),
-            ["Can't change value list on parameter my_parameter because it has parameter values."],
-        )
-        self.assertEqual(updated_ids, set())
-
-    def test_update_parameter_definitions_default_value_that_is_not_on_value_list_gives_error(self):
-        import_functions.import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
-        import_functions.import_object_classes(self._db_map, ("object_class",))
-        import_functions.import_objects(self._db_map, (("object_class", "my_object"),))
-        import_functions.import_object_parameters(self._db_map, (("object_class", "my_parameter", None, "my_list"),))
-        self._db_map.commit_session("Populate with initial data.")
-        updated_ids, errors = self._db_map.update_parameter_definitions(
-            {"id": 1, "name": "my_parameter", "default_value": to_database(23.0)[0]}
-        )
-        self.assertEqual(
-            list(map(str, errors)),
-            ["Invalid default_value '23.0' - it should be one from the parameter value list: '99.0'."],
-        )
-        self.assertEqual(updated_ids, set())
-
-    def test_update_parameter_definition_value_list_when_default_value_not_on_the_list_exists_gives_error(self):
-        import_functions.import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
-        import_functions.import_object_classes(self._db_map, ("object_class",))
-        import_functions.import_objects(self._db_map, (("object_class", "my_object"),))
-        import_functions.import_object_parameters(self._db_map, (("object_class", "my_parameter", 23.0),))
-        self._db_map.commit_session("Populate with initial data.")
-        updated_ids, errors = self._db_map.update_parameter_definitions(
-            {"id": 1, "name": "my_parameter", "parameter_value_list_id": 1}
-        )
-        self.assertEqual(
-            list(map(str, errors)),
-            ["Invalid default_value '23.0' - it should be one from the parameter value list: '99.0'."],
-        )
-        self.assertEqual(updated_ids, set())
-
-    def test_update_object_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
-        self._db_map.commit_session("Add test data")
-        ids, errors = self._db_map.update_ext_entity_metadata(
-            *[{"id": 1, "metadata_name": "key_2", "metadata_value": "new value"}]
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(ids, {1})
-        metadata_entries = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata_entries), 1)
-        self.assertEqual(
-            metadata_entries[0]._asdict(), {"id": 1, "name": "key_2", "value": "new value", "commit_id": 3}
-        )
-        entity_metadata_entries = self._db_map.query(self._db_map.entity_metadata_sq).all()
-        self.assertEqual(len(entity_metadata_entries), 1)
-        self.assertEqual(
-            entity_metadata_entries[0]._asdict(), {"id": 1, "entity_id": 1, "metadata_id": 1, "commit_id": 2}
-        )
-
-    def test_update_object_metadata_reuses_existing_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"), ("my_class", "extra_object")))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}', '{"key 2": "metadata value 2"}'))
-        import_functions.import_object_metadata(
-            self._db_map,
-            (
-                ("my_class", "my_object", '{"title": "My metadata."}'),
-                ("my_class", "extra_object", '{"key 2": "metadata value 2"}'),
-            ),
-        )
-        self._db_map.commit_session("Add test data")
-        ids, errors = self._db_map.update_ext_entity_metadata(
-            *[{"id": 1, "metadata_name": "key 2", "metadata_value": "metadata value 2"}]
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(ids, {1})
-        metadata_entries = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata_entries), 2)
-        self.assertEqual(
-            metadata_entries[0]._asdict(), {"id": 1, "name": "title", "value": "My metadata.", "commit_id": 2}
-        )
-        self.assertEqual(
-            metadata_entries[1]._asdict(), {"id": 2, "name": "key 2", "value": "metadata value 2", "commit_id": 2}
-        )
-        entity_metadata_entries = self._db_map.query(self._db_map.entity_metadata_sq).all()
-        self.assertEqual(len(entity_metadata_entries), 2)
-        self.assertEqual(
-            entity_metadata_entries[0]._asdict(), {"id": 1, "entity_id": 1, "metadata_id": 2, "commit_id": 3}
-        )
-        self.assertEqual(
-            entity_metadata_entries[1]._asdict(), {"id": 2, "entity_id": 2, "metadata_id": 2, "commit_id": 2}
-        )
-
-    def test_update_object_metadata_keeps_metadata_still_in_use(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "object_1"), ("my_class", "object_2")))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_object_metadata(
-            self._db_map,
-            (
-                ("my_class", "object_1", '{"title": "My metadata."}'),
-                ("my_class", "object_2", '{"title": "My metadata."}'),
-            ),
-        )
-        self._db_map.commit_session("Add test data")
-        ids, errors = self._db_map.update_ext_entity_metadata(
-            *[{"id": 1, "metadata_name": "new key", "metadata_value": "new value"}]
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(ids, {1, 2})
-        metadata_entries = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata_entries), 2)
-        self.assertEqual(
-            metadata_entries[0]._asdict(), {"id": 1, "name": "title", "value": "My metadata.", "commit_id": 2}
-        )
-        self.assertEqual(
-            metadata_entries[1]._asdict(), {"id": 2, "name": "new key", "value": "new value", "commit_id": 3}
-        )
-        entity_metadata_entries = self._db_map.query(self._db_map.entity_metadata_sq).all()
-        self.assertEqual(len(entity_metadata_entries), 2)
-        self.assertEqual(
-            entity_metadata_entries[0]._asdict(), {"id": 1, "entity_id": 1, "metadata_id": 2, "commit_id": 3}
-        )
-        self.assertEqual(
-            entity_metadata_entries[1]._asdict(), {"id": 2, "entity_id": 2, "metadata_id": 1, "commit_id": 2}
-        )
-
-    def test_update_parameter_value_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_object_parameter_values(
-            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
-        )
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_object_parameter_value_metadata(
-            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
-        )
-        self._db_map.commit_session("Add test data")
-        ids, errors = self._db_map.update_ext_parameter_value_metadata(
-            *[{"id": 1, "metadata_name": "key_2", "metadata_value": "new value"}]
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(ids, {1})
-        metadata_entries = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata_entries), 1)
-        self.assertEqual(
-            metadata_entries[0]._asdict(), {"id": 1, "name": "key_2", "value": "new value", "commit_id": 3}
-        )
-        value_metadata_entries = self._db_map.query(self._db_map.parameter_value_metadata_sq).all()
-        self.assertEqual(len(value_metadata_entries), 1)
-        self.assertEqual(
-            value_metadata_entries[0]._asdict(), {"id": 1, "parameter_value_id": 1, "metadata_id": 1, "commit_id": 2}
-        )
-
-    def test_update_parameter_value_metadata_will_not_delete_shared_entity_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_object_parameter_values(
-            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
-        )
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
-        import_functions.import_object_parameter_value_metadata(
-            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
-        )
-        self._db_map.commit_session("Add test data")
-        ids, errors = self._db_map.update_ext_parameter_value_metadata(
-            *[{"id": 1, "metadata_name": "key_2", "metadata_value": "new value"}]
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(ids, {1, 2})
-        metadata_entries = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata_entries), 2)
-        self.assertEqual(
-            metadata_entries[0]._asdict(), {"id": 1, "name": "title", "value": "My metadata.", "commit_id": 2}
-        )
-        self.assertEqual(
-            metadata_entries[1]._asdict(), {"id": 2, "name": "key_2", "value": "new value", "commit_id": 3}
-        )
-        value_metadata_entries = self._db_map.query(self._db_map.parameter_value_metadata_sq).all()
-        self.assertEqual(len(value_metadata_entries), 1)
-        self.assertEqual(
-            value_metadata_entries[0]._asdict(), {"id": 1, "parameter_value_id": 1, "metadata_id": 2, "commit_id": 3}
-        )
-        entity_metadata_entries = self._db_map.query(self._db_map.entity_metadata_sq).all()
-        self.assertEqual(len(entity_metadata_entries), 1)
-        self.assertEqual(
-            entity_metadata_entries[0]._asdict(), {"id": 1, "entity_id": 1, "metadata_id": 1, "commit_id": 2}
-        )
-
-    def test_update_metadata(self):
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        self._db_map.commit_session("Add test data.")
-        ids, errors = self._db_map.update_metadata(*({"id": 1, "name": "author", "value": "Prof. T. Est"},))
-        self.assertEqual(errors, [])
-        self.assertEqual(ids, {1})
-        metadata_records = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata_records), 1)
-        self.assertEqual(
-            metadata_records[0]._asdict(), {"id": 1, "name": "author", "value": "Prof. T. Est", "commit_id": 3}
-        )
-
-
-class TestDatabaseMappingRemoveMixin(unittest.TestCase):
-    def setUp(self):
-        self._db_map = DatabaseMapping(IN_MEMORY_DB_URL, create=True)
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_remove_works_when_entity_groups_are_present(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_objects(self._db_map, (("my_class", "my_group"),))
-        import_functions.import_object_groups(self._db_map, (("my_class", "my_group", "my_object"),))
-        self._db_map.commit_session("Add test data.")
-        self._db_map.cascade_remove_items(object={1})  # This shouldn't raise an exception
-        self._db_map.commit_session("Remove object.")
-        objects = self._db_map.query(self._db_map.object_sq).all()
-        self.assertEqual(len(objects), 1)
-        self.assertEqual(objects[0].name, "my_group")
-
-    def test_remove_object_class(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        self._db_map.commit_session("Add test data.")
-        my_class = self._db_map.query(self._db_map.object_class_sq).one_or_none()
-        self.assertIsNotNone(my_class)
-        self._db_map.cascade_remove_items(**{"object_class": {my_class.id}})
-        self._db_map.commit_session("Remove object class.")
-        my_class = self._db_map.query(self._db_map.object_class_sq).one_or_none()
-        self.assertIsNone(my_class)
-
-    def test_remove_relationship_class(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_relationship_classes(self._db_map, (("my_relationship_class", ("my_class",)),))
-        self._db_map.commit_session("Add test data.")
-        my_class = self._db_map.query(self._db_map.relationship_class_sq).one_or_none()
-        self.assertIsNotNone(my_class)
-        self._db_map.cascade_remove_items(**{"relationship_class": {my_class.id}})
-        self._db_map.commit_session("Remove relationship class.")
-        my_class = self._db_map.query(self._db_map.relationship_class_sq).one_or_none()
-        self.assertIsNone(my_class)
-
-    def test_remove_object(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        self._db_map.commit_session("Add test data.")
-        my_object = self._db_map.query(self._db_map.object_sq).one_or_none()
-        self.assertIsNotNone(my_object)
-        self._db_map.cascade_remove_items(**{"object": {my_object.id}})
-        self._db_map.commit_session("Remove object.")
-        my_object = self._db_map.query(self._db_map.object_sq).one_or_none()
-        self.assertIsNone(my_object)
-
-    def test_remove_relationship(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_relationship_classes(self._db_map, (("my_relationship_class", ("my_class",)),))
-        import_functions.import_relationships(self._db_map, (("my_relationship_class", ("my_object",)),))
-        self._db_map.commit_session("Add test data.")
-        my_relationship = self._db_map.query(self._db_map.relationship_sq).one_or_none()
-        self.assertIsNotNone(my_relationship)
-        self._db_map.cascade_remove_items(**{"relationship": {2}})
-        self._db_map.commit_session("Remove relationship.")
-        my_relationship = self._db_map.query(self._db_map.relationship_sq).one_or_none()
-        self.assertIsNone(my_relationship)
-
-    def test_remove_parameter_value(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
-        import_functions.import_object_parameter_values(
-            self._db_map, (("my_class", "my_object", "my_parameter", 23.0),)
-        )
-        self._db_map.commit_session("Add test data.")
-        my_value = self._db_map.query(self._db_map.object_parameter_value_sq).one_or_none()
-        self.assertIsNotNone(my_value)
-        self._db_map.cascade_remove_items(**{"parameter_value": {my_value.id}})
-        self._db_map.commit_session("Remove parameter value.")
-        my_parameter = self._db_map.query(self._db_map.object_parameter_value_sq).one_or_none()
-        self.assertIsNone(my_parameter)
-
-
-class TestDatabaseMappingCommitMixin(unittest.TestCase):
-    def setUp(self):
-        self._db_map = DatabaseMapping(IN_MEMORY_DB_URL, create=True)
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_commit_message(self):
-        """Tests that commit comment ends up in the database."""
-        self._db_map.add_object_classes({"name": "testclass"})
-        self._db_map.commit_session("test commit")
-        self.assertEqual(self._db_map.query(self._db_map.commit_sq).all()[-1].comment, "test commit")
-        self._db_map.connection.close()
-
-    def test_commit_session_raise_with_empty_comment(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        self.assertRaisesRegex(SpineDBAPIError, "Commit message cannot be empty.", self._db_map.commit_session, "")
-
-    def test_commit_session_raise_when_nothing_to_commit(self):
-        self.assertRaisesRegex(SpineDBAPIError, "Nothing to commit.", self._db_map.commit_session, "No changes.")
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for DatabaseMapping class.
+
+"""
+import unittest
+from unittest.mock import patch
+from sqlalchemy.engine.url import URL
+from spinedb_api import (
+    DatabaseMapping,
+    to_database,
+    import_functions,
+    from_database,
+    SpineDBAPIError,
+    SpineIntegrityError,
+)
+
+IN_MEMORY_DB_URL = "sqlite://"
+
+
+class TestDatabaseMappingBase(unittest.TestCase):
+    _db_map = None
+
+    @classmethod
+    def setUpClass(cls):
+        cls._db_map = DatabaseMapping(IN_MEMORY_DB_URL, create=True)
+
+    @classmethod
+    def tearDownClass(cls):
+        cls._db_map.connection.close()
+
+    def test_construction_with_filters(self):
+        db_url = IN_MEMORY_DB_URL + "?spinedbfilter=fltr1&spinedbfilter=fltr2"
+        with patch("spinedb_api.db_mapping.apply_filter_stack") as mock_apply:
+            with patch(
+                "spinedb_api.db_mapping.load_filters", return_value=[{"fltr1": "config1", "fltr2": "config2"}]
+            ) as mock_load:
+                db_map = DatabaseMapping(db_url, create=True)
+                db_map.connection.close()
+                mock_load.assert_called_once_with(["fltr1", "fltr2"])
+                mock_apply.assert_called_once_with(db_map, [{"fltr1": "config1", "fltr2": "config2"}])
+
+    def test_construction_with_sqlalchemy_url_and_filters(self):
+        sa_url = URL("sqlite")
+        sa_url.query = {"spinedbfilter": ["fltr1", "fltr2"]}
+        with patch("spinedb_api.db_mapping.apply_filter_stack") as mock_apply:
+            with patch(
+                "spinedb_api.db_mapping.load_filters", return_value=[{"fltr1": "config1", "fltr2": "config2"}]
+            ) as mock_load:
+                db_map = DatabaseMapping(sa_url, create=True)
+                db_map.connection.close()
+                mock_load.assert_called_once_with(["fltr1", "fltr2"])
+                mock_apply.assert_called_once_with(db_map, [{"fltr1": "config1", "fltr2": "config2"}])
+
+    def test_entity_class_type_sq(self):
+        columns = ["id", "name", "commit_id"]
+        self.assertEqual(len(self._db_map.entity_class_type_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.entity_class_type_sq.c, column_name))
+
+    def test_entity_type_sq(self):
+        columns = ["id", "name", "commit_id"]
+        self.assertEqual(len(self._db_map.entity_type_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.entity_type_sq.c, column_name))
+
+    def test_entity_sq(self):
+        columns = ["id", "type_id", "class_id", "name", "description", "commit_id"]
+        self.assertEqual(len(self._db_map.entity_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.entity_sq.c, column_name))
+
+    def test_object_class_sq(self):
+        columns = ["id", "name", "description", "display_order", "display_icon", "hidden", "commit_id"]
+        self.assertEqual(len(self._db_map.object_class_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.object_class_sq.c, column_name))
+
+    def test_object_sq(self):
+        columns = ["id", "class_id", "name", "description", "commit_id"]
+        self.assertEqual(len(self._db_map.object_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.object_sq.c, column_name))
+
+    def test_relationship_class_sq(self):
+        columns = ["id", "dimension", "object_class_id", "name", "description", "display_icon", "hidden", "commit_id"]
+        self.assertEqual(len(self._db_map.relationship_class_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.relationship_class_sq.c, column_name))
+
+    def test_relationship_sq(self):
+        columns = ["id", "dimension", "object_id", "class_id", "name", "commit_id"]
+        self.assertEqual(len(self._db_map.relationship_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.relationship_sq.c, column_name))
+
+    def test_entity_group_sq(self):
+        columns = ["id", "entity_id", "entity_class_id", "member_id"]
+        self.assertEqual(len(self._db_map.entity_group_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.entity_group_sq.c, column_name))
+
+    def test_parameter_definition_sq(self):
+        columns = [
+            "id",
+            "name",
+            "description",
+            "entity_class_id",
+            "object_class_id",
+            "relationship_class_id",
+            "default_value",
+            "default_type",
+            "list_value_id",
+            "commit_id",
+            "parameter_value_list_id",
+        ]
+        self.assertEqual(len(self._db_map.parameter_definition_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.parameter_definition_sq.c, column_name))
+
+    def test_parameter_value_sq(self):
+        columns = [
+            "id",
+            "parameter_definition_id",
+            "entity_class_id",
+            "entity_id",
+            "object_class_id",
+            "relationship_class_id",
+            "object_id",
+            "relationship_id",
+            "value",
+            "type",
+            "list_value_id",
+            "commit_id",
+            "alternative_id",
+        ]
+        self.assertEqual(len(self._db_map.parameter_value_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.parameter_value_sq.c, column_name))
+
+    def test_parameter_value_list_sq(self):
+        columns = ["id", "name", "commit_id"]
+        self.assertEqual(len(self._db_map.parameter_value_list_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.parameter_value_list_sq.c, column_name))
+
+    def test_ext_object_sq(self):
+        columns = ["id", "class_id", "class_name", "name", "description", "group_id", "commit_id"]
+        self.assertEqual(len(self._db_map.ext_object_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.ext_object_sq.c, column_name))
+
+    def test_ext_relationship_class_sq(self):
+        columns = [
+            "id",
+            "name",
+            "description",
+            "display_icon",
+            "dimension",
+            "object_class_id",
+            "object_class_name",
+            "commit_id",
+        ]
+        self.assertEqual(len(self._db_map.ext_relationship_class_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.ext_relationship_class_sq.c, column_name))
+
+    def test_wide_relationship_class_sq(self):
+        columns = [
+            "id",
+            "name",
+            "description",
+            "display_icon",
+            "commit_id",
+            "object_class_id_list",
+            "object_class_name_list",
+        ]
+        self.assertEqual(len(self._db_map.wide_relationship_class_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.wide_relationship_class_sq.c, column_name))
+
+    def test_ext_relationship_sq(self):
+        columns = [
+            "id",
+            "name",
+            "class_id",
+            "class_name",
+            "dimension",
+            "object_id",
+            "object_name",
+            "object_class_id",
+            "object_class_name",
+            "commit_id",
+        ]
+        self.assertEqual(len(self._db_map.ext_relationship_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.ext_relationship_sq.c, column_name))
+
+    def test_wide_relationship_sq(self):
+        columns = [
+            "id",
+            "name",
+            "class_id",
+            "class_name",
+            "commit_id",
+            "object_id_list",
+            "object_name_list",
+            "object_class_id_list",
+            "object_class_name_list",
+        ]
+        self.assertEqual(len(self._db_map.wide_relationship_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.wide_relationship_sq.c, column_name))
+
+    def test_object_parameter_definition_sq(self):
+        columns = [
+            "id",
+            "entity_class_id",
+            "entity_class_name",
+            "object_class_id",
+            "object_class_name",
+            "parameter_name",
+            "value_list_id",
+            "value_list_name",
+            "default_value",
+            "default_type",
+            "description",
+        ]
+        self.assertEqual(len(self._db_map.object_parameter_definition_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.object_parameter_definition_sq.c, column_name))
+
+    def test_relationship_parameter_definition_sq(self):
+        columns = [
+            "id",
+            "entity_class_id",
+            "entity_class_name",
+            "relationship_class_id",
+            "relationship_class_name",
+            "object_class_id_list",
+            "object_class_name_list",
+            "parameter_name",
+            "value_list_id",
+            "value_list_name",
+            "default_value",
+            "default_type",
+            "description",
+        ]
+        self.assertEqual(len(self._db_map.relationship_parameter_definition_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.relationship_parameter_definition_sq.c, column_name))
+
+    def test_object_parameter_value_sq(self):
+        columns = [
+            "id",
+            "entity_class_id",
+            "object_class_id",
+            "object_class_name",
+            "entity_id",
+            "object_id",
+            "object_name",
+            "parameter_id",
+            "parameter_name",
+            "alternative_id",
+            "alternative_name",
+            "value",
+            "type",
+        ]
+        self.assertEqual(len(self._db_map.object_parameter_value_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.object_parameter_value_sq.c, column_name))
+
+    def test_relationship_parameter_value_sq(self):
+        columns = [
+            "id",
+            "entity_class_id",
+            "relationship_class_id",
+            "relationship_class_name",
+            "object_class_id_list",
+            "object_class_name_list",
+            "entity_id",
+            "relationship_id",
+            "object_id_list",
+            "object_name_list",
+            "parameter_id",
+            "parameter_name",
+            "alternative_id",
+            "alternative_name",
+            "value",
+            "type",
+        ]
+        self.assertEqual(len(self._db_map.relationship_parameter_value_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.relationship_parameter_value_sq.c, column_name))
+
+    def test_wide_parameter_value_list_sq(self):
+        columns = ["id", "name", "value_index_list", "value_id_list", "commit_id"]
+        self.assertEqual(len(self._db_map.wide_parameter_value_list_sq.c), len(columns))
+        for column_name in columns:
+            self.assertTrue(hasattr(self._db_map.wide_parameter_value_list_sq.c, column_name))
+
+
+class TestDatabaseMappingBaseQueries(unittest.TestCase):
+    def setUp(self):
+        self._db_map = DatabaseMapping(IN_MEMORY_DB_URL, create=True)
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def create_object_classes(self):
+        obj_classes = ['class1', 'class2']
+        import_functions.import_object_classes(self._db_map, obj_classes)
+        return obj_classes
+
+    def create_objects(self):
+        objects = [('class1', 'obj11'), ('class1', 'obj12'), ('class2', 'obj21')]
+        import_functions.import_objects(self._db_map, objects)
+        return objects
+
+    def create_relationship_classes(self):
+        relationship_classes = [('rel1', ['class1']), ('rel2', ['class1', 'class2'])]
+        import_functions.import_relationship_classes(self._db_map, relationship_classes)
+        return relationship_classes
+
+    def create_relationships(self):
+        relationships = [('rel1', ['obj11']), ('rel2', ['obj11', 'obj21'])]
+        import_functions.import_relationships(self._db_map, relationships)
+        return relationships
+
+    def test_commit_sq_hides_pending_commit(self):
+        commits = self._db_map.query(self._db_map.commit_sq).all()
+        self.assertEqual(len(commits), 1)
+
+    def test_alternative_sq(self):
+        import_functions.import_alternatives(self._db_map, (("alt1", "test alternative"),))
+        alternative_rows = self._db_map.query(self._db_map.alternative_sq).all()
+        expected_names_and_descriptions = {"Base": "Base alternative", "alt1": "test alternative"}
+        self.assertEqual(len(alternative_rows), len(expected_names_and_descriptions))
+        for row in alternative_rows:
+            self.assertTrue(row.name in expected_names_and_descriptions)
+            self.assertEqual(row.description, expected_names_and_descriptions[row.name])
+            expected_names_and_descriptions.pop(row.name)
+        self.assertEqual(expected_names_and_descriptions, {})
+
+    def test_scenario_sq(self):
+        import_functions.import_scenarios(self._db_map, (("scen1", True, "test scenario"),))
+        scenario_rows = self._db_map.query(self._db_map.scenario_sq).all()
+        self.assertEqual(len(scenario_rows), 1)
+        self.assertEqual(scenario_rows[0].name, "scen1")
+        self.assertEqual(scenario_rows[0].description, "test scenario")
+        self.assertTrue(scenario_rows[0].active)
+
+    def test_ext_linked_scenario_alternative_sq(self):
+        import_functions.import_scenarios(self._db_map, (("scen1", True),))
+        import_functions.import_alternatives(self._db_map, ("alt1", "alt2", "alt3"))
+        import_functions.import_scenario_alternatives(self._db_map, (("scen1", "alt2"),))
+        import_functions.import_scenario_alternatives(self._db_map, (("scen1", "alt3"),))
+        import_functions.import_scenario_alternatives(self._db_map, (("scen1", "alt1"),))
+        scenario_alternative_rows = self._db_map.query(self._db_map.ext_linked_scenario_alternative_sq).all()
+        self.assertEqual(len(scenario_alternative_rows), 3)
+        expected_befores = {"alt2": "alt3", "alt3": "alt1", "alt1": None}
+        expected_ranks = {"alt2": 1, "alt3": 2, "alt1": 3}
+        for row in scenario_alternative_rows:
+            self.assertEqual(row.scenario_name, "scen1")
+            self.assertIn(row.alternative_name, expected_befores)
+            self.assertEqual(row.rank, expected_ranks[row.alternative_name])
+            expected_before_alternative = expected_befores.pop(row.alternative_name)
+            self.assertEqual(row.before_alternative_name, expected_before_alternative)
+            if expected_before_alternative is not None:
+                self.assertIsNotNone(row.before_alternative_id)
+                self.assertEqual(row.before_rank, expected_ranks[row.before_alternative_name])
+            else:
+                self.assertIsNone(row.before_alternative_id)
+                self.assertIsNone(row.before_rank)
+        self.assertEqual(expected_befores, {})
+
+    def test_entity_class_sq(self):
+        obj_classes = self.create_object_classes()
+        relationship_classes = self.create_relationship_classes()
+        results = self._db_map.query(self._db_map.entity_class_sq).all()
+        # Check that number of results matches total entities
+        self.assertEqual(len(results), len(obj_classes) + len(relationship_classes))
+        # Check result values
+        for row, class_name in zip(results, obj_classes + [rel[0] for rel in relationship_classes]):
+            self.assertEqual(row.name, class_name)
+
+    def test_entity_sq(self):
+        self.create_object_classes()
+        objects = self.create_objects()
+        self.create_relationship_classes()
+        relationships = self.create_relationships()
+        entity_rows = self._db_map.query(self._db_map.entity_sq).all()
+        self.assertEqual(len(entity_rows), len(objects) + len(relationships))
+        object_names = [o[1] for o in objects]
+        relationship_names = [r[0] + "_" + "__".join(r[1]) for r in relationships]
+        for row, expected_name in zip(entity_rows, object_names + relationship_names):
+            self.assertEqual(row.name, expected_name)
+
+    def test_object_class_sq_picks_object_classes_only(self):
+        obj_classes = self.create_object_classes()
+        self.create_relationship_classes()
+        class_rows = self._db_map.query(self._db_map.object_class_sq).all()
+        self.assertEqual(len(class_rows), len(obj_classes))
+        for row, expected_name in zip(class_rows, obj_classes):
+            self.assertEqual(row.name, expected_name)
+
+    def test_object_sq_picks_objects_only(self):
+        self.create_object_classes()
+        objects = self.create_objects()
+        self.create_relationship_classes()
+        self.create_relationships()
+        object_rows = self._db_map.query(self._db_map.object_sq).all()
+        self.assertEqual(len(object_rows), len(objects))
+        for row, expected_object in zip(object_rows, objects):
+            self.assertEqual(row.name, expected_object[1])
+
+    def test_wide_relationship_class_sq(self):
+        self.create_object_classes()
+        relationship_classes = self.create_relationship_classes()
+        class_rows = self._db_map.query(self._db_map.wide_relationship_class_sq).all()
+        self.assertEqual(len(class_rows), 2)
+        for row, relationship_class in zip(class_rows, relationship_classes):
+            self.assertEqual(row.name, relationship_class[0])
+            self.assertEqual(row.object_class_name_list, ",".join(relationship_class[1]))
+
+    def test_wide_relationship_sq(self):
+        self.create_object_classes()
+        self.create_objects()
+        relationship_classes = self.create_relationship_classes()
+        object_classes = {rel_class[0]: rel_class[1] for rel_class in relationship_classes}
+        relationships = self.create_relationships()
+        relationship_rows = self._db_map.query(self._db_map.wide_relationship_sq).all()
+        self.assertEqual(len(relationship_rows), 2)
+        for row, relationship in zip(relationship_rows, relationships):
+            self.assertEqual(row.name, relationship[0] + "_" + "__".join(relationship[1]))
+            self.assertEqual(row.class_name, relationship[0])
+            self.assertEqual(row.object_class_name_list, ",".join(object_classes[relationship[0]]))
+            self.assertEqual(row.object_name_list, ",".join(relationship[1]))
+
+    def test_parameter_definition_sq_for_object_class(self):
+        self.create_object_classes()
+        import_functions.import_object_parameters(self._db_map, (("class1", "par1"),))
+        definition_rows = self._db_map.query(self._db_map.parameter_definition_sq).all()
+        self.assertEqual(len(definition_rows), 1)
+        self.assertEqual(definition_rows[0].name, "par1")
+        self.assertIsNotNone(definition_rows[0].object_class_id)
+        self.assertIsNone(definition_rows[0].relationship_class_id)
+
+    def test_parameter_definition_sq_for_relationship_class(self):
+        self.create_object_classes()
+        self.create_relationship_classes()
+        import_functions.import_relationship_parameters(self._db_map, (("rel1", "par1"),))
+        definition_rows = self._db_map.query(self._db_map.parameter_definition_sq).all()
+        self.assertEqual(len(definition_rows), 1)
+        self.assertEqual(definition_rows[0].name, "par1")
+        self.assertIsNone(definition_rows[0].object_class_id)
+        self.assertIsNotNone(definition_rows[0].relationship_class_id)
+
+    def test_entity_parameter_definition_sq_for_object_class(self):
+        self.create_object_classes()
+        self.create_relationship_classes()
+        import_functions.import_object_parameters(self._db_map, (("class1", "par1"),))
+        definition_rows = self._db_map.query(self._db_map.entity_parameter_definition_sq).all()
+        self.assertEqual(len(definition_rows), 1)
+        self.assertEqual(definition_rows[0].parameter_name, "par1")
+        self.assertEqual(definition_rows[0].entity_class_name, "class1")
+        self.assertEqual(definition_rows[0].object_class_name, "class1")
+        self.assertIsNone(definition_rows[0].relationship_class_id)
+        self.assertIsNone(definition_rows[0].relationship_class_name)
+        self.assertIsNone(definition_rows[0].object_class_id_list)
+        self.assertIsNone(definition_rows[0].object_class_name_list)
+
+    def test_entity_parameter_definition_sq_for_relationship_class(self):
+        object_classes = self.create_object_classes()
+        self.create_relationship_classes()
+        import_functions.import_relationship_parameters(self._db_map, (("rel2", "par1"),))
+        definition_rows = self._db_map.query(self._db_map.entity_parameter_definition_sq).all()
+        self.assertEqual(len(definition_rows), 1)
+        self.assertEqual(definition_rows[0].parameter_name, "par1")
+        self.assertEqual(definition_rows[0].entity_class_name, "rel2")
+        self.assertIsNotNone(definition_rows[0].relationship_class_id)
+        self.assertEqual(definition_rows[0].relationship_class_name, "rel2")
+        self.assertIsNotNone(definition_rows[0].object_class_id_list)
+        self.assertEqual(definition_rows[0].object_class_name_list, ",".join(object_classes))
+        self.assertIsNone(definition_rows[0].object_class_name)
+
+    def test_entity_parameter_definition_sq_with_multiple_relationship_classes_but_single_parameter(self):
+        self.create_object_classes()
+        self.create_relationship_classes()
+        obj_parameter_definitions = [('class1', 'par1a'), ('class1', 'par1b')]
+        rel_parameter_definitions = [('rel1', 'rpar1a')]
+        import_functions.import_object_parameters(self._db_map, obj_parameter_definitions)
+        import_functions.import_relationship_parameters(self._db_map, rel_parameter_definitions)
+        results = self._db_map.query(self._db_map.entity_parameter_definition_sq).all()
+        # Check that number of results matches total entities
+        self.assertEqual(len(results), len(obj_parameter_definitions) + len(rel_parameter_definitions))
+        # Check result values
+        for row, par_def in zip(results, obj_parameter_definitions + rel_parameter_definitions):
+            self.assertTupleEqual((row.entity_class_name, row.parameter_name), par_def)
+
+    def test_entity_parameter_values(self):
+        self.create_object_classes()
+        self.create_objects()
+        self.create_relationship_classes()
+        self.create_relationships()
+        obj_parameter_definitions = [('class1', 'par1a'), ('class1', 'par1b'), ('class2', 'par2a')]
+        rel_parameter_definitions = [('rel1', 'rpar1a'), ('rel2', 'rpar2a')]
+        import_functions.import_object_parameters(self._db_map, obj_parameter_definitions)
+        import_functions.import_relationship_parameters(self._db_map, rel_parameter_definitions)
+        object_parameter_values = [
+            ('class1', 'obj11', 'par1a', 123),
+            ('class1', 'obj11', 'par1b', 333),
+            ('class2', 'obj21', 'par2a', 'empty'),
+        ]
+        _, errors = import_functions.import_object_parameter_values(self._db_map, object_parameter_values)
+        self.assertFalse(errors)
+        relationship_parameter_values = [('rel1', ['obj11'], 'rpar1a', 1.1), ('rel2', ['obj11', 'obj21'], 'rpar2a', 42)]
+        _, errors = import_functions.import_relationship_parameter_values(self._db_map, relationship_parameter_values)
+        self.assertFalse(errors)
+        results = self._db_map.query(self._db_map.entity_parameter_value_sq).all()
+        # Check that number of results matches total entities
+        self.assertEqual(len(results), len(object_parameter_values) + len(relationship_parameter_values))
+        # Check result values
+        for row, par_val in zip(results, object_parameter_values + relationship_parameter_values):
+            self.assertEqual(row.entity_class_name, par_val[0])
+            if row.object_name:  # This is an object parameter
+                self.assertEqual(row.object_name, par_val[1])
+            else:  # This is a relationship parameter
+                self.assertEqual(row.object_name_list, ','.join(par_val[1]))
+            self.assertEqual(row.parameter_name, par_val[2])
+            self.assertEqual(from_database(row.value, row.type), par_val[3])
+
+    def test_wide_parameter_value_list_sq(self):
+        _, errors = import_functions.import_parameter_value_lists(
+            self._db_map, (("list1", "value1"), ("list1", "value2"), ("list2", "valueA"))
+        )
+        self.assertEqual(errors, [])
+        value_lists = self._db_map.query(self._db_map.wide_parameter_value_list_sq).all()
+        self.assertEqual(len(value_lists), 2)
+        self.assertEqual(value_lists[0].name, "list1")
+        self.assertEqual(value_lists[1].name, "list2")
+
+
+class TestDatabaseMappingUpdateMixin(unittest.TestCase):
+    def setUp(self):
+        self._db_map = DatabaseMapping(IN_MEMORY_DB_URL, create=True)
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_update_method_of_tool_feature_method(self):
+        import_functions.import_object_classes(self._db_map, ("object_class1", "object_class2"))
+        import_functions.import_parameter_value_lists(
+            self._db_map, (("value_list", "value1"), ("value_list", "value2"))
+        )
+        import_functions.import_object_parameters(
+            self._db_map, (("object_class1", "parameter1", "value1", "value_list"), ("object_class1", "parameter2"))
+        )
+        import_functions.import_features(self._db_map, (("object_class1", "parameter1"),))
+        import_functions.import_tools(self._db_map, ("tool1",))
+        import_functions.import_tool_features(self._db_map, (("tool1", "object_class1", "parameter1"),))
+        import_functions.import_tool_feature_methods(
+            self._db_map, (("tool1", "object_class1", "parameter1", "value2"),)
+        )
+        self._db_map.commit_session("Populate with initial data.")
+        updated_ids, errors = self._db_map.update_tool_feature_methods(
+            {"id": 1, "method_index": 0, "method": to_database("value1")[0]}
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(updated_ids, {1})
+        self._db_map.commit_session("Update data.")
+        tool_feature_methods = self._db_map.query(self._db_map.ext_tool_feature_method_sq).all()
+        self.assertEqual(len(tool_feature_methods), 1)
+        tool_feature_method = tool_feature_methods[0]
+        self.assertEqual(tool_feature_method.method, to_database("value1")[0])
+
+    def test_update_wide_relationship_class(self):
+        _ = import_functions.import_object_classes(self._db_map, ("object_class_1",))
+        _ = import_functions.import_relationship_classes(self._db_map, (("my_class", ("object_class_1",)),))
+        self._db_map.commit_session("Add test data")
+        updated_ids, errors = self._db_map.update_wide_relationship_classes({"id": 2, "name": "renamed"})
+        self.assertEqual(errors, [])
+        self.assertEqual(updated_ids, {2})
+        self._db_map.commit_session("Update data.")
+        classes = self._db_map.query(self._db_map.wide_relationship_class_sq).all()
+        self.assertEqual(len(classes), 1)
+        self.assertEqual(classes[0].name, "renamed")
+
+    def test_update_wide_relationship_class_does_not_update_member_class_id(self):
+        import_functions.import_object_classes(self._db_map, ("object_class_1", "object_class_2"))
+        import_functions.import_relationship_classes(self._db_map, (("my_class", ("object_class_1",)),))
+        self._db_map.commit_session("Add test data")
+        updated_ids, errors = self._db_map.update_wide_relationship_classes(
+            {"id": 3, "name": "renamed", "object_class_id_list": [2]}
+        )
+        self.assertEqual([str(err) for err in errors], ["Can't update fixed fields 'object_class_id_list'"])
+        self.assertEqual(updated_ids, {3})
+        self._db_map.commit_session("Update data.")
+        classes = self._db_map.query(self._db_map.wide_relationship_class_sq).all()
+        self.assertEqual(len(classes), 1)
+        self.assertEqual(classes[0].name, "renamed")
+        self.assertEqual(classes[0].object_class_name_list, "object_class_1")
+
+    def test_update_wide_relationship(self):
+        import_functions.import_object_classes(self._db_map, ("object_class_1", "object_class_2"))
+        import_functions.import_objects(
+            self._db_map,
+            (("object_class_1", "object_11"), ("object_class_1", "object_12"), ("object_class_2", "object_21")),
+        )
+        import_functions.import_relationship_classes(
+            self._db_map, (("my_class", ("object_class_1", "object_class_2")),)
+        )
+        import_functions.import_relationships(self._db_map, (("my_class", ("object_11", "object_21")),))
+        self._db_map.commit_session("Add test data")
+        updated_ids, errors = self._db_map.update_wide_relationships(
+            {"id": 4, "name": "renamed", "object_id_list": [2, 3]}
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(updated_ids, {4})
+        self._db_map.commit_session("Update data.")
+        relationships = self._db_map.query(self._db_map.wide_relationship_sq).all()
+        self.assertEqual(len(relationships), 1)
+        self.assertEqual(relationships[0].name, "renamed")
+        self.assertEqual(relationships[0].object_name_list, "object_12,object_21")
+
+    def test_update_parameter_value_by_id_only(self):
+        import_functions.import_object_classes(self._db_map, ("object_class1",))
+        import_functions.import_object_parameters(self._db_map, (("object_class1", "parameter1"),))
+        import_functions.import_objects(self._db_map, (("object_class1", "object1"),))
+        import_functions.import_object_parameter_values(
+            self._db_map, (("object_class1", "object1", "parameter1", "something"),)
+        )
+        self._db_map.commit_session("Populate with initial data.")
+        updated_ids, errors = self._db_map.update_parameter_values({"id": 1, "value": b"something else"})
+        self.assertEqual(errors, [])
+        self.assertEqual(updated_ids, {1})
+        self._db_map.commit_session("Update data.")
+        pvals = self._db_map.query(self._db_map.parameter_value_sq).all()
+        self.assertEqual(len(pvals), 1)
+        pval = pvals[0]
+        self.assertEqual(pval.value, b"something else")
+
+    def test_update_parameter_definition_by_id_only(self):
+        import_functions.import_object_classes(self._db_map, ("object_class1",))
+        import_functions.import_object_parameters(self._db_map, (("object_class1", "parameter1"),))
+        self._db_map.commit_session("Populate with initial data.")
+        updated_ids, errors = self._db_map.update_parameter_definitions({"id": 1, "name": "parameter2"})
+        self.assertEqual(errors, [])
+        self.assertEqual(updated_ids, {1})
+        self._db_map.commit_session("Update data.")
+        pdefs = self._db_map.query(self._db_map.parameter_definition_sq).all()
+        self.assertEqual(len(pdefs), 1)
+        self.assertEqual(pdefs[0].name, "parameter2")
+
+    def test_update_parameter_definition_value_list(self):
+        import_functions.import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
+        import_functions.import_object_classes(self._db_map, ("object_class",))
+        import_functions.import_object_parameters(self._db_map, (("object_class", "my_parameter"),))
+        self._db_map.commit_session("Populate with initial data.")
+        updated_ids, errors = self._db_map.update_parameter_definitions(
+            {"id": 1, "name": "my_parameter", "parameter_value_list_id": 1}
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(updated_ids, {1})
+        self._db_map.commit_session("Update data.")
+        pdefs = self._db_map.query(self._db_map.parameter_definition_sq).all()
+        self.assertEqual(len(pdefs), 1)
+        self.assertEqual(
+            pdefs[0]._asdict(),
+            {
+                "commit_id": 3,
+                "default_type": None,
+                "default_value": None,
+                "description": None,
+                "entity_class_id": 1,
+                "id": 1,
+                "list_value_id": None,
+                "name": "my_parameter",
+                "object_class_id": 1,
+                "parameter_value_list_id": 1,
+                "relationship_class_id": None,
+            },
+        )
+
+    def test_update_parameter_definition_value_list_when_values_exist_gives_error(self):
+        import_functions.import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
+        import_functions.import_object_classes(self._db_map, ("object_class",))
+        import_functions.import_objects(self._db_map, (("object_class", "my_object"),))
+        import_functions.import_object_parameters(self._db_map, (("object_class", "my_parameter"),))
+        import_functions.import_object_parameter_values(
+            self._db_map, (("object_class", "my_object", "my_parameter", 23.0),)
+        )
+        self._db_map.commit_session("Populate with initial data.")
+        updated_ids, errors = self._db_map.update_parameter_definitions(
+            {"id": 1, "name": "my_parameter", "parameter_value_list_id": 1}
+        )
+        self.assertEqual(
+            list(map(str, errors)),
+            ["Can't change value list on parameter my_parameter because it has parameter values."],
+        )
+        self.assertEqual(updated_ids, set())
+
+    def test_update_parameter_definitions_default_value_that_is_not_on_value_list_gives_error(self):
+        import_functions.import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
+        import_functions.import_object_classes(self._db_map, ("object_class",))
+        import_functions.import_objects(self._db_map, (("object_class", "my_object"),))
+        import_functions.import_object_parameters(self._db_map, (("object_class", "my_parameter", None, "my_list"),))
+        self._db_map.commit_session("Populate with initial data.")
+        updated_ids, errors = self._db_map.update_parameter_definitions(
+            {"id": 1, "name": "my_parameter", "default_value": to_database(23.0)[0]}
+        )
+        self.assertEqual(
+            list(map(str, errors)),
+            ["Invalid default_value '23.0' - it should be one from the parameter value list: '99.0'."],
+        )
+        self.assertEqual(updated_ids, set())
+
+    def test_update_parameter_definition_value_list_when_default_value_not_on_the_list_exists_gives_error(self):
+        import_functions.import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
+        import_functions.import_object_classes(self._db_map, ("object_class",))
+        import_functions.import_objects(self._db_map, (("object_class", "my_object"),))
+        import_functions.import_object_parameters(self._db_map, (("object_class", "my_parameter", 23.0),))
+        self._db_map.commit_session("Populate with initial data.")
+        updated_ids, errors = self._db_map.update_parameter_definitions(
+            {"id": 1, "name": "my_parameter", "parameter_value_list_id": 1}
+        )
+        self.assertEqual(
+            list(map(str, errors)),
+            ["Invalid default_value '23.0' - it should be one from the parameter value list: '99.0'."],
+        )
+        self.assertEqual(updated_ids, set())
+
+    def test_update_object_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
+        self._db_map.commit_session("Add test data")
+        ids, errors = self._db_map.update_ext_entity_metadata(
+            *[{"id": 1, "metadata_name": "key_2", "metadata_value": "new value"}]
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(ids, {1})
+        metadata_entries = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata_entries), 1)
+        self.assertEqual(
+            metadata_entries[0]._asdict(), {"id": 1, "name": "key_2", "value": "new value", "commit_id": 3}
+        )
+        entity_metadata_entries = self._db_map.query(self._db_map.entity_metadata_sq).all()
+        self.assertEqual(len(entity_metadata_entries), 1)
+        self.assertEqual(
+            entity_metadata_entries[0]._asdict(), {"id": 1, "entity_id": 1, "metadata_id": 1, "commit_id": 2}
+        )
+
+    def test_update_object_metadata_reuses_existing_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"), ("my_class", "extra_object")))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}', '{"key 2": "metadata value 2"}'))
+        import_functions.import_object_metadata(
+            self._db_map,
+            (
+                ("my_class", "my_object", '{"title": "My metadata."}'),
+                ("my_class", "extra_object", '{"key 2": "metadata value 2"}'),
+            ),
+        )
+        self._db_map.commit_session("Add test data")
+        ids, errors = self._db_map.update_ext_entity_metadata(
+            *[{"id": 1, "metadata_name": "key 2", "metadata_value": "metadata value 2"}]
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(ids, {1})
+        metadata_entries = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata_entries), 2)
+        self.assertEqual(
+            metadata_entries[0]._asdict(), {"id": 1, "name": "title", "value": "My metadata.", "commit_id": 2}
+        )
+        self.assertEqual(
+            metadata_entries[1]._asdict(), {"id": 2, "name": "key 2", "value": "metadata value 2", "commit_id": 2}
+        )
+        entity_metadata_entries = self._db_map.query(self._db_map.entity_metadata_sq).all()
+        self.assertEqual(len(entity_metadata_entries), 2)
+        self.assertEqual(
+            entity_metadata_entries[0]._asdict(), {"id": 1, "entity_id": 1, "metadata_id": 2, "commit_id": 3}
+        )
+        self.assertEqual(
+            entity_metadata_entries[1]._asdict(), {"id": 2, "entity_id": 2, "metadata_id": 2, "commit_id": 2}
+        )
+
+    def test_update_object_metadata_keeps_metadata_still_in_use(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "object_1"), ("my_class", "object_2")))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_object_metadata(
+            self._db_map,
+            (
+                ("my_class", "object_1", '{"title": "My metadata."}'),
+                ("my_class", "object_2", '{"title": "My metadata."}'),
+            ),
+        )
+        self._db_map.commit_session("Add test data")
+        ids, errors = self._db_map.update_ext_entity_metadata(
+            *[{"id": 1, "metadata_name": "new key", "metadata_value": "new value"}]
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(ids, {1, 2})
+        metadata_entries = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata_entries), 2)
+        self.assertEqual(
+            metadata_entries[0]._asdict(), {"id": 1, "name": "title", "value": "My metadata.", "commit_id": 2}
+        )
+        self.assertEqual(
+            metadata_entries[1]._asdict(), {"id": 2, "name": "new key", "value": "new value", "commit_id": 3}
+        )
+        entity_metadata_entries = self._db_map.query(self._db_map.entity_metadata_sq).all()
+        self.assertEqual(len(entity_metadata_entries), 2)
+        self.assertEqual(
+            entity_metadata_entries[0]._asdict(), {"id": 1, "entity_id": 1, "metadata_id": 2, "commit_id": 3}
+        )
+        self.assertEqual(
+            entity_metadata_entries[1]._asdict(), {"id": 2, "entity_id": 2, "metadata_id": 1, "commit_id": 2}
+        )
+
+    def test_update_parameter_value_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_object_parameter_values(
+            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
+        )
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_object_parameter_value_metadata(
+            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
+        )
+        self._db_map.commit_session("Add test data")
+        ids, errors = self._db_map.update_ext_parameter_value_metadata(
+            *[{"id": 1, "metadata_name": "key_2", "metadata_value": "new value"}]
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(ids, {1})
+        metadata_entries = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata_entries), 1)
+        self.assertEqual(
+            metadata_entries[0]._asdict(), {"id": 1, "name": "key_2", "value": "new value", "commit_id": 3}
+        )
+        value_metadata_entries = self._db_map.query(self._db_map.parameter_value_metadata_sq).all()
+        self.assertEqual(len(value_metadata_entries), 1)
+        self.assertEqual(
+            value_metadata_entries[0]._asdict(), {"id": 1, "parameter_value_id": 1, "metadata_id": 1, "commit_id": 2}
+        )
+
+    def test_update_parameter_value_metadata_will_not_delete_shared_entity_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_object_parameter_values(
+            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
+        )
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
+        import_functions.import_object_parameter_value_metadata(
+            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
+        )
+        self._db_map.commit_session("Add test data")
+        ids, errors = self._db_map.update_ext_parameter_value_metadata(
+            *[{"id": 1, "metadata_name": "key_2", "metadata_value": "new value"}]
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(ids, {1, 2})
+        metadata_entries = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata_entries), 2)
+        self.assertEqual(
+            metadata_entries[0]._asdict(), {"id": 1, "name": "title", "value": "My metadata.", "commit_id": 2}
+        )
+        self.assertEqual(
+            metadata_entries[1]._asdict(), {"id": 2, "name": "key_2", "value": "new value", "commit_id": 3}
+        )
+        value_metadata_entries = self._db_map.query(self._db_map.parameter_value_metadata_sq).all()
+        self.assertEqual(len(value_metadata_entries), 1)
+        self.assertEqual(
+            value_metadata_entries[0]._asdict(), {"id": 1, "parameter_value_id": 1, "metadata_id": 2, "commit_id": 3}
+        )
+        entity_metadata_entries = self._db_map.query(self._db_map.entity_metadata_sq).all()
+        self.assertEqual(len(entity_metadata_entries), 1)
+        self.assertEqual(
+            entity_metadata_entries[0]._asdict(), {"id": 1, "entity_id": 1, "metadata_id": 1, "commit_id": 2}
+        )
+
+    def test_update_metadata(self):
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        self._db_map.commit_session("Add test data.")
+        ids, errors = self._db_map.update_metadata(*({"id": 1, "name": "author", "value": "Prof. T. Est"},))
+        self.assertEqual(errors, [])
+        self.assertEqual(ids, {1})
+        metadata_records = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata_records), 1)
+        self.assertEqual(
+            metadata_records[0]._asdict(), {"id": 1, "name": "author", "value": "Prof. T. Est", "commit_id": 3}
+        )
+
+
+class TestDatabaseMappingRemoveMixin(unittest.TestCase):
+    def setUp(self):
+        self._db_map = DatabaseMapping(IN_MEMORY_DB_URL, create=True)
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_remove_works_when_entity_groups_are_present(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_objects(self._db_map, (("my_class", "my_group"),))
+        import_functions.import_object_groups(self._db_map, (("my_class", "my_group", "my_object"),))
+        self._db_map.commit_session("Add test data.")
+        self._db_map.cascade_remove_items(object={1})  # This shouldn't raise an exception
+        self._db_map.commit_session("Remove object.")
+        objects = self._db_map.query(self._db_map.object_sq).all()
+        self.assertEqual(len(objects), 1)
+        self.assertEqual(objects[0].name, "my_group")
+
+    def test_remove_object_class(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        self._db_map.commit_session("Add test data.")
+        my_class = self._db_map.query(self._db_map.object_class_sq).one_or_none()
+        self.assertIsNotNone(my_class)
+        self._db_map.cascade_remove_items(**{"object_class": {my_class.id}})
+        self._db_map.commit_session("Remove object class.")
+        my_class = self._db_map.query(self._db_map.object_class_sq).one_or_none()
+        self.assertIsNone(my_class)
+
+    def test_remove_relationship_class(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_relationship_classes(self._db_map, (("my_relationship_class", ("my_class",)),))
+        self._db_map.commit_session("Add test data.")
+        my_class = self._db_map.query(self._db_map.relationship_class_sq).one_or_none()
+        self.assertIsNotNone(my_class)
+        self._db_map.cascade_remove_items(**{"relationship_class": {my_class.id}})
+        self._db_map.commit_session("Remove relationship class.")
+        my_class = self._db_map.query(self._db_map.relationship_class_sq).one_or_none()
+        self.assertIsNone(my_class)
+
+    def test_remove_object(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        self._db_map.commit_session("Add test data.")
+        my_object = self._db_map.query(self._db_map.object_sq).one_or_none()
+        self.assertIsNotNone(my_object)
+        self._db_map.cascade_remove_items(**{"object": {my_object.id}})
+        self._db_map.commit_session("Remove object.")
+        my_object = self._db_map.query(self._db_map.object_sq).one_or_none()
+        self.assertIsNone(my_object)
+
+    def test_remove_relationship(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_relationship_classes(self._db_map, (("my_relationship_class", ("my_class",)),))
+        import_functions.import_relationships(self._db_map, (("my_relationship_class", ("my_object",)),))
+        self._db_map.commit_session("Add test data.")
+        my_relationship = self._db_map.query(self._db_map.relationship_sq).one_or_none()
+        self.assertIsNotNone(my_relationship)
+        self._db_map.cascade_remove_items(**{"relationship": {2}})
+        self._db_map.commit_session("Remove relationship.")
+        my_relationship = self._db_map.query(self._db_map.relationship_sq).one_or_none()
+        self.assertIsNone(my_relationship)
+
+    def test_remove_parameter_value(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
+        import_functions.import_object_parameter_values(
+            self._db_map, (("my_class", "my_object", "my_parameter", 23.0),)
+        )
+        self._db_map.commit_session("Add test data.")
+        my_value = self._db_map.query(self._db_map.object_parameter_value_sq).one_or_none()
+        self.assertIsNotNone(my_value)
+        self._db_map.cascade_remove_items(**{"parameter_value": {my_value.id}})
+        self._db_map.commit_session("Remove parameter value.")
+        my_parameter = self._db_map.query(self._db_map.object_parameter_value_sq).one_or_none()
+        self.assertIsNone(my_parameter)
+
+
+class TestDatabaseMappingCommitMixin(unittest.TestCase):
+    def setUp(self):
+        self._db_map = DatabaseMapping(IN_MEMORY_DB_URL, create=True)
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_commit_message(self):
+        """Tests that commit comment ends up in the database."""
+        self._db_map.add_object_classes({"name": "testclass"})
+        self._db_map.commit_session("test commit")
+        self.assertEqual(self._db_map.query(self._db_map.commit_sq).all()[-1].comment, "test commit")
+        self._db_map.connection.close()
+
+    def test_commit_session_raise_with_empty_comment(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        self.assertRaisesRegex(SpineDBAPIError, "Commit message cannot be empty.", self._db_map.commit_session, "")
+
+    def test_commit_session_raise_when_nothing_to_commit(self):
+        self.assertRaisesRegex(SpineDBAPIError, "Nothing to commit.", self._db_map.commit_session, "No changes.")
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/test_DiffDatabaseMapping.py` & `spinedb_api-0.30.4/tests/test_DiffDatabaseMapping.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,1320 +1,1320 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for DiffDatabaseMapping class.
-
-"""
-
-import os.path
-from tempfile import TemporaryDirectory
-import unittest
-from unittest import mock
-from sqlalchemy.engine.url import make_url, URL
-from sqlalchemy.util import KeyedTuple
-from spinedb_api.diff_db_mapping import DiffDatabaseMapping
-from spinedb_api.exception import SpineIntegrityError
-from spinedb_api.db_cache import DBCache
-from spinedb_api import import_functions, SpineDBAPIError
-
-
-def create_query_wrapper(db_map):
-    def query_wrapper(*args, orig_query=db_map.query, **kwargs):
-        arg = args[0]
-        if isinstance(arg, mock.Mock):
-            return arg.value
-        return orig_query(*args, **kwargs)
-
-    return query_wrapper
-
-
-IN_MEMORY_DB_URL = "sqlite://"
-
-
-def create_diff_db_map():
-    return DiffDatabaseMapping(IN_MEMORY_DB_URL, username="UnitTest", create=True)
-
-
-class TestDiffDatabaseMappingConstruction(unittest.TestCase):
-    def test_construction_with_filters(self):
-        db_url = IN_MEMORY_DB_URL + "?spinedbfilter=fltr1&spinedbfilter=fltr2"
-        with mock.patch("spinedb_api.diff_db_mapping.apply_filter_stack") as mock_apply:
-            with mock.patch(
-                "spinedb_api.diff_db_mapping.load_filters", return_value=[{"fltr1": "config1", "fltr2": "config2"}]
-            ) as mock_load:
-                db_map = DiffDatabaseMapping(db_url, create=True)
-                db_map.connection.close()
-                mock_load.assert_called_once_with(["fltr1", "fltr2"])
-                mock_apply.assert_called_once_with(db_map, [{"fltr1": "config1", "fltr2": "config2"}])
-
-    def test_construction_with_sqlalchemy_url_and_filters(self):
-        db_url = IN_MEMORY_DB_URL + "/?spinedbfilter=fltr1&spinedbfilter=fltr2"
-        sa_url = make_url(db_url)
-        with mock.patch("spinedb_api.diff_db_mapping.apply_filter_stack") as mock_apply:
-            with mock.patch(
-                "spinedb_api.diff_db_mapping.load_filters", return_value=[{"fltr1": "config1", "fltr2": "config2"}]
-            ) as mock_load:
-                db_map = DiffDatabaseMapping(sa_url, create=True)
-                db_map.connection.close()
-                mock_load.assert_called_once_with(["fltr1", "fltr2"])
-                mock_apply.assert_called_once_with(db_map, [{"fltr1": "config1", "fltr2": "config2"}])
-
-    def test_shorthand_filter_query_works(self):
-        with TemporaryDirectory() as temp_dir:
-            url = URL("sqlite")
-            url.database = os.path.join(temp_dir, "test_shorthand_filter_query_works.json")
-            out_db = DiffDatabaseMapping(url, create=True)
-            out_db.add_tools({"name": "object_activity_control", "id": 1})
-            out_db.commit_session("Add tool.")
-            out_db.connection.close()
-            try:
-                db_map = DiffDatabaseMapping(url)
-            except:
-                self.fail("DiffDatabaseMapping.__init__() should not raise.")
-            else:
-                db_map.connection.close()
-
-
-class TestDiffDatabaseMappingRemove(unittest.TestCase):
-    def setUp(self):
-        self._db_map = create_diff_db_map()
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_cascade_remove_relationship(self):
-        """Test adding and removing a relationship and committing"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2})
-        ids, _ = self._db_map.add_wide_relationships({"name": "remove_me", "class_id": 3, "object_id_list": [1, 2]})
-        self._db_map.cascade_remove_items(relationship=ids)
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
-        self._db_map.commit_session("delete")
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
-
-    def test_cascade_remove_relationship_from_committed_session(self):
-        """Test removing a relationship from a committed session"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2})
-        ids, _ = self._db_map.add_wide_relationships({"name": "remove_me", "class_id": 3, "object_id_list": [1, 2]})
-        self._db_map.commit_session("add")
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 1)
-        self._db_map.cascade_remove_items(relationship=ids)
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
-        self._db_map.commit_session("Add test data.")
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
-
-    def test_remove_object(self):
-        """Test adding and removing an object and committing"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        ids, _ = self._db_map.add_objects(
-            {"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2}
-        )
-        self._db_map.remove_items(object=ids)
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
-        self._db_map.commit_session("delete")
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
-
-    def test_remove_object_from_committed_session(self):
-        """Test removing an object from a committed session"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        ids, _ = self._db_map.add_objects(
-            {"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2}
-        )
-        self._db_map.commit_session("add")
-        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 2)
-        self._db_map.remove_items(object=ids)
-        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 0)
-        self._db_map.commit_session("Add test data.")
-        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 0)
-
-    def test_remove_entity_group(self):
-        """Test adding and removing an entity group and committing"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
-        ids, _ = self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 1, "member_id": 2})
-        self._db_map.remove_items(entity_group=ids)
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_group_sq).all()), 0)
-        self._db_map.commit_session("delete")
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_group_sq).all()), 0)
-
-    def test_remove_entity_group_from_committed_session(self):
-        """Test removing an entity group from a committed session"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
-        ids, _ = self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 1, "member_id": 2})
-        self._db_map.commit_session("add")
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_group_sq).all()), 1)
-        self._db_map.remove_items(entity_group=ids)
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_group_sq).all()), 0)
-        self._db_map.commit_session("delete")
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_group_sq).all()), 0)
-
-    def test_cascade_remove_relationship_class(self):
-        """Test adding and removing a relationship class and committing"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        ids, _ = self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
-        self._db_map.cascade_remove_items(relationship_class=ids)
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_class_sq).all()), 0)
-        self._db_map.commit_session("delete")
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_class_sq).all()), 0)
-
-    def test_cascade_remove_relationship_class_from_committed_session(self):
-        """Test removing a relationship class from a committed session"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        ids, _ = self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
-        self._db_map.commit_session("add")
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_class_sq).all()), 1)
-        self._db_map.cascade_remove_items(relationship_class=ids)
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_class_sq).all()), 0)
-        self._db_map.commit_session("Add test data.")
-        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_class_sq).all()), 0)
-
-    def test_remove_object_class(self):
-        """Test adding and removing an object class and committing"""
-        ids, _ = self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.remove_items(object_class=ids)
-        self.assertEqual(len(self._db_map.query(self._db_map.object_class_sq).all()), 0)
-        self._db_map.commit_session("delete")
-        self.assertEqual(len(self._db_map.query(self._db_map.object_class_sq).all()), 0)
-
-    def test_remove_object_class_from_committed_session(self):
-        """Test removing an object class from a committed session"""
-        ids, _ = self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.commit_session("add")
-        self.assertEqual(len(self._db_map.query(self._db_map.object_class_sq).all()), 2)
-        self._db_map.remove_items(object_class=ids)
-        self.assertEqual(len(self._db_map.query(self._db_map.object_class_sq).all()), 0)
-        self._db_map.commit_session("Add test data.")
-        self.assertEqual(len(self._db_map.query(self._db_map.object_class_sq).all()), 0)
-
-    def test_remove_parameter_value(self):
-        """Test adding and removing a parameter value and committing"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, strict=True)
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, strict=True)
-        self._db_map.add_parameter_definitions({"name": "param", "id": 1, "object_class_id": 1}, strict=True)
-        self._db_map.add_parameter_values(
-            {
-                "value": b"0",
-                "id": 1,
-                "parameter_definition_id": 1,
-                "object_id": 1,
-                "object_class_id": 1,
-                "alternative_id": 1,
-            },
-            strict=True,
-        )
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 1)
-        self._db_map.remove_items(parameter_value=[1])
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
-        self._db_map.commit_session("delete")
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
-
-    def test_remove_parameter_value_from_committed_session(self):
-        """Test adding and committing a parameter value and then removing it"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, strict=True)
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, strict=True)
-        self._db_map.add_parameter_definitions({"name": "param", "id": 1, "object_class_id": 1}, strict=True)
-        self._db_map.add_parameter_values(
-            {
-                "value": b"0",
-                "id": 1,
-                "parameter_definition_id": 1,
-                "object_id": 1,
-                "object_class_id": 1,
-                "alternative_id": 1,
-            },
-            strict=True,
-        )
-        self._db_map.commit_session("add")
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 1)
-        self._db_map.remove_items(parameter_value=[1])
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
-        self._db_map.commit_session("delete")
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
-
-    def test_cascade_remove_object_removes_parameter_value_as_well(self):
-        """Test adding and removing a parameter value and committing"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, strict=True)
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, strict=True)
-        self._db_map.add_parameter_definitions({"name": "param", "id": 1, "object_class_id": 1}, strict=True)
-        self._db_map.add_parameter_values(
-            {
-                "value": b"0",
-                "id": 1,
-                "parameter_definition_id": 1,
-                "object_id": 1,
-                "object_class_id": 1,
-                "alternative_id": 1,
-            },
-            strict=True,
-        )
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 1)
-        self._db_map.cascade_remove_items(object={1})
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
-        self._db_map.commit_session("delete")
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
-
-    def test_cascade_remove_object_from_committed_session_removes_parameter_value_as_well(self):
-        """Test adding and committing a paramater value and then removing it"""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, strict=True)
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, strict=True)
-        self._db_map.add_parameter_definitions({"name": "param", "id": 1, "object_class_id": 1}, strict=True)
-        self._db_map.add_parameter_values(
-            {
-                "value": b"0",
-                "id": 1,
-                "parameter_definition_id": 1,
-                "object_id": 1,
-                "object_class_id": 1,
-                "alternative_id": 1,
-            },
-            strict=True,
-        )
-        self._db_map.commit_session("add")
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 1)
-        self._db_map.cascade_remove_items(object={1})
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
-        self._db_map.commit_session("delete")
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
-
-    def test_cascade_remove_metadata_removes_corresponding_entity_and_value_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
-        import_functions.import_object_parameter_values(
-            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
-        )
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
-        import_functions.import_object_parameter_value_metadata(
-            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
-        )
-        self._db_map.commit_session("Add test data.")
-        metadata = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata), 1)
-        self._db_map.cascade_remove_items(**{"metadata": {metadata[0].id}})
-        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 1)
-        self.assertEqual(len(self._db_map.query(self._db_map.object_parameter_definition_sq).all()), 1)
-
-    def test_cascade_remove_entity_metadata_removes_corresponding_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
-        self._db_map.commit_session("Add test data.")
-        entity_metadata = self._db_map.query(self._db_map.entity_metadata_sq).all()
-        self.assertEqual(len(entity_metadata), 1)
-        self._db_map.cascade_remove_items(**{"entity_metadata": {entity_metadata[0].id}})
-        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 1)
-
-    def test_cascade_remove_entity_metadata_leaves_metadata_used_by_value_intact(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
-        import_functions.import_object_parameter_values(
-            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
-        )
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
-        import_functions.import_object_parameter_value_metadata(
-            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
-        )
-        self._db_map.commit_session("Add test data.")
-        entity_metadata = self._db_map.query(self._db_map.entity_metadata_sq).all()
-        self.assertEqual(len(entity_metadata), 1)
-        self._db_map.cascade_remove_items(**{"entity_metadata": {entity_metadata[0].id}})
-        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 1)
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_metadata_sq).all()), 1)
-
-    def test_cascade_remove_value_metadata_leaves_metadata_used_by_entity_intact(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
-        import_functions.import_object_parameter_values(
-            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
-        )
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
-        import_functions.import_object_parameter_value_metadata(
-            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
-        )
-        self._db_map.commit_session("Add test data.")
-        parameter_value_metadata = self._db_map.query(self._db_map.parameter_value_metadata_sq).all()
-        self.assertEqual(len(parameter_value_metadata), 1)
-        self._db_map.cascade_remove_items(**{"parameter_value_metadata": {parameter_value_metadata[0].id}})
-        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 1)
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 1)
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_metadata_sq).all()), 0)
-
-    def test_cascade_remove_object_removes_its_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
-        self._db_map.commit_session("Add test data.")
-        self._db_map.cascade_remove_items(**{"object": {1}})
-        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 0)
-
-    def test_cascade_remove_relationship_removes_its_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("my_object_class",))
-        import_functions.import_objects(self._db_map, (("my_object_class", "my_object"),))
-        import_functions.import_relationship_classes(self._db_map, (("my_class", ("my_object_class",)),))
-        import_functions.import_relationships(self._db_map, (("my_class", ("my_object",)),))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_relationship_metadata(
-            self._db_map, (("my_class", ("my_object",), '{"title": "My metadata."}'),)
-        )
-        self._db_map.commit_session("Add test data.")
-        self._db_map.cascade_remove_items(**{"relationship": {2}})
-        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.relationship_sq).all()), 0)
-
-    def test_cascade_remove_parameter_value_removes_its_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
-        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
-        import_functions.import_object_parameter_values(
-            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
-        )
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        import_functions.import_object_parameter_value_metadata(
-            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
-        )
-        self._db_map.commit_session("Add test data.")
-        self._db_map.cascade_remove_items(**{"parameter_value": {1}})
-        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
-        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
-
-
-class TestDiffDatabaseMappingAdd(unittest.TestCase):
-    def setUp(self):
-        self._db_map = create_diff_db_map()
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_add_and_retrieve_many_objects(self):
-        """Tests add many objects into db and retrieving them."""
-        ids, _ = self._db_map.add_object_classes({"name": "testclass"})
-        class_id = next(iter(ids))
-        added = self._db_map.add_objects(*[{"name": str(i), "class_id": class_id} for i in range(1001)])[0]
-        self.assertEqual(len(added), 1001)
-        self._db_map.commit_session("test_commit")
-        self.assertEqual(self._db_map.query(self._db_map.entity_sq).count(), 1001)
-
-    def test_add_object_classes(self):
-        """Test that adding object classes works."""
-        self._db_map.add_object_classes({"name": "fish"}, {"name": "dog"})
-        diff_table = self._db_map._diff_table("entity_class")
-        object_classes = (
-            self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.object_class_type).all()
-        )
-        self.assertEqual(len(object_classes), 2)
-        self.assertEqual(object_classes[0].name, "fish")
-        self.assertEqual(object_classes[1].name, "dog")
-
-    def test_add_object_class_with_invalid_name(self):
-        """Test that adding object classes with empty name raises error"""
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_object_classes({"name": ""}, strict=True)
-
-    def test_add_object_classes_with_same_name(self):
-        """Test that adding two object classes with the same name only adds one of them."""
-        self._db_map.add_object_classes({"name": "fish"}, {"name": "fish"})
-        diff_table = self._db_map._diff_table("entity_class")
-        object_classes = (
-            self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.object_class_type).all()
-        )
-        self.assertEqual(len(object_classes), 1)
-        self.assertEqual(object_classes[0].name, "fish")
-
-    def test_add_object_class_with_same_name_as_existing_one(self):
-        """Test that adding an object class with an already taken name raises an integrity error."""
-        self._db_map.add_object_classes({"name": "fish"}, {"name": "fish"})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_object_classes({"name": "fish"}, strict=True)
-
-    def test_add_objects(self):
-        """Test that adding objects works."""
-        self._db_map.add_object_classes({"name": "fish"})
-        self._db_map.add_objects({"name": "nemo", "class_id": 1}, {"name": "dory", "class_id": 1})
-        diff_table = self._db_map._diff_table("entity")
-        objects = self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.object_entity_type).all()
-        self.assertEqual(len(objects), 2)
-        self.assertEqual(objects[0].name, "nemo")
-        self.assertEqual(objects[0].class_id, 1)
-        self.assertEqual(objects[1].name, "dory")
-        self.assertEqual(objects[1].class_id, 1)
-
-    def test_add_object_with_invalid_name(self):
-        """Test that adding object classes with empty name raises error"""
-        self._db_map.add_object_classes({"name": "fish"})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_objects({"name": "", "class_id": 1}, strict=True)
-
-    def test_add_objects_with_same_name(self):
-        """Test that adding two objects with the same name only adds one of them."""
-        self._db_map.add_object_classes({"name": "fish"})
-        self._db_map.add_objects({"name": "nemo", "class_id": 1}, {"name": "nemo", "class_id": 1})
-        diff_table = self._db_map._diff_table("entity")
-        objects = self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.object_entity_type).all()
-        self.assertEqual(len(objects), 1)
-        self.assertEqual(objects[0].name, "nemo")
-        self.assertEqual(objects[0].class_id, 1)
-
-    def test_add_object_with_same_name_as_existing_one(self):
-        """Test that adding an object with an already taken name raises an integrity error."""
-        self._db_map.add_object_classes({"name": "fish"})
-        self._db_map.add_objects({"name": "nemo", "class_id": 1})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_objects({"name": "nemo", "class_id": 1}, strict=True)
-
-    def test_add_object_with_invalid_class(self):
-        """Test that adding an object with a non existing class raises an integrity error."""
-        self._db_map.add_object_classes({"name": "fish"})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_objects({"name": "pluto", "class_id": 2}, strict=True)
-
-    def test_add_relationship_classes(self):
-        """Test that adding relationship classes works."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.add_wide_relationship_classes(
-            {"name": "rc1", "object_class_id_list": [1, 2]}, {"name": "rc2", "object_class_id_list": [2, 1]}
-        )
-        diff_table = self._db_map._diff_table("relationship_entity_class")
-        rel_ent_clss = self._db_map.query(diff_table).all()
-        diff_table = self._db_map._diff_table("entity_class")
-        rel_clss = (
-            self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.relationship_class_type).all()
-        )
-        self.assertEqual(len(rel_ent_clss), 4)
-        self.assertEqual(rel_clss[0].name, "rc1")
-        self.assertEqual(rel_ent_clss[0].member_class_id, 1)
-        self.assertEqual(rel_ent_clss[1].member_class_id, 2)
-        self.assertEqual(rel_clss[1].name, "rc2")
-        self.assertEqual(rel_ent_clss[2].member_class_id, 2)
-        self.assertEqual(rel_ent_clss[3].member_class_id, 1)
-
-    def test_add_relationship_classes_with_invalid_name(self):
-        """Test that adding object classes with empty name raises error"""
-        self._db_map.add_object_classes({"name": "fish"})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_wide_relationship_classes({"name": "", "object_class_id_list": [1]}, strict=True)
-
-    def test_add_relationship_classes_with_same_name(self):
-        """Test that adding two relationship classes with the same name only adds one of them."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.add_wide_relationship_classes(
-            {"name": "rc1", "object_class_id_list": [1, 2]}, {"name": "rc1", "object_class_id_list": [1, 2]}
-        )
-        diff_table = self._db_map._diff_table("relationship_entity_class")
-        relationship_members = self._db_map.query(diff_table).all()
-        diff_table = self._db_map._diff_table("entity_class")
-        relationships = (
-            self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.relationship_class_type).all()
-        )
-        self.assertEqual(len(relationship_members), 2)
-        self.assertEqual(len(relationships), 1)
-        self.assertEqual(relationships[0].name, "rc1")
-        self.assertEqual(relationship_members[0].member_class_id, 1)
-        self.assertEqual(relationship_members[1].member_class_id, 2)
-
-    def test_add_relationship_class_with_same_name_as_existing_one(self):
-        """Test that adding a relationship class with an already taken name raises an integrity error."""
-        query_wrapper = create_query_wrapper(self._db_map)
-        with mock.patch.object(DiffDatabaseMapping, "query") as mock_query, mock.patch.object(
-            DiffDatabaseMapping, "object_class_sq"
-        ) as mock_object_class_sq, mock.patch.object(
-            DiffDatabaseMapping, "wide_relationship_class_sq"
-        ) as mock_wide_rel_cls_sq:
-            mock_query.side_effect = query_wrapper
-            mock_object_class_sq.return_value = [
-                KeyedTuple([1, "fish"], labels=["id", "name"]),
-                KeyedTuple([2, "dog"], labels=["id", "name"]),
-            ]
-            mock_wide_rel_cls_sq.return_value = [
-                KeyedTuple([1, "1,2", "fish__dog"], labels=["id", "object_class_id_list", "name"])
-            ]
-            with self.assertRaises(SpineIntegrityError):
-                self._db_map.add_wide_relationship_classes(
-                    {"name": "fish__dog", "object_class_id_list": [1, 2]}, strict=True
-                )
-
-    def test_add_relationship_class_with_invalid_object_class(self):
-        """Test that adding a relationship class with a non existing object class raises an integrity error."""
-        query_wrapper = create_query_wrapper(self._db_map)
-        with mock.patch.object(DiffDatabaseMapping, "query") as mock_query, mock.patch.object(
-            DiffDatabaseMapping, "object_class_sq"
-        ) as mock_object_class_sq, mock.patch.object(DiffDatabaseMapping, "wide_relationship_class_sq"):
-            mock_query.side_effect = query_wrapper
-            mock_object_class_sq.return_value = [KeyedTuple([1, "fish"], labels=["id", "name"])]
-            with self.assertRaises(SpineIntegrityError):
-                self._db_map.add_wide_relationship_classes(
-                    {"name": "fish__dog", "object_class_id_list": [1, 2]}, strict=True
-                )
-
-    def test_add_relationships(self):
-        """Test that adding relationships works."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2})
-        self._db_map.add_wide_relationships({"name": "nemo__pluto", "class_id": 3, "object_id_list": [1, 2]})
-        diff_table = self._db_map._diff_table("relationship_entity")
-        rel_ents = self._db_map.query(diff_table).all()
-        diff_table = self._db_map._diff_table("entity")
-        relationships = (
-            self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.relationship_entity_type).all()
-        )
-        self.assertEqual(len(rel_ents), 2)
-        self.assertEqual(len(relationships), 1)
-        self.assertEqual(relationships[0].name, "nemo__pluto")
-        self.assertEqual(rel_ents[0].entity_class_id, 3)
-        self.assertEqual(rel_ents[0].member_id, 1)
-        self.assertEqual(rel_ents[1].entity_class_id, 3)
-        self.assertEqual(rel_ents[1].member_id, 2)
-
-    def test_add_relationship_with_invalid_name(self):
-        """Test that adding object classes with empty name raises error"""
-        self._db_map.add_object_classes({"name": "oc1"}, strict=True)
-        self._db_map.add_wide_relationship_classes({"name": "rc1", "object_class_id_list": [1]}, strict=True)
-        self._db_map.add_objects({"name": "o1", "class_id": 1}, strict=True)
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_wide_relationships({"name": "", "class_id": 1, "object_id_list": [1]}, strict=True)
-
-    def test_add_identical_relationships(self):
-        """Test that adding two relationships with the same class and same objects only adds the first one."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2})
-        self._db_map.add_wide_relationships(
-            {"name": "nemo__pluto", "class_id": 3, "object_id_list": [1, 2]},
-            {"name": "nemo__pluto_duplicate", "class_id": 3, "object_id_list": [1, 2]},
-        )
-        diff_table = self._db_map._diff_table("relationship")
-        relationships = self._db_map.query(diff_table).all()
-        self.assertEqual(len(relationships), 1)
-
-    def test_add_relationship_identical_to_existing_one(self):
-        """Test that adding a relationship with the same class and same objects as an existing one
-        raises an integrity error.
-        """
-        query_wrapper = create_query_wrapper(self._db_map)
-        with mock.patch.object(DiffDatabaseMapping, "query") as mock_query, mock.patch.object(
-            DiffDatabaseMapping, "object_sq"
-        ) as mock_object_sq, mock.patch.object(
-            DiffDatabaseMapping, "wide_relationship_class_sq"
-        ) as mock_wide_rel_cls_sq, mock.patch.object(
-            DiffDatabaseMapping, "wide_relationship_sq"
-        ) as mock_wide_rel_sq:
-            mock_query.side_effect = query_wrapper
-            mock_object_sq.return_value = [
-                KeyedTuple([1, 10, "nemo"], labels=["id", "class_id", "name"]),
-                KeyedTuple([2, 20, "pluto"], labels=["id", "class_id", "name"]),
-            ]
-            mock_wide_rel_cls_sq.return_value = [
-                KeyedTuple([1, "10,20", "fish__dog"], labels=["id", "object_class_id_list", "name"])
-            ]
-            mock_wide_rel_sq.return_value = [
-                KeyedTuple([1, 1, "1,2", "nemo__pluto"], labels=["id", "class_id", "object_id_list", "name"])
-            ]
-            with self.assertRaises(SpineIntegrityError):
-                self._db_map.add_wide_relationships(
-                    {"name": "nemoy__plutoy", "class_id": 1, "object_id_list": [1, 2]}, strict=True
-                )
-
-    def test_add_relationship_with_invalid_class(self):
-        """Test that adding a relationship with an invalid class raises an integrity error."""
-        query_wrapper = create_query_wrapper(self._db_map)
-        with mock.patch.object(DiffDatabaseMapping, "query") as mock_query, mock.patch.object(
-            DiffDatabaseMapping, "object_sq"
-        ) as mock_object_sq, mock.patch.object(
-            DiffDatabaseMapping, "wide_relationship_class_sq"
-        ) as mock_wide_rel_cls_sq, mock.patch.object(
-            DiffDatabaseMapping, "wide_relationship_sq"
-        ):
-            mock_query.side_effect = query_wrapper
-            mock_object_sq.return_value = [
-                KeyedTuple([1, 10, "nemo"], labels=["id", "class_id", "name"]),
-                KeyedTuple([2, 20, "pluto"], labels=["id", "class_id", "name"]),
-            ]
-            mock_wide_rel_cls_sq.return_value = [
-                KeyedTuple([1, "10,20", "fish__dog"], labels=["id", "object_class_id_list", "name"])
-            ]
-            with self.assertRaises(SpineIntegrityError):
-                self._db_map.add_wide_relationships(
-                    {"name": "nemo__pluto", "class_id": 2, "object_id_list": [1, 2]}, strict=True
-                )
-
-    def test_add_relationship_with_invalid_object(self):
-        """Test that adding a relationship with an invalid object raises an integrity error."""
-        query_wrapper = create_query_wrapper(self._db_map)
-        with mock.patch.object(DiffDatabaseMapping, "query") as mock_query, mock.patch.object(
-            DiffDatabaseMapping, "object_sq"
-        ) as mock_object_sq, mock.patch.object(
-            DiffDatabaseMapping, "wide_relationship_class_sq"
-        ) as mock_wide_rel_cls_sq, mock.patch.object(
-            DiffDatabaseMapping, "wide_relationship_sq"
-        ):
-            mock_query.side_effect = query_wrapper
-            mock_object_sq.return_value = [
-                KeyedTuple([1, 10, "nemo"], labels=["id", "class_id", "name"]),
-                KeyedTuple([2, 20, "pluto"], labels=["id", "class_id", "name"]),
-            ]
-            mock_wide_rel_cls_sq.return_value = [
-                KeyedTuple([1, "10,20", "fish__dog"], labels=["id", "object_class_id_list", "name"])
-            ]
-            with self.assertRaises(SpineIntegrityError):
-                self._db_map.add_wide_relationships(
-                    {"name": "nemo__pluto", "class_id": 1, "object_id_list": [1, 3]}, strict=True
-                )
-
-    def test_add_entity_groups(self):
-        """Test that adding group entities works."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
-        self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 1, "member_id": 2})
-        diff_table = self._db_map._diff_table("entity_group")
-        entity_groups = self._db_map.query(diff_table).all()
-        self.assertEqual(len(entity_groups), 1)
-        self.assertEqual(entity_groups[0].entity_id, 1)
-        self.assertEqual(entity_groups[0].entity_class_id, 1)
-        self.assertEqual(entity_groups[0].member_id, 2)
-
-    def test_add_entity_groups_with_invalid_class(self):
-        """Test that adding group entities with an invalid class fails."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 2, "member_id": 2}, strict=True)
-
-    def test_add_entity_groups_with_invalid_entity(self):
-        """Test that adding group entities with an invalid entity fails."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_entity_groups({"entity_id": 3, "entity_class_id": 2, "member_id": 2}, strict=True)
-
-    def test_add_entity_groups_with_invalid_member(self):
-        """Test that adding group entities with an invalid member fails."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 2, "member_id": 3}, strict=True)
-
-    def test_add_repeated_entity_groups(self):
-        """Test that adding repeated group entities fails."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1})
-        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
-        self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 2, "member_id": 2})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 2, "member_id": 2}, strict=True)
-
-    def test_add_parameter_definitions(self):
-        """Test that adding parameter definitions works."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
-        self._db_map.add_parameter_definitions(
-            {"name": "color", "object_class_id": 1, "description": "test1"},
-            {"name": "relative_speed", "relationship_class_id": 3, "description": "test2"},
-        )
-        diff_table = self._db_map._diff_table("parameter_definition")
-        parameter_definitions = self._db_map.query(diff_table).all()
-        self.assertEqual(len(parameter_definitions), 2)
-        self.assertEqual(parameter_definitions[0].name, "color")
-        self.assertEqual(parameter_definitions[0].entity_class_id, 1)
-        self.assertEqual(parameter_definitions[0].description, "test1")
-        self.assertEqual(parameter_definitions[1].name, "relative_speed")
-        self.assertEqual(parameter_definitions[1].entity_class_id, 3)
-        self.assertEqual(parameter_definitions[1].description, "test2")
-
-    def test_add_parameter_with_invalid_name(self):
-        """Test that adding object classes with empty name raises error"""
-        self._db_map.add_object_classes({"name": "oc1"}, strict=True)
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_parameter_definitions({"name": "", "object_class_id": 1}, strict=True)
-
-    def test_add_parameter_definitions_with_same_name(self):
-        """Test that adding two parameter_definitions with the same name adds both of them."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
-        self._db_map.add_parameter_definitions(
-            {"name": "color", "object_class_id": 1}, {"name": "color", "relationship_class_id": 3}
-        )
-        diff_table = self._db_map._diff_table("parameter_definition")
-        parameter_definitions = self._db_map.query(diff_table).all()
-        self.assertEqual(len(parameter_definitions), 2)
-        self.assertEqual(parameter_definitions[0].name, "color")
-        self.assertEqual(parameter_definitions[1].name, "color")
-        self.assertEqual(parameter_definitions[0].entity_class_id, 1)
-
-    def test_add_parameter_with_same_name_as_existing_one(self):
-        """Test that adding parameter_definitions with an already taken name raises and integrity error."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
-        self._db_map.add_parameter_definitions(
-            {"name": "color", "object_class_id": 1}, {"name": "color", "relationship_class_id": 3}
-        )
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_parameter_definitions({"name": "color", "object_class_id": 1}, strict=True)
-
-    def test_add_parameter_with_invalid_class(self):
-        """Test that adding parameter_definitions with an invalid (object or relationship) class raises and integrity error."""
-        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
-        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_parameter_definitions({"name": "color", "object_class_id": 3}, strict=True)
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_parameter_definitions({"name": "color", "relationship_class_id": 1}, strict=True)
-
-    def test_add_parameter_for_both_object_and_relationship_class(self):
-        """Test that adding parameter_definitions associated to both and object and relationship class
-        raises and integrity error."""
-        self._db_map.add_object_classes({"name": "fish", "id": 1}, {"name": "dog", "id": 2})
-        self._db_map.add_wide_relationship_classes({"name": "fish__dog", "id": 10, "object_class_id_list": [1, 2]})
-        with self.assertRaises(SpineIntegrityError):
-            self._db_map.add_parameter_definitions(
-                {"name": "color", "object_class_id": 1, "relationship_class_id": 10}, strict=True
-            )
-
-    def test_add_parameter_values(self):
-        """Test that adding parameter values works."""
-        import_functions.import_object_classes(self._db_map, ["fish", "dog"])
-        import_functions.import_relationship_classes(self._db_map, [("fish_dog", ["fish", "dog"])])
-        import_functions.import_objects(self._db_map, [("fish", "nemo"), ("dog", "pluto")])
-        import_functions.import_relationships(self._db_map, [("fish_dog", ("nemo", "pluto"))])
-        import_functions.import_object_parameters(self._db_map, [("fish", "color")])
-        import_functions.import_relationship_parameters(self._db_map, [("fish_dog", "rel_speed")])
-        color_id = (
-            self._db_map.parameter_definition_list()
-            .filter(self._db_map.parameter_definition_sq.c.name == "color")
-            .first()
-            .id
-        )
-        rel_speed_id = (
-            self._db_map.parameter_definition_list()
-            .filter(self._db_map.parameter_definition_sq.c.name == "rel_speed")
-            .first()
-            .id
-        )
-        nemo_row = self._db_map.object_list().filter(self._db_map.entity_sq.c.name == "nemo").first()
-        nemo__pluto_row = self._db_map.wide_relationship_list().filter().first()
-        self._db_map.add_parameter_values(
-            {
-                "parameter_definition_id": color_id,
-                "entity_id": nemo_row.id,
-                "entity_class_id": nemo_row.class_id,
-                "value": b'"orange"',
-                "alternative_id": 1,
-            },
-            {
-                "parameter_definition_id": rel_speed_id,
-                "entity_id": nemo__pluto_row.id,
-                "entity_class_id": nemo__pluto_row.class_id,
-                "value": b"125",
-                "alternative_id": 1,
-            },
-        )
-        diff_table = self._db_map._diff_table("parameter_value")
-        parameter_values = self._db_map.query(diff_table).all()
-        self.assertEqual(len(parameter_values), 2)
-        self.assertEqual(parameter_values[0].parameter_definition_id, 1)
-        self.assertEqual(parameter_values[0].entity_id, 1)
-        self.assertEqual(parameter_values[0].value, b'"orange"')
-        self.assertEqual(parameter_values[1].parameter_definition_id, 2)
-        self.assertEqual(parameter_values[1].entity_id, 3)
-        self.assertEqual(parameter_values[1].value, b"125")
-
-    def test_add_parameter_value_with_invalid_object_or_relationship(self):
-        """Test that adding a parameter value with an invalid object or relationship raises an
-        integrity error."""
-        import_functions.import_object_classes(self._db_map, ["fish", "dog"])
-        import_functions.import_relationship_classes(self._db_map, [("fish_dog", ["fish", "dog"])])
-        import_functions.import_objects(self._db_map, [("fish", "nemo"), ("dog", "pluto")])
-        import_functions.import_relationships(self._db_map, [("fish_dog", ("nemo", "pluto"))])
-        import_functions.import_object_parameters(self._db_map, [("fish", "color")])
-        import_functions.import_relationship_parameters(self._db_map, [("fish_dog", "rel_speed")])
-        _, errors = self._db_map.add_parameter_values(
-            {"parameter_definition_id": 1, "object_id": 3, "value": b'"orange"', "alternative_id": 1}, strict=False
-        )
-        self.assertEqual([str(e) for e in errors], ["Incorrect entity 'fish_dog_nemo__pluto' for parameter 'color'."])
-        _, errors = self._db_map.add_parameter_values(
-            {"parameter_definition_id": 2, "relationship_id": 2, "value": b"125", "alternative_id": 1}, strict=False
-        )
-        self.assertEqual([str(e) for e in errors], ["Incorrect entity 'pluto' for parameter 'rel_speed'."])
-
-    def test_add_same_parameter_value_twice(self):
-        """Test that adding a parameter value twice only adds the first one."""
-        import_functions.import_object_classes(self._db_map, ["fish"])
-        import_functions.import_objects(self._db_map, [("fish", "nemo")])
-        import_functions.import_object_parameters(self._db_map, [("fish", "color")])
-        color_id = (
-            self._db_map.parameter_definition_list()
-            .filter(self._db_map.parameter_definition_sq.c.name == "color")
-            .first()
-            .id
-        )
-        nemo_row = self._db_map.object_list().filter(self._db_map.entity_sq.c.name == "nemo").first()
-        self._db_map.add_parameter_values(
-            {
-                "parameter_definition_id": color_id,
-                "entity_id": nemo_row.id,
-                "entity_class_id": nemo_row.class_id,
-                "value": b'"orange"',
-                "alternative_id": 1,
-            },
-            {
-                "parameter_definition_id": color_id,
-                "entity_id": nemo_row.id,
-                "entity_class_id": nemo_row.class_id,
-                "value": b'"blue"',
-                "alternative_id": 1,
-            },
-        )
-        diff_table = self._db_map._diff_table("parameter_value")
-        parameter_values = self._db_map.query(diff_table).all()
-        self.assertEqual(len(parameter_values), 1)
-        self.assertEqual(parameter_values[0].parameter_definition_id, 1)
-        self.assertEqual(parameter_values[0].entity_id, 1)
-        self.assertEqual(parameter_values[0].value, b'"orange"')
-
-    def test_add_existing_parameter_value(self):
-        """Test that adding an existing parameter value raises an integrity error."""
-        import_functions.import_object_classes(self._db_map, ["fish"])
-        import_functions.import_objects(self._db_map, [("fish", "nemo")])
-        import_functions.import_object_parameters(self._db_map, [("fish", "color")])
-        import_functions.import_object_parameter_values(self._db_map, [("fish", "nemo", "color", "orange")])
-        _, errors = self._db_map.add_parameter_values(
-            {
-                "parameter_definition_id": 1,
-                "entity_class_id": 1,
-                "entity_id": 1,
-                "value": b'"blue"',
-                "alternative_id": 1,
-            },
-            strict=False,
-        )
-        self.assertEqual(
-            [str(e) for e in errors], ["The value of parameter 'color' for entity 'nemo' is already specified."]
-        )
-
-    def test_add_alternative(self):
-        ids, errors = self._db_map.add_alternatives({"name": "my_alternative"})
-        self.assertEqual(errors, [])
-        self.assertEqual(ids, {2})
-        alternatives = self._db_map.query(self._db_map.alternative_sq).all()
-        self.assertEqual(len(alternatives), 2)
-        self.assertEqual(
-            alternatives[0]._asdict(), {"id": 1, "name": "Base", "description": "Base alternative", "commit_id": 1}
-        )
-        self.assertEqual(
-            alternatives[1]._asdict(), {"id": 2, "name": "my_alternative", "description": None, "commit_id": None}
-        )
-
-    def test_add_scenario(self):
-        ids, errors = self._db_map.add_scenarios({"name": "my_scenario"})
-        self.assertEqual(errors, [])
-        self.assertEqual(ids, {1})
-        scenarios = self._db_map.query(self._db_map.scenario_sq).all()
-        self.assertEqual(len(scenarios), 1)
-        self.assertEqual(
-            scenarios[0]._asdict(),
-            {"id": 1, "name": "my_scenario", "description": None, "active": False, "commit_id": None},
-        )
-
-    def test_add_scenario_alternative(self):
-        import_functions.import_scenarios(self._db_map, ("my_scenario",))
-        self._db_map.commit_session("Add test data.")
-        ids, errors = self._db_map.add_scenario_alternatives({"scenario_id": 1, "alternative_id": 1, "rank": 0})
-        self.assertEqual(errors, [])
-        self.assertEqual(ids, {1})
-        scenario_alternatives = self._db_map.query(self._db_map.scenario_alternative_sq).all()
-        self.assertEqual(len(scenario_alternatives), 1)
-        self.assertEqual(
-            scenario_alternatives[0]._asdict(),
-            {"id": 1, "scenario_id": 1, "alternative_id": 1, "rank": 0, "commit_id": None},
-        )
-
-    def test_add_metadata(self):
-        items, errors = self._db_map.add_metadata({"name": "test name", "value": "test_add_metadata"}, strict=False)
-        self.assertEqual(errors, [])
-        self.assertEqual(items, {1})
-        self._db_map.commit_session("Add metadata")
-        metadata = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata), 1)
-        self.assertEqual(
-            metadata[0]._asdict(), {"name": "test name", "id": 1, "value": "test_add_metadata", "commit_id": 2}
-        )
-
-    def test_add_metadata_that_exists_does_not_add_it(self):
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        self._db_map.commit_session("Add test data.")
-        items, errors = self._db_map.add_metadata({"name": "title", "value": "My metadata."}, strict=False)
-        self.assertEqual(errors, [])
-        self.assertEqual(items, set())
-        metadata = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata), 1)
-        self.assertEqual(metadata[0]._asdict(), {"name": "title", "id": 1, "value": "My metadata.", "commit_id": 2})
-
-    def test_add_entity_metadata_for_object(self):
-        import_functions.import_object_classes(self._db_map, ("fish",))
-        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        self._db_map.commit_session("Add test data.")
-        items, errors = self._db_map.add_entity_metadata({"entity_id": 1, "metadata_id": 1}, strict=False)
-        self.assertEqual(errors, [])
-        self.assertEqual(items, {1})
-        self._db_map.commit_session("Add entity metadata")
-        entity_metadata = self._db_map.query(self._db_map.ext_entity_metadata_sq).all()
-        self.assertEqual(len(entity_metadata), 1)
-        self.assertEqual(
-            entity_metadata[0]._asdict(),
-            {
-                "entity_id": 1,
-                "entity_name": "leviathan",
-                "metadata_name": "title",
-                "metadata_value": "My metadata.",
-                "metadata_id": 1,
-                "id": 1,
-                "commit_id": 3,
-            },
-        )
-
-    def test_add_entity_metadata_for_relationship(self):
-        import_functions.import_object_classes(self._db_map, ("my_object_class",))
-        import_functions.import_objects(self._db_map, (("my_object_class", "my_object"),))
-        import_functions.import_relationship_classes(self._db_map, (("my_relationship_class", ("my_object_class",)),))
-        import_functions.import_relationships(self._db_map, (("my_relationship_class", ("my_object",)),))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        self._db_map.commit_session("Add test data.")
-        items, errors = self._db_map.add_entity_metadata({"entity_id": 2, "metadata_id": 1}, strict=False)
-        self.assertEqual(errors, [])
-        self.assertEqual(items, {1})
-        self._db_map.commit_session("Add entity metadata")
-        entity_metadata = self._db_map.query(self._db_map.ext_entity_metadata_sq).all()
-        self.assertEqual(len(entity_metadata), 1)
-        self.assertEqual(
-            entity_metadata[0]._asdict(),
-            {
-                "entity_id": 2,
-                "entity_name": "my_relationship_class_my_object",
-                "metadata_name": "title",
-                "metadata_value": "My metadata.",
-                "metadata_id": 1,
-                "id": 1,
-                "commit_id": 3,
-            },
-        )
-
-    def test_add_entity_metadata_doesnt_raise_with_empty_cache(self):
-        items, errors = self._db_map.add_entity_metadata(
-            {"entity_id": 1, "metadata_id": 1}, cache=DBCache(lambda *args, **kwargs: None), strict=False
-        )
-        self.assertEqual(items, set())
-        self.assertEqual(len(errors), 1)
-
-    def test_add_ext_entity_metadata_for_object(self):
-        import_functions.import_object_classes(self._db_map, ("fish",))
-        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
-        self._db_map.commit_session("Add test data.")
-        items, errors = self._db_map.add_ext_entity_metadata(
-            {"entity_id": 1, "metadata_name": "key", "metadata_value": "object metadata"}, strict=False
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(items, {1})
-        self._db_map.commit_session("Add entity metadata")
-        entity_metadata = self._db_map.query(self._db_map.ext_entity_metadata_sq).all()
-        self.assertEqual(len(entity_metadata), 1)
-        self.assertEqual(
-            entity_metadata[0]._asdict(),
-            {
-                "entity_id": 1,
-                "entity_name": "leviathan",
-                "metadata_name": "key",
-                "metadata_value": "object metadata",
-                "metadata_id": 1,
-                "id": 1,
-                "commit_id": 3,
-            },
-        )
-
-    def test_adding_ext_entity_metadata_for_object_reuses_existing_metadata_names_and_values(self):
-        import_functions.import_object_classes(self._db_map, ("fish",))
-        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        self._db_map.commit_session("Add test data.")
-        items, errors = self._db_map.add_ext_entity_metadata(
-            {"entity_id": 1, "metadata_name": "title", "metadata_value": "My metadata."}, strict=False
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(items, {1})
-        self._db_map.commit_session("Add entity metadata")
-        metadata = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata), 1)
-        self.assertEqual(metadata[0]._asdict(), {"id": 1, "name": "title", "value": "My metadata.", "commit_id": 2})
-        entity_metadata = self._db_map.query(self._db_map.ext_entity_metadata_sq).all()
-        self.assertEqual(len(entity_metadata), 1)
-        self.assertEqual(
-            entity_metadata[0]._asdict(),
-            {
-                "entity_id": 1,
-                "entity_name": "leviathan",
-                "metadata_name": "title",
-                "metadata_value": "My metadata.",
-                "metadata_id": 1,
-                "id": 1,
-                "commit_id": 3,
-            },
-        )
-
-    def test_add_parameter_value_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("fish",))
-        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
-        import_functions.import_object_parameters(self._db_map, (("fish", "paranormality"),))
-        import_functions.import_object_parameter_values(self._db_map, (("fish", "leviathan", "paranormality", 3.9),))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        self._db_map.commit_session("Add test data.")
-        items, errors = self._db_map.add_parameter_value_metadata(
-            {"parameter_value_id": 1, "metadata_id": 1, "alternative_id": 1}, strict=False
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(items, {1})
-        self._db_map.commit_session("Add value metadata")
-        value_metadata = self._db_map.query(self._db_map.ext_parameter_value_metadata_sq).all()
-        self.assertEqual(len(value_metadata), 1)
-        self.assertEqual(
-            value_metadata[0]._asdict(),
-            {
-                "alternative_name": "Base",
-                "entity_name": "leviathan",
-                "parameter_value_id": 1,
-                "parameter_name": "paranormality",
-                "metadata_name": "title",
-                "metadata_value": "My metadata.",
-                "metadata_id": 1,
-                "id": 1,
-                "commit_id": 3,
-            },
-        )
-
-    def test_add_parameter_value_metadata_doesnt_raise_with_empty_cache(self):
-        items, errors = self._db_map.add_parameter_value_metadata(
-            {"parameter_value_id": 1, "metadata_id": 1, "alternative_id": 1},
-            cache=DBCache(lambda *args, **kwargs: None),
-            strict=False,
-        )
-        self.assertEqual(items, set())
-        self.assertEqual(len(errors), 1)
-
-    def test_add_ext_parameter_value_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("fish",))
-        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
-        import_functions.import_object_parameters(self._db_map, (("fish", "paranormality"),))
-        import_functions.import_object_parameter_values(self._db_map, (("fish", "leviathan", "paranormality", 3.9),))
-        self._db_map.commit_session("Add test data.")
-        items, errors = self._db_map.add_ext_parameter_value_metadata(
-            {
-                "parameter_value_id": 1,
-                "metadata_name": "key",
-                "metadata_value": "parameter metadata",
-                "alternative_id": 1,
-            },
-            strict=False,
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(items, {1})
-        self._db_map.commit_session("Add value metadata")
-        value_metadata = self._db_map.query(self._db_map.ext_parameter_value_metadata_sq).all()
-        self.assertEqual(len(value_metadata), 1)
-        self.assertEqual(
-            value_metadata[0]._asdict(),
-            {
-                "alternative_name": "Base",
-                "entity_name": "leviathan",
-                "parameter_value_id": 1,
-                "parameter_name": "paranormality",
-                "metadata_name": "key",
-                "metadata_value": "parameter metadata",
-                "metadata_id": 1,
-                "id": 1,
-                "commit_id": 3,
-            },
-        )
-
-    def test_add_ext_parameter_value_metadata_reuses_existing_metadata(self):
-        import_functions.import_object_classes(self._db_map, ("fish",))
-        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
-        import_functions.import_object_parameters(self._db_map, (("fish", "paranormality"),))
-        import_functions.import_object_parameter_values(self._db_map, (("fish", "leviathan", "paranormality", 3.9),))
-        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
-        self._db_map.commit_session("Add test data.")
-        items, errors = self._db_map.add_ext_parameter_value_metadata(
-            {"parameter_value_id": 1, "metadata_name": "title", "metadata_value": "My metadata.", "alternative_id": 1},
-            strict=False,
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(items, {1})
-        self._db_map.commit_session("Add value metadata")
-        metadata = self._db_map.query(self._db_map.metadata_sq).all()
-        self.assertEqual(len(metadata), 1)
-        self.assertEqual(metadata[0]._asdict(), {"id": 1, "name": "title", "value": "My metadata.", "commit_id": 2})
-        value_metadata = self._db_map.query(self._db_map.ext_parameter_value_metadata_sq).all()
-        self.assertEqual(len(value_metadata), 1)
-        self.assertEqual(
-            value_metadata[0]._asdict(),
-            {
-                "alternative_name": "Base",
-                "entity_name": "leviathan",
-                "parameter_value_id": 1,
-                "parameter_name": "paranormality",
-                "metadata_name": "title",
-                "metadata_value": "My metadata.",
-                "metadata_id": 1,
-                "id": 1,
-                "commit_id": 3,
-            },
-        )
-
-
-class TestDiffDatabaseMappingUpdate(unittest.TestCase):
-    def setUp(self):
-        self._db_map = create_diff_db_map()
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_update_object_classes(self):
-        """Test that updating object classes works."""
-        self._db_map.add_object_classes({"id": 1, "name": "fish"}, {"id": 2, "name": "dog"})
-        ids, intgr_error_log = self._db_map.update_object_classes(
-            {"id": 1, "name": "octopus"}, {"id": 2, "name": "god"}
-        )
-        sq = self._db_map.object_class_sq
-        object_classes = {x.id: x.name for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))}
-        self.assertEqual(intgr_error_log, [])
-        self.assertEqual(object_classes[1], "octopus")
-        self.assertEqual(object_classes[2], "god")
-
-    def test_update_objects(self):
-        """Test that updating objects works."""
-        self._db_map.add_object_classes({"id": 1, "name": "fish"})
-        self._db_map.add_objects({"id": 1, "name": "nemo", "class_id": 1}, {"id": 2, "name": "dory", "class_id": 1})
-        ids, intgr_error_log = self._db_map.update_objects({"id": 1, "name": "klaus"}, {"id": 2, "name": "squidward"})
-        sq = self._db_map.object_sq
-        objects = {x.id: x.name for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))}
-        self.assertEqual(intgr_error_log, [])
-        self.assertEqual(objects[1], "klaus")
-        self.assertEqual(objects[2], "squidward")
-
-    def test_update_objects_not_committed(self):
-        """Test that updating objects works."""
-        self._db_map.add_object_classes({"id": 1, "name": "some_class"})
-        self._db_map.add_objects({"id": 1, "name": "nemo", "class_id": 1})
-        ids, intgr_error_log = self._db_map.update_objects({"id": 1, "name": "klaus"})
-        sq = self._db_map.object_sq
-        objects = {x.id: x.name for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))}
-        self.assertEqual(intgr_error_log, [])
-        self.assertEqual(objects[1], "klaus")
-        self.assertEqual(self._db_map.query(self._db_map.object_sq).filter_by(id=1).first().name, "klaus")
-        self._db_map.commit_session("update")
-        self.assertEqual(self._db_map.query(self._db_map.object_sq).filter_by(id=1).first().name, "klaus")
-
-    def test_update_committed_object(self):
-        """Test that updating objects works."""
-        self._db_map.add_object_classes({"id": 1, "name": "some_class"})
-        self._db_map.add_objects({"id": 1, "name": "nemo", "class_id": 1})
-        self._db_map.commit_session("update")
-        ids, intgr_error_log = self._db_map.update_objects({"id": 1, "name": "klaus"})
-        sq = self._db_map.object_sq
-        objects = {x.id: x.name for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))}
-        self.assertEqual(intgr_error_log, [])
-        self.assertEqual(objects[1], "klaus")
-        self.assertEqual(self._db_map.query(self._db_map.object_sq).filter_by(id=1).first().name, "klaus")
-        self._db_map.commit_session("update")
-        self.assertEqual(self._db_map.query(self._db_map.object_sq).filter_by(id=1).first().name, "klaus")
-
-    def test_update_relationship_classes(self):
-        """Test that updating relationship classes works."""
-        self._db_map.add_object_classes({"name": "dog", "id": 1}, {"name": "fish", "id": 2})
-        self._db_map.add_wide_relationship_classes(
-            {"id": 3, "name": "dog__fish", "object_class_id_list": [1, 2]},
-            {"id": 4, "name": "fish__dog", "object_class_id_list": [2, 1]},
-        )
-        ids, intgr_error_log = self._db_map.update_wide_relationship_classes(
-            {"id": 3, "name": "god__octopus"}, {"id": 4, "name": "octopus__dog"}
-        )
-        sq = self._db_map.wide_relationship_class_sq
-        rel_clss = {x.id: x.name for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))}
-        self.assertEqual(intgr_error_log, [])
-        self.assertEqual(rel_clss[3], "god__octopus")
-        self.assertEqual(rel_clss[4], "octopus__dog")
-
-    def test_update_relationships(self):
-        """Test that updating relationships works."""
-        self._db_map.add_object_classes({"name": "fish", "id": 1}, {"name": "dog", "id": 2})
-        self._db_map.add_wide_relationship_classes({"name": "fish__dog", "id": 3, "object_class_id_list": [1, 2]})
-        self._db_map.add_objects(
-            {"name": "nemo", "id": 1, "class_id": 1},
-            {"name": "pluto", "id": 2, "class_id": 2},
-            {"name": "scooby", "id": 3, "class_id": 2},
-        )
-        self._db_map.add_wide_relationships(
-            {"id": 4, "name": "nemo__pluto", "class_id": 3, "object_id_list": [1, 2], "object_class_id_list": [1, 2]}
-        )
-        ids, intgr_error_log = self._db_map.update_wide_relationships(
-            {"id": 4, "name": "nemo__scooby", "class_id": 3, "object_id_list": [1, 3], "object_class_id_list": [1, 2]}
-        )
-        sq = self._db_map.wide_relationship_sq
-        rels = {
-            x.id: {"name": x.name, "object_id_list": x.object_id_list}
-            for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))
-        }
-        self.assertEqual(intgr_error_log, [])
-        self.assertEqual(rels[4]["name"], "nemo__scooby")
-        self.assertEqual(rels[4]["object_id_list"], "1,3")
-
-
-class TestDiffDatabaseMappingCommit(unittest.TestCase):
-    def setUp(self):
-        self._db_map = create_diff_db_map()
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_commit_message(self):
-        """Tests that commit comment ends up in the database."""
-        self._db_map.add_object_classes({"name": "testclass"})
-        self._db_map.commit_session("test commit")
-        self.assertEqual(self._db_map.query(self._db_map.commit_sq).all()[-1].comment, "test commit")
-        self._db_map.connection.close()
-
-    def test_commit_session_raise_with_empty_comment(self):
-        import_functions.import_object_classes(self._db_map, ("my_class",))
-        self.assertRaisesRegex(SpineDBAPIError, "Commit message cannot be empty.", self._db_map.commit_session, "")
-
-    def test_commit_session_raise_when_nothing_to_commit(self):
-        self.assertRaisesRegex(SpineDBAPIError, "Nothing to commit.", self._db_map.commit_session, "No changes.")
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for DiffDatabaseMapping class.
+
+"""
+
+import os.path
+from tempfile import TemporaryDirectory
+import unittest
+from unittest import mock
+from sqlalchemy.engine.url import make_url, URL
+from sqlalchemy.util import KeyedTuple
+from spinedb_api.diff_db_mapping import DiffDatabaseMapping
+from spinedb_api.exception import SpineIntegrityError
+from spinedb_api.db_cache import DBCache
+from spinedb_api import import_functions, SpineDBAPIError
+
+
+def create_query_wrapper(db_map):
+    def query_wrapper(*args, orig_query=db_map.query, **kwargs):
+        arg = args[0]
+        if isinstance(arg, mock.Mock):
+            return arg.value
+        return orig_query(*args, **kwargs)
+
+    return query_wrapper
+
+
+IN_MEMORY_DB_URL = "sqlite://"
+
+
+def create_diff_db_map():
+    return DiffDatabaseMapping(IN_MEMORY_DB_URL, username="UnitTest", create=True)
+
+
+class TestDiffDatabaseMappingConstruction(unittest.TestCase):
+    def test_construction_with_filters(self):
+        db_url = IN_MEMORY_DB_URL + "?spinedbfilter=fltr1&spinedbfilter=fltr2"
+        with mock.patch("spinedb_api.diff_db_mapping.apply_filter_stack") as mock_apply:
+            with mock.patch(
+                "spinedb_api.diff_db_mapping.load_filters", return_value=[{"fltr1": "config1", "fltr2": "config2"}]
+            ) as mock_load:
+                db_map = DiffDatabaseMapping(db_url, create=True)
+                db_map.connection.close()
+                mock_load.assert_called_once_with(["fltr1", "fltr2"])
+                mock_apply.assert_called_once_with(db_map, [{"fltr1": "config1", "fltr2": "config2"}])
+
+    def test_construction_with_sqlalchemy_url_and_filters(self):
+        db_url = IN_MEMORY_DB_URL + "/?spinedbfilter=fltr1&spinedbfilter=fltr2"
+        sa_url = make_url(db_url)
+        with mock.patch("spinedb_api.diff_db_mapping.apply_filter_stack") as mock_apply:
+            with mock.patch(
+                "spinedb_api.diff_db_mapping.load_filters", return_value=[{"fltr1": "config1", "fltr2": "config2"}]
+            ) as mock_load:
+                db_map = DiffDatabaseMapping(sa_url, create=True)
+                db_map.connection.close()
+                mock_load.assert_called_once_with(["fltr1", "fltr2"])
+                mock_apply.assert_called_once_with(db_map, [{"fltr1": "config1", "fltr2": "config2"}])
+
+    def test_shorthand_filter_query_works(self):
+        with TemporaryDirectory() as temp_dir:
+            url = URL("sqlite")
+            url.database = os.path.join(temp_dir, "test_shorthand_filter_query_works.json")
+            out_db = DiffDatabaseMapping(url, create=True)
+            out_db.add_tools({"name": "object_activity_control", "id": 1})
+            out_db.commit_session("Add tool.")
+            out_db.connection.close()
+            try:
+                db_map = DiffDatabaseMapping(url)
+            except:
+                self.fail("DiffDatabaseMapping.__init__() should not raise.")
+            else:
+                db_map.connection.close()
+
+
+class TestDiffDatabaseMappingRemove(unittest.TestCase):
+    def setUp(self):
+        self._db_map = create_diff_db_map()
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_cascade_remove_relationship(self):
+        """Test adding and removing a relationship and committing"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2})
+        ids, _ = self._db_map.add_wide_relationships({"name": "remove_me", "class_id": 3, "object_id_list": [1, 2]})
+        self._db_map.cascade_remove_items(relationship=ids)
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
+        self._db_map.commit_session("delete")
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
+
+    def test_cascade_remove_relationship_from_committed_session(self):
+        """Test removing a relationship from a committed session"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2})
+        ids, _ = self._db_map.add_wide_relationships({"name": "remove_me", "class_id": 3, "object_id_list": [1, 2]})
+        self._db_map.commit_session("add")
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 1)
+        self._db_map.cascade_remove_items(relationship=ids)
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
+        self._db_map.commit_session("Add test data.")
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
+
+    def test_remove_object(self):
+        """Test adding and removing an object and committing"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        ids, _ = self._db_map.add_objects(
+            {"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2}
+        )
+        self._db_map.remove_items(object=ids)
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
+        self._db_map.commit_session("delete")
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_sq).all()), 0)
+
+    def test_remove_object_from_committed_session(self):
+        """Test removing an object from a committed session"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        ids, _ = self._db_map.add_objects(
+            {"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2}
+        )
+        self._db_map.commit_session("add")
+        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 2)
+        self._db_map.remove_items(object=ids)
+        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 0)
+        self._db_map.commit_session("Add test data.")
+        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 0)
+
+    def test_remove_entity_group(self):
+        """Test adding and removing an entity group and committing"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
+        ids, _ = self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 1, "member_id": 2})
+        self._db_map.remove_items(entity_group=ids)
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_group_sq).all()), 0)
+        self._db_map.commit_session("delete")
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_group_sq).all()), 0)
+
+    def test_remove_entity_group_from_committed_session(self):
+        """Test removing an entity group from a committed session"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
+        ids, _ = self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 1, "member_id": 2})
+        self._db_map.commit_session("add")
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_group_sq).all()), 1)
+        self._db_map.remove_items(entity_group=ids)
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_group_sq).all()), 0)
+        self._db_map.commit_session("delete")
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_group_sq).all()), 0)
+
+    def test_cascade_remove_relationship_class(self):
+        """Test adding and removing a relationship class and committing"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        ids, _ = self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
+        self._db_map.cascade_remove_items(relationship_class=ids)
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_class_sq).all()), 0)
+        self._db_map.commit_session("delete")
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_class_sq).all()), 0)
+
+    def test_cascade_remove_relationship_class_from_committed_session(self):
+        """Test removing a relationship class from a committed session"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        ids, _ = self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
+        self._db_map.commit_session("add")
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_class_sq).all()), 1)
+        self._db_map.cascade_remove_items(relationship_class=ids)
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_class_sq).all()), 0)
+        self._db_map.commit_session("Add test data.")
+        self.assertEqual(len(self._db_map.query(self._db_map.wide_relationship_class_sq).all()), 0)
+
+    def test_remove_object_class(self):
+        """Test adding and removing an object class and committing"""
+        ids, _ = self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.remove_items(object_class=ids)
+        self.assertEqual(len(self._db_map.query(self._db_map.object_class_sq).all()), 0)
+        self._db_map.commit_session("delete")
+        self.assertEqual(len(self._db_map.query(self._db_map.object_class_sq).all()), 0)
+
+    def test_remove_object_class_from_committed_session(self):
+        """Test removing an object class from a committed session"""
+        ids, _ = self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.commit_session("add")
+        self.assertEqual(len(self._db_map.query(self._db_map.object_class_sq).all()), 2)
+        self._db_map.remove_items(object_class=ids)
+        self.assertEqual(len(self._db_map.query(self._db_map.object_class_sq).all()), 0)
+        self._db_map.commit_session("Add test data.")
+        self.assertEqual(len(self._db_map.query(self._db_map.object_class_sq).all()), 0)
+
+    def test_remove_parameter_value(self):
+        """Test adding and removing a parameter value and committing"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, strict=True)
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, strict=True)
+        self._db_map.add_parameter_definitions({"name": "param", "id": 1, "object_class_id": 1}, strict=True)
+        self._db_map.add_parameter_values(
+            {
+                "value": b"0",
+                "id": 1,
+                "parameter_definition_id": 1,
+                "object_id": 1,
+                "object_class_id": 1,
+                "alternative_id": 1,
+            },
+            strict=True,
+        )
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 1)
+        self._db_map.remove_items(parameter_value=[1])
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
+        self._db_map.commit_session("delete")
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
+
+    def test_remove_parameter_value_from_committed_session(self):
+        """Test adding and committing a parameter value and then removing it"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, strict=True)
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, strict=True)
+        self._db_map.add_parameter_definitions({"name": "param", "id": 1, "object_class_id": 1}, strict=True)
+        self._db_map.add_parameter_values(
+            {
+                "value": b"0",
+                "id": 1,
+                "parameter_definition_id": 1,
+                "object_id": 1,
+                "object_class_id": 1,
+                "alternative_id": 1,
+            },
+            strict=True,
+        )
+        self._db_map.commit_session("add")
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 1)
+        self._db_map.remove_items(parameter_value=[1])
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
+        self._db_map.commit_session("delete")
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
+
+    def test_cascade_remove_object_removes_parameter_value_as_well(self):
+        """Test adding and removing a parameter value and committing"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, strict=True)
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, strict=True)
+        self._db_map.add_parameter_definitions({"name": "param", "id": 1, "object_class_id": 1}, strict=True)
+        self._db_map.add_parameter_values(
+            {
+                "value": b"0",
+                "id": 1,
+                "parameter_definition_id": 1,
+                "object_id": 1,
+                "object_class_id": 1,
+                "alternative_id": 1,
+            },
+            strict=True,
+        )
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 1)
+        self._db_map.cascade_remove_items(object={1})
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
+        self._db_map.commit_session("delete")
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
+
+    def test_cascade_remove_object_from_committed_session_removes_parameter_value_as_well(self):
+        """Test adding and committing a paramater value and then removing it"""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, strict=True)
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, strict=True)
+        self._db_map.add_parameter_definitions({"name": "param", "id": 1, "object_class_id": 1}, strict=True)
+        self._db_map.add_parameter_values(
+            {
+                "value": b"0",
+                "id": 1,
+                "parameter_definition_id": 1,
+                "object_id": 1,
+                "object_class_id": 1,
+                "alternative_id": 1,
+            },
+            strict=True,
+        )
+        self._db_map.commit_session("add")
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 1)
+        self._db_map.cascade_remove_items(object={1})
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
+        self._db_map.commit_session("delete")
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
+
+    def test_cascade_remove_metadata_removes_corresponding_entity_and_value_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
+        import_functions.import_object_parameter_values(
+            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
+        )
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
+        import_functions.import_object_parameter_value_metadata(
+            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
+        )
+        self._db_map.commit_session("Add test data.")
+        metadata = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata), 1)
+        self._db_map.cascade_remove_items(**{"metadata": {metadata[0].id}})
+        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 1)
+        self.assertEqual(len(self._db_map.query(self._db_map.object_parameter_definition_sq).all()), 1)
+
+    def test_cascade_remove_entity_metadata_removes_corresponding_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
+        self._db_map.commit_session("Add test data.")
+        entity_metadata = self._db_map.query(self._db_map.entity_metadata_sq).all()
+        self.assertEqual(len(entity_metadata), 1)
+        self._db_map.cascade_remove_items(**{"entity_metadata": {entity_metadata[0].id}})
+        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 1)
+
+    def test_cascade_remove_entity_metadata_leaves_metadata_used_by_value_intact(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
+        import_functions.import_object_parameter_values(
+            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
+        )
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
+        import_functions.import_object_parameter_value_metadata(
+            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
+        )
+        self._db_map.commit_session("Add test data.")
+        entity_metadata = self._db_map.query(self._db_map.entity_metadata_sq).all()
+        self.assertEqual(len(entity_metadata), 1)
+        self._db_map.cascade_remove_items(**{"entity_metadata": {entity_metadata[0].id}})
+        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 1)
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_metadata_sq).all()), 1)
+
+    def test_cascade_remove_value_metadata_leaves_metadata_used_by_entity_intact(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
+        import_functions.import_object_parameter_values(
+            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
+        )
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
+        import_functions.import_object_parameter_value_metadata(
+            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
+        )
+        self._db_map.commit_session("Add test data.")
+        parameter_value_metadata = self._db_map.query(self._db_map.parameter_value_metadata_sq).all()
+        self.assertEqual(len(parameter_value_metadata), 1)
+        self._db_map.cascade_remove_items(**{"parameter_value_metadata": {parameter_value_metadata[0].id}})
+        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 1)
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 1)
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_metadata_sq).all()), 0)
+
+    def test_cascade_remove_object_removes_its_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_object_metadata(self._db_map, (("my_class", "my_object", '{"title": "My metadata."}'),))
+        self._db_map.commit_session("Add test data.")
+        self._db_map.cascade_remove_items(**{"object": {1}})
+        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.object_sq).all()), 0)
+
+    def test_cascade_remove_relationship_removes_its_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("my_object_class",))
+        import_functions.import_objects(self._db_map, (("my_object_class", "my_object"),))
+        import_functions.import_relationship_classes(self._db_map, (("my_class", ("my_object_class",)),))
+        import_functions.import_relationships(self._db_map, (("my_class", ("my_object",)),))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_relationship_metadata(
+            self._db_map, (("my_class", ("my_object",), '{"title": "My metadata."}'),)
+        )
+        self._db_map.commit_session("Add test data.")
+        self._db_map.cascade_remove_items(**{"relationship": {2}})
+        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.relationship_sq).all()), 0)
+
+    def test_cascade_remove_parameter_value_removes_its_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        import_functions.import_objects(self._db_map, (("my_class", "my_object"),))
+        import_functions.import_object_parameters(self._db_map, (("my_class", "my_parameter"),))
+        import_functions.import_object_parameter_values(
+            self._db_map, (("my_class", "my_object", "my_parameter", 99.0),)
+        )
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        import_functions.import_object_parameter_value_metadata(
+            self._db_map, (("my_class", "my_object", "my_parameter", '{"title": "My metadata."}'),)
+        )
+        self._db_map.commit_session("Add test data.")
+        self._db_map.cascade_remove_items(**{"parameter_value": {1}})
+        self.assertEqual(len(self._db_map.query(self._db_map.metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.entity_metadata_sq).all()), 0)
+        self.assertEqual(len(self._db_map.query(self._db_map.parameter_value_sq).all()), 0)
+
+
+class TestDiffDatabaseMappingAdd(unittest.TestCase):
+    def setUp(self):
+        self._db_map = create_diff_db_map()
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_add_and_retrieve_many_objects(self):
+        """Tests add many objects into db and retrieving them."""
+        ids, _ = self._db_map.add_object_classes({"name": "testclass"})
+        class_id = next(iter(ids))
+        added = self._db_map.add_objects(*[{"name": str(i), "class_id": class_id} for i in range(1001)])[0]
+        self.assertEqual(len(added), 1001)
+        self._db_map.commit_session("test_commit")
+        self.assertEqual(self._db_map.query(self._db_map.entity_sq).count(), 1001)
+
+    def test_add_object_classes(self):
+        """Test that adding object classes works."""
+        self._db_map.add_object_classes({"name": "fish"}, {"name": "dog"})
+        diff_table = self._db_map._diff_table("entity_class")
+        object_classes = (
+            self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.object_class_type).all()
+        )
+        self.assertEqual(len(object_classes), 2)
+        self.assertEqual(object_classes[0].name, "fish")
+        self.assertEqual(object_classes[1].name, "dog")
+
+    def test_add_object_class_with_invalid_name(self):
+        """Test that adding object classes with empty name raises error"""
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_object_classes({"name": ""}, strict=True)
+
+    def test_add_object_classes_with_same_name(self):
+        """Test that adding two object classes with the same name only adds one of them."""
+        self._db_map.add_object_classes({"name": "fish"}, {"name": "fish"})
+        diff_table = self._db_map._diff_table("entity_class")
+        object_classes = (
+            self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.object_class_type).all()
+        )
+        self.assertEqual(len(object_classes), 1)
+        self.assertEqual(object_classes[0].name, "fish")
+
+    def test_add_object_class_with_same_name_as_existing_one(self):
+        """Test that adding an object class with an already taken name raises an integrity error."""
+        self._db_map.add_object_classes({"name": "fish"}, {"name": "fish"})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_object_classes({"name": "fish"}, strict=True)
+
+    def test_add_objects(self):
+        """Test that adding objects works."""
+        self._db_map.add_object_classes({"name": "fish"})
+        self._db_map.add_objects({"name": "nemo", "class_id": 1}, {"name": "dory", "class_id": 1})
+        diff_table = self._db_map._diff_table("entity")
+        objects = self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.object_entity_type).all()
+        self.assertEqual(len(objects), 2)
+        self.assertEqual(objects[0].name, "nemo")
+        self.assertEqual(objects[0].class_id, 1)
+        self.assertEqual(objects[1].name, "dory")
+        self.assertEqual(objects[1].class_id, 1)
+
+    def test_add_object_with_invalid_name(self):
+        """Test that adding object classes with empty name raises error"""
+        self._db_map.add_object_classes({"name": "fish"})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_objects({"name": "", "class_id": 1}, strict=True)
+
+    def test_add_objects_with_same_name(self):
+        """Test that adding two objects with the same name only adds one of them."""
+        self._db_map.add_object_classes({"name": "fish"})
+        self._db_map.add_objects({"name": "nemo", "class_id": 1}, {"name": "nemo", "class_id": 1})
+        diff_table = self._db_map._diff_table("entity")
+        objects = self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.object_entity_type).all()
+        self.assertEqual(len(objects), 1)
+        self.assertEqual(objects[0].name, "nemo")
+        self.assertEqual(objects[0].class_id, 1)
+
+    def test_add_object_with_same_name_as_existing_one(self):
+        """Test that adding an object with an already taken name raises an integrity error."""
+        self._db_map.add_object_classes({"name": "fish"})
+        self._db_map.add_objects({"name": "nemo", "class_id": 1})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_objects({"name": "nemo", "class_id": 1}, strict=True)
+
+    def test_add_object_with_invalid_class(self):
+        """Test that adding an object with a non existing class raises an integrity error."""
+        self._db_map.add_object_classes({"name": "fish"})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_objects({"name": "pluto", "class_id": 2}, strict=True)
+
+    def test_add_relationship_classes(self):
+        """Test that adding relationship classes works."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.add_wide_relationship_classes(
+            {"name": "rc1", "object_class_id_list": [1, 2]}, {"name": "rc2", "object_class_id_list": [2, 1]}
+        )
+        diff_table = self._db_map._diff_table("relationship_entity_class")
+        rel_ent_clss = self._db_map.query(diff_table).all()
+        diff_table = self._db_map._diff_table("entity_class")
+        rel_clss = (
+            self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.relationship_class_type).all()
+        )
+        self.assertEqual(len(rel_ent_clss), 4)
+        self.assertEqual(rel_clss[0].name, "rc1")
+        self.assertEqual(rel_ent_clss[0].member_class_id, 1)
+        self.assertEqual(rel_ent_clss[1].member_class_id, 2)
+        self.assertEqual(rel_clss[1].name, "rc2")
+        self.assertEqual(rel_ent_clss[2].member_class_id, 2)
+        self.assertEqual(rel_ent_clss[3].member_class_id, 1)
+
+    def test_add_relationship_classes_with_invalid_name(self):
+        """Test that adding object classes with empty name raises error"""
+        self._db_map.add_object_classes({"name": "fish"})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_wide_relationship_classes({"name": "", "object_class_id_list": [1]}, strict=True)
+
+    def test_add_relationship_classes_with_same_name(self):
+        """Test that adding two relationship classes with the same name only adds one of them."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.add_wide_relationship_classes(
+            {"name": "rc1", "object_class_id_list": [1, 2]}, {"name": "rc1", "object_class_id_list": [1, 2]}
+        )
+        diff_table = self._db_map._diff_table("relationship_entity_class")
+        relationship_members = self._db_map.query(diff_table).all()
+        diff_table = self._db_map._diff_table("entity_class")
+        relationships = (
+            self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.relationship_class_type).all()
+        )
+        self.assertEqual(len(relationship_members), 2)
+        self.assertEqual(len(relationships), 1)
+        self.assertEqual(relationships[0].name, "rc1")
+        self.assertEqual(relationship_members[0].member_class_id, 1)
+        self.assertEqual(relationship_members[1].member_class_id, 2)
+
+    def test_add_relationship_class_with_same_name_as_existing_one(self):
+        """Test that adding a relationship class with an already taken name raises an integrity error."""
+        query_wrapper = create_query_wrapper(self._db_map)
+        with mock.patch.object(DiffDatabaseMapping, "query") as mock_query, mock.patch.object(
+            DiffDatabaseMapping, "object_class_sq"
+        ) as mock_object_class_sq, mock.patch.object(
+            DiffDatabaseMapping, "wide_relationship_class_sq"
+        ) as mock_wide_rel_cls_sq:
+            mock_query.side_effect = query_wrapper
+            mock_object_class_sq.return_value = [
+                KeyedTuple([1, "fish"], labels=["id", "name"]),
+                KeyedTuple([2, "dog"], labels=["id", "name"]),
+            ]
+            mock_wide_rel_cls_sq.return_value = [
+                KeyedTuple([1, "1,2", "fish__dog"], labels=["id", "object_class_id_list", "name"])
+            ]
+            with self.assertRaises(SpineIntegrityError):
+                self._db_map.add_wide_relationship_classes(
+                    {"name": "fish__dog", "object_class_id_list": [1, 2]}, strict=True
+                )
+
+    def test_add_relationship_class_with_invalid_object_class(self):
+        """Test that adding a relationship class with a non existing object class raises an integrity error."""
+        query_wrapper = create_query_wrapper(self._db_map)
+        with mock.patch.object(DiffDatabaseMapping, "query") as mock_query, mock.patch.object(
+            DiffDatabaseMapping, "object_class_sq"
+        ) as mock_object_class_sq, mock.patch.object(DiffDatabaseMapping, "wide_relationship_class_sq"):
+            mock_query.side_effect = query_wrapper
+            mock_object_class_sq.return_value = [KeyedTuple([1, "fish"], labels=["id", "name"])]
+            with self.assertRaises(SpineIntegrityError):
+                self._db_map.add_wide_relationship_classes(
+                    {"name": "fish__dog", "object_class_id_list": [1, 2]}, strict=True
+                )
+
+    def test_add_relationships(self):
+        """Test that adding relationships works."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2})
+        self._db_map.add_wide_relationships({"name": "nemo__pluto", "class_id": 3, "object_id_list": [1, 2]})
+        diff_table = self._db_map._diff_table("relationship_entity")
+        rel_ents = self._db_map.query(diff_table).all()
+        diff_table = self._db_map._diff_table("entity")
+        relationships = (
+            self._db_map.query(diff_table).filter(diff_table.c.type_id == self._db_map.relationship_entity_type).all()
+        )
+        self.assertEqual(len(rel_ents), 2)
+        self.assertEqual(len(relationships), 1)
+        self.assertEqual(relationships[0].name, "nemo__pluto")
+        self.assertEqual(rel_ents[0].entity_class_id, 3)
+        self.assertEqual(rel_ents[0].member_id, 1)
+        self.assertEqual(rel_ents[1].entity_class_id, 3)
+        self.assertEqual(rel_ents[1].member_id, 2)
+
+    def test_add_relationship_with_invalid_name(self):
+        """Test that adding object classes with empty name raises error"""
+        self._db_map.add_object_classes({"name": "oc1"}, strict=True)
+        self._db_map.add_wide_relationship_classes({"name": "rc1", "object_class_id_list": [1]}, strict=True)
+        self._db_map.add_objects({"name": "o1", "class_id": 1}, strict=True)
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_wide_relationships({"name": "", "class_id": 1, "object_id_list": [1]}, strict=True)
+
+    def test_add_identical_relationships(self):
+        """Test that adding two relationships with the same class and same objects only adds the first one."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 2})
+        self._db_map.add_wide_relationships(
+            {"name": "nemo__pluto", "class_id": 3, "object_id_list": [1, 2]},
+            {"name": "nemo__pluto_duplicate", "class_id": 3, "object_id_list": [1, 2]},
+        )
+        diff_table = self._db_map._diff_table("relationship")
+        relationships = self._db_map.query(diff_table).all()
+        self.assertEqual(len(relationships), 1)
+
+    def test_add_relationship_identical_to_existing_one(self):
+        """Test that adding a relationship with the same class and same objects as an existing one
+        raises an integrity error.
+        """
+        query_wrapper = create_query_wrapper(self._db_map)
+        with mock.patch.object(DiffDatabaseMapping, "query") as mock_query, mock.patch.object(
+            DiffDatabaseMapping, "object_sq"
+        ) as mock_object_sq, mock.patch.object(
+            DiffDatabaseMapping, "wide_relationship_class_sq"
+        ) as mock_wide_rel_cls_sq, mock.patch.object(
+            DiffDatabaseMapping, "wide_relationship_sq"
+        ) as mock_wide_rel_sq:
+            mock_query.side_effect = query_wrapper
+            mock_object_sq.return_value = [
+                KeyedTuple([1, 10, "nemo"], labels=["id", "class_id", "name"]),
+                KeyedTuple([2, 20, "pluto"], labels=["id", "class_id", "name"]),
+            ]
+            mock_wide_rel_cls_sq.return_value = [
+                KeyedTuple([1, "10,20", "fish__dog"], labels=["id", "object_class_id_list", "name"])
+            ]
+            mock_wide_rel_sq.return_value = [
+                KeyedTuple([1, 1, "1,2", "nemo__pluto"], labels=["id", "class_id", "object_id_list", "name"])
+            ]
+            with self.assertRaises(SpineIntegrityError):
+                self._db_map.add_wide_relationships(
+                    {"name": "nemoy__plutoy", "class_id": 1, "object_id_list": [1, 2]}, strict=True
+                )
+
+    def test_add_relationship_with_invalid_class(self):
+        """Test that adding a relationship with an invalid class raises an integrity error."""
+        query_wrapper = create_query_wrapper(self._db_map)
+        with mock.patch.object(DiffDatabaseMapping, "query") as mock_query, mock.patch.object(
+            DiffDatabaseMapping, "object_sq"
+        ) as mock_object_sq, mock.patch.object(
+            DiffDatabaseMapping, "wide_relationship_class_sq"
+        ) as mock_wide_rel_cls_sq, mock.patch.object(
+            DiffDatabaseMapping, "wide_relationship_sq"
+        ):
+            mock_query.side_effect = query_wrapper
+            mock_object_sq.return_value = [
+                KeyedTuple([1, 10, "nemo"], labels=["id", "class_id", "name"]),
+                KeyedTuple([2, 20, "pluto"], labels=["id", "class_id", "name"]),
+            ]
+            mock_wide_rel_cls_sq.return_value = [
+                KeyedTuple([1, "10,20", "fish__dog"], labels=["id", "object_class_id_list", "name"])
+            ]
+            with self.assertRaises(SpineIntegrityError):
+                self._db_map.add_wide_relationships(
+                    {"name": "nemo__pluto", "class_id": 2, "object_id_list": [1, 2]}, strict=True
+                )
+
+    def test_add_relationship_with_invalid_object(self):
+        """Test that adding a relationship with an invalid object raises an integrity error."""
+        query_wrapper = create_query_wrapper(self._db_map)
+        with mock.patch.object(DiffDatabaseMapping, "query") as mock_query, mock.patch.object(
+            DiffDatabaseMapping, "object_sq"
+        ) as mock_object_sq, mock.patch.object(
+            DiffDatabaseMapping, "wide_relationship_class_sq"
+        ) as mock_wide_rel_cls_sq, mock.patch.object(
+            DiffDatabaseMapping, "wide_relationship_sq"
+        ):
+            mock_query.side_effect = query_wrapper
+            mock_object_sq.return_value = [
+                KeyedTuple([1, 10, "nemo"], labels=["id", "class_id", "name"]),
+                KeyedTuple([2, 20, "pluto"], labels=["id", "class_id", "name"]),
+            ]
+            mock_wide_rel_cls_sq.return_value = [
+                KeyedTuple([1, "10,20", "fish__dog"], labels=["id", "object_class_id_list", "name"])
+            ]
+            with self.assertRaises(SpineIntegrityError):
+                self._db_map.add_wide_relationships(
+                    {"name": "nemo__pluto", "class_id": 1, "object_id_list": [1, 3]}, strict=True
+                )
+
+    def test_add_entity_groups(self):
+        """Test that adding group entities works."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
+        self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 1, "member_id": 2})
+        diff_table = self._db_map._diff_table("entity_group")
+        entity_groups = self._db_map.query(diff_table).all()
+        self.assertEqual(len(entity_groups), 1)
+        self.assertEqual(entity_groups[0].entity_id, 1)
+        self.assertEqual(entity_groups[0].entity_class_id, 1)
+        self.assertEqual(entity_groups[0].member_id, 2)
+
+    def test_add_entity_groups_with_invalid_class(self):
+        """Test that adding group entities with an invalid class fails."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 2, "member_id": 2}, strict=True)
+
+    def test_add_entity_groups_with_invalid_entity(self):
+        """Test that adding group entities with an invalid entity fails."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_entity_groups({"entity_id": 3, "entity_class_id": 2, "member_id": 2}, strict=True)
+
+    def test_add_entity_groups_with_invalid_member(self):
+        """Test that adding group entities with an invalid member fails."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 2, "member_id": 3}, strict=True)
+
+    def test_add_repeated_entity_groups(self):
+        """Test that adding repeated group entities fails."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1})
+        self._db_map.add_objects({"name": "o1", "id": 1, "class_id": 1}, {"name": "o2", "id": 2, "class_id": 1})
+        self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 2, "member_id": 2})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_entity_groups({"entity_id": 1, "entity_class_id": 2, "member_id": 2}, strict=True)
+
+    def test_add_parameter_definitions(self):
+        """Test that adding parameter definitions works."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
+        self._db_map.add_parameter_definitions(
+            {"name": "color", "object_class_id": 1, "description": "test1"},
+            {"name": "relative_speed", "relationship_class_id": 3, "description": "test2"},
+        )
+        diff_table = self._db_map._diff_table("parameter_definition")
+        parameter_definitions = self._db_map.query(diff_table).all()
+        self.assertEqual(len(parameter_definitions), 2)
+        self.assertEqual(parameter_definitions[0].name, "color")
+        self.assertEqual(parameter_definitions[0].entity_class_id, 1)
+        self.assertEqual(parameter_definitions[0].description, "test1")
+        self.assertEqual(parameter_definitions[1].name, "relative_speed")
+        self.assertEqual(parameter_definitions[1].entity_class_id, 3)
+        self.assertEqual(parameter_definitions[1].description, "test2")
+
+    def test_add_parameter_with_invalid_name(self):
+        """Test that adding object classes with empty name raises error"""
+        self._db_map.add_object_classes({"name": "oc1"}, strict=True)
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_parameter_definitions({"name": "", "object_class_id": 1}, strict=True)
+
+    def test_add_parameter_definitions_with_same_name(self):
+        """Test that adding two parameter_definitions with the same name adds both of them."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
+        self._db_map.add_parameter_definitions(
+            {"name": "color", "object_class_id": 1}, {"name": "color", "relationship_class_id": 3}
+        )
+        diff_table = self._db_map._diff_table("parameter_definition")
+        parameter_definitions = self._db_map.query(diff_table).all()
+        self.assertEqual(len(parameter_definitions), 2)
+        self.assertEqual(parameter_definitions[0].name, "color")
+        self.assertEqual(parameter_definitions[1].name, "color")
+        self.assertEqual(parameter_definitions[0].entity_class_id, 1)
+
+    def test_add_parameter_with_same_name_as_existing_one(self):
+        """Test that adding parameter_definitions with an already taken name raises and integrity error."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
+        self._db_map.add_parameter_definitions(
+            {"name": "color", "object_class_id": 1}, {"name": "color", "relationship_class_id": 3}
+        )
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_parameter_definitions({"name": "color", "object_class_id": 1}, strict=True)
+
+    def test_add_parameter_with_invalid_class(self):
+        """Test that adding parameter_definitions with an invalid (object or relationship) class raises and integrity error."""
+        self._db_map.add_object_classes({"name": "oc1", "id": 1}, {"name": "oc2", "id": 2})
+        self._db_map.add_wide_relationship_classes({"name": "rc1", "id": 3, "object_class_id_list": [1, 2]})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_parameter_definitions({"name": "color", "object_class_id": 3}, strict=True)
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_parameter_definitions({"name": "color", "relationship_class_id": 1}, strict=True)
+
+    def test_add_parameter_for_both_object_and_relationship_class(self):
+        """Test that adding parameter_definitions associated to both and object and relationship class
+        raises and integrity error."""
+        self._db_map.add_object_classes({"name": "fish", "id": 1}, {"name": "dog", "id": 2})
+        self._db_map.add_wide_relationship_classes({"name": "fish__dog", "id": 10, "object_class_id_list": [1, 2]})
+        with self.assertRaises(SpineIntegrityError):
+            self._db_map.add_parameter_definitions(
+                {"name": "color", "object_class_id": 1, "relationship_class_id": 10}, strict=True
+            )
+
+    def test_add_parameter_values(self):
+        """Test that adding parameter values works."""
+        import_functions.import_object_classes(self._db_map, ["fish", "dog"])
+        import_functions.import_relationship_classes(self._db_map, [("fish_dog", ["fish", "dog"])])
+        import_functions.import_objects(self._db_map, [("fish", "nemo"), ("dog", "pluto")])
+        import_functions.import_relationships(self._db_map, [("fish_dog", ("nemo", "pluto"))])
+        import_functions.import_object_parameters(self._db_map, [("fish", "color")])
+        import_functions.import_relationship_parameters(self._db_map, [("fish_dog", "rel_speed")])
+        color_id = (
+            self._db_map.parameter_definition_list()
+            .filter(self._db_map.parameter_definition_sq.c.name == "color")
+            .first()
+            .id
+        )
+        rel_speed_id = (
+            self._db_map.parameter_definition_list()
+            .filter(self._db_map.parameter_definition_sq.c.name == "rel_speed")
+            .first()
+            .id
+        )
+        nemo_row = self._db_map.object_list().filter(self._db_map.entity_sq.c.name == "nemo").first()
+        nemo__pluto_row = self._db_map.wide_relationship_list().filter().first()
+        self._db_map.add_parameter_values(
+            {
+                "parameter_definition_id": color_id,
+                "entity_id": nemo_row.id,
+                "entity_class_id": nemo_row.class_id,
+                "value": b'"orange"',
+                "alternative_id": 1,
+            },
+            {
+                "parameter_definition_id": rel_speed_id,
+                "entity_id": nemo__pluto_row.id,
+                "entity_class_id": nemo__pluto_row.class_id,
+                "value": b"125",
+                "alternative_id": 1,
+            },
+        )
+        diff_table = self._db_map._diff_table("parameter_value")
+        parameter_values = self._db_map.query(diff_table).all()
+        self.assertEqual(len(parameter_values), 2)
+        self.assertEqual(parameter_values[0].parameter_definition_id, 1)
+        self.assertEqual(parameter_values[0].entity_id, 1)
+        self.assertEqual(parameter_values[0].value, b'"orange"')
+        self.assertEqual(parameter_values[1].parameter_definition_id, 2)
+        self.assertEqual(parameter_values[1].entity_id, 3)
+        self.assertEqual(parameter_values[1].value, b"125")
+
+    def test_add_parameter_value_with_invalid_object_or_relationship(self):
+        """Test that adding a parameter value with an invalid object or relationship raises an
+        integrity error."""
+        import_functions.import_object_classes(self._db_map, ["fish", "dog"])
+        import_functions.import_relationship_classes(self._db_map, [("fish_dog", ["fish", "dog"])])
+        import_functions.import_objects(self._db_map, [("fish", "nemo"), ("dog", "pluto")])
+        import_functions.import_relationships(self._db_map, [("fish_dog", ("nemo", "pluto"))])
+        import_functions.import_object_parameters(self._db_map, [("fish", "color")])
+        import_functions.import_relationship_parameters(self._db_map, [("fish_dog", "rel_speed")])
+        _, errors = self._db_map.add_parameter_values(
+            {"parameter_definition_id": 1, "object_id": 3, "value": b'"orange"', "alternative_id": 1}, strict=False
+        )
+        self.assertEqual([str(e) for e in errors], ["Incorrect entity 'fish_dog_nemo__pluto' for parameter 'color'."])
+        _, errors = self._db_map.add_parameter_values(
+            {"parameter_definition_id": 2, "relationship_id": 2, "value": b"125", "alternative_id": 1}, strict=False
+        )
+        self.assertEqual([str(e) for e in errors], ["Incorrect entity 'pluto' for parameter 'rel_speed'."])
+
+    def test_add_same_parameter_value_twice(self):
+        """Test that adding a parameter value twice only adds the first one."""
+        import_functions.import_object_classes(self._db_map, ["fish"])
+        import_functions.import_objects(self._db_map, [("fish", "nemo")])
+        import_functions.import_object_parameters(self._db_map, [("fish", "color")])
+        color_id = (
+            self._db_map.parameter_definition_list()
+            .filter(self._db_map.parameter_definition_sq.c.name == "color")
+            .first()
+            .id
+        )
+        nemo_row = self._db_map.object_list().filter(self._db_map.entity_sq.c.name == "nemo").first()
+        self._db_map.add_parameter_values(
+            {
+                "parameter_definition_id": color_id,
+                "entity_id": nemo_row.id,
+                "entity_class_id": nemo_row.class_id,
+                "value": b'"orange"',
+                "alternative_id": 1,
+            },
+            {
+                "parameter_definition_id": color_id,
+                "entity_id": nemo_row.id,
+                "entity_class_id": nemo_row.class_id,
+                "value": b'"blue"',
+                "alternative_id": 1,
+            },
+        )
+        diff_table = self._db_map._diff_table("parameter_value")
+        parameter_values = self._db_map.query(diff_table).all()
+        self.assertEqual(len(parameter_values), 1)
+        self.assertEqual(parameter_values[0].parameter_definition_id, 1)
+        self.assertEqual(parameter_values[0].entity_id, 1)
+        self.assertEqual(parameter_values[0].value, b'"orange"')
+
+    def test_add_existing_parameter_value(self):
+        """Test that adding an existing parameter value raises an integrity error."""
+        import_functions.import_object_classes(self._db_map, ["fish"])
+        import_functions.import_objects(self._db_map, [("fish", "nemo")])
+        import_functions.import_object_parameters(self._db_map, [("fish", "color")])
+        import_functions.import_object_parameter_values(self._db_map, [("fish", "nemo", "color", "orange")])
+        _, errors = self._db_map.add_parameter_values(
+            {
+                "parameter_definition_id": 1,
+                "entity_class_id": 1,
+                "entity_id": 1,
+                "value": b'"blue"',
+                "alternative_id": 1,
+            },
+            strict=False,
+        )
+        self.assertEqual(
+            [str(e) for e in errors], ["The value of parameter 'color' for entity 'nemo' is already specified."]
+        )
+
+    def test_add_alternative(self):
+        ids, errors = self._db_map.add_alternatives({"name": "my_alternative"})
+        self.assertEqual(errors, [])
+        self.assertEqual(ids, {2})
+        alternatives = self._db_map.query(self._db_map.alternative_sq).all()
+        self.assertEqual(len(alternatives), 2)
+        self.assertEqual(
+            alternatives[0]._asdict(), {"id": 1, "name": "Base", "description": "Base alternative", "commit_id": 1}
+        )
+        self.assertEqual(
+            alternatives[1]._asdict(), {"id": 2, "name": "my_alternative", "description": None, "commit_id": None}
+        )
+
+    def test_add_scenario(self):
+        ids, errors = self._db_map.add_scenarios({"name": "my_scenario"})
+        self.assertEqual(errors, [])
+        self.assertEqual(ids, {1})
+        scenarios = self._db_map.query(self._db_map.scenario_sq).all()
+        self.assertEqual(len(scenarios), 1)
+        self.assertEqual(
+            scenarios[0]._asdict(),
+            {"id": 1, "name": "my_scenario", "description": None, "active": False, "commit_id": None},
+        )
+
+    def test_add_scenario_alternative(self):
+        import_functions.import_scenarios(self._db_map, ("my_scenario",))
+        self._db_map.commit_session("Add test data.")
+        ids, errors = self._db_map.add_scenario_alternatives({"scenario_id": 1, "alternative_id": 1, "rank": 0})
+        self.assertEqual(errors, [])
+        self.assertEqual(ids, {1})
+        scenario_alternatives = self._db_map.query(self._db_map.scenario_alternative_sq).all()
+        self.assertEqual(len(scenario_alternatives), 1)
+        self.assertEqual(
+            scenario_alternatives[0]._asdict(),
+            {"id": 1, "scenario_id": 1, "alternative_id": 1, "rank": 0, "commit_id": None},
+        )
+
+    def test_add_metadata(self):
+        items, errors = self._db_map.add_metadata({"name": "test name", "value": "test_add_metadata"}, strict=False)
+        self.assertEqual(errors, [])
+        self.assertEqual(items, {1})
+        self._db_map.commit_session("Add metadata")
+        metadata = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata), 1)
+        self.assertEqual(
+            metadata[0]._asdict(), {"name": "test name", "id": 1, "value": "test_add_metadata", "commit_id": 2}
+        )
+
+    def test_add_metadata_that_exists_does_not_add_it(self):
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        self._db_map.commit_session("Add test data.")
+        items, errors = self._db_map.add_metadata({"name": "title", "value": "My metadata."}, strict=False)
+        self.assertEqual(errors, [])
+        self.assertEqual(items, set())
+        metadata = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata), 1)
+        self.assertEqual(metadata[0]._asdict(), {"name": "title", "id": 1, "value": "My metadata.", "commit_id": 2})
+
+    def test_add_entity_metadata_for_object(self):
+        import_functions.import_object_classes(self._db_map, ("fish",))
+        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        self._db_map.commit_session("Add test data.")
+        items, errors = self._db_map.add_entity_metadata({"entity_id": 1, "metadata_id": 1}, strict=False)
+        self.assertEqual(errors, [])
+        self.assertEqual(items, {1})
+        self._db_map.commit_session("Add entity metadata")
+        entity_metadata = self._db_map.query(self._db_map.ext_entity_metadata_sq).all()
+        self.assertEqual(len(entity_metadata), 1)
+        self.assertEqual(
+            entity_metadata[0]._asdict(),
+            {
+                "entity_id": 1,
+                "entity_name": "leviathan",
+                "metadata_name": "title",
+                "metadata_value": "My metadata.",
+                "metadata_id": 1,
+                "id": 1,
+                "commit_id": 3,
+            },
+        )
+
+    def test_add_entity_metadata_for_relationship(self):
+        import_functions.import_object_classes(self._db_map, ("my_object_class",))
+        import_functions.import_objects(self._db_map, (("my_object_class", "my_object"),))
+        import_functions.import_relationship_classes(self._db_map, (("my_relationship_class", ("my_object_class",)),))
+        import_functions.import_relationships(self._db_map, (("my_relationship_class", ("my_object",)),))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        self._db_map.commit_session("Add test data.")
+        items, errors = self._db_map.add_entity_metadata({"entity_id": 2, "metadata_id": 1}, strict=False)
+        self.assertEqual(errors, [])
+        self.assertEqual(items, {1})
+        self._db_map.commit_session("Add entity metadata")
+        entity_metadata = self._db_map.query(self._db_map.ext_entity_metadata_sq).all()
+        self.assertEqual(len(entity_metadata), 1)
+        self.assertEqual(
+            entity_metadata[0]._asdict(),
+            {
+                "entity_id": 2,
+                "entity_name": "my_relationship_class_my_object",
+                "metadata_name": "title",
+                "metadata_value": "My metadata.",
+                "metadata_id": 1,
+                "id": 1,
+                "commit_id": 3,
+            },
+        )
+
+    def test_add_entity_metadata_doesnt_raise_with_empty_cache(self):
+        items, errors = self._db_map.add_entity_metadata(
+            {"entity_id": 1, "metadata_id": 1}, cache=DBCache(lambda *args, **kwargs: None), strict=False
+        )
+        self.assertEqual(items, set())
+        self.assertEqual(len(errors), 1)
+
+    def test_add_ext_entity_metadata_for_object(self):
+        import_functions.import_object_classes(self._db_map, ("fish",))
+        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
+        self._db_map.commit_session("Add test data.")
+        items, errors = self._db_map.add_ext_entity_metadata(
+            {"entity_id": 1, "metadata_name": "key", "metadata_value": "object metadata"}, strict=False
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(items, {1})
+        self._db_map.commit_session("Add entity metadata")
+        entity_metadata = self._db_map.query(self._db_map.ext_entity_metadata_sq).all()
+        self.assertEqual(len(entity_metadata), 1)
+        self.assertEqual(
+            entity_metadata[0]._asdict(),
+            {
+                "entity_id": 1,
+                "entity_name": "leviathan",
+                "metadata_name": "key",
+                "metadata_value": "object metadata",
+                "metadata_id": 1,
+                "id": 1,
+                "commit_id": 3,
+            },
+        )
+
+    def test_adding_ext_entity_metadata_for_object_reuses_existing_metadata_names_and_values(self):
+        import_functions.import_object_classes(self._db_map, ("fish",))
+        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        self._db_map.commit_session("Add test data.")
+        items, errors = self._db_map.add_ext_entity_metadata(
+            {"entity_id": 1, "metadata_name": "title", "metadata_value": "My metadata."}, strict=False
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(items, {1})
+        self._db_map.commit_session("Add entity metadata")
+        metadata = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata), 1)
+        self.assertEqual(metadata[0]._asdict(), {"id": 1, "name": "title", "value": "My metadata.", "commit_id": 2})
+        entity_metadata = self._db_map.query(self._db_map.ext_entity_metadata_sq).all()
+        self.assertEqual(len(entity_metadata), 1)
+        self.assertEqual(
+            entity_metadata[0]._asdict(),
+            {
+                "entity_id": 1,
+                "entity_name": "leviathan",
+                "metadata_name": "title",
+                "metadata_value": "My metadata.",
+                "metadata_id": 1,
+                "id": 1,
+                "commit_id": 3,
+            },
+        )
+
+    def test_add_parameter_value_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("fish",))
+        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
+        import_functions.import_object_parameters(self._db_map, (("fish", "paranormality"),))
+        import_functions.import_object_parameter_values(self._db_map, (("fish", "leviathan", "paranormality", 3.9),))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        self._db_map.commit_session("Add test data.")
+        items, errors = self._db_map.add_parameter_value_metadata(
+            {"parameter_value_id": 1, "metadata_id": 1, "alternative_id": 1}, strict=False
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(items, {1})
+        self._db_map.commit_session("Add value metadata")
+        value_metadata = self._db_map.query(self._db_map.ext_parameter_value_metadata_sq).all()
+        self.assertEqual(len(value_metadata), 1)
+        self.assertEqual(
+            value_metadata[0]._asdict(),
+            {
+                "alternative_name": "Base",
+                "entity_name": "leviathan",
+                "parameter_value_id": 1,
+                "parameter_name": "paranormality",
+                "metadata_name": "title",
+                "metadata_value": "My metadata.",
+                "metadata_id": 1,
+                "id": 1,
+                "commit_id": 3,
+            },
+        )
+
+    def test_add_parameter_value_metadata_doesnt_raise_with_empty_cache(self):
+        items, errors = self._db_map.add_parameter_value_metadata(
+            {"parameter_value_id": 1, "metadata_id": 1, "alternative_id": 1},
+            cache=DBCache(lambda *args, **kwargs: None),
+            strict=False,
+        )
+        self.assertEqual(items, set())
+        self.assertEqual(len(errors), 1)
+
+    def test_add_ext_parameter_value_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("fish",))
+        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
+        import_functions.import_object_parameters(self._db_map, (("fish", "paranormality"),))
+        import_functions.import_object_parameter_values(self._db_map, (("fish", "leviathan", "paranormality", 3.9),))
+        self._db_map.commit_session("Add test data.")
+        items, errors = self._db_map.add_ext_parameter_value_metadata(
+            {
+                "parameter_value_id": 1,
+                "metadata_name": "key",
+                "metadata_value": "parameter metadata",
+                "alternative_id": 1,
+            },
+            strict=False,
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(items, {1})
+        self._db_map.commit_session("Add value metadata")
+        value_metadata = self._db_map.query(self._db_map.ext_parameter_value_metadata_sq).all()
+        self.assertEqual(len(value_metadata), 1)
+        self.assertEqual(
+            value_metadata[0]._asdict(),
+            {
+                "alternative_name": "Base",
+                "entity_name": "leviathan",
+                "parameter_value_id": 1,
+                "parameter_name": "paranormality",
+                "metadata_name": "key",
+                "metadata_value": "parameter metadata",
+                "metadata_id": 1,
+                "id": 1,
+                "commit_id": 3,
+            },
+        )
+
+    def test_add_ext_parameter_value_metadata_reuses_existing_metadata(self):
+        import_functions.import_object_classes(self._db_map, ("fish",))
+        import_functions.import_objects(self._db_map, (("fish", "leviathan"),))
+        import_functions.import_object_parameters(self._db_map, (("fish", "paranormality"),))
+        import_functions.import_object_parameter_values(self._db_map, (("fish", "leviathan", "paranormality", 3.9),))
+        import_functions.import_metadata(self._db_map, ('{"title": "My metadata."}',))
+        self._db_map.commit_session("Add test data.")
+        items, errors = self._db_map.add_ext_parameter_value_metadata(
+            {"parameter_value_id": 1, "metadata_name": "title", "metadata_value": "My metadata.", "alternative_id": 1},
+            strict=False,
+        )
+        self.assertEqual(errors, [])
+        self.assertEqual(items, {1})
+        self._db_map.commit_session("Add value metadata")
+        metadata = self._db_map.query(self._db_map.metadata_sq).all()
+        self.assertEqual(len(metadata), 1)
+        self.assertEqual(metadata[0]._asdict(), {"id": 1, "name": "title", "value": "My metadata.", "commit_id": 2})
+        value_metadata = self._db_map.query(self._db_map.ext_parameter_value_metadata_sq).all()
+        self.assertEqual(len(value_metadata), 1)
+        self.assertEqual(
+            value_metadata[0]._asdict(),
+            {
+                "alternative_name": "Base",
+                "entity_name": "leviathan",
+                "parameter_value_id": 1,
+                "parameter_name": "paranormality",
+                "metadata_name": "title",
+                "metadata_value": "My metadata.",
+                "metadata_id": 1,
+                "id": 1,
+                "commit_id": 3,
+            },
+        )
+
+
+class TestDiffDatabaseMappingUpdate(unittest.TestCase):
+    def setUp(self):
+        self._db_map = create_diff_db_map()
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_update_object_classes(self):
+        """Test that updating object classes works."""
+        self._db_map.add_object_classes({"id": 1, "name": "fish"}, {"id": 2, "name": "dog"})
+        ids, intgr_error_log = self._db_map.update_object_classes(
+            {"id": 1, "name": "octopus"}, {"id": 2, "name": "god"}
+        )
+        sq = self._db_map.object_class_sq
+        object_classes = {x.id: x.name for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))}
+        self.assertEqual(intgr_error_log, [])
+        self.assertEqual(object_classes[1], "octopus")
+        self.assertEqual(object_classes[2], "god")
+
+    def test_update_objects(self):
+        """Test that updating objects works."""
+        self._db_map.add_object_classes({"id": 1, "name": "fish"})
+        self._db_map.add_objects({"id": 1, "name": "nemo", "class_id": 1}, {"id": 2, "name": "dory", "class_id": 1})
+        ids, intgr_error_log = self._db_map.update_objects({"id": 1, "name": "klaus"}, {"id": 2, "name": "squidward"})
+        sq = self._db_map.object_sq
+        objects = {x.id: x.name for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))}
+        self.assertEqual(intgr_error_log, [])
+        self.assertEqual(objects[1], "klaus")
+        self.assertEqual(objects[2], "squidward")
+
+    def test_update_objects_not_committed(self):
+        """Test that updating objects works."""
+        self._db_map.add_object_classes({"id": 1, "name": "some_class"})
+        self._db_map.add_objects({"id": 1, "name": "nemo", "class_id": 1})
+        ids, intgr_error_log = self._db_map.update_objects({"id": 1, "name": "klaus"})
+        sq = self._db_map.object_sq
+        objects = {x.id: x.name for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))}
+        self.assertEqual(intgr_error_log, [])
+        self.assertEqual(objects[1], "klaus")
+        self.assertEqual(self._db_map.query(self._db_map.object_sq).filter_by(id=1).first().name, "klaus")
+        self._db_map.commit_session("update")
+        self.assertEqual(self._db_map.query(self._db_map.object_sq).filter_by(id=1).first().name, "klaus")
+
+    def test_update_committed_object(self):
+        """Test that updating objects works."""
+        self._db_map.add_object_classes({"id": 1, "name": "some_class"})
+        self._db_map.add_objects({"id": 1, "name": "nemo", "class_id": 1})
+        self._db_map.commit_session("update")
+        ids, intgr_error_log = self._db_map.update_objects({"id": 1, "name": "klaus"})
+        sq = self._db_map.object_sq
+        objects = {x.id: x.name for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))}
+        self.assertEqual(intgr_error_log, [])
+        self.assertEqual(objects[1], "klaus")
+        self.assertEqual(self._db_map.query(self._db_map.object_sq).filter_by(id=1).first().name, "klaus")
+        self._db_map.commit_session("update")
+        self.assertEqual(self._db_map.query(self._db_map.object_sq).filter_by(id=1).first().name, "klaus")
+
+    def test_update_relationship_classes(self):
+        """Test that updating relationship classes works."""
+        self._db_map.add_object_classes({"name": "dog", "id": 1}, {"name": "fish", "id": 2})
+        self._db_map.add_wide_relationship_classes(
+            {"id": 3, "name": "dog__fish", "object_class_id_list": [1, 2]},
+            {"id": 4, "name": "fish__dog", "object_class_id_list": [2, 1]},
+        )
+        ids, intgr_error_log = self._db_map.update_wide_relationship_classes(
+            {"id": 3, "name": "god__octopus"}, {"id": 4, "name": "octopus__dog"}
+        )
+        sq = self._db_map.wide_relationship_class_sq
+        rel_clss = {x.id: x.name for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))}
+        self.assertEqual(intgr_error_log, [])
+        self.assertEqual(rel_clss[3], "god__octopus")
+        self.assertEqual(rel_clss[4], "octopus__dog")
+
+    def test_update_relationships(self):
+        """Test that updating relationships works."""
+        self._db_map.add_object_classes({"name": "fish", "id": 1}, {"name": "dog", "id": 2})
+        self._db_map.add_wide_relationship_classes({"name": "fish__dog", "id": 3, "object_class_id_list": [1, 2]})
+        self._db_map.add_objects(
+            {"name": "nemo", "id": 1, "class_id": 1},
+            {"name": "pluto", "id": 2, "class_id": 2},
+            {"name": "scooby", "id": 3, "class_id": 2},
+        )
+        self._db_map.add_wide_relationships(
+            {"id": 4, "name": "nemo__pluto", "class_id": 3, "object_id_list": [1, 2], "object_class_id_list": [1, 2]}
+        )
+        ids, intgr_error_log = self._db_map.update_wide_relationships(
+            {"id": 4, "name": "nemo__scooby", "class_id": 3, "object_id_list": [1, 3], "object_class_id_list": [1, 2]}
+        )
+        sq = self._db_map.wide_relationship_sq
+        rels = {
+            x.id: {"name": x.name, "object_id_list": x.object_id_list}
+            for x in self._db_map.query(sq).filter(sq.c.id.in_(ids))
+        }
+        self.assertEqual(intgr_error_log, [])
+        self.assertEqual(rels[4]["name"], "nemo__scooby")
+        self.assertEqual(rels[4]["object_id_list"], "1,3")
+
+
+class TestDiffDatabaseMappingCommit(unittest.TestCase):
+    def setUp(self):
+        self._db_map = create_diff_db_map()
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_commit_message(self):
+        """Tests that commit comment ends up in the database."""
+        self._db_map.add_object_classes({"name": "testclass"})
+        self._db_map.commit_session("test commit")
+        self.assertEqual(self._db_map.query(self._db_map.commit_sq).all()[-1].comment, "test commit")
+        self._db_map.connection.close()
+
+    def test_commit_session_raise_with_empty_comment(self):
+        import_functions.import_object_classes(self._db_map, ("my_class",))
+        self.assertRaisesRegex(SpineDBAPIError, "Commit message cannot be empty.", self._db_map.commit_session, "")
+
+    def test_commit_session_raise_when_nothing_to_commit(self):
+        self.assertRaisesRegex(SpineDBAPIError, "Nothing to commit.", self._db_map.commit_session, "No changes.")
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/test_check_functions.py` & `spinedb_api-0.30.4/tests/test_check_functions.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-import json
-from numbers import Number
-import unittest
-
-from spinedb_api.db_cache import DBCache, ParameterValueItem
-from spinedb_api.exception import SpineIntegrityError
-
-from spinedb_api.check_functions import replace_parameter_values_with_list_references
-
-
-class TestCheckFunctions(unittest.TestCase):
-    def setUp(self):
-        self.data = [
-            (bool, (b'"TRUE"', b'"FALSE"', b'"T"', b'"True"', b'"False"'), (b'true', b'false')),
-            (int, (b'32', b'3.14'), (b'42', b'-2')),
-            (str, (b'"FOO"', b'"bar"'), (b'"foo"', b'"Bar"', b'"BAZ"')),
-        ]
-        self.parameter_definitions = {
-            1: {'name': 'par1', 'entity_class_id': 1, 'parameter_value_list_id': 1},
-            2: {'name': 'par2', 'entity_class_id': 1, 'parameter_value_list_id': 2},
-            3: {'name': 'par2', 'entity_class_id': 1, 'parameter_value_list_id': 3},
-        }
-        self.value_type = {bool: 1, int: 2, str: 3}
-        self.parameter_value_lists = {1: (1, 2), 2: (3, 4), 3: (5, 6, 7)}
-        self.list_values = {1: True, 2: False, 3: 42, 4: -2, 5: 'foo', 6: 'Bar', 7: 'BAZ'}
-
-    def get_item(self, _type: type, val: bytes):
-        _id = self.value_type[_type]  # setup: parameter definition/value list ids are equal
-        kwargs = {
-            'id': 1,
-            'parameter_definition_id': _id,
-            'entity_class_id': 1,
-            'entity_id': 1,
-            'object_class_id': 1,
-            'object_id': 1,
-            'value': val,
-            'commit_id': 3,
-            'alternative_id': 1,
-            'object_class_name': 'test_objcls',
-            'alternative_name': 'Base',
-            'object_name': 'obj1',
-        }
-        return ParameterValueItem(DBCache(lambda *_, **__: None), item_type="value", **kwargs)
-
-    def test_replace_parameter_or_default_values_with_list_references(self):
-        # regression test for spine-tools/Spine-Toolbox#1878
-        for _type, _fail, _pass in self.data:
-            for data in _fail:
-                with self.subTest(_type=_type, data=data):
-                    expect_in = json.loads(data.decode('utf8'))
-                    if isinstance(expect_in, Number):
-                        expect_in = float(expect_in)
-                    ref = [self.list_values[i] for i in self.parameter_value_lists[self.value_type[_type]]]
-                    expect_ref = ", ".join(f"{json.dumps(i)!r}" for i in ref)
-                    self.assertRaisesRegex(
-                        SpineIntegrityError,
-                        fr"{expect_in!r}.+{expect_ref}",
-                        replace_parameter_values_with_list_references,
-                        self.get_item(_type, data),
-                        self.parameter_definitions,
-                        self.parameter_value_lists,
-                        self.list_values,
-                    )
-
-            for data in _pass:
-                with self.subTest(_type=_type, data=data):
-                    replace_parameter_values_with_list_references(
-                        self.get_item(_type, data),
-                        self.parameter_definitions,
-                        self.parameter_value_lists,
-                        self.list_values,
-                    )
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+import json
+from numbers import Number
+import unittest
+
+from spinedb_api.db_cache import DBCache, ParameterValueItem
+from spinedb_api.exception import SpineIntegrityError
+
+from spinedb_api.check_functions import replace_parameter_values_with_list_references
+
+
+class TestCheckFunctions(unittest.TestCase):
+    def setUp(self):
+        self.data = [
+            (bool, (b'"TRUE"', b'"FALSE"', b'"T"', b'"True"', b'"False"'), (b'true', b'false')),
+            (int, (b'32', b'3.14'), (b'42', b'-2')),
+            (str, (b'"FOO"', b'"bar"'), (b'"foo"', b'"Bar"', b'"BAZ"')),
+        ]
+        self.parameter_definitions = {
+            1: {'name': 'par1', 'entity_class_id': 1, 'parameter_value_list_id': 1},
+            2: {'name': 'par2', 'entity_class_id': 1, 'parameter_value_list_id': 2},
+            3: {'name': 'par2', 'entity_class_id': 1, 'parameter_value_list_id': 3},
+        }
+        self.value_type = {bool: 1, int: 2, str: 3}
+        self.parameter_value_lists = {1: (1, 2), 2: (3, 4), 3: (5, 6, 7)}
+        self.list_values = {1: True, 2: False, 3: 42, 4: -2, 5: 'foo', 6: 'Bar', 7: 'BAZ'}
+
+    def get_item(self, _type: type, val: bytes):
+        _id = self.value_type[_type]  # setup: parameter definition/value list ids are equal
+        kwargs = {
+            'id': 1,
+            'parameter_definition_id': _id,
+            'entity_class_id': 1,
+            'entity_id': 1,
+            'object_class_id': 1,
+            'object_id': 1,
+            'value': val,
+            'commit_id': 3,
+            'alternative_id': 1,
+            'object_class_name': 'test_objcls',
+            'alternative_name': 'Base',
+            'object_name': 'obj1',
+        }
+        return ParameterValueItem(DBCache(lambda *_, **__: None), item_type="value", **kwargs)
+
+    def test_replace_parameter_or_default_values_with_list_references(self):
+        # regression test for spine-tools/Spine-Toolbox#1878
+        for _type, _fail, _pass in self.data:
+            for data in _fail:
+                with self.subTest(_type=_type, data=data):
+                    expect_in = json.loads(data.decode('utf8'))
+                    if isinstance(expect_in, Number):
+                        expect_in = float(expect_in)
+                    ref = [self.list_values[i] for i in self.parameter_value_lists[self.value_type[_type]]]
+                    expect_ref = ", ".join(f"{json.dumps(i)!r}" for i in ref)
+                    self.assertRaisesRegex(
+                        SpineIntegrityError,
+                        fr"{expect_in!r}.+{expect_ref}",
+                        replace_parameter_values_with_list_references,
+                        self.get_item(_type, data),
+                        self.parameter_definitions,
+                        self.parameter_value_lists,
+                        self.list_values,
+                    )
+
+            for data in _pass:
+                with self.subTest(_type=_type, data=data):
+                    replace_parameter_values_with_list_references(
+                        self.get_item(_type, data),
+                        self.parameter_definitions,
+                        self.parameter_value_lists,
+                        self.list_values,
+                    )
```

### Comparing `spinedb_api-0.30.3/tests/test_export_functions.py` & `spinedb_api-0.30.4/tests/test_export_functions.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,168 +1,168 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
-# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
-# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for export_functions.
-
-"""
-
-import unittest
-from spinedb_api import (
-    DiffDatabaseMapping,
-    export_alternatives,
-    export_data,
-    export_scenarios,
-    export_scenario_alternatives,
-    export_tools,
-    export_features,
-    export_tool_features,
-    export_tool_feature_methods,
-    import_alternatives,
-    import_object_classes,
-    import_object_parameter_values,
-    import_object_parameters,
-    import_objects,
-    import_parameter_value_lists,
-    import_relationship_classes,
-    import_relationship_parameter_values,
-    import_relationship_parameters,
-    import_relationships,
-    import_scenarios,
-    import_scenario_alternatives,
-    import_tools,
-    import_features,
-    import_tool_features,
-    import_tool_feature_methods,
-)
-
-
-class TestExportFunctions(unittest.TestCase):
-    def setUp(self):
-        db_url = "sqlite://"
-        self._db_map = DiffDatabaseMapping(db_url, username="UnitTest", create=True)
-
-    def tearDown(self):
-        self._db_map.connection.close()
-
-    def test_export_tools(self):
-        import_tools(self._db_map, [("tool", "Description")])
-        exported = export_tools(self._db_map)
-        self.assertEqual(exported, [("tool", "Description")])
-
-    def test_export_features(self):
-        import_object_classes(self._db_map, ["object_class1", "object_class2"])
-        import_parameter_value_lists(self._db_map, [['value_list', 'value1'], ['value_list', 'value2']])
-        import_object_parameters(self._db_map, [["object_class1", "parameter1", "value1", "value_list"]])
-        import_features(self._db_map, [["object_class1", "parameter1", "Description"]])
-        exported = export_features(self._db_map)
-        self.assertEqual(exported, [("object_class1", "parameter1", "value_list", "Description")])
-
-    def test_export_tool_features(self):
-        import_object_classes(self._db_map, ["object_class1", "object_class2"])
-        import_parameter_value_lists(self._db_map, [['value_list', 'value1'], ['value_list', 'value2']])
-        import_object_parameters(self._db_map, [["object_class1", "parameter1", "value1", "value_list"]])
-        import_features(self._db_map, [["object_class1", "parameter1", "Description"]])
-        import_tools(self._db_map, ["tool1"])
-        import_tool_features(self._db_map, [["tool1", "object_class1", "parameter1"]])
-        exported = export_tool_features(self._db_map)
-        self.assertEqual(exported, [("tool1", "object_class1", "parameter1", False)])
-
-    def test_export_tool_feature_methods(self):
-        import_object_classes(self._db_map, ["object_class1", "object_class2"])
-        import_parameter_value_lists(self._db_map, [['value_list', 'value1'], ['value_list', 'value2']])
-        import_object_parameters(self._db_map, [["object_class1", "parameter1", "value1", "value_list"]])
-        import_features(self._db_map, [["object_class1", "parameter1", "Description"]])
-        import_tools(self._db_map, ["tool1"])
-        import_tool_features(self._db_map, [["tool1", "object_class1", "parameter1"]])
-        import_tool_feature_methods(self._db_map, [["tool1", "object_class1", "parameter1", "value2"]])
-        exported = export_tool_feature_methods(self._db_map)
-        self.assertEqual(exported, [("tool1", "object_class1", "parameter1", "value2")])
-
-    def test_export_alternatives(self):
-        import_alternatives(self._db_map, [("alternative", "Description")])
-        exported = export_alternatives(self._db_map)
-        self.assertEqual(exported, [("Base", "Base alternative"), ("alternative", "Description")])
-
-    def test_export_scenarios(self):
-        import_scenarios(self._db_map, [("scenario", False, "Description")])
-        exported = export_scenarios(self._db_map)
-        self.assertEqual(exported, [("scenario", False, "Description")])
-
-    def test_export_scenario_alternatives(self):
-        import_alternatives(self._db_map, ["alternative"])
-        import_scenarios(self._db_map, ["scenario"])
-        import_scenario_alternatives(self._db_map, (("scenario", "alternative"),))
-        exported = export_scenario_alternatives(self._db_map)
-        self.assertEqual(exported, [("scenario", "alternative", None)])
-
-    def test_export_multiple_scenario_alternatives(self):
-        import_alternatives(self._db_map, ["alternative1"])
-        import_alternatives(self._db_map, ["alternative2"])
-        import_scenarios(self._db_map, ["scenario"])
-        import_scenario_alternatives(self._db_map, (("scenario", "alternative1"),))
-        import_scenario_alternatives(self._db_map, (("scenario", "alternative2", "alternative1"),))
-        exported = export_scenario_alternatives(self._db_map)
-        self.assertEqual(
-            set(exported), {("scenario", "alternative2", "alternative1"), ("scenario", "alternative1", None)}
-        )
-
-    def test_export_data(self):
-        import_object_classes(self._db_map, ["object_class"])
-        import_object_parameters(self._db_map, [("object_class", "object_parameter")])
-        import_objects(self._db_map, [("object_class", "object")])
-        import_object_parameter_values(self._db_map, [("object_class", "object", "object_parameter", 2.3)])
-        import_relationship_classes(self._db_map, [("relationship_class", ["object_class"])])
-        import_relationship_parameters(self._db_map, [("relationship_class", "relationship_parameter")])
-        import_relationships(self._db_map, [("relationship_class", ["object"])])
-        import_relationship_parameter_values(
-            self._db_map, [("relationship_class", ["object"], "relationship_parameter", 3.14)]
-        )
-        import_parameter_value_lists(self._db_map, [("value_list", "5.5"), ("value_list", "6.4")])
-        import_alternatives(self._db_map, ["alternative"])
-        import_scenarios(self._db_map, ["scenario"])
-        import_scenario_alternatives(self._db_map, [("scenario", "alternative")])
-        exported = export_data(self._db_map)
-        self.assertEqual(len(exported), 12)
-        self.assertIn("object_classes", exported)
-        self.assertEqual(exported["object_classes"], [("object_class", None, None)])
-        self.assertIn("object_parameters", exported)
-        self.assertEqual(exported["object_parameters"], [("object_class", "object_parameter", None, None, None)])
-        self.assertIn("objects", exported)
-        self.assertEqual(exported["objects"], [("object_class", "object", None)])
-        self.assertIn("object_parameter_values", exported)
-        self.assertEqual(
-            exported["object_parameter_values"], [("object_class", "object", "object_parameter", 2.3, "Base")]
-        )
-        self.assertIn("relationship_classes", exported)
-        self.assertEqual(exported["relationship_classes"], [("relationship_class", ("object_class",), None, None)])
-        self.assertIn("relationship_parameters", exported)
-        self.assertEqual(
-            exported["relationship_parameters"], [("relationship_class", "relationship_parameter", None, None, None)]
-        )
-        self.assertIn("relationships", exported)
-        self.assertEqual(exported["relationships"], [("relationship_class", ("object",))])
-        self.assertIn("relationship_parameter_values", exported)
-        self.assertEqual(
-            exported["relationship_parameter_values"],
-            [("relationship_class", ("object",), "relationship_parameter", 3.14, "Base")],
-        )
-        self.assertIn("parameter_value_lists", exported)
-        self.assertEqual(exported["parameter_value_lists"], [("value_list", "5.5"), ("value_list", "6.4")])
-        self.assertIn("alternatives", exported)
-        self.assertEqual(exported["alternatives"], [("Base", "Base alternative"), ("alternative", None)])
-        self.assertIn("scenarios", exported)
-        self.assertEqual(exported["scenarios"], [("scenario", False, None)])
-        self.assertIn("scenario_alternatives", exported)
-        self.assertEqual(exported["scenario_alternatives"], [("scenario", "alternative", None)])
-
-
-if __name__ == '__main__':
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for export_functions.
+
+"""
+
+import unittest
+from spinedb_api import (
+    DiffDatabaseMapping,
+    export_alternatives,
+    export_data,
+    export_scenarios,
+    export_scenario_alternatives,
+    export_tools,
+    export_features,
+    export_tool_features,
+    export_tool_feature_methods,
+    import_alternatives,
+    import_object_classes,
+    import_object_parameter_values,
+    import_object_parameters,
+    import_objects,
+    import_parameter_value_lists,
+    import_relationship_classes,
+    import_relationship_parameter_values,
+    import_relationship_parameters,
+    import_relationships,
+    import_scenarios,
+    import_scenario_alternatives,
+    import_tools,
+    import_features,
+    import_tool_features,
+    import_tool_feature_methods,
+)
+
+
+class TestExportFunctions(unittest.TestCase):
+    def setUp(self):
+        db_url = "sqlite://"
+        self._db_map = DiffDatabaseMapping(db_url, username="UnitTest", create=True)
+
+    def tearDown(self):
+        self._db_map.connection.close()
+
+    def test_export_tools(self):
+        import_tools(self._db_map, [("tool", "Description")])
+        exported = export_tools(self._db_map)
+        self.assertEqual(exported, [("tool", "Description")])
+
+    def test_export_features(self):
+        import_object_classes(self._db_map, ["object_class1", "object_class2"])
+        import_parameter_value_lists(self._db_map, [['value_list', 'value1'], ['value_list', 'value2']])
+        import_object_parameters(self._db_map, [["object_class1", "parameter1", "value1", "value_list"]])
+        import_features(self._db_map, [["object_class1", "parameter1", "Description"]])
+        exported = export_features(self._db_map)
+        self.assertEqual(exported, [("object_class1", "parameter1", "value_list", "Description")])
+
+    def test_export_tool_features(self):
+        import_object_classes(self._db_map, ["object_class1", "object_class2"])
+        import_parameter_value_lists(self._db_map, [['value_list', 'value1'], ['value_list', 'value2']])
+        import_object_parameters(self._db_map, [["object_class1", "parameter1", "value1", "value_list"]])
+        import_features(self._db_map, [["object_class1", "parameter1", "Description"]])
+        import_tools(self._db_map, ["tool1"])
+        import_tool_features(self._db_map, [["tool1", "object_class1", "parameter1"]])
+        exported = export_tool_features(self._db_map)
+        self.assertEqual(exported, [("tool1", "object_class1", "parameter1", False)])
+
+    def test_export_tool_feature_methods(self):
+        import_object_classes(self._db_map, ["object_class1", "object_class2"])
+        import_parameter_value_lists(self._db_map, [['value_list', 'value1'], ['value_list', 'value2']])
+        import_object_parameters(self._db_map, [["object_class1", "parameter1", "value1", "value_list"]])
+        import_features(self._db_map, [["object_class1", "parameter1", "Description"]])
+        import_tools(self._db_map, ["tool1"])
+        import_tool_features(self._db_map, [["tool1", "object_class1", "parameter1"]])
+        import_tool_feature_methods(self._db_map, [["tool1", "object_class1", "parameter1", "value2"]])
+        exported = export_tool_feature_methods(self._db_map)
+        self.assertEqual(exported, [("tool1", "object_class1", "parameter1", "value2")])
+
+    def test_export_alternatives(self):
+        import_alternatives(self._db_map, [("alternative", "Description")])
+        exported = export_alternatives(self._db_map)
+        self.assertEqual(exported, [("Base", "Base alternative"), ("alternative", "Description")])
+
+    def test_export_scenarios(self):
+        import_scenarios(self._db_map, [("scenario", False, "Description")])
+        exported = export_scenarios(self._db_map)
+        self.assertEqual(exported, [("scenario", False, "Description")])
+
+    def test_export_scenario_alternatives(self):
+        import_alternatives(self._db_map, ["alternative"])
+        import_scenarios(self._db_map, ["scenario"])
+        import_scenario_alternatives(self._db_map, (("scenario", "alternative"),))
+        exported = export_scenario_alternatives(self._db_map)
+        self.assertEqual(exported, [("scenario", "alternative", None)])
+
+    def test_export_multiple_scenario_alternatives(self):
+        import_alternatives(self._db_map, ["alternative1"])
+        import_alternatives(self._db_map, ["alternative2"])
+        import_scenarios(self._db_map, ["scenario"])
+        import_scenario_alternatives(self._db_map, (("scenario", "alternative1"),))
+        import_scenario_alternatives(self._db_map, (("scenario", "alternative2", "alternative1"),))
+        exported = export_scenario_alternatives(self._db_map)
+        self.assertEqual(
+            set(exported), {("scenario", "alternative2", "alternative1"), ("scenario", "alternative1", None)}
+        )
+
+    def test_export_data(self):
+        import_object_classes(self._db_map, ["object_class"])
+        import_object_parameters(self._db_map, [("object_class", "object_parameter")])
+        import_objects(self._db_map, [("object_class", "object")])
+        import_object_parameter_values(self._db_map, [("object_class", "object", "object_parameter", 2.3)])
+        import_relationship_classes(self._db_map, [("relationship_class", ["object_class"])])
+        import_relationship_parameters(self._db_map, [("relationship_class", "relationship_parameter")])
+        import_relationships(self._db_map, [("relationship_class", ["object"])])
+        import_relationship_parameter_values(
+            self._db_map, [("relationship_class", ["object"], "relationship_parameter", 3.14)]
+        )
+        import_parameter_value_lists(self._db_map, [("value_list", "5.5"), ("value_list", "6.4")])
+        import_alternatives(self._db_map, ["alternative"])
+        import_scenarios(self._db_map, ["scenario"])
+        import_scenario_alternatives(self._db_map, [("scenario", "alternative")])
+        exported = export_data(self._db_map)
+        self.assertEqual(len(exported), 12)
+        self.assertIn("object_classes", exported)
+        self.assertEqual(exported["object_classes"], [("object_class", None, None)])
+        self.assertIn("object_parameters", exported)
+        self.assertEqual(exported["object_parameters"], [("object_class", "object_parameter", None, None, None)])
+        self.assertIn("objects", exported)
+        self.assertEqual(exported["objects"], [("object_class", "object", None)])
+        self.assertIn("object_parameter_values", exported)
+        self.assertEqual(
+            exported["object_parameter_values"], [("object_class", "object", "object_parameter", 2.3, "Base")]
+        )
+        self.assertIn("relationship_classes", exported)
+        self.assertEqual(exported["relationship_classes"], [("relationship_class", ("object_class",), None, None)])
+        self.assertIn("relationship_parameters", exported)
+        self.assertEqual(
+            exported["relationship_parameters"], [("relationship_class", "relationship_parameter", None, None, None)]
+        )
+        self.assertIn("relationships", exported)
+        self.assertEqual(exported["relationships"], [("relationship_class", ("object",))])
+        self.assertIn("relationship_parameter_values", exported)
+        self.assertEqual(
+            exported["relationship_parameter_values"],
+            [("relationship_class", ("object",), "relationship_parameter", 3.14, "Base")],
+        )
+        self.assertIn("parameter_value_lists", exported)
+        self.assertEqual(exported["parameter_value_lists"], [("value_list", "5.5"), ("value_list", "6.4")])
+        self.assertIn("alternatives", exported)
+        self.assertEqual(exported["alternatives"], [("Base", "Base alternative"), ("alternative", None)])
+        self.assertIn("scenarios", exported)
+        self.assertEqual(exported["scenarios"], [("scenario", False, None)])
+        self.assertIn("scenario_alternatives", exported)
+        self.assertEqual(exported["scenario_alternatives"], [("scenario", "alternative", None)])
+
+
+if __name__ == '__main__':
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/test_helpers.py` & `spinedb_api-0.30.4/tests/test_helpers.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,44 +1,44 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for helpers.py.
-
-"""
-
-
-import unittest
-from spinedb_api.helpers import compare_schemas, create_new_spine_database
-
-
-class TestHelpers(unittest.TestCase):
-    def setUp(self):
-        pass
-
-    def tearDown(self):
-        pass
-
-    def test_same_schema(self):
-        """Test that importing object class works"""
-        engine1 = create_new_spine_database('sqlite://')
-        engine2 = create_new_spine_database('sqlite://')
-        self.assertTrue(compare_schemas(engine1, engine2))
-
-    def test_different_schema(self):
-        """Test that importing object class works"""
-        engine1 = create_new_spine_database('sqlite://')
-        engine2 = create_new_spine_database('sqlite://')
-        engine2.execute("drop table entity_type")
-        self.assertFalse(compare_schemas(engine1, engine2))
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for helpers.py.
+
+"""
+
+
+import unittest
+from spinedb_api.helpers import compare_schemas, create_new_spine_database
+
+
+class TestHelpers(unittest.TestCase):
+    def setUp(self):
+        pass
+
+    def tearDown(self):
+        pass
+
+    def test_same_schema(self):
+        """Test that importing object class works"""
+        engine1 = create_new_spine_database('sqlite://')
+        engine2 = create_new_spine_database('sqlite://')
+        self.assertTrue(compare_schemas(engine1, engine2))
+
+    def test_different_schema(self):
+        """Test that importing object class works"""
+        engine1 = create_new_spine_database('sqlite://')
+        engine2 = create_new_spine_database('sqlite://')
+        engine2.execute("drop table entity_type")
+        self.assertFalse(compare_schemas(engine1, engine2))
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/test_mapping.py` & `spinedb_api-0.30.4/tests/test_mapping.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,52 +1,52 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for :mod:`spinedb_api.mapping`.
-
-"""
-import unittest
-from spinedb_api.mapping import Mapping, Position, value_index, unflatten
-
-
-class TestMapping(unittest.TestCase):
-    def test_value_index(self):
-        mapping = Mapping(0)
-        self.assertEqual(value_index(mapping.flatten()), 0)
-        mapping.position = Position.hidden
-        self.assertEqual(value_index(mapping.flatten()), -1)
-        mapping.child = Mapping(0)
-        self.assertEqual(value_index(mapping.flatten()), 1)
-        mapping.child.position = Position.hidden
-        self.assertEqual(value_index(mapping.flatten()), -1)
-        mapping.position = 0
-        self.assertEqual(value_index(mapping.flatten()), 0)
-
-    def test_non_pivoted_columns(self):
-        root_mapping = unflatten([Mapping(5), Mapping(Position.hidden)])
-        self.assertEqual(root_mapping.non_pivoted_columns(), [5])
-
-    def test_non_pivoted_columns_when_non_tail_mapping_is_pivoted(self):
-        root_mapping = unflatten([Mapping(5), Mapping(Position.hidden), Mapping(-1), Mapping(13), Mapping(23)])
-        self.assertEqual(root_mapping.non_pivoted_columns(), [5, 13])
-
-    def test_is_pivoted_returns_true_when_position_is_pivoted(self):
-        mapping = Mapping(-1)
-        self.assertTrue(mapping.is_pivoted())
-
-    def test_is_pivoted_returns_false_when_all_mappings_are_non_pivoted(self):
-        mappings = [Mapping(0), Mapping(1)]
-        root = unflatten(mappings)
-        self.assertFalse(root.is_pivoted())
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for :mod:`spinedb_api.mapping`.
+
+"""
+import unittest
+from spinedb_api.mapping import Mapping, Position, value_index, unflatten
+
+
+class TestMapping(unittest.TestCase):
+    def test_value_index(self):
+        mapping = Mapping(0)
+        self.assertEqual(value_index(mapping.flatten()), 0)
+        mapping.position = Position.hidden
+        self.assertEqual(value_index(mapping.flatten()), -1)
+        mapping.child = Mapping(0)
+        self.assertEqual(value_index(mapping.flatten()), 1)
+        mapping.child.position = Position.hidden
+        self.assertEqual(value_index(mapping.flatten()), -1)
+        mapping.position = 0
+        self.assertEqual(value_index(mapping.flatten()), 0)
+
+    def test_non_pivoted_columns(self):
+        root_mapping = unflatten([Mapping(5), Mapping(Position.hidden)])
+        self.assertEqual(root_mapping.non_pivoted_columns(), [5])
+
+    def test_non_pivoted_columns_when_non_tail_mapping_is_pivoted(self):
+        root_mapping = unflatten([Mapping(5), Mapping(Position.hidden), Mapping(-1), Mapping(13), Mapping(23)])
+        self.assertEqual(root_mapping.non_pivoted_columns(), [5, 13])
+
+    def test_is_pivoted_returns_true_when_position_is_pivoted(self):
+        mapping = Mapping(-1)
+        self.assertTrue(mapping.is_pivoted())
+
+    def test_is_pivoted_returns_false_when_all_mappings_are_non_pivoted(self):
+        mappings = [Mapping(0), Mapping(1)]
+        root = unflatten(mappings)
+        self.assertFalse(root.is_pivoted())
+
+
+if __name__ == "__main__":
+    unittest.main()
```

### Comparing `spinedb_api-0.30.3/tests/test_migration.py` & `spinedb_api-0.30.4/tests/test_migration.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,141 +1,141 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Unit tests for migration scripts.
-
-"""
-import os.path
-from tempfile import TemporaryDirectory
-import unittest
-from sqlalchemy import inspect
-from sqlalchemy.engine.url import URL
-from spinedb_api.helpers import create_new_spine_database, _create_first_spine_database, is_head_engine, schema_dict
-from spinedb_api import DiffDatabaseMapping
-
-
-class TestMigration(unittest.TestCase):
-    @unittest.skip(
-        "default_values's server_default has been changed from 0 to NULL in the create scrip, but there's no associated upgrade script yet."
-    )
-    def test_upgrade_schema(self):
-        """Tests that the upgrade scripts produce the same schema as the function to create
-        a Spine db anew.
-        """
-        left_engine = _create_first_spine_database("sqlite://")
-        is_head_engine(left_engine, upgrade=True)
-        left_insp = inspect(left_engine)
-        left_dict = schema_dict(left_insp)
-        right_engine = create_new_spine_database("sqlite://")
-        right_insp = inspect(right_engine)
-        right_dict = schema_dict(right_insp)
-        self.maxDiff = None
-        self.assertEqual(str(left_dict), str(right_dict))
-
-        left_ver = left_engine.execute("SELECT version_num FROM alembic_version").fetchall()
-        right_ver = right_engine.execute("SELECT version_num FROM alembic_version").fetchall()
-        self.assertEqual(left_ver, right_ver)
-
-        left_ent_typ = left_engine.execute("SELECT * FROM entity_type").fetchall()
-        right_ent_typ = right_engine.execute("SELECT * FROM entity_type").fetchall()
-        left_ent_cls_typ = left_engine.execute("SELECT * FROM entity_class_type").fetchall()
-        right_ent_cls_typ = right_engine.execute("SELECT * FROM entity_class_type").fetchall()
-        self.assertEqual(left_ent_typ, right_ent_typ)
-        self.assertEqual(left_ent_cls_typ, right_ent_cls_typ)
-
-    def test_upgrade_content(self):
-        """Tests that the upgrade scripts when applied on a db that has some contents
-        persist that content entirely.
-        """
-        with TemporaryDirectory() as temp_dir:
-            db_url = URL("sqlite")
-            db_url.database = os.path.join(temp_dir, "test_upgrade_content.sqlite")
-            # Create *first* spine db
-            engine = _create_first_spine_database(db_url)
-            # Insert basic stuff
-            engine.execute("INSERT INTO object_class (id, name) VALUES (1, 'dog')")
-            engine.execute("INSERT INTO object_class (id, name) VALUES (2, 'fish')")
-            engine.execute("INSERT INTO object (id, class_id, name) VALUES (1, 1, 'pluto')")
-            engine.execute("INSERT INTO object (id, class_id, name) VALUES (2, 1, 'scooby')")
-            engine.execute("INSERT INTO object (id, class_id, name) VALUES (3, 2, 'nemo')")
-            engine.execute(
-                "INSERT INTO relationship_class (id, name, dimension, object_class_id) VALUES (1, 'dog__fish', 0, 1)"
-            )
-            engine.execute(
-                "INSERT INTO relationship_class (id, name, dimension, object_class_id) VALUES (1, 'dog__fish', 1, 2)"
-            )
-            engine.execute(
-                "INSERT INTO relationship (id, class_id, name, dimension, object_id) VALUES (1, 1, 'pluto__nemo', 0, 1)"
-            )
-            engine.execute(
-                "INSERT INTO relationship (id, class_id, name, dimension, object_id) VALUES (1, 1, 'pluto__nemo', 1, 3)"
-            )
-            engine.execute(
-                "INSERT INTO relationship (id, class_id, name, dimension, object_id) VALUES (2, 1, 'scooby__nemo', 0, 2)"
-            )
-            engine.execute(
-                "INSERT INTO relationship (id, class_id, name, dimension, object_id) VALUES (2, 1, 'scooby__nemo', 1, 3)"
-            )
-            engine.execute("INSERT INTO parameter (id, object_class_id, name) VALUES (1, 1, 'breed')")
-            engine.execute("INSERT INTO parameter (id, object_class_id, name) VALUES (2, 2, 'water')")
-            engine.execute("INSERT INTO parameter (id, relationship_class_id, name) VALUES (3, 1, 'relative_speed')")
-            engine.execute("INSERT INTO parameter_value (parameter_id, object_id, value) VALUES (1, 1, '\"labrador\"')")
-            engine.execute("INSERT INTO parameter_value (parameter_id, object_id, value) VALUES (1, 2, '\"big dane\"')")
-            engine.execute("INSERT INTO parameter_value (parameter_id, relationship_id, value) VALUES (3, 1, '100')")
-            engine.execute("INSERT INTO parameter_value (parameter_id, relationship_id, value) VALUES (3, 2, '-1')")
-            # Upgrade the db and check that our stuff is still there
-            db_map = DiffDatabaseMapping(db_url, upgrade=True)
-            object_classes = {x.id: x.name for x in db_map.object_class_list()}
-            objects = {x.id: (object_classes[x.class_id], x.name) for x in db_map.object_list()}
-            rel_clss = {x.id: (x.name, x.object_class_name_list) for x in db_map.wide_relationship_class_list()}
-            rels = {
-                x.id: (rel_clss[x.class_id][0], x.name, x.object_name_list) for x in db_map.wide_relationship_list()
-            }
-            obj_par_defs = {
-                x.id: (object_classes[x.object_class_id], x.parameter_name)
-                for x in db_map.object_parameter_definition_list()
-            }
-            rel_par_defs = {
-                x.id: (rel_clss[x.relationship_class_id][0], x.parameter_name)
-                for x in db_map.relationship_parameter_definition_list()
-            }
-            obj_par_vals = {
-                (obj_par_defs[x.parameter_id][1], objects[x.object_id][1], x.value)
-                for x in db_map.object_parameter_value_list()
-            }
-            rel_par_vals = {
-                (rel_par_defs[x.parameter_id][1], rels[x.relationship_id][1], x.value)
-                for x in db_map.relationship_parameter_value_list()
-            }
-            self.assertTrue(len(object_classes), 2)
-            self.assertTrue(len(objects), 3)
-            self.assertTrue(len(rel_clss), 1)
-            self.assertTrue(len(rels), 2)
-            self.assertTrue(len(obj_par_defs), 2)
-            self.assertTrue(len(rel_par_defs), 1)
-            self.assertTrue(len(obj_par_vals), 2)
-            self.assertTrue(len(rel_par_vals), 2)
-            self.assertTrue('dog' in object_classes.values())
-            self.assertTrue('fish' in object_classes.values())
-            self.assertTrue(('dog', 'pluto') in objects.values())
-            self.assertTrue(('dog', 'scooby') in objects.values())
-            self.assertTrue(('fish', 'nemo') in objects.values())
-            self.assertTrue(('dog__fish', 'dog,fish') in rel_clss.values())
-            self.assertTrue(('dog__fish', 'pluto__nemo', 'pluto,nemo') in rels.values())
-            self.assertTrue(('dog__fish', 'scooby__nemo', 'scooby,nemo') in rels.values())
-            self.assertTrue(('dog', 'breed') in obj_par_defs.values())
-            self.assertTrue(('fish', 'water') in obj_par_defs.values())
-            self.assertTrue(('dog__fish', 'relative_speed') in rel_par_defs.values())
-            self.assertTrue(('breed', 'scooby', b'"big dane"') in obj_par_vals)
-            self.assertTrue(('breed', 'pluto', b'"labrador"') in obj_par_vals)
-            self.assertTrue(('relative_speed', 'pluto__nemo', b'100') in rel_par_vals)
-            self.assertTrue(('relative_speed', 'scooby__nemo', b'-1') in rel_par_vals)
-            db_map.connection.close()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Unit tests for migration scripts.
+
+"""
+import os.path
+from tempfile import TemporaryDirectory
+import unittest
+from sqlalchemy import inspect
+from sqlalchemy.engine.url import URL
+from spinedb_api.helpers import create_new_spine_database, _create_first_spine_database, is_head_engine, schema_dict
+from spinedb_api import DiffDatabaseMapping
+
+
+class TestMigration(unittest.TestCase):
+    @unittest.skip(
+        "default_values's server_default has been changed from 0 to NULL in the create scrip, but there's no associated upgrade script yet."
+    )
+    def test_upgrade_schema(self):
+        """Tests that the upgrade scripts produce the same schema as the function to create
+        a Spine db anew.
+        """
+        left_engine = _create_first_spine_database("sqlite://")
+        is_head_engine(left_engine, upgrade=True)
+        left_insp = inspect(left_engine)
+        left_dict = schema_dict(left_insp)
+        right_engine = create_new_spine_database("sqlite://")
+        right_insp = inspect(right_engine)
+        right_dict = schema_dict(right_insp)
+        self.maxDiff = None
+        self.assertEqual(str(left_dict), str(right_dict))
+
+        left_ver = left_engine.execute("SELECT version_num FROM alembic_version").fetchall()
+        right_ver = right_engine.execute("SELECT version_num FROM alembic_version").fetchall()
+        self.assertEqual(left_ver, right_ver)
+
+        left_ent_typ = left_engine.execute("SELECT * FROM entity_type").fetchall()
+        right_ent_typ = right_engine.execute("SELECT * FROM entity_type").fetchall()
+        left_ent_cls_typ = left_engine.execute("SELECT * FROM entity_class_type").fetchall()
+        right_ent_cls_typ = right_engine.execute("SELECT * FROM entity_class_type").fetchall()
+        self.assertEqual(left_ent_typ, right_ent_typ)
+        self.assertEqual(left_ent_cls_typ, right_ent_cls_typ)
+
+    def test_upgrade_content(self):
+        """Tests that the upgrade scripts when applied on a db that has some contents
+        persist that content entirely.
+        """
+        with TemporaryDirectory() as temp_dir:
+            db_url = URL("sqlite")
+            db_url.database = os.path.join(temp_dir, "test_upgrade_content.sqlite")
+            # Create *first* spine db
+            engine = _create_first_spine_database(db_url)
+            # Insert basic stuff
+            engine.execute("INSERT INTO object_class (id, name) VALUES (1, 'dog')")
+            engine.execute("INSERT INTO object_class (id, name) VALUES (2, 'fish')")
+            engine.execute("INSERT INTO object (id, class_id, name) VALUES (1, 1, 'pluto')")
+            engine.execute("INSERT INTO object (id, class_id, name) VALUES (2, 1, 'scooby')")
+            engine.execute("INSERT INTO object (id, class_id, name) VALUES (3, 2, 'nemo')")
+            engine.execute(
+                "INSERT INTO relationship_class (id, name, dimension, object_class_id) VALUES (1, 'dog__fish', 0, 1)"
+            )
+            engine.execute(
+                "INSERT INTO relationship_class (id, name, dimension, object_class_id) VALUES (1, 'dog__fish', 1, 2)"
+            )
+            engine.execute(
+                "INSERT INTO relationship (id, class_id, name, dimension, object_id) VALUES (1, 1, 'pluto__nemo', 0, 1)"
+            )
+            engine.execute(
+                "INSERT INTO relationship (id, class_id, name, dimension, object_id) VALUES (1, 1, 'pluto__nemo', 1, 3)"
+            )
+            engine.execute(
+                "INSERT INTO relationship (id, class_id, name, dimension, object_id) VALUES (2, 1, 'scooby__nemo', 0, 2)"
+            )
+            engine.execute(
+                "INSERT INTO relationship (id, class_id, name, dimension, object_id) VALUES (2, 1, 'scooby__nemo', 1, 3)"
+            )
+            engine.execute("INSERT INTO parameter (id, object_class_id, name) VALUES (1, 1, 'breed')")
+            engine.execute("INSERT INTO parameter (id, object_class_id, name) VALUES (2, 2, 'water')")
+            engine.execute("INSERT INTO parameter (id, relationship_class_id, name) VALUES (3, 1, 'relative_speed')")
+            engine.execute("INSERT INTO parameter_value (parameter_id, object_id, value) VALUES (1, 1, '\"labrador\"')")
+            engine.execute("INSERT INTO parameter_value (parameter_id, object_id, value) VALUES (1, 2, '\"big dane\"')")
+            engine.execute("INSERT INTO parameter_value (parameter_id, relationship_id, value) VALUES (3, 1, '100')")
+            engine.execute("INSERT INTO parameter_value (parameter_id, relationship_id, value) VALUES (3, 2, '-1')")
+            # Upgrade the db and check that our stuff is still there
+            db_map = DiffDatabaseMapping(db_url, upgrade=True)
+            object_classes = {x.id: x.name for x in db_map.object_class_list()}
+            objects = {x.id: (object_classes[x.class_id], x.name) for x in db_map.object_list()}
+            rel_clss = {x.id: (x.name, x.object_class_name_list) for x in db_map.wide_relationship_class_list()}
+            rels = {
+                x.id: (rel_clss[x.class_id][0], x.name, x.object_name_list) for x in db_map.wide_relationship_list()
+            }
+            obj_par_defs = {
+                x.id: (object_classes[x.object_class_id], x.parameter_name)
+                for x in db_map.object_parameter_definition_list()
+            }
+            rel_par_defs = {
+                x.id: (rel_clss[x.relationship_class_id][0], x.parameter_name)
+                for x in db_map.relationship_parameter_definition_list()
+            }
+            obj_par_vals = {
+                (obj_par_defs[x.parameter_id][1], objects[x.object_id][1], x.value)
+                for x in db_map.object_parameter_value_list()
+            }
+            rel_par_vals = {
+                (rel_par_defs[x.parameter_id][1], rels[x.relationship_id][1], x.value)
+                for x in db_map.relationship_parameter_value_list()
+            }
+            self.assertTrue(len(object_classes), 2)
+            self.assertTrue(len(objects), 3)
+            self.assertTrue(len(rel_clss), 1)
+            self.assertTrue(len(rels), 2)
+            self.assertTrue(len(obj_par_defs), 2)
+            self.assertTrue(len(rel_par_defs), 1)
+            self.assertTrue(len(obj_par_vals), 2)
+            self.assertTrue(len(rel_par_vals), 2)
+            self.assertTrue('dog' in object_classes.values())
+            self.assertTrue('fish' in object_classes.values())
+            self.assertTrue(('dog', 'pluto') in objects.values())
+            self.assertTrue(('dog', 'scooby') in objects.values())
+            self.assertTrue(('fish', 'nemo') in objects.values())
+            self.assertTrue(('dog__fish', 'dog,fish') in rel_clss.values())
+            self.assertTrue(('dog__fish', 'pluto__nemo', 'pluto,nemo') in rels.values())
+            self.assertTrue(('dog__fish', 'scooby__nemo', 'scooby,nemo') in rels.values())
+            self.assertTrue(('dog', 'breed') in obj_par_defs.values())
+            self.assertTrue(('fish', 'water') in obj_par_defs.values())
+            self.assertTrue(('dog__fish', 'relative_speed') in rel_par_defs.values())
+            self.assertTrue(('breed', 'scooby', b'"big dane"') in obj_par_vals)
+            self.assertTrue(('breed', 'pluto', b'"labrador"') in obj_par_vals)
+            self.assertTrue(('relative_speed', 'pluto__nemo', b'100') in rel_par_vals)
+            self.assertTrue(('relative_speed', 'scooby__nemo', b'-1') in rel_par_vals)
+            db_map.connection.close()
```

### Comparing `spinedb_api-0.30.3/tests/test_parameter_value.py` & `spinedb_api-0.30.4/tests/test_parameter_value.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,1004 +1,1004 @@
-######################################################################################################################
-# Copyright (C) 2017-2022 Spine project consortium
-# This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
-# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
-# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
-# this program. If not, see <http://www.gnu.org/licenses/>.
-######################################################################################################################
-
-"""
-Tests for the parameter_value module.
-
-"""
-
-from datetime import datetime
-import json
-import unittest
-import dateutil.parser
-from dateutil.relativedelta import relativedelta
-import numpy as np
-import numpy.testing
-from spinedb_api.parameter_value import (
-    convert_containers_to_maps,
-    convert_leaf_maps_to_specialized_containers,
-    convert_map_to_table,
-    duration_to_relativedelta,
-    relativedelta_to_duration,
-    from_database,
-    to_database,
-    Array,
-    DateTime,
-    Duration,
-    IndexedNumberArray,
-    Map,
-    TimePattern,
-    TimeSeriesFixedResolution,
-    TimeSeriesVariableResolution,
-    TimeSeries,
-)
-
-
-class TestParameterValue(unittest.TestCase):
-    """Test for the free functions and classes in parameter_value."""
-
-    def test_duration_to_relativedelta_seconds(self):
-        delta = duration_to_relativedelta("7s")
-        self.assertEqual(delta, relativedelta(seconds=7))
-        delta = duration_to_relativedelta("1 second")
-        self.assertEqual(delta, relativedelta(seconds=1))
-        delta = duration_to_relativedelta("7 seconds")
-        self.assertEqual(delta, relativedelta(seconds=7))
-        delta = duration_to_relativedelta("99 seconds")
-        self.assertEqual(delta, relativedelta(minutes=1, seconds=39))
-
-    def test_relativedelta_to_duration_seconds(self):
-        delta = duration_to_relativedelta("7s")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "7s")
-        delta = duration_to_relativedelta("9999999s")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "9999999s")
-
-    def test_duration_to_relativedelta_minutes(self):
-        delta = duration_to_relativedelta("7m")
-        self.assertEqual(delta, relativedelta(minutes=7))
-        delta = duration_to_relativedelta("1 minute")
-        self.assertEqual(delta, relativedelta(minutes=1))
-        delta = duration_to_relativedelta("7 minutes")
-        self.assertEqual(delta, relativedelta(minutes=7))
-
-    def test_relativedelta_to_duration_minutes(self):
-        delta = duration_to_relativedelta("7m")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "7m")
-        delta = duration_to_relativedelta("999999m")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "999999m")
-
-    def test_duration_to_relativedelta_hours(self):
-        delta = duration_to_relativedelta("7h")
-        self.assertEqual(delta, relativedelta(hours=7))
-        delta = duration_to_relativedelta("1 hour")
-        self.assertEqual(delta, relativedelta(hours=1))
-        delta = duration_to_relativedelta("7 hours")
-        self.assertEqual(delta, relativedelta(hours=7))
-
-    def test_relativedelta_to_duration_hours(self):
-        delta = duration_to_relativedelta("7h")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "7h")
-        delta = duration_to_relativedelta("99999h")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "99999h")
-
-    def test_duration_to_relativedelta_days(self):
-        delta = duration_to_relativedelta("7D")
-        self.assertEqual(delta, relativedelta(days=7))
-        delta = duration_to_relativedelta("1 day")
-        self.assertEqual(delta, relativedelta(days=1))
-        delta = duration_to_relativedelta("7 days")
-        self.assertEqual(delta, relativedelta(days=7))
-
-    def test_relativedelta_to_duration_days(self):
-        delta = duration_to_relativedelta("7D")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "7D")
-        delta = duration_to_relativedelta("9999D")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "9999D")
-
-    def test_duration_to_relativedelta_months(self):
-        delta = duration_to_relativedelta("7M")
-        self.assertEqual(delta, relativedelta(months=7))
-        delta = duration_to_relativedelta("1 month")
-        self.assertEqual(delta, relativedelta(months=1))
-        delta = duration_to_relativedelta("7 months")
-        self.assertEqual(delta, relativedelta(months=7))
-
-    def test_relativedelta_to_duration_months(self):
-        delta = duration_to_relativedelta("7M")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "7M")
-        delta = duration_to_relativedelta("99M")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "99M")
-
-    def test_duration_to_relativedelta_years(self):
-        delta = duration_to_relativedelta("7Y")
-        self.assertEqual(delta, relativedelta(years=7))
-        delta = duration_to_relativedelta("7Y")
-        self.assertEqual(delta, relativedelta(years=7))
-        delta = duration_to_relativedelta("1 year")
-        self.assertEqual(delta, relativedelta(years=1))
-        delta = duration_to_relativedelta("7 years")
-        self.assertEqual(delta, relativedelta(years=7))
-
-    def test_relativedelta_to_duration_years(self):
-        delta = duration_to_relativedelta("7Y")
-        duration = relativedelta_to_duration(delta)
-        self.assertEqual(duration, "7Y")
-
-    def test_from_database_plain_number(self):
-        database_value = b"23.0"
-        value = from_database(database_value, value_type=None)
-        self.assertTrue(isinstance(value, float))
-        self.assertEqual(value, 23.0)
-
-    def test_from_database_boolean(self):
-        database_value = b"true"
-        value = from_database(database_value, value_type=None)
-        self.assertTrue(isinstance(value, bool))
-        self.assertEqual(value, True)
-
-    def test_to_database_plain_number(self):
-        value = 23.0
-        database_value, value_type = to_database(value)
-        value_as_float = json.loads(database_value)
-        self.assertEqual(value_as_float, value)
-        self.assertIsNone(value_type)
-
-    def test_to_database_DateTime(self):
-        value = DateTime(datetime(year=2019, month=6, day=26, hour=12, minute=50, second=13))
-        database_value, value_type = to_database(value)
-        value_as_dict = json.loads(database_value)
-        self.assertEqual(value_as_dict, {"data": "2019-06-26T12:50:13"})
-        self.assertEqual(value_type, "date_time")
-
-    def test_from_database_DateTime(self):
-        database_value = b'{"data": "2019-06-01T22:15:00+01:00"}'
-        value = from_database(database_value, value_type="date_time")
-        self.assertEqual(value.value, dateutil.parser.parse("2019-06-01T22:15:00+01:00"))
-
-    def test_DateTime_to_database(self):
-        value = DateTime(datetime(year=2019, month=6, day=26, hour=10, minute=50, second=34))
-        database_value, value_type = value.to_database()
-        value_dict = json.loads(database_value)
-        self.assertEqual(value_dict, {"data": "2019-06-26T10:50:34"})
-        self.assertEqual(value_type, "date_time")
-
-    def test_from_database_Duration(self):
-        database_value = b'{"data": "4 seconds"}'
-        value = from_database(database_value, value_type="duration")
-        self.assertEqual(value.value, relativedelta(seconds=4))
-
-    def test_from_database_Duration_default_units(self):
-        database_value = b'{"data": 23}'
-        value = from_database(database_value, value_type="duration")
-        self.assertEqual(value.value, relativedelta(minutes=23))
-
-    def test_from_database_Duration_legacy_list_format_converted_to_Array(self):
-        database_value = b'{"data": ["1 hour", "1h", 60, "2 hours"]}'
-        value = from_database(database_value, value_type="duration")
-        expected = Array([Duration("1h"), Duration("1h"), Duration("1h"), Duration("2h")])
-        self.assertEqual(value, expected)
-
-    def test_Duration_to_database(self):
-        value = Duration(duration_to_relativedelta("8 years"))
-        database_value, value_type = value.to_database()
-        value_as_dict = json.loads(database_value)
-        self.assertEqual(value_as_dict, {"data": "8Y"})
-        self.assertEqual(value_type, "duration")
-
-    def test_from_database_TimePattern(self):
-        database_value = b"""
-        {
-          "data": {
-            "m1-4,m9-12": 300,
-            "m5-8": 221.5
-          }
-        }
-        """
-        value = from_database(database_value, value_type="time_pattern")
-        self.assertEqual(len(value), 2)
-        self.assertEqual(value.indexes, ["m1-4,m9-12", "m5-8"])
-        numpy.testing.assert_equal(value.values, numpy.array([300.0, 221.5]))
-        self.assertEqual(value.index_name, "p")
-
-    def test_from_database_TimePattern_with_index_name(self):
-        database_value = b"""
-        {
-          "index_name": "index",
-          "data": {
-            "M1-12": 300
-          }
-        }
-        """
-        value = from_database(database_value, value_type="time_pattern")
-        self.assertEqual(value.indexes, ["M1-12"])
-        numpy.testing.assert_equal(value.values, numpy.array([300.0]))
-        self.assertEqual(value.index_name, "index")
-
-    def test_TimePattern_to_database(self):
-        value = TimePattern(["M1-4,M9-12", "M5-8"], numpy.array([300.0, 221.5]))
-        database_value, value_type = value.to_database()
-        value_as_dict = json.loads(database_value)
-        self.assertEqual(value_as_dict, {"data": {"M1-4,M9-12": 300.0, "M5-8": 221.5}})
-        self.assertEqual(value_type, "time_pattern")
-
-    def test_TimePattern_to_database_with_integer_values(self):
-        value = TimePattern(["M1-4,M9-12", "M5-8"], [300, 221])
-        database_value, value_type = value.to_database()
-        value_as_dict = json.loads(database_value)
-        self.assertEqual(value_as_dict, {"data": {"M1-4,M9-12": 300.0, "M5-8": 221.0}})
-        self.assertEqual(value_type, "time_pattern")
-
-    def test_TimePattern_to_database_with_index_name(self):
-        value = TimePattern(["M1-12"], [300.0])
-        value.index_name = "index"
-        database_value, value_type = value.to_database()
-        value_as_dict = json.loads(database_value)
-        self.assertEqual(value_as_dict, {"index_name": "index", "data": {"M1-12": 300.0}})
-        self.assertEqual(value_type, "time_pattern")
-
-    def test_TimePattern_index_length_is_not_limited(self):
-        value = TimePattern(["M1-4", "M5-12"], [300, 221])
-        value.indexes[0] = "M1-2,M3-4,M5-6,M7-8,M9-10,M11-12"
-        value.indexes[1] = "M2-3,M4-5,M6-7,M8-9,M10-11"
-        self.assertEqual(list(value.indexes), ["M1-2,M3-4,M5-6,M7-8,M9-10,M11-12", "M2-3,M4-5,M6-7,M8-9,M10-11"])
-
-    def test_from_database_TimeSeriesVariableResolution_as_dictionary(self):
-        releases = b"""{
-                          "data": {
-                              "1977-05-25": 4,
-                              "1980-05-21": 5,
-                              "1983-05-25": 6
-                          }
-                      }"""
-        time_series = from_database(releases, value_type="time_series")
-        self.assertEqual(
-            time_series.indexes,
-            numpy.array(
-                [numpy.datetime64("1977-05-25"), numpy.datetime64("1980-05-21"), numpy.datetime64("1983-05-25")],
-                dtype="datetime64[D]",
-            ),
-        )
-        self.assertEqual(len(time_series), 3)
-        self.assertTrue(isinstance(time_series.values, numpy.ndarray))
-        numpy.testing.assert_equal(time_series.values, numpy.array([4, 5, 6]))
-        self.assertEqual(time_series.index_name, "t")
-
-    def test_from_database_TimeSeriesVariableResolution_as_dictionary_with_index_name(self):
-        releases = b"""{
-                          "data": {
-                              "1977-05-25": 4,
-                              "1980-05-21": 5
-                          },
-                          "index_name": "index"
-                      }"""
-        time_series = from_database(releases, value_type="time_series")
-        self.assertEqual(time_series.index_name, "index")
-
-    def test_from_database_TimeSeriesVariableResolution_as_two_column_array(self):
-        releases = b"""{
-                          "data": [
-                              ["1977-05-25", 4],
-                              ["1980-05-21", 5],
-                              ["1983-05-25", 6]
-                          ]
-                      }"""
-        time_series = from_database(releases, value_type="time_series")
-        self.assertEqual(
-            time_series.indexes,
-            numpy.array(
-                [numpy.datetime64("1977-05-25"), numpy.datetime64("1980-05-21"), numpy.datetime64("1983-05-25")],
-                dtype="datetime64[D]",
-            ),
-        )
-        self.assertEqual(len(time_series), 3)
-        self.assertTrue(isinstance(time_series.values, numpy.ndarray))
-        numpy.testing.assert_equal(time_series.values, numpy.array([4, 5, 6]))
-        self.assertEqual(time_series.index_name, "t")
-
-    def test_from_database_TimeSeriesVariableResolution_as_two_column_array_with_index_name(self):
-        releases = b"""{
-                          "data": [
-                              ["1977-05-25", 4],
-                              ["1980-05-21", 5]
-                          ],
-                          "index_name": "index"
-                      }"""
-        time_series = from_database(releases, value_type="time_series")
-        self.assertEqual(time_series.index_name, "index")
-
-    def test_from_database_TimeSeriesFixedResolution_default_repeat(self):
-        database_value = b"""{
-                                   "index": {
-                                       "ignore_year": true
-                                   },
-                                   "data": [["2019-07-02T10:00:00", 7.0],
-                                            ["2019-07-02T10:00:01", 4.0]]
-                               }"""
-        time_series = from_database(database_value, value_type="time_series")
-        self.assertTrue(time_series.ignore_year)
-        self.assertFalse(time_series.repeat)
-
-    def test_TimeSeriesVariableResolution_to_database(self):
-        dates = numpy.array(["1999-05-19", "2002-05-16", "2005-05-19"], dtype="datetime64[D]")
-        episodes = numpy.array([1, 2, 3], dtype=float)
-        value = TimeSeriesVariableResolution(dates, episodes, False, False)
-        db_value, value_type = value.to_database()
-        releases = json.loads(db_value)
-        self.assertEqual(releases, {"data": {"1999-05-19": 1, "2002-05-16": 2, "2005-05-19": 3}})
-        self.assertEqual(value_type, "time_series")
-
-    def test_TimeSeriesVariableResolution_to_database_with_index_name(self):
-        dates = numpy.array(["2002-05-16", "2005-05-19"], dtype="datetime64[D]")
-        episodes = numpy.array([1, 2], dtype=float)
-        value = TimeSeriesVariableResolution(dates, episodes, False, False, "index")
-        db_value, value_type = value.to_database()
-        releases = json.loads(db_value)
-        self.assertEqual(releases, {"index_name": "index", "data": {"2002-05-16": 1, "2005-05-19": 2}})
-        self.assertEqual(value_type, "time_series")
-
-    def test_TimeSeriesVariableResolution_to_database_with_ignore_year_and_repeat(self):
-        dates = numpy.array(["1999-05-19", "2002-05-16", "2005-05-19"], dtype="datetime64[D]")
-        episodes = numpy.array([1, 2, 3], dtype=float)
-        value = TimeSeriesVariableResolution(dates, episodes, True, True)
-        db_value, value_type = value.to_database()
-        releases = json.loads(db_value)
-        self.assertEqual(
-            releases,
-            {
-                "data": {"1999-05-19": 1, "2002-05-16": 2, "2005-05-19": 3},
-                "index": {"ignore_year": True, "repeat": True},
-            },
-        )
-        self.assertEqual(value_type, "time_series")
-
-    def test_from_database_TimeSeriesFixedResolution(self):
-        days_of_our_lives = b"""{
-                                   "index": {
-                                       "start": "2019-03-23",
-                                       "resolution": "1 day",
-                                       "ignore_year": false,
-                                       "repeat": false
-                                   },
-                                   "data": [7.0, 5.0, 8.1]
-                               }"""
-        time_series = from_database(days_of_our_lives, value_type="time_series")
-        self.assertEqual(len(time_series), 3)
-        self.assertEqual(
-            time_series.indexes,
-            numpy.array(
-                [numpy.datetime64("2019-03-23"), numpy.datetime64("2019-03-24"), numpy.datetime64("2019-03-25")],
-                dtype="datetime64[s]",
-            ),
-        )
-        self.assertTrue(isinstance(time_series.values, numpy.ndarray))
-        numpy.testing.assert_equal(time_series.values, numpy.array([7.0, 5.0, 8.1]))
-        self.assertEqual(time_series.start, dateutil.parser.parse("2019-03-23"))
-        self.assertEqual(len(time_series.resolution), 1)
-        self.assertEqual(time_series.resolution[0], relativedelta(days=1))
-        self.assertFalse(time_series.ignore_year)
-        self.assertFalse(time_series.repeat)
-        self.assertEqual(time_series.index_name, "t")
-
-    def test_from_database_TimeSeriesFixedResolution_no_index(self):
-        database_value = b"""{
-                                "data": [1, 2, 3, 4, 5, 8]
-                            }
-        """
-        time_series = from_database(database_value, value_type="time_series")
-        self.assertEqual(len(time_series), 6)
-        self.assertEqual(
-            time_series.indexes,
-            numpy.array(
-                [
-                    numpy.datetime64("0001-01-01T00:00:00"),
-                    numpy.datetime64("0001-01-01T01:00:00"),
-                    numpy.datetime64("0001-01-01T02:00:00"),
-                    numpy.datetime64("0001-01-01T03:00:00"),
-                    numpy.datetime64("0001-01-01T04:00:00"),
-                    numpy.datetime64("0001-01-01T05:00:00"),
-                ],
-                dtype="datetime64[s]",
-            ),
-        )
-        numpy.testing.assert_equal(time_series.values, numpy.array([1.0, 2.0, 3.0, 4.0, 5.0, 8.0]))
-        self.assertEqual(time_series.start, dateutil.parser.parse("0001-01-01T00:00:00"))
-        self.assertEqual(len(time_series.resolution), 1)
-        self.assertEqual(time_series.resolution[0], relativedelta(hours=1))
-        self.assertTrue(time_series.ignore_year)
-        self.assertTrue(time_series.repeat)
-
-    def test_from_database_TimeSeriesFixedResolution_index_name(self):
-        database_value = b"""{
-                                "data": [1],
-                                "index_name": "index"
-                            }
-        """
-        time_series = from_database(database_value, value_type="time_series")
-        self.assertEqual(time_series.index_name, "index")
-
-    def test_from_database_TimeSeriesFixedResolution_resolution_list(self):
-        database_value = b"""{
-                                "index": {
-                                    "start": "2019-01-31",
-                                    "resolution": ["1 day", "1M"],
-                                    "ignore_year": false,
-                                    "repeat": false
-                                },
-                                "data": [7.0, 5.0, 8.1, -4.1]
-                            }"""
-        time_series = from_database(database_value, value_type="time_series")
-        self.assertEqual(len(time_series), 4)
-        self.assertEqual(
-            time_series.indexes,
-            numpy.array(
-                [
-                    numpy.datetime64("2019-01-31"),
-                    numpy.datetime64("2019-02-01"),
-                    numpy.datetime64("2019-03-01"),
-                    numpy.datetime64("2019-03-02"),
-                ],
-                dtype="datetime64[s]",
-            ),
-        )
-        numpy.testing.assert_equal(time_series.values, numpy.array([7.0, 5.0, 8.1, -4.1]))
-        self.assertEqual(time_series.start, dateutil.parser.parse("2019-01-31"))
-        self.assertEqual(len(time_series.resolution), 2)
-        self.assertEqual(time_series.resolution, [relativedelta(days=1), relativedelta(months=1)])
-        self.assertFalse(time_series.ignore_year)
-        self.assertFalse(time_series.repeat)
-
-    def test_from_database_TimeSeriesFixedResolution_default_resolution_is_1hour(self):
-        database_value = b"""{
-                                   "index": {
-                                       "start": "2019-03-23",
-                                       "ignore_year": false,
-                                       "repeat": false
-                                   },
-                                   "data": [7.0, 5.0, 8.1]
-                               }"""
-        time_series = from_database(database_value, value_type="time_series")
-        self.assertEqual(len(time_series), 3)
-        self.assertEqual(len(time_series.resolution), 1)
-        self.assertEqual(time_series.resolution[0], relativedelta(hours=1))
-
-    def test_from_database_TimeSeriesFixedResolution_default_resolution_unit_is_minutes(self):
-        database_value = b"""{
-                                   "index": {
-                                       "start": "2019-03-23",
-                                       "resolution": 30
-                                   },
-                                   "data": [7.0, 5.0, 8.1]
-                               }"""
-        time_series = from_database(database_value, value_type="time_series")
-        self.assertEqual(len(time_series), 3)
-        self.assertEqual(len(time_series.resolution), 1)
-        self.assertEqual(time_series.resolution[0], relativedelta(minutes=30))
-        database_value = b"""{
-                                   "index": {
-                                       "start": "2019-03-23",
-                                       "resolution": [30, 45]
-                                   },
-                                   "data": [7.0, 5.0, 8.1]
-                               }"""
-        time_series = from_database(database_value, value_type="time_series")
-        self.assertEqual(len(time_series), 3)
-        self.assertEqual(len(time_series.resolution), 2)
-        self.assertEqual(time_series.resolution[0], relativedelta(minutes=30))
-        self.assertEqual(time_series.resolution[1], relativedelta(minutes=45))
-
-    def test_from_database_TimeSeriesFixedResolution_default_ignore_year(self):
-        # Should be false if start is given
-        database_value = b"""{
-                                   "index": {
-                                       "start": "2019-03-23",
-                                       "resolution": "1 day",
-                                       "repeat": false
-                                   },
-                                   "data": [7.0, 5.0, 8.1]
-                               }"""
-        time_series = from_database(database_value, value_type="time_series")
-        self.assertFalse(time_series.ignore_year)
-        # Should be true if start is omitted
-        database_value = b"""{
-                                   "index": {
-                                       "resolution": "1 day",
-                                       "repeat": false
-                                   },
-                                   "data": [7.0, 5.0, 8.1]
-                               }"""
-        time_series = from_database(database_value, value_type="time_series")
-        self.assertTrue(time_series.ignore_year)
-
-    def test_TimeSeriesFixedResolution_to_database(self):
-        values = numpy.array([3, 2, 4], dtype=float)
-        resolution = [duration_to_relativedelta("1 months")]
-        start = datetime(year=2007, month=6, day=1)
-        value = TimeSeriesFixedResolution(start, resolution, values, True, True)
-        db_value, value_type = value.to_database()
-        releases = json.loads(db_value)
-        self.assertEqual(
-            releases,
-            {
-                "index": {"start": "2007-06-01 00:00:00", "resolution": "1M", "ignore_year": True, "repeat": True},
-                "data": [3, 2, 4],
-            },
-        )
-        self.assertEqual(value_type, "time_series")
-
-    def test_TimeSeriesFixedResolution_to_database_with_index_type(self):
-        values = numpy.array([3, 2, 4], dtype=float)
-        resolution = [duration_to_relativedelta("1 months")]
-        start = datetime(year=2007, month=6, day=1)
-        value = TimeSeriesFixedResolution(start, resolution, values, True, True, "index")
-        db_value, value_type = value.to_database()
-        releases = json.loads(db_value)
-        self.assertEqual(
-            releases,
-            {
-                "index_name": "index",
-                "index": {"start": "2007-06-01 00:00:00", "resolution": "1M", "ignore_year": True, "repeat": True},
-                "data": [3, 2, 4],
-            },
-        )
-        self.assertEqual(value_type, "time_series")
-
-    def test_TimeSeriesFixedResolution_resolution_list_to_database(self):
-        start = datetime(year=2007, month=1, day=1)
-        resolutions = ["1 month", "1 year"]
-        resolutions = [duration_to_relativedelta(r) for r in resolutions]
-        values = numpy.array([3.0, 2.0, 4.0])
-        value = TimeSeriesFixedResolution(start, resolutions, values, True, True)
-        db_value, value_type = value.to_database()
-        releases = json.loads(db_value)
-        self.assertEqual(
-            releases,
-            {
-                "index": {
-                    "start": "2007-01-01 00:00:00",
-                    "resolution": ["1M", "1Y"],
-                    "ignore_year": True,
-                    "repeat": True,
-                },
-                "data": [3.0, 2.0, 4.0],
-            },
-        )
-        self.assertEqual(value_type, "time_series")
-
-    def test_TimeSeriesFixedResolution_init_conversions(self):
-        series = TimeSeriesFixedResolution("2019-01-03T00:30:33", "1D", [3.0, 2.0, 1.0], False, False)
-        self.assertTrue(isinstance(series.start, datetime))
-        self.assertTrue(isinstance(series.resolution, list))
-        for element in series.resolution:
-            self.assertTrue(isinstance(element, relativedelta))
-        self.assertTrue(isinstance(series.values, numpy.ndarray))
-        series = TimeSeriesFixedResolution("2019-01-03T00:30:33", ["2h", "4h"], [3.0, 2.0, 1.0], False, False)
-        self.assertTrue(isinstance(series.resolution, list))
-        for element in series.resolution:
-            self.assertTrue(isinstance(element, relativedelta))
-
-    def test_TimeSeriesVariableResolution_init_conversion(self):
-        series = TimeSeriesVariableResolution(["2008-07-08T03:00", "2008-08-08T13:30"], [3.3, 4.4], True, True)
-        self.assertTrue(isinstance(series.indexes, np.ndarray))
-        for index in series.indexes:
-            self.assertTrue(isinstance(index, np.datetime64))
-        self.assertTrue(isinstance(series.values, np.ndarray))
-
-    def test_from_database_Map_with_index_name(self):
-        database_value = b'{"index_type":"str", "index_name": "index", "data":[["a", 1.1]]}'
-        value = from_database(database_value, value_type="map")
-        self.assertIsInstance(value, Map)
-        self.assertEqual(value.indexes, ["a"])
-        self.assertEqual(value.values, [1.1])
-        self.assertEqual(value.index_name, "index")
-
-    def test_from_database_Map_dictionary_format(self):
-        database_value = b'{"index_type":"str", "data":{"a": 1.1, "b": 2.2}}'
-        value = from_database(database_value, value_type="map")
-        self.assertIsInstance(value, Map)
-        self.assertEqual(value.indexes, ["a", "b"])
-        self.assertEqual(value.values, [1.1, 2.2])
-        self.assertEqual(value.index_name, "x")
-
-    def test_from_database_Map_two_column_array_format(self):
-        database_value = b'{"index_type":"float", "data":[[1.1, "a"], [2.2, "b"]]}'
-        value = from_database(database_value, value_type="map")
-        self.assertIsInstance(value, Map)
-        self.assertEqual(value.indexes, [1.1, 2.2])
-        self.assertEqual(value.values, ["a", "b"])
-        self.assertEqual(value.index_name, "x")
-
-    def test_from_database_Map_nested_maps(self):
-        database_value = b'''
-        {
-             "index_type": "duration",
-              "data":[["1 hour", {"type": "map",
-                                  "index_type": "date_time",
-                                  "data": {"2020-01-01T12:00": {"type":"duration", "data":"3 hours"}}}]]
-        }'''
-        value = from_database(database_value, value_type="map")
-        self.assertEqual(value.indexes, [Duration("1 hour")])
-        nested_map = value.values[0]
-        self.assertIsInstance(nested_map, Map)
-        self.assertEqual(nested_map.indexes, [DateTime("2020-01-01T12:00")])
-        self.assertEqual(nested_map.values, [Duration("3 hours")])
-
-    def test_from_database_Map_with_TimeSeries_values(self):
-        database_value = b'''
-        {
-             "index_type": "duration",
-              "data":[["1 hour", {"type": "time_series",
-                                  "data": [["2020-01-01T12:00", -3.0], ["2020-01-02T12:00", -9.3]]
-                                 }
-                     ]]
-        }'''
-        value = from_database(database_value, value_type="map")
-        self.assertEqual(value.indexes, [Duration("1 hour")])
-        self.assertEqual(
-            value.values,
-            [TimeSeriesVariableResolution(["2020-01-01T12:00", "2020-01-02T12:00"], [-3.0, -9.3], False, False)],
-        )
-
-    def test_from_database_Map_with_Array_values(self):
-        database_value = b'''
-        {
-             "index_type": "duration",
-              "data":[["1 hour", {"type": "array", "data": [-3.0, -9.3]}]]
-        }'''
-        value = from_database(database_value, value_type="map")
-        self.assertEqual(value.indexes, [Duration("1 hour")])
-        self.assertEqual(value.values, [Array([-3.0, -9.3])])
-
-    def test_from_database_Map_with_TimePattern_values(self):
-        database_value = b'''
-        {
-             "index_type": "float",
-              "data":[["2.3", {"type": "time_pattern", "data": {"M1-2": -9.3, "M3-12": -3.9}}]]
-        }'''
-        value = from_database(database_value, value_type="map")
-        self.assertEqual(value.indexes, [2.3])
-        self.assertEqual(value.values, [TimePattern(["M1-2", "M3-12"], [-9.3, -3.9])])
-
-    def test_Map_to_database(self):
-        map_value = Map(["a", "b"], [1.1, 2.2])
-        db_value, value_type = to_database(map_value)
-        raw = json.loads(db_value)
-        self.assertEqual(raw, {"index_type": "str", "data": [["a", 1.1], ["b", 2.2]]})
-        self.assertEqual(value_type, "map")
-
-    def test_Map_to_database_with_index_names(self):
-        nested_map = Map(["a"], [0.3])
-        nested_map.index_name = "nested index"
-        map_value = Map(["A"], [nested_map])
-        map_value.index_name = "index"
-        db_value, value_type = to_database(map_value)
-        raw = json.loads(db_value)
-        self.assertEqual(
-            raw,
-            {
-                "index_type": "str",
-                "index_name": "index",
-                "data": [
-                    ["A", {"type": "map", "index_type": "str", "index_name": "nested index", "data": [["a", 0.3]]}]
-                ],
-            },
-        )
-        self.assertEqual(value_type, "map")
-
-    def test_Map_to_database_with_TimeSeries_values(self):
-        time_series1 = TimeSeriesVariableResolution(["2020-01-01T12:00", "2020-01-02T12:00"], [2.3, 4.5], False, False)
-        time_series2 = TimeSeriesVariableResolution(
-            ["2020-01-01T12:00", "2020-01-02T12:00"], [-4.5, -2.3], False, False
-        )
-        map_value = Map(["a", "b"], [time_series1, time_series2])
-        db_value, value_type = to_database(map_value)
-        raw = json.loads(db_value)
-        expected = {
-            "index_type": "str",
-            "data": [
-                ["a", {"type": "time_series", "data": {"2020-01-01T12:00:00": 2.3, "2020-01-02T12:00:00": 4.5}}],
-                ["b", {"type": "time_series", "data": {"2020-01-01T12:00:00": -4.5, "2020-01-02T12:00:00": -2.3}}],
-            ],
-        }
-        self.assertEqual(raw, expected)
-        self.assertEqual(value_type, "map")
-
-    def test_Map_to_database_nested_maps(self):
-        nested_map = Map([Duration("2 months")], [Duration("5 days")])
-        map_value = Map([DateTime("2020-01-01T13:00")], [nested_map])
-        db_value, value_type = to_database(map_value)
-        raw = json.loads(db_value)
-        self.assertEqual(
-            raw,
-            {
-                "index_type": "date_time",
-                "data": [
-                    [
-                        "2020-01-01T13:00:00",
-                        {"type": "map", "index_type": "duration", "data": [["2M", {"type": "duration", "data": "5D"}]]},
-                    ]
-                ],
-            },
-        )
-        self.assertEqual(value_type, "map")
-
-    def test_Array_of_floats_to_database(self):
-        array = Array([-1.1, -2.2, -3.3])
-        db_value, value_type = to_database(array)
-        raw = json.loads(db_value)
-        self.assertEqual(raw, {"value_type": "float", "data": [-1.1, -2.2, -3.3]})
-        self.assertEqual(value_type, "array")
-
-    def test_Array_of_strings_to_database(self):
-        array = Array(["a", "b"])
-        db_value, value_type = to_database(array)
-        raw = json.loads(db_value)
-        self.assertEqual(raw, {"value_type": "str", "data": ["a", "b"]})
-        self.assertEqual(value_type, "array")
-
-    def test_Array_of_DateTimes_to_database(self):
-        array = Array([DateTime("2020-01-01T13:00")])
-        db_value, value_type = to_database(array)
-        raw = json.loads(db_value)
-        self.assertEqual(raw, {"value_type": "date_time", "data": ["2020-01-01T13:00:00"]})
-        self.assertEqual(value_type, "array")
-
-    def test_Array_of_Durations_to_database(self):
-        array = Array([Duration("4 months")])
-        db_value, value_type = to_database(array)
-        raw = json.loads(db_value)
-        self.assertEqual(raw, {"value_type": "duration", "data": ["4M"]})
-        self.assertEqual(value_type, "array")
-
-    def test_Array_of_floats_from_database(self):
-        database_value = b"""{
-            "value_type": "float",
-            "data": [1.2, 2.3]
-        }"""
-        array = from_database(database_value, value_type="array")
-        self.assertEqual(array.values, [1.2, 2.3])
-        self.assertEqual(array.indexes, [0, 1])
-        self.assertEqual(array.index_name, "i")
-
-    def test_Array_of_default_value_type_from_database(self):
-        database_value = b"""{
-            "data": [1.2, 2.3]
-        }"""
-        array = from_database(database_value, value_type="array")
-        self.assertEqual(array.values, [1.2, 2.3])
-        self.assertEqual(array.indexes, [0, 1])
-        self.assertEqual(array.index_name, "i")
-
-    def test_Array_of_strings_from_database(self):
-        database_value = b"""{
-            "value_type": "str",
-            "data": ["A", "B"]
-        }"""
-        array = from_database(database_value, value_type="array")
-        self.assertEqual(array.values, ["A", "B"])
-        self.assertEqual(array.indexes, [0, 1])
-        self.assertEqual(array.index_name, "i")
-
-    def test_Array_of_DateTimes_from_database(self):
-        database_value = b"""{
-            "value_type": "date_time",
-            "data": ["2020-03-25T10:34:00"]
-        }"""
-        array = from_database(database_value, value_type="array")
-        self.assertEqual(array.values, [DateTime("2020-03-25T10:34:00")])
-        self.assertEqual(array.indexes, [0])
-        self.assertEqual(array.index_name, "i")
-
-    def test_Array_of_Durations_from_database(self):
-        database_value = b"""{
-            "value_type": "duration",
-            "data": ["2 years", "7 seconds"]
-        }"""
-        array = from_database(database_value, value_type="array")
-        self.assertEqual(array.values, [Duration("2 years"), Duration("7s")])
-        self.assertEqual(array.indexes, [0, 1])
-        self.assertEqual(array.index_name, "i")
-
-    def test_Array_from_database_with_index_name(self):
-        database_value = b"""{
-            "value_type": "float",
-            "index_name": "index",
-            "data": [1.2]
-        }"""
-        array = from_database(database_value, value_type="array")
-        self.assertEqual(array.values, [1.2])
-        self.assertEqual(array.indexes, [0])
-        self.assertEqual(array.index_name, "index")
-
-    def test_Array_constructor_converts_ints_to_floats(self):
-        array = Array([9, 5])
-        self.assertIs(array.value_type, float)
-        self.assertEqual(array.values, [9.0, 5.0])
-
-    def test_DateTime_copy_construction(self):
-        date_time = DateTime("2019-07-03T09:09:09")
-        copied = DateTime(date_time)
-        self.assertEqual(copied, date_time)
-
-    def test_Duration_copy_construction(self):
-        duration = Duration("3 minutes")
-        copied = Duration(duration)
-        self.assertEqual(copied, duration)
-
-    def test_DateTime_equality(self):
-        date_time = DateTime(dateutil.parser.parse("2019-07-03T09:09:09"))
-        self.assertEqual(date_time, date_time)
-        equal_date_time = DateTime(dateutil.parser.parse("2019-07-03T09:09:09"))
-        self.assertEqual(date_time, equal_date_time)
-        inequal_date_time = DateTime(dateutil.parser.parse("2018-07-03T09:09:09"))
-        self.assertNotEqual(date_time, inequal_date_time)
-
-    def test_Duration_equality(self):
-        duration = Duration(duration_to_relativedelta("3 minutes"))
-        self.assertEqual(duration, duration)
-        equal_duration = Duration(duration_to_relativedelta("3m"))
-        self.assertEqual(duration, equal_duration)
-        inequal_duration = Duration(duration_to_relativedelta("3 seconds"))
-        self.assertNotEqual(duration, inequal_duration)
-
-    def test_Map_equality(self):
-        map_value = Map(["a"], [-2.3])
-        self.assertEqual(map_value, Map(["a"], [-2.3]))
-        nested_map = Map(["a"], [-2.3])
-        map_value = Map(["A"], [nested_map])
-        self.assertEqual(map_value, Map(["A"], [Map(["a"], [-2.3])]))
-
-    def test_TimePattern_equality(self):
-        pattern = TimePattern(["D1-2", "D3-7"], np.array([-2.3, -5.0]))
-        self.assertEqual(pattern, pattern)
-        equal_pattern = TimePattern(["D1-2", "D3-7"], np.array([-2.3, -5.0]))
-        self.assertEqual(pattern, equal_pattern)
-        inequal_pattern = TimePattern(["M1-3", "M4-12"], np.array([-5.0, 23.0]))
-        self.assertNotEqual(pattern, inequal_pattern)
-
-    def test_TimeSeriesFixedResolution_equality(self):
-        series = TimeSeriesFixedResolution("2019-01-03T00:30:33", "1D", [3.0, 2.0, 1.0], False, False)
-        self.assertEqual(series, series)
-        equal_series = TimeSeriesFixedResolution("2019-01-03T00:30:33", "1D", [3.0, 2.0, 1.0], False, False)
-        self.assertEqual(series, equal_series)
-        inequal_series = TimeSeriesFixedResolution("2019-01-03T00:30:33", "1D", [3.0, 2.0, 1.0], True, False)
-        self.assertNotEqual(series, inequal_series)
-
-    def test_TimeSeriesVariableResolution_equality(self):
-        series = TimeSeriesVariableResolution(["2000-01-01T00:00", "2001-01-01T00:00"], [4.2, 2.4], True, True)
-        self.assertEqual(series, series)
-        equal_series = TimeSeriesVariableResolution(["2000-01-01T00:00", "2001-01-01T00:00"], [4.2, 2.4], True, True)
-        self.assertEqual(series, equal_series)
-        inequal_series = TimeSeriesVariableResolution(["2000-01-01T00:00", "2002-01-01T00:00"], [4.2, 2.4], False, True)
-        self.assertNotEqual(series, inequal_series)
-
-    def test_IndexedValue_constructor_converts_values_to_floats(self):
-        value = IndexedNumberArray("", [4, -9, 11])
-        self.assertEqual(value.values.dtype, np.dtype(float))
-        numpy.testing.assert_equal(value.values, numpy.array([4.0, -9.0, 11.0]))
-        value = IndexedNumberArray("", numpy.array([16, -251, 99]))
-        self.assertEqual(value.values.dtype, np.dtype(float))
-        numpy.testing.assert_equal(value.values, numpy.array([16.0, -251.0, 99.0]))
-
-    def test_Map_is_nested(self):
-        map_value = Map(["a"], [-2.3])
-        self.assertFalse(map_value.is_nested())
-        nested_map = Map(["a"], [-2.3])
-        map_value = Map(["A"], [nested_map])
-        self.assertTrue(map_value.is_nested())
-
-    def test_convert_leaf_maps_to_specialized_containers_non_nested_map(self):
-        map_value = Map([DateTime("2000-01-01T00:00"), DateTime("2000-01-02T00:00")], [-3.2, -2.3])
-        converted = convert_leaf_maps_to_specialized_containers(map_value)
-        self.assertEqual(
-            converted,
-            TimeSeriesVariableResolution(
-                ["2000-01-01T00:00", "2000-01-02T00:00"], [-3.2, -2.3], False, False, index_name=Map.DEFAULT_INDEX_NAME
-            ),
-        )
-
-    def test_convert_leaf_maps_to_specialized_containers_no_conversion(self):
-        map_value = Map(["a", "b"], [-3.2, -2.3])
-        converted = convert_leaf_maps_to_specialized_containers(map_value)
-        self.assertEqual(converted, map_value)
-
-    def test_convert_leaf_maps_to_specialized_containers_nested_map(self):
-        nested1 = Map(["a", "b"], [-3.2, -2.3])
-        nested2 = Map([DateTime("2000-01-01T00:00"), DateTime("2000-01-02T00:00")], [-3.2, -2.3])
-        map_value = Map([1.0, 2.0, 3.0], [nested1, nested2, 4.4])
-        converted = convert_leaf_maps_to_specialized_containers(map_value)
-        time_series = TimeSeriesVariableResolution(
-            ["2000-01-01T00:00", "2000-01-02T00:00"], [-3.2, -2.3], False, False, index_name=Map.DEFAULT_INDEX_NAME
-        )
-        expected = Map([1.0, 2.0, 3.0], [nested1, time_series, 4.4])
-        self.assertEqual(converted, expected)
-
-    def test_convert_non_nested_map_to_table(self):
-        map_ = Map(["a", "b"], [-3.2, -2.3])
-        table = convert_map_to_table(map_)
-        self.assertEqual(table, [["a", -3.2], ["b", -2.3]])
-
-    def test_convert_nested_map_to_table(self):
-        map1 = Map(["a", "b"], [-3.2, -2.3])
-        map2 = Map(["c", "d"], [3.2, 2.3])
-        nested_map = Map(["A", "B"], [map1, map2])
-        table = convert_map_to_table(nested_map)
-        self.assertEqual(table, [["A", "a", -3.2], ["A", "b", -2.3], ["B", "c", 3.2], ["B", "d", 2.3]])
-
-    def test_convert_uneven_nested_map_to_table(self):
-        map1 = Map(["a", "b"], [-3.2, -2.3])
-        nested_map = Map(["A", "B"], [map1, 42.0])
-        table = convert_map_to_table(nested_map, False)
-        self.assertEqual(table, [["A", "a", -3.2], ["A", "b", -2.3], ["B", 42.0]])
-
-    def test_convert_nested_map_to_table_with_padding(self):
-        map1 = Map(["a", "b"], [-3.2, -2.3])
-        nested_map = Map(["A", "B"], [map1, 42.0])
-        table = convert_map_to_table(nested_map, True)
-        self.assertEqual(table, [["A", "a", -3.2], ["A", "b", -2.3], ["B", 42.0, None]])
-
-    def test_convert_nested_map_to_table_using_special_empty_cells(self):
-        map1 = Map(["a"], [-2.3])
-        nested_map = Map(["A", "B", "C"], [map1, 42.0, map1])
-        empty = object()
-        table = convert_map_to_table(nested_map, True, empty=empty)
-        self.assertEqual(table, [["A", "a", -2.3], ["B", 42.0, empty], ["C", "a", -2.3]])
-        self.assertIs(table[1][2], empty)
-
-    def test_convert_containers_to_maps_empty_map(self):
-        self.assertEqual(convert_containers_to_maps(Map([], [], str)), Map([], [], str))
-
-    def test_convert_containers_to_maps_time_series(self):
-        time_series = TimeSeriesVariableResolution(["2020-11-27T12:55", "2020-11-27T13:00"], [2.5, 2.3], False, False)
-        map_ = convert_containers_to_maps(time_series)
-        self.assertEqual(
-            map_,
-            Map(
-                [DateTime("2020-11-27T12:55"), DateTime("2020-11-27T13:00")],
-                [2.5, 2.3],
-                index_name=TimeSeries.DEFAULT_INDEX_NAME,
-            ),
-        )
-
-    def test_convert_containers_to_maps_map_with_time_series(self):
-        time_series = TimeSeriesVariableResolution(["2020-11-27T12:55", "2020-11-27T13:00"], [2.5, 2.3], False, False)
-        map_ = Map(["a", "b"], [-1.1, time_series])
-        converted = convert_containers_to_maps(map_)
-        expected = Map(
-            ["a", "b"],
-            [
-                -1.1,
-                Map(
-                    [DateTime("2020-11-27T12:55"), DateTime("2020-11-27T13:00")],
-                    [2.5, 2.3],
-                    index_name=TimeSeries.DEFAULT_INDEX_NAME,
-                ),
-            ],
-        )
-        self.assertEqual(converted, expected)
-
-    def convert_map_to_dict(self):
-        map1 = Map(["a", "b"], [-3.2, -2.3])
-        map2 = Map(["c", "d"], [3.2, 2.3])
-        nested_map = Map(["A", "B"], [map1, map2])
-        self.assertEqual(nested_map, {"A": {"a": -3.2, "b": -2.3}, "B": {"c": 3.2, "d": 2.3}})
-
-
-if __name__ == "__main__":
-    unittest.main()
+######################################################################################################################
+# Copyright (C) 2017-2022 Spine project consortium
+# This file is part of Spine Database API.
+# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
+# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
+# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
+# Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
+# this program. If not, see <http://www.gnu.org/licenses/>.
+######################################################################################################################
+
+"""
+Tests for the parameter_value module.
+
+"""
+
+from datetime import datetime
+import json
+import unittest
+import dateutil.parser
+from dateutil.relativedelta import relativedelta
+import numpy as np
+import numpy.testing
+from spinedb_api.parameter_value import (
+    convert_containers_to_maps,
+    convert_leaf_maps_to_specialized_containers,
+    convert_map_to_table,
+    duration_to_relativedelta,
+    relativedelta_to_duration,
+    from_database,
+    to_database,
+    Array,
+    DateTime,
+    Duration,
+    IndexedNumberArray,
+    Map,
+    TimePattern,
+    TimeSeriesFixedResolution,
+    TimeSeriesVariableResolution,
+    TimeSeries,
+)
+
+
+class TestParameterValue(unittest.TestCase):
+    """Test for the free functions and classes in parameter_value."""
+
+    def test_duration_to_relativedelta_seconds(self):
+        delta = duration_to_relativedelta("7s")
+        self.assertEqual(delta, relativedelta(seconds=7))
+        delta = duration_to_relativedelta("1 second")
+        self.assertEqual(delta, relativedelta(seconds=1))
+        delta = duration_to_relativedelta("7 seconds")
+        self.assertEqual(delta, relativedelta(seconds=7))
+        delta = duration_to_relativedelta("99 seconds")
+        self.assertEqual(delta, relativedelta(minutes=1, seconds=39))
+
+    def test_relativedelta_to_duration_seconds(self):
+        delta = duration_to_relativedelta("7s")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "7s")
+        delta = duration_to_relativedelta("9999999s")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "9999999s")
+
+    def test_duration_to_relativedelta_minutes(self):
+        delta = duration_to_relativedelta("7m")
+        self.assertEqual(delta, relativedelta(minutes=7))
+        delta = duration_to_relativedelta("1 minute")
+        self.assertEqual(delta, relativedelta(minutes=1))
+        delta = duration_to_relativedelta("7 minutes")
+        self.assertEqual(delta, relativedelta(minutes=7))
+
+    def test_relativedelta_to_duration_minutes(self):
+        delta = duration_to_relativedelta("7m")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "7m")
+        delta = duration_to_relativedelta("999999m")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "999999m")
+
+    def test_duration_to_relativedelta_hours(self):
+        delta = duration_to_relativedelta("7h")
+        self.assertEqual(delta, relativedelta(hours=7))
+        delta = duration_to_relativedelta("1 hour")
+        self.assertEqual(delta, relativedelta(hours=1))
+        delta = duration_to_relativedelta("7 hours")
+        self.assertEqual(delta, relativedelta(hours=7))
+
+    def test_relativedelta_to_duration_hours(self):
+        delta = duration_to_relativedelta("7h")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "7h")
+        delta = duration_to_relativedelta("99999h")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "99999h")
+
+    def test_duration_to_relativedelta_days(self):
+        delta = duration_to_relativedelta("7D")
+        self.assertEqual(delta, relativedelta(days=7))
+        delta = duration_to_relativedelta("1 day")
+        self.assertEqual(delta, relativedelta(days=1))
+        delta = duration_to_relativedelta("7 days")
+        self.assertEqual(delta, relativedelta(days=7))
+
+    def test_relativedelta_to_duration_days(self):
+        delta = duration_to_relativedelta("7D")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "7D")
+        delta = duration_to_relativedelta("9999D")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "9999D")
+
+    def test_duration_to_relativedelta_months(self):
+        delta = duration_to_relativedelta("7M")
+        self.assertEqual(delta, relativedelta(months=7))
+        delta = duration_to_relativedelta("1 month")
+        self.assertEqual(delta, relativedelta(months=1))
+        delta = duration_to_relativedelta("7 months")
+        self.assertEqual(delta, relativedelta(months=7))
+
+    def test_relativedelta_to_duration_months(self):
+        delta = duration_to_relativedelta("7M")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "7M")
+        delta = duration_to_relativedelta("99M")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "99M")
+
+    def test_duration_to_relativedelta_years(self):
+        delta = duration_to_relativedelta("7Y")
+        self.assertEqual(delta, relativedelta(years=7))
+        delta = duration_to_relativedelta("7Y")
+        self.assertEqual(delta, relativedelta(years=7))
+        delta = duration_to_relativedelta("1 year")
+        self.assertEqual(delta, relativedelta(years=1))
+        delta = duration_to_relativedelta("7 years")
+        self.assertEqual(delta, relativedelta(years=7))
+
+    def test_relativedelta_to_duration_years(self):
+        delta = duration_to_relativedelta("7Y")
+        duration = relativedelta_to_duration(delta)
+        self.assertEqual(duration, "7Y")
+
+    def test_from_database_plain_number(self):
+        database_value = b"23.0"
+        value = from_database(database_value, value_type=None)
+        self.assertTrue(isinstance(value, float))
+        self.assertEqual(value, 23.0)
+
+    def test_from_database_boolean(self):
+        database_value = b"true"
+        value = from_database(database_value, value_type=None)
+        self.assertTrue(isinstance(value, bool))
+        self.assertEqual(value, True)
+
+    def test_to_database_plain_number(self):
+        value = 23.0
+        database_value, value_type = to_database(value)
+        value_as_float = json.loads(database_value)
+        self.assertEqual(value_as_float, value)
+        self.assertIsNone(value_type)
+
+    def test_to_database_DateTime(self):
+        value = DateTime(datetime(year=2019, month=6, day=26, hour=12, minute=50, second=13))
+        database_value, value_type = to_database(value)
+        value_as_dict = json.loads(database_value)
+        self.assertEqual(value_as_dict, {"data": "2019-06-26T12:50:13"})
+        self.assertEqual(value_type, "date_time")
+
+    def test_from_database_DateTime(self):
+        database_value = b'{"data": "2019-06-01T22:15:00+01:00"}'
+        value = from_database(database_value, value_type="date_time")
+        self.assertEqual(value.value, dateutil.parser.parse("2019-06-01T22:15:00+01:00"))
+
+    def test_DateTime_to_database(self):
+        value = DateTime(datetime(year=2019, month=6, day=26, hour=10, minute=50, second=34))
+        database_value, value_type = value.to_database()
+        value_dict = json.loads(database_value)
+        self.assertEqual(value_dict, {"data": "2019-06-26T10:50:34"})
+        self.assertEqual(value_type, "date_time")
+
+    def test_from_database_Duration(self):
+        database_value = b'{"data": "4 seconds"}'
+        value = from_database(database_value, value_type="duration")
+        self.assertEqual(value.value, relativedelta(seconds=4))
+
+    def test_from_database_Duration_default_units(self):
+        database_value = b'{"data": 23}'
+        value = from_database(database_value, value_type="duration")
+        self.assertEqual(value.value, relativedelta(minutes=23))
+
+    def test_from_database_Duration_legacy_list_format_converted_to_Array(self):
+        database_value = b'{"data": ["1 hour", "1h", 60, "2 hours"]}'
+        value = from_database(database_value, value_type="duration")
+        expected = Array([Duration("1h"), Duration("1h"), Duration("1h"), Duration("2h")])
+        self.assertEqual(value, expected)
+
+    def test_Duration_to_database(self):
+        value = Duration(duration_to_relativedelta("8 years"))
+        database_value, value_type = value.to_database()
+        value_as_dict = json.loads(database_value)
+        self.assertEqual(value_as_dict, {"data": "8Y"})
+        self.assertEqual(value_type, "duration")
+
+    def test_from_database_TimePattern(self):
+        database_value = b"""
+        {
+          "data": {
+            "m1-4,m9-12": 300,
+            "m5-8": 221.5
+          }
+        }
+        """
+        value = from_database(database_value, value_type="time_pattern")
+        self.assertEqual(len(value), 2)
+        self.assertEqual(value.indexes, ["m1-4,m9-12", "m5-8"])
+        numpy.testing.assert_equal(value.values, numpy.array([300.0, 221.5]))
+        self.assertEqual(value.index_name, "p")
+
+    def test_from_database_TimePattern_with_index_name(self):
+        database_value = b"""
+        {
+          "index_name": "index",
+          "data": {
+            "M1-12": 300
+          }
+        }
+        """
+        value = from_database(database_value, value_type="time_pattern")
+        self.assertEqual(value.indexes, ["M1-12"])
+        numpy.testing.assert_equal(value.values, numpy.array([300.0]))
+        self.assertEqual(value.index_name, "index")
+
+    def test_TimePattern_to_database(self):
+        value = TimePattern(["M1-4,M9-12", "M5-8"], numpy.array([300.0, 221.5]))
+        database_value, value_type = value.to_database()
+        value_as_dict = json.loads(database_value)
+        self.assertEqual(value_as_dict, {"data": {"M1-4,M9-12": 300.0, "M5-8": 221.5}})
+        self.assertEqual(value_type, "time_pattern")
+
+    def test_TimePattern_to_database_with_integer_values(self):
+        value = TimePattern(["M1-4,M9-12", "M5-8"], [300, 221])
+        database_value, value_type = value.to_database()
+        value_as_dict = json.loads(database_value)
+        self.assertEqual(value_as_dict, {"data": {"M1-4,M9-12": 300.0, "M5-8": 221.0}})
+        self.assertEqual(value_type, "time_pattern")
+
+    def test_TimePattern_to_database_with_index_name(self):
+        value = TimePattern(["M1-12"], [300.0])
+        value.index_name = "index"
+        database_value, value_type = value.to_database()
+        value_as_dict = json.loads(database_value)
+        self.assertEqual(value_as_dict, {"index_name": "index", "data": {"M1-12": 300.0}})
+        self.assertEqual(value_type, "time_pattern")
+
+    def test_TimePattern_index_length_is_not_limited(self):
+        value = TimePattern(["M1-4", "M5-12"], [300, 221])
+        value.indexes[0] = "M1-2,M3-4,M5-6,M7-8,M9-10,M11-12"
+        value.indexes[1] = "M2-3,M4-5,M6-7,M8-9,M10-11"
+        self.assertEqual(list(value.indexes), ["M1-2,M3-4,M5-6,M7-8,M9-10,M11-12", "M2-3,M4-5,M6-7,M8-9,M10-11"])
+
+    def test_from_database_TimeSeriesVariableResolution_as_dictionary(self):
+        releases = b"""{
+                          "data": {
+                              "1977-05-25": 4,
+                              "1980-05-21": 5,
+                              "1983-05-25": 6
+                          }
+                      }"""
+        time_series = from_database(releases, value_type="time_series")
+        self.assertEqual(
+            time_series.indexes,
+            numpy.array(
+                [numpy.datetime64("1977-05-25"), numpy.datetime64("1980-05-21"), numpy.datetime64("1983-05-25")],
+                dtype="datetime64[D]",
+            ),
+        )
+        self.assertEqual(len(time_series), 3)
+        self.assertTrue(isinstance(time_series.values, numpy.ndarray))
+        numpy.testing.assert_equal(time_series.values, numpy.array([4, 5, 6]))
+        self.assertEqual(time_series.index_name, "t")
+
+    def test_from_database_TimeSeriesVariableResolution_as_dictionary_with_index_name(self):
+        releases = b"""{
+                          "data": {
+                              "1977-05-25": 4,
+                              "1980-05-21": 5
+                          },
+                          "index_name": "index"
+                      }"""
+        time_series = from_database(releases, value_type="time_series")
+        self.assertEqual(time_series.index_name, "index")
+
+    def test_from_database_TimeSeriesVariableResolution_as_two_column_array(self):
+        releases = b"""{
+                          "data": [
+                              ["1977-05-25", 4],
+                              ["1980-05-21", 5],
+                              ["1983-05-25", 6]
+                          ]
+                      }"""
+        time_series = from_database(releases, value_type="time_series")
+        self.assertEqual(
+            time_series.indexes,
+            numpy.array(
+                [numpy.datetime64("1977-05-25"), numpy.datetime64("1980-05-21"), numpy.datetime64("1983-05-25")],
+                dtype="datetime64[D]",
+            ),
+        )
+        self.assertEqual(len(time_series), 3)
+        self.assertTrue(isinstance(time_series.values, numpy.ndarray))
+        numpy.testing.assert_equal(time_series.values, numpy.array([4, 5, 6]))
+        self.assertEqual(time_series.index_name, "t")
+
+    def test_from_database_TimeSeriesVariableResolution_as_two_column_array_with_index_name(self):
+        releases = b"""{
+                          "data": [
+                              ["1977-05-25", 4],
+                              ["1980-05-21", 5]
+                          ],
+                          "index_name": "index"
+                      }"""
+        time_series = from_database(releases, value_type="time_series")
+        self.assertEqual(time_series.index_name, "index")
+
+    def test_from_database_TimeSeriesFixedResolution_default_repeat(self):
+        database_value = b"""{
+                                   "index": {
+                                       "ignore_year": true
+                                   },
+                                   "data": [["2019-07-02T10:00:00", 7.0],
+                                            ["2019-07-02T10:00:01", 4.0]]
+                               }"""
+        time_series = from_database(database_value, value_type="time_series")
+        self.assertTrue(time_series.ignore_year)
+        self.assertFalse(time_series.repeat)
+
+    def test_TimeSeriesVariableResolution_to_database(self):
+        dates = numpy.array(["1999-05-19", "2002-05-16", "2005-05-19"], dtype="datetime64[D]")
+        episodes = numpy.array([1, 2, 3], dtype=float)
+        value = TimeSeriesVariableResolution(dates, episodes, False, False)
+        db_value, value_type = value.to_database()
+        releases = json.loads(db_value)
+        self.assertEqual(releases, {"data": {"1999-05-19": 1, "2002-05-16": 2, "2005-05-19": 3}})
+        self.assertEqual(value_type, "time_series")
+
+    def test_TimeSeriesVariableResolution_to_database_with_index_name(self):
+        dates = numpy.array(["2002-05-16", "2005-05-19"], dtype="datetime64[D]")
+        episodes = numpy.array([1, 2], dtype=float)
+        value = TimeSeriesVariableResolution(dates, episodes, False, False, "index")
+        db_value, value_type = value.to_database()
+        releases = json.loads(db_value)
+        self.assertEqual(releases, {"index_name": "index", "data": {"2002-05-16": 1, "2005-05-19": 2}})
+        self.assertEqual(value_type, "time_series")
+
+    def test_TimeSeriesVariableResolution_to_database_with_ignore_year_and_repeat(self):
+        dates = numpy.array(["1999-05-19", "2002-05-16", "2005-05-19"], dtype="datetime64[D]")
+        episodes = numpy.array([1, 2, 3], dtype=float)
+        value = TimeSeriesVariableResolution(dates, episodes, True, True)
+        db_value, value_type = value.to_database()
+        releases = json.loads(db_value)
+        self.assertEqual(
+            releases,
+            {
+                "data": {"1999-05-19": 1, "2002-05-16": 2, "2005-05-19": 3},
+                "index": {"ignore_year": True, "repeat": True},
+            },
+        )
+        self.assertEqual(value_type, "time_series")
+
+    def test_from_database_TimeSeriesFixedResolution(self):
+        days_of_our_lives = b"""{
+                                   "index": {
+                                       "start": "2019-03-23",
+                                       "resolution": "1 day",
+                                       "ignore_year": false,
+                                       "repeat": false
+                                   },
+                                   "data": [7.0, 5.0, 8.1]
+                               }"""
+        time_series = from_database(days_of_our_lives, value_type="time_series")
+        self.assertEqual(len(time_series), 3)
+        self.assertEqual(
+            time_series.indexes,
+            numpy.array(
+                [numpy.datetime64("2019-03-23"), numpy.datetime64("2019-03-24"), numpy.datetime64("2019-03-25")],
+                dtype="datetime64[s]",
+            ),
+        )
+        self.assertTrue(isinstance(time_series.values, numpy.ndarray))
+        numpy.testing.assert_equal(time_series.values, numpy.array([7.0, 5.0, 8.1]))
+        self.assertEqual(time_series.start, dateutil.parser.parse("2019-03-23"))
+        self.assertEqual(len(time_series.resolution), 1)
+        self.assertEqual(time_series.resolution[0], relativedelta(days=1))
+        self.assertFalse(time_series.ignore_year)
+        self.assertFalse(time_series.repeat)
+        self.assertEqual(time_series.index_name, "t")
+
+    def test_from_database_TimeSeriesFixedResolution_no_index(self):
+        database_value = b"""{
+                                "data": [1, 2, 3, 4, 5, 8]
+                            }
+        """
+        time_series = from_database(database_value, value_type="time_series")
+        self.assertEqual(len(time_series), 6)
+        self.assertEqual(
+            time_series.indexes,
+            numpy.array(
+                [
+                    numpy.datetime64("0001-01-01T00:00:00"),
+                    numpy.datetime64("0001-01-01T01:00:00"),
+                    numpy.datetime64("0001-01-01T02:00:00"),
+                    numpy.datetime64("0001-01-01T03:00:00"),
+                    numpy.datetime64("0001-01-01T04:00:00"),
+                    numpy.datetime64("0001-01-01T05:00:00"),
+                ],
+                dtype="datetime64[s]",
+            ),
+        )
+        numpy.testing.assert_equal(time_series.values, numpy.array([1.0, 2.0, 3.0, 4.0, 5.0, 8.0]))
+        self.assertEqual(time_series.start, dateutil.parser.parse("0001-01-01T00:00:00"))
+        self.assertEqual(len(time_series.resolution), 1)
+        self.assertEqual(time_series.resolution[0], relativedelta(hours=1))
+        self.assertTrue(time_series.ignore_year)
+        self.assertTrue(time_series.repeat)
+
+    def test_from_database_TimeSeriesFixedResolution_index_name(self):
+        database_value = b"""{
+                                "data": [1],
+                                "index_name": "index"
+                            }
+        """
+        time_series = from_database(database_value, value_type="time_series")
+        self.assertEqual(time_series.index_name, "index")
+
+    def test_from_database_TimeSeriesFixedResolution_resolution_list(self):
+        database_value = b"""{
+                                "index": {
+                                    "start": "2019-01-31",
+                                    "resolution": ["1 day", "1M"],
+                                    "ignore_year": false,
+                                    "repeat": false
+                                },
+                                "data": [7.0, 5.0, 8.1, -4.1]
+                            }"""
+        time_series = from_database(database_value, value_type="time_series")
+        self.assertEqual(len(time_series), 4)
+        self.assertEqual(
+            time_series.indexes,
+            numpy.array(
+                [
+                    numpy.datetime64("2019-01-31"),
+                    numpy.datetime64("2019-02-01"),
+                    numpy.datetime64("2019-03-01"),
+                    numpy.datetime64("2019-03-02"),
+                ],
+                dtype="datetime64[s]",
+            ),
+        )
+        numpy.testing.assert_equal(time_series.values, numpy.array([7.0, 5.0, 8.1, -4.1]))
+        self.assertEqual(time_series.start, dateutil.parser.parse("2019-01-31"))
+        self.assertEqual(len(time_series.resolution), 2)
+        self.assertEqual(time_series.resolution, [relativedelta(days=1), relativedelta(months=1)])
+        self.assertFalse(time_series.ignore_year)
+        self.assertFalse(time_series.repeat)
+
+    def test_from_database_TimeSeriesFixedResolution_default_resolution_is_1hour(self):
+        database_value = b"""{
+                                   "index": {
+                                       "start": "2019-03-23",
+                                       "ignore_year": false,
+                                       "repeat": false
+                                   },
+                                   "data": [7.0, 5.0, 8.1]
+                               }"""
+        time_series = from_database(database_value, value_type="time_series")
+        self.assertEqual(len(time_series), 3)
+        self.assertEqual(len(time_series.resolution), 1)
+        self.assertEqual(time_series.resolution[0], relativedelta(hours=1))
+
+    def test_from_database_TimeSeriesFixedResolution_default_resolution_unit_is_minutes(self):
+        database_value = b"""{
+                                   "index": {
+                                       "start": "2019-03-23",
+                                       "resolution": 30
+                                   },
+                                   "data": [7.0, 5.0, 8.1]
+                               }"""
+        time_series = from_database(database_value, value_type="time_series")
+        self.assertEqual(len(time_series), 3)
+        self.assertEqual(len(time_series.resolution), 1)
+        self.assertEqual(time_series.resolution[0], relativedelta(minutes=30))
+        database_value = b"""{
+                                   "index": {
+                                       "start": "2019-03-23",
+                                       "resolution": [30, 45]
+                                   },
+                                   "data": [7.0, 5.0, 8.1]
+                               }"""
+        time_series = from_database(database_value, value_type="time_series")
+        self.assertEqual(len(time_series), 3)
+        self.assertEqual(len(time_series.resolution), 2)
+        self.assertEqual(time_series.resolution[0], relativedelta(minutes=30))
+        self.assertEqual(time_series.resolution[1], relativedelta(minutes=45))
+
+    def test_from_database_TimeSeriesFixedResolution_default_ignore_year(self):
+        # Should be false if start is given
+        database_value = b"""{
+                                   "index": {
+                                       "start": "2019-03-23",
+                                       "resolution": "1 day",
+                                       "repeat": false
+                                   },
+                                   "data": [7.0, 5.0, 8.1]
+                               }"""
+        time_series = from_database(database_value, value_type="time_series")
+        self.assertFalse(time_series.ignore_year)
+        # Should be true if start is omitted
+        database_value = b"""{
+                                   "index": {
+                                       "resolution": "1 day",
+                                       "repeat": false
+                                   },
+                                   "data": [7.0, 5.0, 8.1]
+                               }"""
+        time_series = from_database(database_value, value_type="time_series")
+        self.assertTrue(time_series.ignore_year)
+
+    def test_TimeSeriesFixedResolution_to_database(self):
+        values = numpy.array([3, 2, 4], dtype=float)
+        resolution = [duration_to_relativedelta("1 months")]
+        start = datetime(year=2007, month=6, day=1)
+        value = TimeSeriesFixedResolution(start, resolution, values, True, True)
+        db_value, value_type = value.to_database()
+        releases = json.loads(db_value)
+        self.assertEqual(
+            releases,
+            {
+                "index": {"start": "2007-06-01 00:00:00", "resolution": "1M", "ignore_year": True, "repeat": True},
+                "data": [3, 2, 4],
+            },
+        )
+        self.assertEqual(value_type, "time_series")
+
+    def test_TimeSeriesFixedResolution_to_database_with_index_type(self):
+        values = numpy.array([3, 2, 4], dtype=float)
+        resolution = [duration_to_relativedelta("1 months")]
+        start = datetime(year=2007, month=6, day=1)
+        value = TimeSeriesFixedResolution(start, resolution, values, True, True, "index")
+        db_value, value_type = value.to_database()
+        releases = json.loads(db_value)
+        self.assertEqual(
+            releases,
+            {
+                "index_name": "index",
+                "index": {"start": "2007-06-01 00:00:00", "resolution": "1M", "ignore_year": True, "repeat": True},
+                "data": [3, 2, 4],
+            },
+        )
+        self.assertEqual(value_type, "time_series")
+
+    def test_TimeSeriesFixedResolution_resolution_list_to_database(self):
+        start = datetime(year=2007, month=1, day=1)
+        resolutions = ["1 month", "1 year"]
+        resolutions = [duration_to_relativedelta(r) for r in resolutions]
+        values = numpy.array([3.0, 2.0, 4.0])
+        value = TimeSeriesFixedResolution(start, resolutions, values, True, True)
+        db_value, value_type = value.to_database()
+        releases = json.loads(db_value)
+        self.assertEqual(
+            releases,
+            {
+                "index": {
+                    "start": "2007-01-01 00:00:00",
+                    "resolution": ["1M", "1Y"],
+                    "ignore_year": True,
+                    "repeat": True,
+                },
+                "data": [3.0, 2.0, 4.0],
+            },
+        )
+        self.assertEqual(value_type, "time_series")
+
+    def test_TimeSeriesFixedResolution_init_conversions(self):
+        series = TimeSeriesFixedResolution("2019-01-03T00:30:33", "1D", [3.0, 2.0, 1.0], False, False)
+        self.assertTrue(isinstance(series.start, datetime))
+        self.assertTrue(isinstance(series.resolution, list))
+        for element in series.resolution:
+            self.assertTrue(isinstance(element, relativedelta))
+        self.assertTrue(isinstance(series.values, numpy.ndarray))
+        series = TimeSeriesFixedResolution("2019-01-03T00:30:33", ["2h", "4h"], [3.0, 2.0, 1.0], False, False)
+        self.assertTrue(isinstance(series.resolution, list))
+        for element in series.resolution:
+            self.assertTrue(isinstance(element, relativedelta))
+
+    def test_TimeSeriesVariableResolution_init_conversion(self):
+        series = TimeSeriesVariableResolution(["2008-07-08T03:00", "2008-08-08T13:30"], [3.3, 4.4], True, True)
+        self.assertTrue(isinstance(series.indexes, np.ndarray))
+        for index in series.indexes:
+            self.assertTrue(isinstance(index, np.datetime64))
+        self.assertTrue(isinstance(series.values, np.ndarray))
+
+    def test_from_database_Map_with_index_name(self):
+        database_value = b'{"index_type":"str", "index_name": "index", "data":[["a", 1.1]]}'
+        value = from_database(database_value, value_type="map")
+        self.assertIsInstance(value, Map)
+        self.assertEqual(value.indexes, ["a"])
+        self.assertEqual(value.values, [1.1])
+        self.assertEqual(value.index_name, "index")
+
+    def test_from_database_Map_dictionary_format(self):
+        database_value = b'{"index_type":"str", "data":{"a": 1.1, "b": 2.2}}'
+        value = from_database(database_value, value_type="map")
+        self.assertIsInstance(value, Map)
+        self.assertEqual(value.indexes, ["a", "b"])
+        self.assertEqual(value.values, [1.1, 2.2])
+        self.assertEqual(value.index_name, "x")
+
+    def test_from_database_Map_two_column_array_format(self):
+        database_value = b'{"index_type":"float", "data":[[1.1, "a"], [2.2, "b"]]}'
+        value = from_database(database_value, value_type="map")
+        self.assertIsInstance(value, Map)
+        self.assertEqual(value.indexes, [1.1, 2.2])
+        self.assertEqual(value.values, ["a", "b"])
+        self.assertEqual(value.index_name, "x")
+
+    def test_from_database_Map_nested_maps(self):
+        database_value = b'''
+        {
+             "index_type": "duration",
+              "data":[["1 hour", {"type": "map",
+                                  "index_type": "date_time",
+                                  "data": {"2020-01-01T12:00": {"type":"duration", "data":"3 hours"}}}]]
+        }'''
+        value = from_database(database_value, value_type="map")
+        self.assertEqual(value.indexes, [Duration("1 hour")])
+        nested_map = value.values[0]
+        self.assertIsInstance(nested_map, Map)
+        self.assertEqual(nested_map.indexes, [DateTime("2020-01-01T12:00")])
+        self.assertEqual(nested_map.values, [Duration("3 hours")])
+
+    def test_from_database_Map_with_TimeSeries_values(self):
+        database_value = b'''
+        {
+             "index_type": "duration",
+              "data":[["1 hour", {"type": "time_series",
+                                  "data": [["2020-01-01T12:00", -3.0], ["2020-01-02T12:00", -9.3]]
+                                 }
+                     ]]
+        }'''
+        value = from_database(database_value, value_type="map")
+        self.assertEqual(value.indexes, [Duration("1 hour")])
+        self.assertEqual(
+            value.values,
+            [TimeSeriesVariableResolution(["2020-01-01T12:00", "2020-01-02T12:00"], [-3.0, -9.3], False, False)],
+        )
+
+    def test_from_database_Map_with_Array_values(self):
+        database_value = b'''
+        {
+             "index_type": "duration",
+              "data":[["1 hour", {"type": "array", "data": [-3.0, -9.3]}]]
+        }'''
+        value = from_database(database_value, value_type="map")
+        self.assertEqual(value.indexes, [Duration("1 hour")])
+        self.assertEqual(value.values, [Array([-3.0, -9.3])])
+
+    def test_from_database_Map_with_TimePattern_values(self):
+        database_value = b'''
+        {
+             "index_type": "float",
+              "data":[["2.3", {"type": "time_pattern", "data": {"M1-2": -9.3, "M3-12": -3.9}}]]
+        }'''
+        value = from_database(database_value, value_type="map")
+        self.assertEqual(value.indexes, [2.3])
+        self.assertEqual(value.values, [TimePattern(["M1-2", "M3-12"], [-9.3, -3.9])])
+
+    def test_Map_to_database(self):
+        map_value = Map(["a", "b"], [1.1, 2.2])
+        db_value, value_type = to_database(map_value)
+        raw = json.loads(db_value)
+        self.assertEqual(raw, {"index_type": "str", "data": [["a", 1.1], ["b", 2.2]]})
+        self.assertEqual(value_type, "map")
+
+    def test_Map_to_database_with_index_names(self):
+        nested_map = Map(["a"], [0.3])
+        nested_map.index_name = "nested index"
+        map_value = Map(["A"], [nested_map])
+        map_value.index_name = "index"
+        db_value, value_type = to_database(map_value)
+        raw = json.loads(db_value)
+        self.assertEqual(
+            raw,
+            {
+                "index_type": "str",
+                "index_name": "index",
+                "data": [
+                    ["A", {"type": "map", "index_type": "str", "index_name": "nested index", "data": [["a", 0.3]]}]
+                ],
+            },
+        )
+        self.assertEqual(value_type, "map")
+
+    def test_Map_to_database_with_TimeSeries_values(self):
+        time_series1 = TimeSeriesVariableResolution(["2020-01-01T12:00", "2020-01-02T12:00"], [2.3, 4.5], False, False)
+        time_series2 = TimeSeriesVariableResolution(
+            ["2020-01-01T12:00", "2020-01-02T12:00"], [-4.5, -2.3], False, False
+        )
+        map_value = Map(["a", "b"], [time_series1, time_series2])
+        db_value, value_type = to_database(map_value)
+        raw = json.loads(db_value)
+        expected = {
+            "index_type": "str",
+            "data": [
+                ["a", {"type": "time_series", "data": {"2020-01-01T12:00:00": 2.3, "2020-01-02T12:00:00": 4.5}}],
+                ["b", {"type": "time_series", "data": {"2020-01-01T12:00:00": -4.5, "2020-01-02T12:00:00": -2.3}}],
+            ],
+        }
+        self.assertEqual(raw, expected)
+        self.assertEqual(value_type, "map")
+
+    def test_Map_to_database_nested_maps(self):
+        nested_map = Map([Duration("2 months")], [Duration("5 days")])
+        map_value = Map([DateTime("2020-01-01T13:00")], [nested_map])
+        db_value, value_type = to_database(map_value)
+        raw = json.loads(db_value)
+        self.assertEqual(
+            raw,
+            {
+                "index_type": "date_time",
+                "data": [
+                    [
+                        "2020-01-01T13:00:00",
+                        {"type": "map", "index_type": "duration", "data": [["2M", {"type": "duration", "data": "5D"}]]},
+                    ]
+                ],
+            },
+        )
+        self.assertEqual(value_type, "map")
+
+    def test_Array_of_floats_to_database(self):
+        array = Array([-1.1, -2.2, -3.3])
+        db_value, value_type = to_database(array)
+        raw = json.loads(db_value)
+        self.assertEqual(raw, {"value_type": "float", "data": [-1.1, -2.2, -3.3]})
+        self.assertEqual(value_type, "array")
+
+    def test_Array_of_strings_to_database(self):
+        array = Array(["a", "b"])
+        db_value, value_type = to_database(array)
+        raw = json.loads(db_value)
+        self.assertEqual(raw, {"value_type": "str", "data": ["a", "b"]})
+        self.assertEqual(value_type, "array")
+
+    def test_Array_of_DateTimes_to_database(self):
+        array = Array([DateTime("2020-01-01T13:00")])
+        db_value, value_type = to_database(array)
+        raw = json.loads(db_value)
+        self.assertEqual(raw, {"value_type": "date_time", "data": ["2020-01-01T13:00:00"]})
+        self.assertEqual(value_type, "array")
+
+    def test_Array_of_Durations_to_database(self):
+        array = Array([Duration("4 months")])
+        db_value, value_type = to_database(array)
+        raw = json.loads(db_value)
+        self.assertEqual(raw, {"value_type": "duration", "data": ["4M"]})
+        self.assertEqual(value_type, "array")
+
+    def test_Array_of_floats_from_database(self):
+        database_value = b"""{
+            "value_type": "float",
+            "data": [1.2, 2.3]
+        }"""
+        array = from_database(database_value, value_type="array")
+        self.assertEqual(array.values, [1.2, 2.3])
+        self.assertEqual(array.indexes, [0, 1])
+        self.assertEqual(array.index_name, "i")
+
+    def test_Array_of_default_value_type_from_database(self):
+        database_value = b"""{
+            "data": [1.2, 2.3]
+        }"""
+        array = from_database(database_value, value_type="array")
+        self.assertEqual(array.values, [1.2, 2.3])
+        self.assertEqual(array.indexes, [0, 1])
+        self.assertEqual(array.index_name, "i")
+
+    def test_Array_of_strings_from_database(self):
+        database_value = b"""{
+            "value_type": "str",
+            "data": ["A", "B"]
+        }"""
+        array = from_database(database_value, value_type="array")
+        self.assertEqual(array.values, ["A", "B"])
+        self.assertEqual(array.indexes, [0, 1])
+        self.assertEqual(array.index_name, "i")
+
+    def test_Array_of_DateTimes_from_database(self):
+        database_value = b"""{
+            "value_type": "date_time",
+            "data": ["2020-03-25T10:34:00"]
+        }"""
+        array = from_database(database_value, value_type="array")
+        self.assertEqual(array.values, [DateTime("2020-03-25T10:34:00")])
+        self.assertEqual(array.indexes, [0])
+        self.assertEqual(array.index_name, "i")
+
+    def test_Array_of_Durations_from_database(self):
+        database_value = b"""{
+            "value_type": "duration",
+            "data": ["2 years", "7 seconds"]
+        }"""
+        array = from_database(database_value, value_type="array")
+        self.assertEqual(array.values, [Duration("2 years"), Duration("7s")])
+        self.assertEqual(array.indexes, [0, 1])
+        self.assertEqual(array.index_name, "i")
+
+    def test_Array_from_database_with_index_name(self):
+        database_value = b"""{
+            "value_type": "float",
+            "index_name": "index",
+            "data": [1.2]
+        }"""
+        array = from_database(database_value, value_type="array")
+        self.assertEqual(array.values, [1.2])
+        self.assertEqual(array.indexes, [0])
+        self.assertEqual(array.index_name, "index")
+
+    def test_Array_constructor_converts_ints_to_floats(self):
+        array = Array([9, 5])
+        self.assertIs(array.value_type, float)
+        self.assertEqual(array.values, [9.0, 5.0])
+
+    def test_DateTime_copy_construction(self):
+        date_time = DateTime("2019-07-03T09:09:09")
+        copied = DateTime(date_time)
+        self.assertEqual(copied, date_time)
+
+    def test_Duration_copy_construction(self):
+        duration = Duration("3 minutes")
+        copied = Duration(duration)
+        self.assertEqual(copied, duration)
+
+    def test_DateTime_equality(self):
+        date_time = DateTime(dateutil.parser.parse("2019-07-03T09:09:09"))
+        self.assertEqual(date_time, date_time)
+        equal_date_time = DateTime(dateutil.parser.parse("2019-07-03T09:09:09"))
+        self.assertEqual(date_time, equal_date_time)
+        inequal_date_time = DateTime(dateutil.parser.parse("2018-07-03T09:09:09"))
+        self.assertNotEqual(date_time, inequal_date_time)
+
+    def test_Duration_equality(self):
+        duration = Duration(duration_to_relativedelta("3 minutes"))
+        self.assertEqual(duration, duration)
+        equal_duration = Duration(duration_to_relativedelta("3m"))
+        self.assertEqual(duration, equal_duration)
+        inequal_duration = Duration(duration_to_relativedelta("3 seconds"))
+        self.assertNotEqual(duration, inequal_duration)
+
+    def test_Map_equality(self):
+        map_value = Map(["a"], [-2.3])
+        self.assertEqual(map_value, Map(["a"], [-2.3]))
+        nested_map = Map(["a"], [-2.3])
+        map_value = Map(["A"], [nested_map])
+        self.assertEqual(map_value, Map(["A"], [Map(["a"], [-2.3])]))
+
+    def test_TimePattern_equality(self):
+        pattern = TimePattern(["D1-2", "D3-7"], np.array([-2.3, -5.0]))
+        self.assertEqual(pattern, pattern)
+        equal_pattern = TimePattern(["D1-2", "D3-7"], np.array([-2.3, -5.0]))
+        self.assertEqual(pattern, equal_pattern)
+        inequal_pattern = TimePattern(["M1-3", "M4-12"], np.array([-5.0, 23.0]))
+        self.assertNotEqual(pattern, inequal_pattern)
+
+    def test_TimeSeriesFixedResolution_equality(self):
+        series = TimeSeriesFixedResolution("2019-01-03T00:30:33", "1D", [3.0, 2.0, 1.0], False, False)
+        self.assertEqual(series, series)
+        equal_series = TimeSeriesFixedResolution("2019-01-03T00:30:33", "1D", [3.0, 2.0, 1.0], False, False)
+        self.assertEqual(series, equal_series)
+        inequal_series = TimeSeriesFixedResolution("2019-01-03T00:30:33", "1D", [3.0, 2.0, 1.0], True, False)
+        self.assertNotEqual(series, inequal_series)
+
+    def test_TimeSeriesVariableResolution_equality(self):
+        series = TimeSeriesVariableResolution(["2000-01-01T00:00", "2001-01-01T00:00"], [4.2, 2.4], True, True)
+        self.assertEqual(series, series)
+        equal_series = TimeSeriesVariableResolution(["2000-01-01T00:00", "2001-01-01T00:00"], [4.2, 2.4], True, True)
+        self.assertEqual(series, equal_series)
+        inequal_series = TimeSeriesVariableResolution(["2000-01-01T00:00", "2002-01-01T00:00"], [4.2, 2.4], False, True)
+        self.assertNotEqual(series, inequal_series)
+
+    def test_IndexedValue_constructor_converts_values_to_floats(self):
+        value = IndexedNumberArray("", [4, -9, 11])
+        self.assertEqual(value.values.dtype, np.dtype(float))
+        numpy.testing.assert_equal(value.values, numpy.array([4.0, -9.0, 11.0]))
+        value = IndexedNumberArray("", numpy.array([16, -251, 99]))
+        self.assertEqual(value.values.dtype, np.dtype(float))
+        numpy.testing.assert_equal(value.values, numpy.array([16.0, -251.0, 99.0]))
+
+    def test_Map_is_nested(self):
+        map_value = Map(["a"], [-2.3])
+        self.assertFalse(map_value.is_nested())
+        nested_map = Map(["a"], [-2.3])
+        map_value = Map(["A"], [nested_map])
+        self.assertTrue(map_value.is_nested())
+
+    def test_convert_leaf_maps_to_specialized_containers_non_nested_map(self):
+        map_value = Map([DateTime("2000-01-01T00:00"), DateTime("2000-01-02T00:00")], [-3.2, -2.3])
+        converted = convert_leaf_maps_to_specialized_containers(map_value)
+        self.assertEqual(
+            converted,
+            TimeSeriesVariableResolution(
+                ["2000-01-01T00:00", "2000-01-02T00:00"], [-3.2, -2.3], False, False, index_name=Map.DEFAULT_INDEX_NAME
+            ),
+        )
+
+    def test_convert_leaf_maps_to_specialized_containers_no_conversion(self):
+        map_value = Map(["a", "b"], [-3.2, -2.3])
+        converted = convert_leaf_maps_to_specialized_containers(map_value)
+        self.assertEqual(converted, map_value)
+
+    def test_convert_leaf_maps_to_specialized_containers_nested_map(self):
+        nested1 = Map(["a", "b"], [-3.2, -2.3])
+        nested2 = Map([DateTime("2000-01-01T00:00"), DateTime("2000-01-02T00:00")], [-3.2, -2.3])
+        map_value = Map([1.0, 2.0, 3.0], [nested1, nested2, 4.4])
+        converted = convert_leaf_maps_to_specialized_containers(map_value)
+        time_series = TimeSeriesVariableResolution(
+            ["2000-01-01T00:00", "2000-01-02T00:00"], [-3.2, -2.3], False, False, index_name=Map.DEFAULT_INDEX_NAME
+        )
+        expected = Map([1.0, 2.0, 3.0], [nested1, time_series, 4.4])
+        self.assertEqual(converted, expected)
+
+    def test_convert_non_nested_map_to_table(self):
+        map_ = Map(["a", "b"], [-3.2, -2.3])
+        table = convert_map_to_table(map_)
+        self.assertEqual(table, [["a", -3.2], ["b", -2.3]])
+
+    def test_convert_nested_map_to_table(self):
+        map1 = Map(["a", "b"], [-3.2, -2.3])
+        map2 = Map(["c", "d"], [3.2, 2.3])
+        nested_map = Map(["A", "B"], [map1, map2])
+        table = convert_map_to_table(nested_map)
+        self.assertEqual(table, [["A", "a", -3.2], ["A", "b", -2.3], ["B", "c", 3.2], ["B", "d", 2.3]])
+
+    def test_convert_uneven_nested_map_to_table(self):
+        map1 = Map(["a", "b"], [-3.2, -2.3])
+        nested_map = Map(["A", "B"], [map1, 42.0])
+        table = convert_map_to_table(nested_map, False)
+        self.assertEqual(table, [["A", "a", -3.2], ["A", "b", -2.3], ["B", 42.0]])
+
+    def test_convert_nested_map_to_table_with_padding(self):
+        map1 = Map(["a", "b"], [-3.2, -2.3])
+        nested_map = Map(["A", "B"], [map1, 42.0])
+        table = convert_map_to_table(nested_map, True)
+        self.assertEqual(table, [["A", "a", -3.2], ["A", "b", -2.3], ["B", 42.0, None]])
+
+    def test_convert_nested_map_to_table_using_special_empty_cells(self):
+        map1 = Map(["a"], [-2.3])
+        nested_map = Map(["A", "B", "C"], [map1, 42.0, map1])
+        empty = object()
+        table = convert_map_to_table(nested_map, True, empty=empty)
+        self.assertEqual(table, [["A", "a", -2.3], ["B", 42.0, empty], ["C", "a", -2.3]])
+        self.assertIs(table[1][2], empty)
+
+    def test_convert_containers_to_maps_empty_map(self):
+        self.assertEqual(convert_containers_to_maps(Map([], [], str)), Map([], [], str))
+
+    def test_convert_containers_to_maps_time_series(self):
+        time_series = TimeSeriesVariableResolution(["2020-11-27T12:55", "2020-11-27T13:00"], [2.5, 2.3], False, False)
+        map_ = convert_containers_to_maps(time_series)
+        self.assertEqual(
+            map_,
+            Map(
+                [DateTime("2020-11-27T12:55"), DateTime("2020-11-27T13:00")],
+                [2.5, 2.3],
+                index_name=TimeSeries.DEFAULT_INDEX_NAME,
+            ),
+        )
+
+    def test_convert_containers_to_maps_map_with_time_series(self):
+        time_series = TimeSeriesVariableResolution(["2020-11-27T12:55", "2020-11-27T13:00"], [2.5, 2.3], False, False)
+        map_ = Map(["a", "b"], [-1.1, time_series])
+        converted = convert_containers_to_maps(map_)
+        expected = Map(
+            ["a", "b"],
+            [
+                -1.1,
+                Map(
+                    [DateTime("2020-11-27T12:55"), DateTime("2020-11-27T13:00")],
+                    [2.5, 2.3],
+                    index_name=TimeSeries.DEFAULT_INDEX_NAME,
+                ),
+            ],
+        )
+        self.assertEqual(converted, expected)
+
+    def convert_map_to_dict(self):
+        map1 = Map(["a", "b"], [-3.2, -2.3])
+        map2 = Map(["c", "d"], [3.2, 2.3])
+        nested_map = Map(["A", "B"], [map1, map2])
+        self.assertEqual(nested_map, {"A": {"a": -3.2, "b": -2.3}, "B": {"c": 3.2, "d": 2.3}})
+
+
+if __name__ == "__main__":
+    unittest.main()
```

