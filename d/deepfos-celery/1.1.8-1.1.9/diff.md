# Comparing `tmp/deepfos_celery-1.1.8-py3-none-any.whl.zip` & `tmp/deepfos_celery-1.1.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,44 @@
-Zip file size: 439755 bytes, number of entries: 184
--rw-rw-rw-  2.0 fat     6078 b- defN 24-Jan-29 10:58 celery/__init__.py
+Zip file size: 439951 bytes, number of entries: 184
+-rw-rw-rw-  2.0 fat     6254 b- defN 24-Jan-30 03:14 celery/__init__.py
 -rw-rw-rw-  2.0 fat      401 b- defN 22-Sep-23 05:35 celery/__main__.py
 -rw-rw-rw-  2.0 fat     5049 b- defN 22-Sep-23 05:35 celery/_state.py
 -rw-rw-rw-  2.0 fat    24135 b- defN 22-Sep-23 05:35 celery/beat.py
 -rw-rw-rw-  2.0 fat    12297 b- defN 23-Aug-08 02:44 celery/bootsteps.py
 -rw-rw-rw-  2.0 fat    58315 b- defN 23-Mar-20 08:36 celery/canvas.py
 -rw-rw-rw-  2.0 fat     8601 b- defN 22-Sep-23 05:35 celery/exceptions.py
 -rw-rw-rw-  2.0 fat    17016 b- defN 22-Sep-23 05:35 celery/local.py
 -rw-rw-rw-  2.0 fat    24279 b- defN 22-Sep-23 05:35 celery/platforms.py
 -rw-rw-rw-  2.0 fat    34176 b- defN 23-Aug-08 05:47 celery/result.py
 -rw-rw-rw-  2.0 fat    29259 b- defN 22-Sep-23 05:35 celery/schedules.py
 -rw-rw-rw-  2.0 fat     4273 b- defN 22-Sep-23 05:23 celery/signals.py
 -rw-rw-rw-  2.0 fat     3260 b- defN 22-Sep-23 05:35 celery/states.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-11 10:12 celery/aio/__init__.py
--rw-rw-rw-  2.0 fat     6304 b- defN 23-Sep-01 03:32 celery/aio/amqp.py
--rw-rw-rw-  2.0 fat    24479 b- defN 23-Oct-08 02:07 celery/aio/backend.py
--rw-rw-rw-  2.0 fat     3008 b- defN 23-Aug-10 11:13 celery/aio/canvas.py
--rw-rw-rw-  2.0 fat     8918 b- defN 23-Aug-10 11:13 celery/aio/control.py
--rw-rw-rw-  2.0 fat     6242 b- defN 23-Aug-10 11:13 celery/aio/dispatcher.py
--rw-rw-rw-  2.0 fat      145 b- defN 23-Apr-11 10:12 celery/aio/events.py
--rw-rw-rw-  2.0 fat    13301 b- defN 23-Sep-04 10:11 celery/aio/result.py
--rw-rw-rw-  2.0 fat     3017 b- defN 23-Aug-10 11:13 celery/aio/task.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-Jan-30 03:06 celery/aio/__init__.py
+-rw-rw-rw-  2.0 fat     6461 b- defN 24-Jan-30 03:06 celery/aio/amqp.py
+-rw-rw-rw-  2.0 fat    25158 b- defN 24-Jan-30 03:06 celery/aio/backend.py
+-rw-rw-rw-  2.0 fat     3008 b- defN 24-Jan-30 03:06 celery/aio/canvas.py
+-rw-rw-rw-  2.0 fat     8918 b- defN 24-Jan-30 03:06 celery/aio/control.py
+-rw-rw-rw-  2.0 fat     6242 b- defN 24-Jan-30 03:06 celery/aio/dispatcher.py
+-rw-rw-rw-  2.0 fat      145 b- defN 24-Jan-30 03:06 celery/aio/events.py
+-rw-rw-rw-  2.0 fat    13301 b- defN 24-Jan-30 03:06 celery/aio/result.py
+-rw-rw-rw-  2.0 fat     3017 b- defN 24-Jan-30 03:06 celery/aio/task.py
 -rw-rw-rw-  2.0 fat     2459 b- defN 22-Sep-23 05:35 celery/app/__init__.py
--rw-rw-rw-  2.0 fat    23519 b- defN 23-Sep-01 03:33 celery/app/amqp.py
+-rw-rw-rw-  2.0 fat    24144 b- defN 24-Jan-30 03:06 celery/app/amqp.py
 -rw-rw-rw-  2.0 fat     1445 b- defN 22-Sep-23 05:23 celery/app/annotations.py
 -rw-rw-rw-  2.0 fat     2288 b- defN 22-Sep-23 05:35 celery/app/autoretry.py
--rw-rw-rw-  2.0 fat     2826 b- defN 23-Aug-08 07:24 celery/app/backends.py
--rw-rw-rw-  2.0 fat    56868 b- defN 23-Aug-08 07:25 celery/app/base.py
+-rw-rw-rw-  2.0 fat     2897 b- defN 24-Jan-30 03:06 celery/app/backends.py
+-rw-rw-rw-  2.0 fat    58408 b- defN 24-Jan-30 03:06 celery/app/base.py
 -rw-rw-rw-  2.0 fat     6673 b- defN 22-Sep-23 05:23 celery/app/builtins.py
 -rw-rw-rw-  2.0 fat    16748 b- defN 22-Sep-23 05:35 celery/app/control.py
 -rw-rw-rw-  2.0 fat    14654 b- defN 24-Jan-29 10:55 celery/app/defaults.py
 -rw-rw-rw-  2.0 fat     1326 b- defN 22-Sep-23 05:23 celery/app/events.py
 -rw-rw-rw-  2.0 fat     9169 b- defN 22-Sep-23 05:35 celery/app/log.py
 -rw-rw-rw-  2.0 fat     2002 b- defN 22-Sep-23 05:35 celery/app/registry.py
 -rw-rw-rw-  2.0 fat     4685 b- defN 22-Sep-23 05:35 celery/app/routes.py
--rw-rw-rw-  2.0 fat    41027 b- defN 23-Apr-11 10:12 celery/app/task.py
+-rw-rw-rw-  2.0 fat    41027 b- defN 24-Jan-30 03:06 celery/app/task.py
 -rw-rw-rw-  2.0 fat    25909 b- defN 22-Sep-23 05:35 celery/app/trace.py
 -rw-rw-rw-  2.0 fat    13160 b- defN 22-Sep-23 05:35 celery/app/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Sep-23 05:23 celery/apps/__init__.py
 -rw-rw-rw-  2.0 fat     5161 b- defN 22-Sep-23 05:35 celery/apps/beat.py
 -rw-rw-rw-  2.0 fat    16418 b- defN 22-Sep-23 05:35 celery/apps/multi.py
 -rw-rw-rw-  2.0 fat    13474 b- defN 22-Sep-23 05:35 celery/apps/worker.py
 -rw-rw-rw-  2.0 fat       23 b- defN 22-Sep-23 05:23 celery/backends/__init__.py
@@ -52,15 +52,15 @@
 -rw-rw-rw-  2.0 fat     6847 b- defN 22-Sep-23 05:35 celery/backends/cosmosdbsql.py
 -rw-rw-rw-  2.0 fat     3173 b- defN 22-Sep-23 05:35 celery/backends/couchbase.py
 -rw-rw-rw-  2.0 fat     2919 b- defN 22-Sep-23 05:35 celery/backends/couchdb.py
 -rw-rw-rw-  2.0 fat    17262 b- defN 22-Sep-23 05:35 celery/backends/dynamodb.py
 -rw-rw-rw-  2.0 fat     8469 b- defN 22-Sep-23 05:35 celery/backends/elasticsearch.py
 -rw-rw-rw-  2.0 fat     2925 b- defN 22-Sep-23 05:35 celery/backends/filesystem.py
 -rw-rw-rw-  2.0 fat    11189 b- defN 22-Sep-23 05:35 celery/backends/mongodb.py
--rw-rw-rw-  2.0 fat    23218 b- defN 23-Sep-04 03:49 celery/backends/redis.py
+-rw-rw-rw-  2.0 fat    23835 b- defN 24-Jan-30 03:06 celery/backends/redis.py
 -rw-rw-rw-  2.0 fat    12077 b- defN 22-Sep-23 05:23 celery/backends/rpc.py
 -rw-rw-rw-  2.0 fat     2752 b- defN 22-Sep-23 05:23 celery/backends/s3.py
 -rw-rw-rw-  2.0 fat     7771 b- defN 22-Sep-23 05:23 celery/backends/database/__init__.py
 -rw-rw-rw-  2.0 fat     3351 b- defN 22-Sep-23 05:23 celery/backends/database/models.py
 -rw-rw-rw-  2.0 fat     2866 b- defN 22-Sep-23 05:35 celery/backends/database/session.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Sep-23 05:23 celery/bin/__init__.py
 -rw-rw-rw-  2.0 fat     9960 b- defN 22-Sep-23 05:35 celery/bin/amqp.py
@@ -77,19 +77,19 @@
 -rw-rw-rw-  2.0 fat    15363 b- defN 22-Sep-23 05:35 celery/bin/multi.py
 -rw-rw-rw-  2.0 fat     2578 b- defN 22-Sep-23 05:35 celery/bin/purge.py
 -rw-rw-rw-  2.0 fat     1007 b- defN 22-Sep-23 05:35 celery/bin/result.py
 -rw-rw-rw-  2.0 fat     4809 b- defN 22-Sep-23 05:35 celery/bin/shell.py
 -rw-rw-rw-  2.0 fat     3072 b- defN 22-Sep-23 05:35 celery/bin/upgrade.py
 -rw-rw-rw-  2.0 fat    11751 b- defN 23-Aug-08 03:36 celery/bin/worker.py
 -rw-rw-rw-  2.0 fat      852 b- defN 22-Sep-23 05:35 celery/concurrency/__init__.py
--rw-rw-rw-  2.0 fat    57584 b- defN 24-Jan-29 10:53 celery/concurrency/asynpool.py
+-rw-rw-rw-  2.0 fat    58104 b- defN 24-Jan-30 03:10 celery/concurrency/asynpool.py
 -rw-rw-rw-  2.0 fat     4373 b- defN 23-Aug-15 06:14 celery/concurrency/base.py
 -rw-rw-rw-  2.0 fat     4156 b- defN 22-Sep-23 05:35 celery/concurrency/eventlet.py
 -rw-rw-rw-  2.0 fat     3415 b- defN 22-Sep-23 05:35 celery/concurrency/gevent.py
--rw-rw-rw-  2.0 fat     5931 b- defN 23-Aug-08 03:35 celery/concurrency/prefork.py
+-rw-rw-rw-  2.0 fat     5931 b- defN 24-Jan-30 03:06 celery/concurrency/prefork.py
 -rw-rw-rw-  2.0 fat      693 b- defN 22-Sep-23 05:23 celery/concurrency/solo.py
 -rw-rw-rw-  2.0 fat     1248 b- defN 22-Sep-23 05:23 celery/concurrency/thread.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Sep-23 05:23 celery/contrib/__init__.py
 -rw-rw-rw-  2.0 fat     5091 b- defN 22-Sep-23 05:35 celery/contrib/abortable.py
 -rw-rw-rw-  2.0 fat    14163 b- defN 22-Sep-23 05:35 celery/contrib/migrate.py
 -rw-rw-rw-  2.0 fat     6507 b- defN 22-Sep-23 05:35 celery/contrib/pytest.py
 -rw-rw-rw-  2.0 fat     5021 b- defN 22-Sep-23 05:35 celery/contrib/rdb.py
@@ -100,36 +100,36 @@
 -rw-rw-rw-  2.0 fat     3394 b- defN 22-Sep-23 05:35 celery/contrib/testing/mocks.py
 -rw-rw-rw-  2.0 fat      208 b- defN 22-Sep-23 05:23 celery/contrib/testing/tasks.py
 -rw-rw-rw-  2.0 fat     5040 b- defN 22-Sep-23 05:35 celery/contrib/testing/worker.py
 -rw-rw-rw-  2.0 fat      477 b- defN 22-Sep-23 05:23 celery/events/__init__.py
 -rw-rw-rw-  2.0 fat    18050 b- defN 22-Sep-23 05:35 celery/events/cursesmon.py
 -rw-rw-rw-  2.0 fat     8987 b- defN 22-Sep-23 05:23 celery/events/dispatcher.py
 -rw-rw-rw-  2.0 fat     3116 b- defN 22-Sep-23 05:23 celery/events/dumper.py
--rw-rw-rw-  2.0 fat     1755 b- defN 23-Nov-28 02:03 celery/events/event.py
+-rw-rw-rw-  2.0 fat     1818 b- defN 24-Jan-30 03:06 celery/events/event.py
 -rw-rw-rw-  2.0 fat     4998 b- defN 22-Sep-23 05:23 celery/events/receiver.py
 -rw-rw-rw-  2.0 fat     3274 b- defN 22-Sep-23 05:35 celery/events/snapshot.py
 -rw-rw-rw-  2.0 fat    25773 b- defN 22-Sep-23 05:35 celery/events/state.py
 -rw-rw-rw-  2.0 fat       14 b- defN 22-Sep-23 05:23 celery/fixups/__init__.py
 -rw-rw-rw-  2.0 fat     6596 b- defN 22-Sep-23 05:35 celery/fixups/django.py
 -rw-rw-rw-  2.0 fat      490 b- defN 22-Sep-23 05:23 celery/loaders/__init__.py
 -rw-rw-rw-  2.0 fat      199 b- defN 22-Sep-23 05:23 celery/loaders/app.py
 -rw-rw-rw-  2.0 fat     8731 b- defN 22-Sep-23 05:35 celery/loaders/base.py
 -rw-rw-rw-  2.0 fat     1520 b- defN 22-Sep-23 05:23 celery/loaders/default.py
--rw-rw-rw-  2.0 fat      120 b- defN 23-Aug-08 09:47 celery/patches/__init__.py
--rw-rw-rw-  2.0 fat      201 b- defN 23-Aug-08 09:47 celery/patches/kombu/__init__.py
--rw-rw-rw-  2.0 fat     6656 b- defN 23-Aug-10 11:13 celery/patches/kombu/common.py
--rw-rw-rw-  2.0 fat    12680 b- defN 23-Aug-10 11:13 celery/patches/kombu/connection.py
--rw-rw-rw-  2.0 fat     4396 b- defN 23-Aug-10 11:13 celery/patches/kombu/entity.py
--rw-rw-rw-  2.0 fat     2702 b- defN 23-Aug-10 11:13 celery/patches/kombu/exchange.py
--rw-rw-rw-  2.0 fat     8775 b- defN 23-Aug-10 11:13 celery/patches/kombu/messaging.py
--rw-rw-rw-  2.0 fat     1991 b- defN 23-Aug-10 11:13 celery/patches/kombu/pools.py
--rw-rw-rw-  2.0 fat    37932 b- defN 23-Oct-07 05:45 celery/patches/kombu/redis.py
--rw-rw-rw-  2.0 fat     9057 b- defN 23-Oct-08 07:01 celery/patches/kombu/redis_cluster.py
--rw-rw-rw-  2.0 fat     6623 b- defN 23-Aug-10 11:13 celery/patches/kombu/utils.py
--rw-rw-rw-  2.0 fat     4729 b- defN 23-Sep-04 10:13 celery/patches/redis/__init__.py
+-rw-rw-rw-  2.0 fat      128 b- defN 24-Jan-30 03:06 celery/patches/__init__.py
+-rw-rw-rw-  2.0 fat      210 b- defN 24-Jan-30 03:06 celery/patches/kombu/__init__.py
+-rw-rw-rw-  2.0 fat     6656 b- defN 24-Jan-30 03:06 celery/patches/kombu/common.py
+-rw-rw-rw-  2.0 fat    12680 b- defN 24-Jan-30 03:06 celery/patches/kombu/connection.py
+-rw-rw-rw-  2.0 fat     4396 b- defN 24-Jan-30 03:06 celery/patches/kombu/entity.py
+-rw-rw-rw-  2.0 fat     2702 b- defN 24-Jan-30 03:06 celery/patches/kombu/exchange.py
+-rw-rw-rw-  2.0 fat     8775 b- defN 24-Jan-30 03:06 celery/patches/kombu/messaging.py
+-rw-rw-rw-  2.0 fat     1991 b- defN 24-Jan-30 03:06 celery/patches/kombu/pools.py
+-rw-rw-rw-  2.0 fat    39028 b- defN 24-Jan-30 03:06 celery/patches/kombu/redis.py
+-rw-rw-rw-  2.0 fat     9349 b- defN 24-Jan-30 03:06 celery/patches/kombu/redis_cluster.py
+-rw-rw-rw-  2.0 fat     6623 b- defN 24-Jan-30 03:06 celery/patches/kombu/utils.py
+-rw-rw-rw-  2.0 fat     4870 b- defN 24-Jan-30 03:06 celery/patches/redis/__init__.py
 -rw-rw-rw-  2.0 fat     2278 b- defN 22-Sep-23 05:35 celery/security/__init__.py
 -rw-rw-rw-  2.0 fat     2993 b- defN 22-Sep-23 05:35 celery/security/certificate.py
 -rw-rw-rw-  2.0 fat     1042 b- defN 22-Sep-23 05:35 celery/security/key.py
 -rw-rw-rw-  2.0 fat     4206 b- defN 22-Sep-23 05:35 celery/security/serialization.py
 -rw-rw-rw-  2.0 fat      845 b- defN 22-Sep-23 05:23 celery/security/utils.py
 -rw-rw-rw-  2.0 fat      935 b- defN 22-Sep-23 05:23 celery/utils/__init__.py
 -rw-rw-rw-  2.0 fat     2874 b- defN 22-Sep-23 05:23 celery/utils/abstract.py
@@ -138,49 +138,49 @@
 -rw-rw-rw-  2.0 fat     3620 b- defN 22-Sep-23 05:23 celery/utils/deprecated.py
 -rw-rw-rw-  2.0 fat    10550 b- defN 22-Sep-23 05:35 celery/utils/functional.py
 -rw-rw-rw-  2.0 fat     9041 b- defN 22-Sep-23 05:23 celery/utils/graph.py
 -rw-rw-rw-  2.0 fat     4826 b- defN 22-Sep-23 05:35 celery/utils/imports.py
 -rw-rw-rw-  2.0 fat     2776 b- defN 22-Sep-23 05:23 celery/utils/iso8601.py
 -rw-rw-rw-  2.0 fat     8626 b- defN 22-Sep-23 05:35 celery/utils/log.py
 -rw-rw-rw-  2.0 fat     2858 b- defN 22-Sep-23 05:23 celery/utils/nodenames.py
--rw-rw-rw-  2.0 fat     6136 b- defN 23-Apr-11 10:12 celery/utils/objects.py
--rw-rw-rw-  2.0 fat     6788 b- defN 23-Jun-19 06:33 celery/utils/psutil.py
+-rw-rw-rw-  2.0 fat     6136 b- defN 24-Jan-30 03:06 celery/utils/objects.py
+-rw-rw-rw-  2.0 fat     6788 b- defN 24-Jan-30 03:06 celery/utils/psutil.py
 -rw-rw-rw-  2.0 fat     9084 b- defN 22-Sep-23 05:35 celery/utils/saferepr.py
 -rw-rw-rw-  2.0 fat     8069 b- defN 22-Sep-23 05:35 celery/utils/serialization.py
 -rw-rw-rw-  2.0 fat     1093 b- defN 22-Sep-23 05:35 celery/utils/sysinfo.py
 -rw-rw-rw-  2.0 fat     4568 b- defN 22-Sep-23 05:23 celery/utils/term.py
 -rw-rw-rw-  2.0 fat     5779 b- defN 22-Sep-23 05:35 celery/utils/text.py
 -rw-rw-rw-  2.0 fat     9626 b- defN 22-Sep-23 05:35 celery/utils/threads.py
 -rw-rw-rw-  2.0 fat    12375 b- defN 22-Sep-23 05:35 celery/utils/time.py
 -rw-rw-rw-  2.0 fat     4533 b- defN 22-Sep-23 05:35 celery/utils/timer2.py
 -rw-rw-rw-  2.0 fat       74 b- defN 22-Sep-23 05:23 celery/utils/dispatch/__init__.py
--rw-rw-rw-  2.0 fat    17707 b- defN 23-Aug-10 11:13 celery/utils/dispatch/signal.py
+-rw-rw-rw-  2.0 fat    17707 b- defN 24-Jan-30 03:06 celery/utils/dispatch/signal.py
 -rw-rw-rw-  2.0 fat      299 b- defN 22-Sep-23 05:23 celery/utils/static/__init__.py
 -rw-rw-rw-  2.0 fat     2556 b- defN 22-Sep-23 05:23 celery/utils/static/celery_128.png
 -rw-rw-rw-  2.0 fat       95 b- defN 22-Sep-23 05:23 celery/worker/__init__.py
 -rw-rw-rw-  2.0 fat     4593 b- defN 22-Sep-23 05:23 celery/worker/autoscale.py
 -rw-rw-rw-  2.0 fat     7517 b- defN 22-Sep-23 05:23 celery/worker/components.py
 -rw-rw-rw-  2.0 fat    16771 b- defN 22-Sep-23 05:35 celery/worker/control.py
 -rw-rw-rw-  2.0 fat     2107 b- defN 22-Sep-23 05:23 celery/worker/heartbeat.py
 -rw-rw-rw-  2.0 fat     3857 b- defN 22-Sep-23 05:35 celery/worker/loops.py
 -rw-rw-rw-  2.0 fat     3630 b- defN 22-Sep-23 05:23 celery/worker/pidbox.py
 -rw-rw-rw-  2.0 fat    22670 b- defN 23-Aug-15 06:12 celery/worker/request.py
 -rw-rw-rw-  2.0 fat     7639 b- defN 22-Sep-23 05:35 celery/worker/state.py
 -rw-rw-rw-  2.0 fat     6893 b- defN 22-Sep-23 05:35 celery/worker/strategy.py
--rw-rw-rw-  2.0 fat    14621 b- defN 23-Nov-13 09:08 celery/worker/worker.py
+-rw-rw-rw-  2.0 fat    15033 b- defN 24-Jan-30 03:06 celery/worker/worker.py
 -rw-rw-rw-  2.0 fat      391 b- defN 22-Sep-23 05:23 celery/worker/consumer/__init__.py
 -rw-rw-rw-  2.0 fat      525 b- defN 22-Sep-23 05:23 celery/worker/consumer/agent.py
 -rw-rw-rw-  2.0 fat     1026 b- defN 22-Sep-23 05:23 celery/worker/consumer/connection.py
 -rw-rw-rw-  2.0 fat    21497 b- defN 22-Sep-23 05:35 celery/worker/consumer/consumer.py
 -rw-rw-rw-  2.0 fat      946 b- defN 22-Sep-23 05:23 celery/worker/consumer/control.py
 -rw-rw-rw-  2.0 fat     2054 b- defN 22-Sep-23 05:23 celery/worker/consumer/events.py
 -rw-rw-rw-  2.0 fat     6833 b- defN 22-Sep-23 05:23 celery/worker/consumer/gossip.py
 -rw-rw-rw-  2.0 fat      930 b- defN 22-Sep-23 05:23 celery/worker/consumer/heart.py
 -rw-rw-rw-  2.0 fat     2519 b- defN 22-Sep-23 05:23 celery/worker/consumer/mingle.py
 -rw-rw-rw-  2.0 fat     1959 b- defN 22-Sep-23 05:35 celery/worker/consumer/tasks.py
--rw-rw-rw-  2.0 fat     2631 b- defN 24-Jan-29 10:59 deepfos_celery-1.1.8.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     6651 b- defN 24-Jan-29 10:59 deepfos_celery-1.1.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Jan-29 10:59 deepfos_celery-1.1.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       49 b- defN 24-Jan-29 10:59 deepfos_celery-1.1.8.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat        7 b- defN 24-Jan-29 10:59 deepfos_celery-1.1.8.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    15041 b- defN 24-Jan-29 10:59 deepfos_celery-1.1.8.dist-info/RECORD
-184 files, 1464241 bytes uncompressed, 416519 bytes compressed:  71.6%
+-rw-rw-rw-  2.0 fat     2631 b- defN 24-Jan-30 03:34 deepfos_celery-1.1.9.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     6651 b- defN 24-Jan-30 03:34 deepfos_celery-1.1.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Jan-30 03:34 deepfos_celery-1.1.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       49 b- defN 24-Jan-30 03:34 deepfos_celery-1.1.9.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat        7 b- defN 24-Jan-30 03:34 deepfos_celery-1.1.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    15041 b- defN 24-Jan-30 03:34 deepfos_celery-1.1.9.dist-info/RECORD
+184 files, 1470647 bytes uncompressed, 416715 bytes compressed:  71.7%
```

## zipnote {}

```diff
@@ -528,26 +528,26 @@
 
 Filename: celery/worker/consumer/mingle.py
 Comment: 
 
 Filename: celery/worker/consumer/tasks.py
 Comment: 
 
-Filename: deepfos_celery-1.1.8.dist-info/LICENSE
+Filename: deepfos_celery-1.1.9.dist-info/LICENSE
 Comment: 
 
-Filename: deepfos_celery-1.1.8.dist-info/METADATA
+Filename: deepfos_celery-1.1.9.dist-info/METADATA
 Comment: 
 
-Filename: deepfos_celery-1.1.8.dist-info/WHEEL
+Filename: deepfos_celery-1.1.9.dist-info/WHEEL
 Comment: 
 
-Filename: deepfos_celery-1.1.8.dist-info/entry_points.txt
+Filename: deepfos_celery-1.1.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: deepfos_celery-1.1.8.dist-info/top_level.txt
+Filename: deepfos_celery-1.1.9.dist-info/top_level.txt
 Comment: 
 
-Filename: deepfos_celery-1.1.8.dist-info/RECORD
+Filename: deepfos_celery-1.1.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## celery/__init__.py

```diff
@@ -1,176 +1,176 @@
-"""Distributed Task Queue."""
-# :copyright: (c) 2016-20206 Asif Saif Uddin, celery core and individual
-#                 contributors, All rights reserved.
-# :copyright: (c) 2015-2016 Ask Solem.  All rights reserved.
-# :copyright: (c) 2012-2014 GoPivotal, Inc., All rights reserved.
-# :copyright: (c) 2009 - 2012 Ask Solem and individual contributors,
-#                 All rights reserved.
-# :license:   BSD (3 Clause), see LICENSE for more details.
-
-import os
-import re
-import sys
-from collections import namedtuple
-
-# Lazy loading
-from . import local  # noqa
-from celery.patches import apply_patches
-
-apply_patches()
-
-SERIES = 'singularity'
-
-__version__ = '1.1.8'
-__author__ = 'DeepFOS'
-__contact__ = 'python@deepfinance.com'
-__homepage__ = 'http://celeryproject.org'
-__docformat__ = 'restructuredtext'
-__keywords__ = 'task job queue distributed messaging actor'
-
-# -eof meta-
-
-__all__ = (
-    'Celery', 'bugreport', 'shared_task', 'task', 'Task',
-    'current_app', 'current_task', 'maybe_signature',
-    'chain', 'chord', 'chunks', 'group', 'signature',
-    'xmap', 'xstarmap', 'uuid',
-)
-
-VERSION_BANNER = f'{__version__} ({SERIES})'
-
-version_info_t = namedtuple('version_info_t', (
-    'major', 'minor', 'micro', 'releaselevel', 'serial',
-))
-
-# bumpversion can only search for {current_version}
-# so we have to parse the version here.
-_temp = re.match(
-    r'(\d+)\.(\d+).(\d+)(.+)?', __version__).groups()
-VERSION = version_info = version_info_t(
-    int(_temp[0]), int(_temp[1]), int(_temp[2]), _temp[3] or '', '')
-del _temp
-del re
-
-if os.environ.get('C_IMPDEBUG'):  # pragma: no cover
-    import builtins
-
-    def debug_import(name, locals=None, globals=None,
-                     fromlist=None, level=-1, real_import=builtins.__import__):
-        glob = globals or getattr(sys, 'emarfteg_'[::-1])(1).f_globals
-        importer_name = glob and glob.get('__name__') or 'unknown'
-        print(f'-- {importer_name} imports {name}')
-        return real_import(name, locals, globals, fromlist, level)
-    builtins.__import__ = debug_import
-
-# This is never executed, but tricks static analyzers (PyDev, PyCharm,
-# pylint, etc.) into knowing the types of these symbols, and what
-# they contain.
-STATICA_HACK = True
-globals()['kcah_acitats'[::-1].upper()] = False
-if STATICA_HACK:  # pragma: no cover
-    from celery._state import current_app, current_task  # noqa
-    from celery.app import shared_task  # noqa
-    from celery.app.base import Celery  # noqa
-    from celery.app.task import Task  # noqa
-    from celery.app.utils import bugreport  # noqa
-    from celery.canvas import (chain, chord, chunks, group,  # noqa
-                               maybe_signature, signature, subtask, xmap,
-                               xstarmap)
-    from celery.utils import uuid  # noqa
-
-# Eventlet/gevent patching must happen before importing
-# anything else, so these tools must be at top-level.
-
-
-def _find_option_with_arg(argv, short_opts=None, long_opts=None):
-    """Search argv for options specifying short and longopt alternatives.
-
-    Returns:
-        str: value for option found
-    Raises:
-        KeyError: if option not found.
-    """
-    for i, arg in enumerate(argv):
-        if arg.startswith('-'):
-            if long_opts and arg.startswith('--'):
-                name, sep, val = arg.partition('=')
-                if name in long_opts:
-                    return val if sep else argv[i + 1]
-            if short_opts and arg in short_opts:
-                return argv[i + 1]
-    raise KeyError('|'.join(short_opts or [] + long_opts or []))
-
-
-def _patch_eventlet():
-    import eventlet.debug
-
-    eventlet.monkey_patch()
-    blockdetect = float(os.environ.get('EVENTLET_NOBLOCK', 0))
-    if blockdetect:
-        eventlet.debug.hub_blocking_detection(blockdetect, blockdetect)
-
-
-def _patch_gevent():
-    import gevent.monkey
-    import gevent.signal
-
-    gevent.monkey.patch_all()
-
-
-def maybe_patch_concurrency(argv=None, short_opts=None,
-                            long_opts=None, patches=None):
-    """Apply eventlet/gevent monkeypatches.
-
-    With short and long opt alternatives that specify the command line
-    option to set the pool, this makes sure that anything that needs
-    to be patched is completed as early as possible.
-    (e.g., eventlet/gevent monkey patches).
-    """
-    argv = argv if argv else sys.argv
-    short_opts = short_opts if short_opts else ['-P']
-    long_opts = long_opts if long_opts else ['--pool']
-    patches = patches if patches else {'eventlet': _patch_eventlet,
-                                       'gevent': _patch_gevent}
-    try:
-        pool = _find_option_with_arg(argv, short_opts, long_opts)
-    except KeyError:
-        pass
-    else:
-        try:
-            patcher = patches[pool]
-        except KeyError:
-            pass
-        else:
-            patcher()
-
-        # set up eventlet/gevent environments ASAP
-        from celery import concurrency
-        concurrency.get_implementation(pool)
-
-
-# this just creates a new module, that imports stuff on first attribute
-# access.  This makes the library faster to use.
-old_module, new_module = local.recreate_module(  # pragma: no cover
-    __name__,
-    by_module={
-        'celery.app': ['Celery', 'bugreport', 'shared_task'],
-        'celery.app.task': ['Task'],
-        'celery._state': ['current_app', 'current_task'],
-        'celery.canvas': [
-            'Signature', 'chain', 'chord', 'chunks', 'group',
-            'signature', 'maybe_signature', 'subtask',
-            'xmap', 'xstarmap',
-        ],
-        'celery.utils': ['uuid'],
-    },
-    direct={'task': 'celery.task'},
-    __package__='celery', __file__=__file__,
-    __path__=__path__, __doc__=__doc__, __version__=__version__,
-    __author__=__author__, __contact__=__contact__,
-    __homepage__=__homepage__, __docformat__=__docformat__, local=local,
-    VERSION=VERSION, SERIES=SERIES, VERSION_BANNER=VERSION_BANNER,
-    version_info_t=version_info_t,
-    version_info=version_info,
-    maybe_patch_concurrency=maybe_patch_concurrency,
-    _find_option_with_arg=_find_option_with_arg,
-)
+"""Distributed Task Queue."""
+# :copyright: (c) 2016-20206 Asif Saif Uddin, celery core and individual
+#                 contributors, All rights reserved.
+# :copyright: (c) 2015-2016 Ask Solem.  All rights reserved.
+# :copyright: (c) 2012-2014 GoPivotal, Inc., All rights reserved.
+# :copyright: (c) 2009 - 2012 Ask Solem and individual contributors,
+#                 All rights reserved.
+# :license:   BSD (3 Clause), see LICENSE for more details.
+
+import os
+import re
+import sys
+from collections import namedtuple
+
+# Lazy loading
+from . import local  # noqa
+from celery.patches import apply_patches
+
+apply_patches()
+
+SERIES = 'singularity'
+
+__version__ = '1.1.9'
+__author__ = 'DeepFOS'
+__contact__ = 'python@deepfinance.com'
+__homepage__ = 'http://celeryproject.org'
+__docformat__ = 'restructuredtext'
+__keywords__ = 'task job queue distributed messaging actor'
+
+# -eof meta-
+
+__all__ = (
+    'Celery', 'bugreport', 'shared_task', 'task', 'Task',
+    'current_app', 'current_task', 'maybe_signature',
+    'chain', 'chord', 'chunks', 'group', 'signature',
+    'xmap', 'xstarmap', 'uuid',
+)
+
+VERSION_BANNER = f'{__version__} ({SERIES})'
+
+version_info_t = namedtuple('version_info_t', (
+    'major', 'minor', 'micro', 'releaselevel', 'serial',
+))
+
+# bumpversion can only search for {current_version}
+# so we have to parse the version here.
+_temp = re.match(
+    r'(\d+)\.(\d+).(\d+)(.+)?', __version__).groups()
+VERSION = version_info = version_info_t(
+    int(_temp[0]), int(_temp[1]), int(_temp[2]), _temp[3] or '', '')
+del _temp
+del re
+
+if os.environ.get('C_IMPDEBUG'):  # pragma: no cover
+    import builtins
+
+    def debug_import(name, locals=None, globals=None,
+                     fromlist=None, level=-1, real_import=builtins.__import__):
+        glob = globals or getattr(sys, 'emarfteg_'[::-1])(1).f_globals
+        importer_name = glob and glob.get('__name__') or 'unknown'
+        print(f'-- {importer_name} imports {name}')
+        return real_import(name, locals, globals, fromlist, level)
+    builtins.__import__ = debug_import
+
+# This is never executed, but tricks static analyzers (PyDev, PyCharm,
+# pylint, etc.) into knowing the types of these symbols, and what
+# they contain.
+STATICA_HACK = True
+globals()['kcah_acitats'[::-1].upper()] = False
+if STATICA_HACK:  # pragma: no cover
+    from celery._state import current_app, current_task  # noqa
+    from celery.app import shared_task  # noqa
+    from celery.app.base import Celery  # noqa
+    from celery.app.task import Task  # noqa
+    from celery.app.utils import bugreport  # noqa
+    from celery.canvas import (chain, chord, chunks, group,  # noqa
+                               maybe_signature, signature, subtask, xmap,
+                               xstarmap)
+    from celery.utils import uuid  # noqa
+
+# Eventlet/gevent patching must happen before importing
+# anything else, so these tools must be at top-level.
+
+
+def _find_option_with_arg(argv, short_opts=None, long_opts=None):
+    """Search argv for options specifying short and longopt alternatives.
+
+    Returns:
+        str: value for option found
+    Raises:
+        KeyError: if option not found.
+    """
+    for i, arg in enumerate(argv):
+        if arg.startswith('-'):
+            if long_opts and arg.startswith('--'):
+                name, sep, val = arg.partition('=')
+                if name in long_opts:
+                    return val if sep else argv[i + 1]
+            if short_opts and arg in short_opts:
+                return argv[i + 1]
+    raise KeyError('|'.join(short_opts or [] + long_opts or []))
+
+
+def _patch_eventlet():
+    import eventlet.debug
+
+    eventlet.monkey_patch()
+    blockdetect = float(os.environ.get('EVENTLET_NOBLOCK', 0))
+    if blockdetect:
+        eventlet.debug.hub_blocking_detection(blockdetect, blockdetect)
+
+
+def _patch_gevent():
+    import gevent.monkey
+    import gevent.signal
+
+    gevent.monkey.patch_all()
+
+
+def maybe_patch_concurrency(argv=None, short_opts=None,
+                            long_opts=None, patches=None):
+    """Apply eventlet/gevent monkeypatches.
+
+    With short and long opt alternatives that specify the command line
+    option to set the pool, this makes sure that anything that needs
+    to be patched is completed as early as possible.
+    (e.g., eventlet/gevent monkey patches).
+    """
+    argv = argv if argv else sys.argv
+    short_opts = short_opts if short_opts else ['-P']
+    long_opts = long_opts if long_opts else ['--pool']
+    patches = patches if patches else {'eventlet': _patch_eventlet,
+                                       'gevent': _patch_gevent}
+    try:
+        pool = _find_option_with_arg(argv, short_opts, long_opts)
+    except KeyError:
+        pass
+    else:
+        try:
+            patcher = patches[pool]
+        except KeyError:
+            pass
+        else:
+            patcher()
+
+        # set up eventlet/gevent environments ASAP
+        from celery import concurrency
+        concurrency.get_implementation(pool)
+
+
+# this just creates a new module, that imports stuff on first attribute
+# access.  This makes the library faster to use.
+old_module, new_module = local.recreate_module(  # pragma: no cover
+    __name__,
+    by_module={
+        'celery.app': ['Celery', 'bugreport', 'shared_task'],
+        'celery.app.task': ['Task'],
+        'celery._state': ['current_app', 'current_task'],
+        'celery.canvas': [
+            'Signature', 'chain', 'chord', 'chunks', 'group',
+            'signature', 'maybe_signature', 'subtask',
+            'xmap', 'xstarmap',
+        ],
+        'celery.utils': ['uuid'],
+    },
+    direct={'task': 'celery.task'},
+    __package__='celery', __file__=__file__,
+    __path__=__path__, __doc__=__doc__, __version__=__version__,
+    __author__=__author__, __contact__=__contact__,
+    __homepage__=__homepage__, __docformat__=__docformat__, local=local,
+    VERSION=VERSION, SERIES=SERIES, VERSION_BANNER=VERSION_BANNER,
+    version_info_t=version_info_t,
+    version_info=version_info,
+    maybe_patch_concurrency=maybe_patch_concurrency,
+    _find_option_with_arg=_find_option_with_arg,
+)
```

## celery/aio/amqp.py

 * *Ordering differences only*

```diff
@@ -1,157 +1,157 @@
-from kombu import Exchange
-from kombu.common import Broadcast
-from kombu.utils import cached_property
-
-from celery.app.amqp import AMQP, Queues
-from celery.patches.kombu import pools
-from celery.patches.kombu.connection import Connection
-from celery.patches.kombu.messaging import Producer
-from celery.patches.kombu.entity import Queue
-from celery import signals
-
-
-class AioQueues(Queues):
-    queue_cls = Queue
-
-
-class AioAMQP(AMQP):
-    Producer = Producer
-    Connection = Connection
-    queues_cls = AioQueues
-
-    @property
-    def producer_pool(self):
-        if self._producer_pool is None:
-            self._producer_pool = pools.producers[
-                self.app.aconnection_for_write()]
-            self._producer_pool.limit = self.app.pool.limit
-        return self._producer_pool
-
-    def _create_task_sender(self):
-        default_retry = self.app.conf.task_publish_retry
-        default_policy = self.app.conf.task_publish_retry_policy
-        default_delivery_mode = self.app.conf.task_default_delivery_mode
-        default_queue = self.default_queue
-        queues = self.queues
-        send_before_publish = signals.before_task_publish.asend
-        before_receivers = signals.before_task_publish.areceivers
-        send_after_publish = signals.after_task_publish.asend
-        after_receivers = signals.after_task_publish.areceivers
-
-        send_task_sent = signals.task_sent.asend   # XXX compat
-        sent_receivers = signals.task_sent.areceivers
-
-        default_evd = self._event_dispatcher
-        default_exchange = self.default_exchange
-
-        default_rkey = self.app.conf.task_default_routing_key
-        default_serializer = self.app.conf.task_serializer
-        default_compressor = self.app.conf.result_compression
-
-        async def send_task_message(
-            producer, name, message,
-            exchange=None, routing_key=None, queue=None,
-            event_dispatcher=None,
-            retry=None, retry_policy=None,
-            serializer=None, delivery_mode=None,
-            compression=None, declare=None,
-            headers=None, exchange_type=None, **kwargs
-        ):
-            retry = default_retry if retry is None else retry
-            headers2, properties, body, sent_event = message
-            if headers:
-                headers2.update(headers)
-            if kwargs:
-                properties.update(kwargs)
-
-            qname = queue
-            if queue is None and exchange is None:
-                queue = default_queue
-            if queue is not None:
-                if isinstance(queue, str):
-                    qname, queue = queue, queues[queue]
-                else:
-                    qname = queue.name
-
-            if delivery_mode is None:
-                try:
-                    delivery_mode = queue.exchange.delivery_mode
-                except AttributeError:
-                    pass
-                delivery_mode = delivery_mode or default_delivery_mode
-
-            if exchange_type is None:
-                try:
-                    exchange_type = queue.exchange.type
-                except AttributeError:
-                    exchange_type = 'direct'
-
-            # convert to anon-exchange, when exchange not set and direct ex.
-            if (not exchange or not routing_key) and exchange_type == 'direct':
-                exchange, routing_key = '', qname
-            elif exchange is None:
-                # not topic exchange, and exchange not undefined
-                exchange = queue.exchange.name or default_exchange
-                routing_key = routing_key or queue.routing_key or default_rkey
-            if declare is None and queue and not isinstance(queue, Broadcast):
-                declare = [queue]
-
-            # merge default and custom policy
-            retry = default_retry if retry is None else retry
-            _rp = (dict(default_policy, **retry_policy) if retry_policy
-                   else default_policy)
-
-            if before_receivers:
-                await send_before_publish(
-                    sender=name, body=body,
-                    exchange=exchange, routing_key=routing_key,
-                    declare=declare, headers=headers2,
-                    properties=properties, retry_policy=retry_policy,
-                )
-            ret = await producer.publish(
-                body,
-                exchange=exchange,
-                routing_key=routing_key,
-                serializer=serializer or default_serializer,
-                compression=compression or default_compressor,
-                retry=retry, retry_policy=_rp,
-                delivery_mode=delivery_mode, declare=declare,
-                headers=headers2,
-                **properties
-            )
-            if after_receivers:
-                await send_after_publish(
-                    sender=name, body=body, headers=headers2,
-                    exchange=exchange, routing_key=routing_key
-                )
-            if sent_receivers:  # XXX deprecated
-                if isinstance(body, tuple):  # protocol version 2
-                    await send_task_sent(
-                        sender=name, task_id=headers2['id'], task=name,
-                        args=body[0], kwargs=body[1],
-                        eta=headers2['eta'], taskset=headers2['group'],
-                    )
-                else:  # protocol version 1
-                    await send_task_sent(
-                        sender=name, task_id=body['id'], task=name,
-                        args=body['args'], kwargs=body['kwargs'],
-                        eta=body['eta'], taskset=body['taskset'],
-                    )
-            if sent_event:
-                evd = event_dispatcher or default_evd
-                exname = exchange
-                if isinstance(exname, Exchange):
-                    exname = exname.name
-                sent_event.update({
-                    'queue': qname,
-                    'exchange': exname,
-                    'routing_key': routing_key,
-                })
-                await evd.publish('task-sent', sent_event,
-                            producer, retry=retry, retry_policy=retry_policy)
-            return ret
-        return send_task_message
-
-    @cached_property
-    def _event_dispatcher(self):
-        return self.app.aio_events.Dispatcher(enabled=False)
+from kombu import Exchange
+from kombu.common import Broadcast
+from kombu.utils import cached_property
+
+from celery.app.amqp import AMQP, Queues
+from celery.patches.kombu import pools
+from celery.patches.kombu.connection import Connection
+from celery.patches.kombu.messaging import Producer
+from celery.patches.kombu.entity import Queue
+from celery import signals
+
+
+class AioQueues(Queues):
+    queue_cls = Queue
+
+
+class AioAMQP(AMQP):
+    Producer = Producer
+    Connection = Connection
+    queues_cls = AioQueues
+
+    @property
+    def producer_pool(self):
+        if self._producer_pool is None:
+            self._producer_pool = pools.producers[
+                self.app.aconnection_for_write()]
+            self._producer_pool.limit = self.app.pool.limit
+        return self._producer_pool
+
+    def _create_task_sender(self):
+        default_retry = self.app.conf.task_publish_retry
+        default_policy = self.app.conf.task_publish_retry_policy
+        default_delivery_mode = self.app.conf.task_default_delivery_mode
+        default_queue = self.default_queue
+        queues = self.queues
+        send_before_publish = signals.before_task_publish.asend
+        before_receivers = signals.before_task_publish.areceivers
+        send_after_publish = signals.after_task_publish.asend
+        after_receivers = signals.after_task_publish.areceivers
+
+        send_task_sent = signals.task_sent.asend   # XXX compat
+        sent_receivers = signals.task_sent.areceivers
+
+        default_evd = self._event_dispatcher
+        default_exchange = self.default_exchange
+
+        default_rkey = self.app.conf.task_default_routing_key
+        default_serializer = self.app.conf.task_serializer
+        default_compressor = self.app.conf.result_compression
+
+        async def send_task_message(
+            producer, name, message,
+            exchange=None, routing_key=None, queue=None,
+            event_dispatcher=None,
+            retry=None, retry_policy=None,
+            serializer=None, delivery_mode=None,
+            compression=None, declare=None,
+            headers=None, exchange_type=None, **kwargs
+        ):
+            retry = default_retry if retry is None else retry
+            headers2, properties, body, sent_event = message
+            if headers:
+                headers2.update(headers)
+            if kwargs:
+                properties.update(kwargs)
+
+            qname = queue
+            if queue is None and exchange is None:
+                queue = default_queue
+            if queue is not None:
+                if isinstance(queue, str):
+                    qname, queue = queue, queues[queue]
+                else:
+                    qname = queue.name
+
+            if delivery_mode is None:
+                try:
+                    delivery_mode = queue.exchange.delivery_mode
+                except AttributeError:
+                    pass
+                delivery_mode = delivery_mode or default_delivery_mode
+
+            if exchange_type is None:
+                try:
+                    exchange_type = queue.exchange.type
+                except AttributeError:
+                    exchange_type = 'direct'
+
+            # convert to anon-exchange, when exchange not set and direct ex.
+            if (not exchange or not routing_key) and exchange_type == 'direct':
+                exchange, routing_key = '', qname
+            elif exchange is None:
+                # not topic exchange, and exchange not undefined
+                exchange = queue.exchange.name or default_exchange
+                routing_key = routing_key or queue.routing_key or default_rkey
+            if declare is None and queue and not isinstance(queue, Broadcast):
+                declare = [queue]
+
+            # merge default and custom policy
+            retry = default_retry if retry is None else retry
+            _rp = (dict(default_policy, **retry_policy) if retry_policy
+                   else default_policy)
+
+            if before_receivers:
+                await send_before_publish(
+                    sender=name, body=body,
+                    exchange=exchange, routing_key=routing_key,
+                    declare=declare, headers=headers2,
+                    properties=properties, retry_policy=retry_policy,
+                )
+            ret = await producer.publish(
+                body,
+                exchange=exchange,
+                routing_key=routing_key,
+                serializer=serializer or default_serializer,
+                compression=compression or default_compressor,
+                retry=retry, retry_policy=_rp,
+                delivery_mode=delivery_mode, declare=declare,
+                headers=headers2,
+                **properties
+            )
+            if after_receivers:
+                await send_after_publish(
+                    sender=name, body=body, headers=headers2,
+                    exchange=exchange, routing_key=routing_key
+                )
+            if sent_receivers:  # XXX deprecated
+                if isinstance(body, tuple):  # protocol version 2
+                    await send_task_sent(
+                        sender=name, task_id=headers2['id'], task=name,
+                        args=body[0], kwargs=body[1],
+                        eta=headers2['eta'], taskset=headers2['group'],
+                    )
+                else:  # protocol version 1
+                    await send_task_sent(
+                        sender=name, task_id=body['id'], task=name,
+                        args=body['args'], kwargs=body['kwargs'],
+                        eta=body['eta'], taskset=body['taskset'],
+                    )
+            if sent_event:
+                evd = event_dispatcher or default_evd
+                exname = exchange
+                if isinstance(exname, Exchange):
+                    exname = exname.name
+                sent_event.update({
+                    'queue': qname,
+                    'exchange': exname,
+                    'routing_key': routing_key,
+                })
+                await evd.publish('task-sent', sent_event,
+                            producer, retry=retry, retry_policy=retry_policy)
+            return ret
+        return send_task_message
+
+    @cached_property
+    def _event_dispatcher(self):
+        return self.app.aio_events.Dispatcher(enabled=False)
```

## celery/aio/backend.py

 * *Ordering differences only*

```diff
@@ -1,679 +1,679 @@
-import asyncio
-import logging
-from collections import deque
-from typing import *
-from functools import partial
-from contextlib import asynccontextmanager
-import time
-import socket
-
-import redis.asyncio as aioredis
-from redis.asyncio import cluster
-
-from celery.backends.redis import (
-    RedisBackend as SyncRedisBackend,
-    SentinelBackend as SyncSentinelBackend,
-    ResultConsumer as SyncResultConsumer,
-    task_join_will_block,
-    states,
-    logger,
-    E_RETRY_LIMIT_EXCEEDED,
-)
-from celery.backends.asynchronous import (
-    Empty,
-    Drainer as SyncDrainer,
-    register_drainer,
-    drainers,
-    detect_environment
-)
-from celery.backends.base import (
-    bytes_to_str,
-    get_exponential_backoff_interval,
-    raise_with_context,
-    BackendGetMetaError,
-    BackendStoreError,
-)
-from celery.exceptions import TimeoutError
-from celery.result import GroupResult
-
-from celery.aio.result import result_from_tuple
-from celery.patches.kombu.redis import get_redis_error_classes
-from celery.patches.kombu.utils import retry_over_time
-
-
-@register_drainer('aio-default')
-class Drainer(SyncDrainer):
-    if TYPE_CHECKING:
-        result_consumer: 'ResultConsumer'
-
-    def __init__(self, result_consumer):
-        self.result_consumer = result_consumer
-        self._started = False
-        self._drainer: Optional[asyncio.Task] = None
-
-    async def drain_events_until(self, r, timeout=None, interval=1, on_interval=None, wait=None):
-        wait = wait or self.result_consumer.drain_events
-        p = r.on_ready
-        time_start = time.monotonic()
-
-        while 1:
-            # Total time spent may exceed a single call to wait()
-            if timeout and time.monotonic() - time_start >= timeout:
-                raise socket.timeout()
-            try:
-                await wait(r, interval)
-            except socket.timeout:
-                pass
-            if on_interval:
-                await on_interval()
-            if p.ready:  # got event on the wanted channel.
-                break
-
-    async def start(self):
-        await self.result_consumer.spawn_drainer()
-
-    def stop(self):
-        self.result_consumer.stop_drainer()
-
-
-class ResultConsumer(SyncResultConsumer):
-    if TYPE_CHECKING:
-        from redis.asyncio.client import PubSub
-
-        backend: 'RedisBackend'
-        _pubsub: Optional[PubSub]
-        drainer: Drainer
-        _running_drainer: Optional[asyncio.Task]
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.drainer = drainers['aio-' + detect_environment()](self)
-        self._running_drainer = None
-        self._pubsub_lock = asyncio.Lock()
-
-    async def on_after_fork(self):
-        try:
-            self.backend.client.connection_pool.reset()
-            if self._pubsub is not None:
-                await self._pubsub.close()
-        except KeyError as e:
-            logger.warning(str(e))
-
-    async def _reconnect_pubsub(self):
-        self._pubsub = None
-        self.backend.client.connection_pool.reset()
-        # task state might have changed when the connection was down so we
-        # retrieve meta for all subscribed tasks before going into pubsub mode
-        metas = await self.backend.client.mget(self.subscribed_to)
-        metas = [meta for meta in metas if meta]
-        for meta in metas:
-            await self.on_state_change(self._decode_result(meta), None)
-        self._pubsub = self.backend.client.pubsub(
-            ignore_subscribe_messages=True,
-        )
-        if self.subscribed_to:
-            await self._pubsub.subscribe(*self.subscribed_to)
-
-    @asynccontextmanager
-    async def reconnect_on_error(self):
-        try:
-            yield
-        except self._connection_errors:
-            try:
-                await self._ensure(self._reconnect_pubsub, ())
-            except self._connection_errors:
-                logger.critical(E_RETRY_LIMIT_EXCEEDED)
-                raise
-
-    async def _maybe_cancel_ready_task(self, meta):
-        if meta['status'] in states.READY_STATES:
-            await self.cancel_for(meta['task_id'])
-
-    async def on_state_change(self, meta, message):
-        if self.on_message:
-            self.on_message(meta)
-        if meta['status'] in states.READY_STATES:
-            task_id = meta['task_id']
-            try:
-                result = self._get_pending_result(task_id)
-            except KeyError:
-                # send to buffer in case we received this result
-                # before it was added to _pending_results.
-                self._pending_messages.put(task_id, meta)
-            else:
-                await result._maybe_set_cache(meta)
-                buckets = self.buckets
-                try:
-                    # remove bucket for this result, since it's fulfilled
-                    bucket = buckets.pop(result)
-                except KeyError:
-                    pass
-                else:
-                    # send to waiter via bucket
-                    bucket.append(result)
-        await asyncio.sleep(0)
-        await self._maybe_cancel_ready_task(meta)
-
-    async def start(self, initial_task_id, **kwargs):
-        self._pubsub = self.backend.client.pubsub(
-            ignore_subscribe_messages=True,
-        )
-        await self._consume_from(initial_task_id)
-
-    async def on_wait_for_pending(self, result, **kwargs):
-        for meta in await result._iter_meta(**kwargs):
-            if meta is not None:
-                await self.on_state_change(meta, None)
-
-    async def stop(self):
-        if self._pubsub is not None:
-            await self._pubsub.close()
-
-    async def _drain_events(self):
-        logger.debug(f'start drainer on channel: [{self._pubsub.channels}]')
-        async with self.reconnect_on_error():
-            async for message in self._pubsub.listen():
-                if message and message['type'] == 'message':
-                    if logger.isEnabledFor(logging.DEBUG):
-                        logger.debug(f'got message: {message}')
-                    await self.on_state_change(
-                        self._decode_result(message['data']), message
-                    )
-
-        logger.debug(f'stoping drainer... | '
-                     f'subscribed: {self._pubsub.subscribed}')
-        self._running_drainer = None
-
-    async def spawn_drainer(self) -> Optional[asyncio.Task]:
-        if not self._pubsub:
-            return
-
-        if self._running_drainer is None:
-            self._running_drainer = asyncio.create_task(self._drain_events())
-            await asyncio.sleep(0)
-        return self._running_drainer
-
-    def stop_drainer(self):
-        if self._running_drainer is not None:
-            self._running_drainer.cancel()
-            self._running_drainer = None
-
-    async def drain_events(self, result, timeout=None):
-        await self.spawn_drainer()
-        await self.backend.wait_for_future(result, timeout)
-
-    async def consume_from(self, task_id):
-        if self._pubsub is None:
-            return await self.start(task_id)
-        await self._consume_from(task_id)
-
-    async def _consume_from(self, task_id):
-        key = self._get_key_for_task(task_id)
-        if key not in self.subscribed_to:
-            self.subscribed_to.add(key)
-            async with self.reconnect_on_error():
-                async with self._pubsub_lock:
-                    if self._pubsub:  # double check needed
-                        await self._pubsub.subscribe(key)
-
-    async def cancel_for(self, task_id):
-        key = self._get_key_for_task(task_id)
-        self.subscribed_to.discard(key)
-        if self._pubsub:
-            async with self.reconnect_on_error():
-                async with self._pubsub_lock:
-                    if self._pubsub:  # double check needed
-                        await self._pubsub.unsubscribe(key)
-
-    # -------------------------------------------------------
-    # celery.backends.asynchronou::BaseResultConsumer
-    async def _wait_for_pending(
-        self, result, timeout=None,
-        on_interval=None, on_message=None, **kwargs
-    ):
-        await self.on_wait_for_pending(result, timeout=timeout, **kwargs)
-        prev_on_m, self.on_message = self.on_message, on_message
-        try:
-            await self.drain_events_until(
-                result, timeout=timeout,
-                on_interval=on_interval
-            )
-        except socket.timeout:
-            raise TimeoutError('The operation timed out.')
-        finally:
-            self.on_message = prev_on_m
-
-    async def drain_events_until(self, r, timeout=None, on_interval=None):
-        return await self.drainer.drain_events_until(
-            r, timeout=timeout, on_interval=on_interval)
-
-
-class RedisBackend(SyncRedisBackend):
-    if TYPE_CHECKING:
-        @property
-        def client(self) -> aioredis.Redis: ...  # noqa
-        result_consumer: ResultConsumer
-
-    ResultConsumer = ResultConsumer
-    redis = aioredis
-
-    def __init__(self, host=None, port=None, db=None, password=None,
-                 max_connections=None, url=None,
-                 connection_pool=None, **kwargs):
-        super().__init__(
-            host=host, port=port, db=db, password=password,
-            max_connections=max_connections, url=url,
-            connection_pool=connection_pool, **kwargs
-        )
-        self._pending_future: Dict[str, asyncio.Future] = {}
-
-    def get_redis_error_classes(self):
-        return get_redis_error_classes()
-
-    async def on_task_call(self, producer, task_id):
-        if not task_join_will_block():
-            await self.result_consumer.consume_from(task_id)
-
-    async def get(self, key):
-        return await self.client.get(key)
-
-    async def mget(self, keys):
-        return await self.client.mget(keys)
-
-    async def ensure(self, fun, args, **policy):
-        retry_policy = dict(self.retry_policy, **policy)
-        max_retries = retry_policy.get('max_retries')
-        return await retry_over_time(
-            fun, self.connection_errors, args, {},
-            partial(self.on_connection_error, max_retries),
-            **retry_policy
-        )
-
-    async def set(self, key, value, **retry_policy):
-        return await self.ensure(self._set, (key, value), **retry_policy)
-
-    async def _set(self, key, value):
-        async with self.client.pipeline() as pipe:
-            if self.expires:
-                pipe.setex(key, self.expires, value)
-            else:
-                pipe.set(key, value)
-            pipe.publish(key, value)
-            await pipe.execute()
-
-    async def remove_pending_result(self, result):
-        self._remove_pending_result(result.id)
-        await self.on_result_fulfilled(result)
-        return result
-
-    async def _forget(self, task_id):
-        await self.delete(self.get_key_for_task(task_id))
-
-    async def forget(self, task_id):
-        self._cache.pop(task_id, None)
-        await self._forget(task_id)
-        await self.result_consumer.cancel_for(task_id)
-
-    async def delete(self, key):
-        await self.client.delete(key)
-
-    async def incr(self, key):
-        return await self.client.incr(key)
-
-    async def expire(self, key, value):
-        return await self.client.expire(key, value)
-
-    async def add_to_chord(self, group_id, result):
-        await self.client.incr(self.get_key_for_group(group_id, '.t'), 1)
-
-    async def on_result_fulfilled(self, result):
-        await self.result_consumer.cancel_for(result.id)
-
-    async def add_pending_result(self, result, weak=False, start_drainer=True):
-        if start_drainer:
-            await self.result_consumer.drainer.start()
-        try:
-            await self._maybe_resolve_from_buffer(result)
-        except Empty:
-            await self._add_pending_result(result.id, result, weak=weak)
-        return result
-
-    async def _maybe_resolve_from_buffer(self, result):
-        await result._maybe_set_cache(self._pending_messages.take(result.id))
-
-    async def _add_pending_result(self, task_id, result, weak=False):
-        concrete, weak_ = self._pending_results
-        if task_id not in weak_ and result.id not in concrete:
-            (weak_ if weak else concrete)[task_id] = result
-            await self.result_consumer.consume_from(task_id)
-            self._pending_future[task_id] = \
-                asyncio.get_running_loop().create_future()
-        else:
-            ori_result = weak_.get(task_id, concrete.get(task_id))
-            await ori_result.on_ready.then(result.on_ready)
-
-    async def add_pending_results(self, results, weak=False):
-        await self.result_consumer.drainer.start()
-        return [
-            await self.add_pending_result(
-                result, weak=weak, start_drainer=False)
-            for result in results
-        ]
-
-    async def wait_for_pending(self, result,
-                         callback=None, propagate=True, **kwargs):
-        self._ensure_not_eager()
-        await self._wait_for_pending(result, **kwargs)
-        return await result.maybe_throw(callback=callback, propagate=propagate)
-
-    async def _wait_for_pending(
-        self, result, timeout=None, on_interval=None,
-        on_message=None, **kwargs
-    ):
-        return await self.result_consumer._wait_for_pending(
-            result, timeout=timeout,
-            on_interval=on_interval, on_message=on_message,
-            **kwargs
-        )
-
-    async def _get_task_meta_for(self, task_id):
-        """Get task meta-data for a task by id."""
-        meta = await self.get(self.get_key_for_task(task_id))
-        if not meta:
-            return {'status': states.PENDING, 'result': None}
-        return self.decode_result(meta)
-
-    async def _store_result(self, task_id, result, state,
-                      traceback=None, request=None, **kwargs):
-        meta = self._get_result_meta(result=result, state=state,
-                                     traceback=traceback, request=request)
-        meta['task_id'] = bytes_to_str(task_id)
-
-        # Retrieve metadata from the backend, if the status
-        # is a success then we ignore any following update to the state.
-        # This solves a task deduplication issue because of network
-        # partitioning or lost workers. This issue involved a race condition
-        # making a lost task overwrite the last successful result in the
-        # result backend.
-        current_meta = await self._get_task_meta_for(task_id)
-
-        if current_meta['status'] == states.SUCCESS:
-            return result
-
-        await self._set_with_state(
-            self.get_key_for_task(task_id),
-            self.encode(meta), state
-        )
-        return result
-
-    async def get_task_meta(self, task_id, cache=True):
-        """Get task meta from backend.
-
-        if always_retry_backend_operation is activated, in the event of a recoverable exception,
-        then retry operation with an exponential backoff until a limit has been reached.
-        """
-        self._ensure_not_eager()
-        if cache:
-            try:
-                return self._cache[task_id]
-            except KeyError:
-                pass
-        retries = 0
-        while True:
-            try:
-                meta = await self._get_task_meta_for(task_id)
-                break
-            except Exception as exc:
-                if self.always_retry and self.exception_safe_to_retry(exc):
-                    if retries < self.max_retries:
-                        retries += 1
-
-                        # get_exponential_backoff_interval computes integers
-                        # and time.sleep accept floats for sub second sleep
-                        sleep_amount = get_exponential_backoff_interval(
-                            self.base_sleep_between_retries_ms, retries,
-                            self.max_sleep_between_retries_ms, True) / 1000
-                        await self._sleep(sleep_amount)
-                    else:
-                        raise_with_context(
-                            BackendGetMetaError("failed to get meta", task_id=task_id),
-                        )
-                else:
-                    raise
-
-        if cache and meta.get('status') == states.SUCCESS:
-            self._cache[task_id] = meta
-        return meta
-
-    async def store_result(self, task_id, result, state,
-                     traceback=None, request=None, **kwargs):
-        """Update task state and result.
-
-        if always_retry_backend_operation is activated, in the event of a recoverable exception,
-        then retry operation with an exponential backoff until a limit has been reached.
-        """
-        result = self.encode_result(result, state)
-
-        retries = 0
-
-        while True:
-            try:
-                await self._store_result(
-                    task_id, result, state, traceback,
-                    request=request, **kwargs)
-                return result
-            except Exception as exc:
-                if self.always_retry and self.exception_safe_to_retry(exc):
-                    if retries < self.max_retries:
-                        retries += 1
-
-                        # get_exponential_backoff_interval computes integers
-                        # and time.sleep accept floats for sub second sleep
-                        sleep_amount = get_exponential_backoff_interval(
-                            self.base_sleep_between_retries_ms, retries,
-                            self.max_sleep_between_retries_ms, True) / 1000
-                        await self._sleep(sleep_amount)
-                    else:
-                        raise_with_context(
-                            BackendStoreError("failed to store result on the backend", task_id=task_id, state=state),
-                        )
-                else:
-                    raise
-
-    async def _sleep(self, amount):
-        await asyncio.sleep(amount)
-
-    async def _set_with_state(self, key, value, state):
-        return await self.set(key, value)
-
-    async def _wait_for_single_future(self, result, timeout):
-        future = self._pending_future[result.id]
-        try:
-            await asyncio.wait_for(asyncio.shield(future), timeout=timeout)
-        except asyncio.TimeoutError:
-            pass
-
-    async def wait_for_future(self, result, timeout):
-        if isinstance(result, GroupResult):
-            results = result.results
-        else:
-            results = [result]
-
-        await asyncio.gather(*[
-            self._wait_for_single_future(r, timeout)
-            for r in results
-        ])
-
-    def maybe_set_future(self, task_id, result):
-        if task_id not in self._pending_future:
-            future = asyncio.get_running_loop().create_future()
-            self._pending_future[task_id] = future
-        else:
-            future: asyncio.Future = self._pending_future[task_id]
-
-        if not future.done():
-            logger.debug(f'Set result for task[{task_id}]: {result}')
-            future.set_result(result)
-
-    def remove_pending_future(self, task_id):
-        self._pending_future.pop(task_id, None)
-
-    async def restore_group(self, group_id, cache=True):
-        meta = await self.get_group_meta(group_id, cache=cache)
-        if meta:
-            return meta['result']
-
-    async def get_group_meta(self, group_id, cache=True):
-        self._ensure_not_eager()
-        if cache:
-            try:
-                return self._cache[group_id]
-            except KeyError:
-                pass
-
-        meta = await self._restore_group(group_id)
-        if cache and meta is not None:
-            self._cache[group_id] = meta
-        return meta
-
-    async def _restore_group(self, group_id):
-        """Get task meta-data for a task by id."""
-        meta = await self.get(self.get_key_for_group(group_id))
-        # previously this was always pickled, but later this
-        # was extended to support other serializers, so the
-        # structure is kind of weird.
-        if meta:
-            meta = self.decode(meta)
-            result = meta['result']
-            meta['result'] = await result_from_tuple(result, self.app)
-            return meta
-
-    async def _save_group(self, group_id, result):
-        await self._set_with_state(
-            self.get_key_for_group(group_id),
-            self.encode({'result': result.as_tuple()}),
-            states.SUCCESS
-        )
-        return result
-
-    async def iter_native(self, result, no_ack=True, **kwargs):
-        self._ensure_not_eager()
-
-        results = result.results
-        if not results:
-            raise StopIteration()
-
-        # we tell the result consumer to put consumed results
-        # into these buckets.
-        bucket = deque()
-        for node in results:
-            if not hasattr(node, '_cache'):
-                bucket.append(node)
-            elif node._cache:
-                bucket.append(node)
-            else:
-                self._collect_into(node, bucket)
-
-        await self._wait_for_pending(result, no_ack=no_ack, **kwargs)
-        while bucket:
-            node = bucket.popleft()
-            if not hasattr(node, '_cache'):
-                yield node.id, node.children
-            else:
-                yield node.id, node._cache
-
-        while bucket:
-            node = bucket.popleft()
-            yield node.id, node._cache
-
-    async def get_many(
-        self, task_ids, timeout=None, interval=0.5, no_ack=True,
-        on_message=None, on_interval=None, max_iterations=None,
-        READY_STATES=states.READY_STATES
-    ):
-        interval = 0.5 if interval is None else interval
-        ids = task_ids if isinstance(task_ids, set) else set(task_ids)
-        cached_ids = set()
-        cache = self._cache
-        for task_id in ids:
-            try:
-                cached = cache[task_id]
-            except KeyError:
-                pass
-            else:
-                if cached['status'] in READY_STATES:
-                    yield bytes_to_str(task_id), cached
-                    cached_ids.add(task_id)
-
-        ids.difference_update(cached_ids)
-        iterations = 0
-        while ids:
-            keys = list(ids)
-            r = self._mget_to_results(
-                await self.mget(
-                    [self.get_key_for_task(k)
-                     for k in keys]
-                ), keys, READY_STATES
-            )
-            cache.update(r)
-            ids.difference_update({bytes_to_str(v) for v in r})
-            for key, value in r.items():
-                if on_message is not None:
-                    on_message(value)
-                yield bytes_to_str(key), value
-            if timeout and iterations * interval >= timeout:
-                raise TimeoutError(f'Operation timed out ({timeout})')
-            if on_interval:
-                on_interval()
-            time.sleep(interval)  # don't busy loop.
-            iterations += 1
-            if max_iterations and iterations >= max_iterations:
-                break
-
-
-class SentinelBackend(RedisBackend):
-    sentinel = getattr(aioredis, "sentinel", None)
-
-    if TYPE_CHECKING:
-        from redis.asyncio.sentinel import Sentinel
-        def _get_sentinel_instance(self, **params) -> Sentinel: ...
-
-    def _params_from_url(self, url, defaults):
-        chunks = url.split(";")
-        connparams = dict(defaults, hosts=[])
-        for chunk in chunks:
-            data = super()._params_from_url(
-                url=chunk, defaults=defaults)
-            connparams['hosts'].append(data)
-        for param in ("host", "port", "db", "password"):
-            connparams.pop(param)
-
-        # Adding db/password in connparams to connect to the correct instance
-        for param in ("db", "password"):
-            if connparams['hosts'] and param in connparams['hosts'][0]:
-                connparams[param] = connparams['hosts'][0].get(param)
-        return connparams
-
-    _get_sentinel_instance = SyncSentinelBackend._get_sentinel_instance # noqa
-    _get_pool = SyncSentinelBackend._get_pool # noqa
-
-
-class ClusterBackend(RedisBackend):
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.connparams.pop('db')
-
-    def _get_pool(self, **params):
-        return NotImplementedError
-
-    def _create_client(self, **params):
-        return cluster.RedisCluster(**params)
-
-    async def _set(self, key, value):
-        async with self.client.pipeline() as pipe:
-            if self.expires:
-                pipe.setex(key, self.expires, value)
-            else:
-                pipe.set(key, value)
-            await pipe.execute()
-        await self.client.publish(key, value)
+import asyncio
+import logging
+from collections import deque
+from typing import *
+from functools import partial
+from contextlib import asynccontextmanager
+import time
+import socket
+
+import redis.asyncio as aioredis
+from redis.asyncio import cluster
+
+from celery.backends.redis import (
+    RedisBackend as SyncRedisBackend,
+    SentinelBackend as SyncSentinelBackend,
+    ResultConsumer as SyncResultConsumer,
+    task_join_will_block,
+    states,
+    logger,
+    E_RETRY_LIMIT_EXCEEDED,
+)
+from celery.backends.asynchronous import (
+    Empty,
+    Drainer as SyncDrainer,
+    register_drainer,
+    drainers,
+    detect_environment
+)
+from celery.backends.base import (
+    bytes_to_str,
+    get_exponential_backoff_interval,
+    raise_with_context,
+    BackendGetMetaError,
+    BackendStoreError,
+)
+from celery.exceptions import TimeoutError
+from celery.result import GroupResult
+
+from celery.aio.result import result_from_tuple
+from celery.patches.kombu.redis import get_redis_error_classes
+from celery.patches.kombu.utils import retry_over_time
+
+
+@register_drainer('aio-default')
+class Drainer(SyncDrainer):
+    if TYPE_CHECKING:
+        result_consumer: 'ResultConsumer'
+
+    def __init__(self, result_consumer):
+        self.result_consumer = result_consumer
+        self._started = False
+        self._drainer: Optional[asyncio.Task] = None
+
+    async def drain_events_until(self, r, timeout=None, interval=1, on_interval=None, wait=None):
+        wait = wait or self.result_consumer.drain_events
+        p = r.on_ready
+        time_start = time.monotonic()
+
+        while 1:
+            # Total time spent may exceed a single call to wait()
+            if timeout and time.monotonic() - time_start >= timeout:
+                raise socket.timeout()
+            try:
+                await wait(r, interval)
+            except socket.timeout:
+                pass
+            if on_interval:
+                await on_interval()
+            if p.ready:  # got event on the wanted channel.
+                break
+
+    async def start(self):
+        await self.result_consumer.spawn_drainer()
+
+    def stop(self):
+        self.result_consumer.stop_drainer()
+
+
+class ResultConsumer(SyncResultConsumer):
+    if TYPE_CHECKING:
+        from redis.asyncio.client import PubSub
+
+        backend: 'RedisBackend'
+        _pubsub: Optional[PubSub]
+        drainer: Drainer
+        _running_drainer: Optional[asyncio.Task]
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.drainer = drainers['aio-' + detect_environment()](self)
+        self._running_drainer = None
+        self._pubsub_lock = asyncio.Lock()
+
+    async def on_after_fork(self):
+        try:
+            self.backend.client.connection_pool.reset()
+            if self._pubsub is not None:
+                await self._pubsub.close()
+        except KeyError as e:
+            logger.warning(str(e))
+
+    async def _reconnect_pubsub(self):
+        self._pubsub = None
+        self.backend.client.connection_pool.reset()
+        # task state might have changed when the connection was down so we
+        # retrieve meta for all subscribed tasks before going into pubsub mode
+        metas = await self.backend.client.mget(self.subscribed_to)
+        metas = [meta for meta in metas if meta]
+        for meta in metas:
+            await self.on_state_change(self._decode_result(meta), None)
+        self._pubsub = self.backend.client.pubsub(
+            ignore_subscribe_messages=True,
+        )
+        if self.subscribed_to:
+            await self._pubsub.subscribe(*self.subscribed_to)
+
+    @asynccontextmanager
+    async def reconnect_on_error(self):
+        try:
+            yield
+        except self._connection_errors:
+            try:
+                await self._ensure(self._reconnect_pubsub, ())
+            except self._connection_errors:
+                logger.critical(E_RETRY_LIMIT_EXCEEDED)
+                raise
+
+    async def _maybe_cancel_ready_task(self, meta):
+        if meta['status'] in states.READY_STATES:
+            await self.cancel_for(meta['task_id'])
+
+    async def on_state_change(self, meta, message):
+        if self.on_message:
+            self.on_message(meta)
+        if meta['status'] in states.READY_STATES:
+            task_id = meta['task_id']
+            try:
+                result = self._get_pending_result(task_id)
+            except KeyError:
+                # send to buffer in case we received this result
+                # before it was added to _pending_results.
+                self._pending_messages.put(task_id, meta)
+            else:
+                await result._maybe_set_cache(meta)
+                buckets = self.buckets
+                try:
+                    # remove bucket for this result, since it's fulfilled
+                    bucket = buckets.pop(result)
+                except KeyError:
+                    pass
+                else:
+                    # send to waiter via bucket
+                    bucket.append(result)
+        await asyncio.sleep(0)
+        await self._maybe_cancel_ready_task(meta)
+
+    async def start(self, initial_task_id, **kwargs):
+        self._pubsub = self.backend.client.pubsub(
+            ignore_subscribe_messages=True,
+        )
+        await self._consume_from(initial_task_id)
+
+    async def on_wait_for_pending(self, result, **kwargs):
+        for meta in await result._iter_meta(**kwargs):
+            if meta is not None:
+                await self.on_state_change(meta, None)
+
+    async def stop(self):
+        if self._pubsub is not None:
+            await self._pubsub.close()
+
+    async def _drain_events(self):
+        logger.debug(f'start drainer on channel: [{self._pubsub.channels}]')
+        async with self.reconnect_on_error():
+            async for message in self._pubsub.listen():
+                if message and message['type'] == 'message':
+                    if logger.isEnabledFor(logging.DEBUG):
+                        logger.debug(f'got message: {message}')
+                    await self.on_state_change(
+                        self._decode_result(message['data']), message
+                    )
+
+        logger.debug(f'stoping drainer... | '
+                     f'subscribed: {self._pubsub.subscribed}')
+        self._running_drainer = None
+
+    async def spawn_drainer(self) -> Optional[asyncio.Task]:
+        if not self._pubsub:
+            return
+
+        if self._running_drainer is None:
+            self._running_drainer = asyncio.create_task(self._drain_events())
+            await asyncio.sleep(0)
+        return self._running_drainer
+
+    def stop_drainer(self):
+        if self._running_drainer is not None:
+            self._running_drainer.cancel()
+            self._running_drainer = None
+
+    async def drain_events(self, result, timeout=None):
+        await self.spawn_drainer()
+        await self.backend.wait_for_future(result, timeout)
+
+    async def consume_from(self, task_id):
+        if self._pubsub is None:
+            return await self.start(task_id)
+        await self._consume_from(task_id)
+
+    async def _consume_from(self, task_id):
+        key = self._get_key_for_task(task_id)
+        if key not in self.subscribed_to:
+            self.subscribed_to.add(key)
+            async with self.reconnect_on_error():
+                async with self._pubsub_lock:
+                    if self._pubsub:  # double check needed
+                        await self._pubsub.subscribe(key)
+
+    async def cancel_for(self, task_id):
+        key = self._get_key_for_task(task_id)
+        self.subscribed_to.discard(key)
+        if self._pubsub:
+            async with self.reconnect_on_error():
+                async with self._pubsub_lock:
+                    if self._pubsub:  # double check needed
+                        await self._pubsub.unsubscribe(key)
+
+    # -------------------------------------------------------
+    # celery.backends.asynchronou::BaseResultConsumer
+    async def _wait_for_pending(
+        self, result, timeout=None,
+        on_interval=None, on_message=None, **kwargs
+    ):
+        await self.on_wait_for_pending(result, timeout=timeout, **kwargs)
+        prev_on_m, self.on_message = self.on_message, on_message
+        try:
+            await self.drain_events_until(
+                result, timeout=timeout,
+                on_interval=on_interval
+            )
+        except socket.timeout:
+            raise TimeoutError('The operation timed out.')
+        finally:
+            self.on_message = prev_on_m
+
+    async def drain_events_until(self, r, timeout=None, on_interval=None):
+        return await self.drainer.drain_events_until(
+            r, timeout=timeout, on_interval=on_interval)
+
+
+class RedisBackend(SyncRedisBackend):
+    if TYPE_CHECKING:
+        @property
+        def client(self) -> aioredis.Redis: ...  # noqa
+        result_consumer: ResultConsumer
+
+    ResultConsumer = ResultConsumer
+    redis = aioredis
+
+    def __init__(self, host=None, port=None, db=None, password=None,
+                 max_connections=None, url=None,
+                 connection_pool=None, **kwargs):
+        super().__init__(
+            host=host, port=port, db=db, password=password,
+            max_connections=max_connections, url=url,
+            connection_pool=connection_pool, **kwargs
+        )
+        self._pending_future: Dict[str, asyncio.Future] = {}
+
+    def get_redis_error_classes(self):
+        return get_redis_error_classes()
+
+    async def on_task_call(self, producer, task_id):
+        if not task_join_will_block():
+            await self.result_consumer.consume_from(task_id)
+
+    async def get(self, key):
+        return await self.client.get(key)
+
+    async def mget(self, keys):
+        return await self.client.mget(keys)
+
+    async def ensure(self, fun, args, **policy):
+        retry_policy = dict(self.retry_policy, **policy)
+        max_retries = retry_policy.get('max_retries')
+        return await retry_over_time(
+            fun, self.connection_errors, args, {},
+            partial(self.on_connection_error, max_retries),
+            **retry_policy
+        )
+
+    async def set(self, key, value, **retry_policy):
+        return await self.ensure(self._set, (key, value), **retry_policy)
+
+    async def _set(self, key, value):
+        async with self.client.pipeline() as pipe:
+            if self.expires:
+                pipe.setex(key, self.expires, value)
+            else:
+                pipe.set(key, value)
+            pipe.publish(key, value)
+            await pipe.execute()
+
+    async def remove_pending_result(self, result):
+        self._remove_pending_result(result.id)
+        await self.on_result_fulfilled(result)
+        return result
+
+    async def _forget(self, task_id):
+        await self.delete(self.get_key_for_task(task_id))
+
+    async def forget(self, task_id):
+        self._cache.pop(task_id, None)
+        await self._forget(task_id)
+        await self.result_consumer.cancel_for(task_id)
+
+    async def delete(self, key):
+        await self.client.delete(key)
+
+    async def incr(self, key):
+        return await self.client.incr(key)
+
+    async def expire(self, key, value):
+        return await self.client.expire(key, value)
+
+    async def add_to_chord(self, group_id, result):
+        await self.client.incr(self.get_key_for_group(group_id, '.t'), 1)
+
+    async def on_result_fulfilled(self, result):
+        await self.result_consumer.cancel_for(result.id)
+
+    async def add_pending_result(self, result, weak=False, start_drainer=True):
+        if start_drainer:
+            await self.result_consumer.drainer.start()
+        try:
+            await self._maybe_resolve_from_buffer(result)
+        except Empty:
+            await self._add_pending_result(result.id, result, weak=weak)
+        return result
+
+    async def _maybe_resolve_from_buffer(self, result):
+        await result._maybe_set_cache(self._pending_messages.take(result.id))
+
+    async def _add_pending_result(self, task_id, result, weak=False):
+        concrete, weak_ = self._pending_results
+        if task_id not in weak_ and result.id not in concrete:
+            (weak_ if weak else concrete)[task_id] = result
+            await self.result_consumer.consume_from(task_id)
+            self._pending_future[task_id] = \
+                asyncio.get_running_loop().create_future()
+        else:
+            ori_result = weak_.get(task_id, concrete.get(task_id))
+            await ori_result.on_ready.then(result.on_ready)
+
+    async def add_pending_results(self, results, weak=False):
+        await self.result_consumer.drainer.start()
+        return [
+            await self.add_pending_result(
+                result, weak=weak, start_drainer=False)
+            for result in results
+        ]
+
+    async def wait_for_pending(self, result,
+                         callback=None, propagate=True, **kwargs):
+        self._ensure_not_eager()
+        await self._wait_for_pending(result, **kwargs)
+        return await result.maybe_throw(callback=callback, propagate=propagate)
+
+    async def _wait_for_pending(
+        self, result, timeout=None, on_interval=None,
+        on_message=None, **kwargs
+    ):
+        return await self.result_consumer._wait_for_pending(
+            result, timeout=timeout,
+            on_interval=on_interval, on_message=on_message,
+            **kwargs
+        )
+
+    async def _get_task_meta_for(self, task_id):
+        """Get task meta-data for a task by id."""
+        meta = await self.get(self.get_key_for_task(task_id))
+        if not meta:
+            return {'status': states.PENDING, 'result': None}
+        return self.decode_result(meta)
+
+    async def _store_result(self, task_id, result, state,
+                      traceback=None, request=None, **kwargs):
+        meta = self._get_result_meta(result=result, state=state,
+                                     traceback=traceback, request=request)
+        meta['task_id'] = bytes_to_str(task_id)
+
+        # Retrieve metadata from the backend, if the status
+        # is a success then we ignore any following update to the state.
+        # This solves a task deduplication issue because of network
+        # partitioning or lost workers. This issue involved a race condition
+        # making a lost task overwrite the last successful result in the
+        # result backend.
+        current_meta = await self._get_task_meta_for(task_id)
+
+        if current_meta['status'] == states.SUCCESS:
+            return result
+
+        await self._set_with_state(
+            self.get_key_for_task(task_id),
+            self.encode(meta), state
+        )
+        return result
+
+    async def get_task_meta(self, task_id, cache=True):
+        """Get task meta from backend.
+
+        if always_retry_backend_operation is activated, in the event of a recoverable exception,
+        then retry operation with an exponential backoff until a limit has been reached.
+        """
+        self._ensure_not_eager()
+        if cache:
+            try:
+                return self._cache[task_id]
+            except KeyError:
+                pass
+        retries = 0
+        while True:
+            try:
+                meta = await self._get_task_meta_for(task_id)
+                break
+            except Exception as exc:
+                if self.always_retry and self.exception_safe_to_retry(exc):
+                    if retries < self.max_retries:
+                        retries += 1
+
+                        # get_exponential_backoff_interval computes integers
+                        # and time.sleep accept floats for sub second sleep
+                        sleep_amount = get_exponential_backoff_interval(
+                            self.base_sleep_between_retries_ms, retries,
+                            self.max_sleep_between_retries_ms, True) / 1000
+                        await self._sleep(sleep_amount)
+                    else:
+                        raise_with_context(
+                            BackendGetMetaError("failed to get meta", task_id=task_id),
+                        )
+                else:
+                    raise
+
+        if cache and meta.get('status') == states.SUCCESS:
+            self._cache[task_id] = meta
+        return meta
+
+    async def store_result(self, task_id, result, state,
+                     traceback=None, request=None, **kwargs):
+        """Update task state and result.
+
+        if always_retry_backend_operation is activated, in the event of a recoverable exception,
+        then retry operation with an exponential backoff until a limit has been reached.
+        """
+        result = self.encode_result(result, state)
+
+        retries = 0
+
+        while True:
+            try:
+                await self._store_result(
+                    task_id, result, state, traceback,
+                    request=request, **kwargs)
+                return result
+            except Exception as exc:
+                if self.always_retry and self.exception_safe_to_retry(exc):
+                    if retries < self.max_retries:
+                        retries += 1
+
+                        # get_exponential_backoff_interval computes integers
+                        # and time.sleep accept floats for sub second sleep
+                        sleep_amount = get_exponential_backoff_interval(
+                            self.base_sleep_between_retries_ms, retries,
+                            self.max_sleep_between_retries_ms, True) / 1000
+                        await self._sleep(sleep_amount)
+                    else:
+                        raise_with_context(
+                            BackendStoreError("failed to store result on the backend", task_id=task_id, state=state),
+                        )
+                else:
+                    raise
+
+    async def _sleep(self, amount):
+        await asyncio.sleep(amount)
+
+    async def _set_with_state(self, key, value, state):
+        return await self.set(key, value)
+
+    async def _wait_for_single_future(self, result, timeout):
+        future = self._pending_future[result.id]
+        try:
+            await asyncio.wait_for(asyncio.shield(future), timeout=timeout)
+        except asyncio.TimeoutError:
+            pass
+
+    async def wait_for_future(self, result, timeout):
+        if isinstance(result, GroupResult):
+            results = result.results
+        else:
+            results = [result]
+
+        await asyncio.gather(*[
+            self._wait_for_single_future(r, timeout)
+            for r in results
+        ])
+
+    def maybe_set_future(self, task_id, result):
+        if task_id not in self._pending_future:
+            future = asyncio.get_running_loop().create_future()
+            self._pending_future[task_id] = future
+        else:
+            future: asyncio.Future = self._pending_future[task_id]
+
+        if not future.done():
+            logger.debug(f'Set result for task[{task_id}]: {result}')
+            future.set_result(result)
+
+    def remove_pending_future(self, task_id):
+        self._pending_future.pop(task_id, None)
+
+    async def restore_group(self, group_id, cache=True):
+        meta = await self.get_group_meta(group_id, cache=cache)
+        if meta:
+            return meta['result']
+
+    async def get_group_meta(self, group_id, cache=True):
+        self._ensure_not_eager()
+        if cache:
+            try:
+                return self._cache[group_id]
+            except KeyError:
+                pass
+
+        meta = await self._restore_group(group_id)
+        if cache and meta is not None:
+            self._cache[group_id] = meta
+        return meta
+
+    async def _restore_group(self, group_id):
+        """Get task meta-data for a task by id."""
+        meta = await self.get(self.get_key_for_group(group_id))
+        # previously this was always pickled, but later this
+        # was extended to support other serializers, so the
+        # structure is kind of weird.
+        if meta:
+            meta = self.decode(meta)
+            result = meta['result']
+            meta['result'] = await result_from_tuple(result, self.app)
+            return meta
+
+    async def _save_group(self, group_id, result):
+        await self._set_with_state(
+            self.get_key_for_group(group_id),
+            self.encode({'result': result.as_tuple()}),
+            states.SUCCESS
+        )
+        return result
+
+    async def iter_native(self, result, no_ack=True, **kwargs):
+        self._ensure_not_eager()
+
+        results = result.results
+        if not results:
+            raise StopIteration()
+
+        # we tell the result consumer to put consumed results
+        # into these buckets.
+        bucket = deque()
+        for node in results:
+            if not hasattr(node, '_cache'):
+                bucket.append(node)
+            elif node._cache:
+                bucket.append(node)
+            else:
+                self._collect_into(node, bucket)
+
+        await self._wait_for_pending(result, no_ack=no_ack, **kwargs)
+        while bucket:
+            node = bucket.popleft()
+            if not hasattr(node, '_cache'):
+                yield node.id, node.children
+            else:
+                yield node.id, node._cache
+
+        while bucket:
+            node = bucket.popleft()
+            yield node.id, node._cache
+
+    async def get_many(
+        self, task_ids, timeout=None, interval=0.5, no_ack=True,
+        on_message=None, on_interval=None, max_iterations=None,
+        READY_STATES=states.READY_STATES
+    ):
+        interval = 0.5 if interval is None else interval
+        ids = task_ids if isinstance(task_ids, set) else set(task_ids)
+        cached_ids = set()
+        cache = self._cache
+        for task_id in ids:
+            try:
+                cached = cache[task_id]
+            except KeyError:
+                pass
+            else:
+                if cached['status'] in READY_STATES:
+                    yield bytes_to_str(task_id), cached
+                    cached_ids.add(task_id)
+
+        ids.difference_update(cached_ids)
+        iterations = 0
+        while ids:
+            keys = list(ids)
+            r = self._mget_to_results(
+                await self.mget(
+                    [self.get_key_for_task(k)
+                     for k in keys]
+                ), keys, READY_STATES
+            )
+            cache.update(r)
+            ids.difference_update({bytes_to_str(v) for v in r})
+            for key, value in r.items():
+                if on_message is not None:
+                    on_message(value)
+                yield bytes_to_str(key), value
+            if timeout and iterations * interval >= timeout:
+                raise TimeoutError(f'Operation timed out ({timeout})')
+            if on_interval:
+                on_interval()
+            time.sleep(interval)  # don't busy loop.
+            iterations += 1
+            if max_iterations and iterations >= max_iterations:
+                break
+
+
+class SentinelBackend(RedisBackend):
+    sentinel = getattr(aioredis, "sentinel", None)
+
+    if TYPE_CHECKING:
+        from redis.asyncio.sentinel import Sentinel
+        def _get_sentinel_instance(self, **params) -> Sentinel: ...
+
+    def _params_from_url(self, url, defaults):
+        chunks = url.split(";")
+        connparams = dict(defaults, hosts=[])
+        for chunk in chunks:
+            data = super()._params_from_url(
+                url=chunk, defaults=defaults)
+            connparams['hosts'].append(data)
+        for param in ("host", "port", "db", "password"):
+            connparams.pop(param)
+
+        # Adding db/password in connparams to connect to the correct instance
+        for param in ("db", "password"):
+            if connparams['hosts'] and param in connparams['hosts'][0]:
+                connparams[param] = connparams['hosts'][0].get(param)
+        return connparams
+
+    _get_sentinel_instance = SyncSentinelBackend._get_sentinel_instance # noqa
+    _get_pool = SyncSentinelBackend._get_pool # noqa
+
+
+class ClusterBackend(RedisBackend):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.connparams.pop('db')
+
+    def _get_pool(self, **params):
+        return NotImplementedError
+
+    def _create_client(self, **params):
+        return cluster.RedisCluster(**params)
+
+    async def _set(self, key, value):
+        async with self.client.pipeline() as pipe:
+            if self.expires:
+                pipe.setex(key, self.expires, value)
+            else:
+                pipe.set(key, value)
+            await pipe.execute()
+        await self.client.publish(key, value)
```

## celery/app/amqp.py

 * *Ordering differences only*

```diff
@@ -1,625 +1,625 @@
-"""Sending/Receiving Messages (Kombu integration)."""
-import numbers
-from collections import namedtuple
-from collections.abc import Mapping
-from datetime import timedelta
-from weakref import WeakValueDictionary
-
-from kombu import Connection, Consumer, Exchange, Producer, Queue, pools
-from kombu.common import Broadcast
-from kombu.utils.functional import maybe_list
-from kombu.utils.objects import cached_property
-
-from celery import signals
-from celery.utils.nodenames import anon_nodename
-from celery.utils.saferepr import saferepr
-from celery.utils.text import indent as textindent
-from celery.utils.time import maybe_make_aware
-
-from . import routes as _routes
-
-__all__ = ('AMQP', 'Queues', 'task_message')
-
-#: earliest date supported by time.mktime.
-INT_MIN = -2147483648
-
-#: Human readable queue declaration.
-QUEUE_FORMAT = """
-.> {0.name:<16} exchange={0.exchange.name}({0.exchange.type}) \
-key={0.routing_key}
-"""
-
-task_message = namedtuple('task_message',
-                          ('headers', 'properties', 'body', 'sent_event'))
-
-
-def utf8dict(d, encoding='utf-8'):
-    return {k.decode(encoding) if isinstance(k, bytes) else k: v
-            for k, v in d.items()}
-
-
-class Queues(dict):
-    """Queue name⇒ declaration mapping.
-
-    Arguments:
-        queues (Iterable): Initial list/tuple or dict of queues.
-        create_missing (bool): By default any unknown queues will be
-            added automatically, but if this flag is disabled the occurrence
-            of unknown queues in `wanted` will raise :exc:`KeyError`.
-        max_priority (int): Default x-max-priority for queues with none set.
-    """
-
-    #: If set, this is a subset of queues to consume from.
-    #: The rest of the queues are then used for routing only.
-    _consume_from = None
-    queue_cls = Queue
-
-    def __init__(self, queues=None, default_exchange=None,
-                 create_missing=True, autoexchange=None,
-                 max_priority=None, default_routing_key=None):
-        dict.__init__(self)
-        self.aliases = WeakValueDictionary()
-        self.default_exchange = default_exchange
-        self.default_routing_key = default_routing_key
-        self.create_missing = create_missing
-        self.autoexchange = Exchange if autoexchange is None else autoexchange
-        self.max_priority = max_priority
-        if queues is not None and not isinstance(queues, Mapping):
-            queues = {q.name: q for q in queues}
-        queues = queues or {}
-        for name, q in queues.items():
-            if isinstance(q, self.queue_cls):
-                self.add(q)
-            else:
-                self.add_compat(name, **q)
-
-    def __getitem__(self, name):
-        try:
-            return self.aliases[name]
-        except KeyError:
-            return dict.__getitem__(self, name)
-
-    def __setitem__(self, name, queue):
-        if self.default_exchange and not queue.exchange:
-            queue.exchange = self.default_exchange
-        dict.__setitem__(self, name, queue)
-        if queue.alias:
-            self.aliases[queue.alias] = queue
-
-    def __missing__(self, name):
-        if self.create_missing:
-            return self.add(self.new_missing(name))
-        raise KeyError(name)
-
-    def add(self, queue, **kwargs):
-        """Add new queue.
-
-        The first argument can either be a :class:`kombu.Queue` instance,
-        or the name of a queue.  If the former the rest of the keyword
-        arguments are ignored, and options are simply taken from the queue
-        instance.
-
-        Arguments:
-            queue (kombu.Queue, str): Queue to add.
-            exchange (kombu.Exchange, str):
-                if queue is str, specifies exchange name.
-            routing_key (str): if queue is str, specifies binding key.
-            exchange_type (str): if queue is str, specifies type of exchange.
-            **options (Any): Additional declaration options used when
-                queue is a str.
-        """
-        if not isinstance(queue, self.queue_cls):
-            return self.add_compat(queue, **kwargs)
-        return self._add(queue)
-
-    def add_compat(self, name, **options):
-        # docs used to use binding_key as routing key
-        options.setdefault('routing_key', options.get('binding_key'))
-        if options['routing_key'] is None:
-            options['routing_key'] = name
-        return self._add(self.queue_cls.from_dict(name, **options))
-
-    def _add(self, queue):
-        if queue.exchange is None or queue.exchange.name == '':
-            queue.exchange = self.default_exchange
-        if not queue.routing_key:
-            queue.routing_key = self.default_routing_key
-        if self.max_priority is not None:
-            if queue.queue_arguments is None:
-                queue.queue_arguments = {}
-            self._set_max_priority(queue.queue_arguments)
-        self[queue.name] = queue
-        return queue
-
-    def _set_max_priority(self, args):
-        if 'x-max-priority' not in args and self.max_priority is not None:
-            return args.update({'x-max-priority': self.max_priority})
-
-    def format(self, indent=0, indent_first=True):
-        """Format routing table into string for log dumps."""
-        active = self.consume_from
-        if not active:
-            return ''
-        info = [QUEUE_FORMAT.strip().format(q)
-                for _, q in sorted(active.items())]
-        if indent_first:
-            return textindent('\n'.join(info), indent)
-        return info[0] + '\n' + textindent('\n'.join(info[1:]), indent)
-
-    def select_add(self, queue, **kwargs):
-        """Add new task queue that'll be consumed from.
-
-        The queue will be active even when a subset has been selected
-        using the :option:`celery worker -Q` option.
-        """
-        q = self.add(queue, **kwargs)
-        if self._consume_from is not None:
-            self._consume_from[q.name] = q
-        return q
-
-    def select(self, include):
-        """Select a subset of currently defined queues to consume from.
-
-        Arguments:
-            include (Sequence[str], str): Names of queues to consume from.
-        """
-        if include:
-            self._consume_from = {
-                name: self[name] for name in maybe_list(include)
-            }
-
-    def deselect(self, exclude):
-        """Deselect queues so that they won't be consumed from.
-
-        Arguments:
-            exclude (Sequence[str], str): Names of queues to avoid
-                consuming from.
-        """
-        if exclude:
-            exclude = maybe_list(exclude)
-            if self._consume_from is None:
-                # using all queues
-                return self.select(k for k in self if k not in exclude)
-            # using selection
-            for queue in exclude:
-                self._consume_from.pop(queue, None)
-
-    def new_missing(self, name):
-        return self.queue_cls(name, self.autoexchange(name), name)
-
-    @property
-    def consume_from(self):
-        if self._consume_from is not None:
-            return self._consume_from
-        return self
-
-
-class AMQP:
-    """App AMQP API: app.amqp."""
-
-    Connection = Connection
-    Consumer = Consumer
-    Producer = Producer
-
-    #: compat alias to Connection
-    BrokerConnection = Connection
-
-    queues_cls = Queues
-
-    #: Cached and prepared routing table.
-    _rtable = None
-
-    #: Underlying producer pool instance automatically
-    #: set by the :attr:`producer_pool`.
-    _producer_pool = None
-
-    # Exchange class/function used when defining automatic queues.
-    # For example, you can use ``autoexchange = lambda n: None`` to use the
-    # AMQP default exchange: a shortcut to bypass routing
-    # and instead send directly to the queue named in the routing key.
-    autoexchange = None
-
-    #: Max size of positional argument representation used for
-    #: logging purposes.
-    argsrepr_maxsize = 1024
-
-    #: Max size of keyword argument representation used for logging purposes.
-    kwargsrepr_maxsize = 1024
-
-    def __init__(self, app):
-        self.app = app
-        self.task_protocols = {
-            1: self.as_task_v1,
-            2: self.as_task_v2,
-        }
-        self.app._conf.bind_to(self._handle_conf_update)
-
-    @cached_property
-    def create_task_message(self):
-        return self.task_protocols[self.app.conf.task_protocol]
-
-    @cached_property
-    def send_task_message(self):
-        return self._create_task_sender()
-
-    def Queues(self, queues, create_missing=None,
-               autoexchange=None, max_priority=None):
-        # Create new :class:`Queues` instance, using queue defaults
-        # from the current configuration.
-        conf = self.app.conf
-        default_routing_key = conf.task_default_routing_key
-        if create_missing is None:
-            create_missing = conf.task_create_missing_queues
-        if max_priority is None:
-            max_priority = conf.task_queue_max_priority
-        if not queues and conf.task_default_queue:
-            queues = (self.queues_cls.queue_cls(
-                conf.task_default_queue,
-                exchange=self.default_exchange,
-                routing_key=default_routing_key),)
-        autoexchange = (self.autoexchange if autoexchange is None
-                        else autoexchange)
-        return self.queues_cls(
-            queues, self.default_exchange, create_missing,
-            autoexchange, max_priority, default_routing_key,
-        )
-
-    def Router(self, queues=None, create_missing=None):
-        """Return the current task router."""
-        return _routes.Router(self.routes, queues or self.queues,
-                              self.app.either('task_create_missing_queues',
-                                              create_missing), app=self.app)
-
-    def flush_routes(self):
-        self._rtable = _routes.prepare(self.app.conf.task_routes)
-
-    def TaskConsumer(self, channel, queues=None, accept=None, **kw):
-        if accept is None:
-            accept = self.app.conf.accept_content
-        return self.Consumer(
-            channel, accept=accept,
-            queues=queues or list(self.queues.consume_from.values()),
-            **kw
-        )
-
-    def as_task_v2(self, task_id, name, args=None, kwargs=None,
-                   countdown=None, eta=None, group_id=None, group_index=None,
-                   expires=None, retries=0, chord=None,
-                   callbacks=None, errbacks=None, reply_to=None,
-                   time_limit=None, soft_time_limit=None,
-                   create_sent_event=False, root_id=None, parent_id=None,
-                   shadow=None, chain=None, now=None, timezone=None,
-                   origin=None, argsrepr=None, kwargsrepr=None):
-        args = args or ()
-        kwargs = kwargs or {}
-        if not isinstance(args, (list, tuple)):
-            raise TypeError('task args must be a list or tuple')
-        if not isinstance(kwargs, Mapping):
-            raise TypeError('task keyword arguments must be a mapping')
-        if countdown:  # convert countdown to ETA
-            self._verify_seconds(countdown, 'countdown')
-            now = now or self.app.now()
-            timezone = timezone or self.app.timezone
-            eta = maybe_make_aware(
-                now + timedelta(seconds=countdown), tz=timezone,
-            )
-        if isinstance(expires, numbers.Real):
-            self._verify_seconds(expires, 'expires')
-            now = now or self.app.now()
-            timezone = timezone or self.app.timezone
-            expires = maybe_make_aware(
-                now + timedelta(seconds=expires), tz=timezone,
-            )
-        if not isinstance(eta, str):
-            eta = eta and eta.isoformat()
-        # If we retry a task `expires` will already be ISO8601-formatted.
-        if not isinstance(expires, str):
-            expires = expires and expires.isoformat()
-
-        if argsrepr is None:
-            argsrepr = saferepr(args, self.argsrepr_maxsize)
-        if kwargsrepr is None:
-            kwargsrepr = saferepr(kwargs, self.kwargsrepr_maxsize)
-
-        if callbacks:
-            callbacks = [utf8dict(callback) for callback in callbacks]
-        if errbacks:
-            errbacks = [utf8dict(errback) for errback in errbacks]
-        if chord:
-            chord = utf8dict(chord)
-
-        if not root_id:  # empty root_id defaults to task_id
-            root_id = task_id
-
-        return task_message(
-            headers={
-                'lang': 'py',
-                'task': name,
-                'id': task_id,
-                'shadow': shadow,
-                'eta': eta,
-                'expires': expires,
-                'group': group_id,
-                'group_index': group_index,
-                'retries': retries,
-                'timelimit': [time_limit, soft_time_limit],
-                'root_id': root_id,
-                'parent_id': parent_id,
-                'argsrepr': argsrepr,
-                'kwargsrepr': kwargsrepr,
-                'origin': origin or anon_nodename()
-            },
-            properties={
-                'correlation_id': task_id,
-                'reply_to': reply_to or '',
-            },
-            body=(
-                args, kwargs, {
-                    'callbacks': callbacks,
-                    'errbacks': errbacks,
-                    'chain': chain,
-                    'chord': chord,
-                },
-            ),
-            sent_event={
-                'uuid': task_id,
-                'root_id': root_id,
-                'parent_id': parent_id,
-                'name': name,
-                'args': argsrepr,
-                'kwargs': kwargsrepr,
-                'retries': retries,
-                'eta': eta,
-                'expires': expires,
-            } if create_sent_event else None,
-        )
-
-    def as_task_v1(self, task_id, name, args=None, kwargs=None,
-                   countdown=None, eta=None, group_id=None, group_index=None,
-                   expires=None, retries=0,
-                   chord=None, callbacks=None, errbacks=None, reply_to=None,
-                   time_limit=None, soft_time_limit=None,
-                   create_sent_event=False, root_id=None, parent_id=None,
-                   shadow=None, now=None, timezone=None,
-                   **compat_kwargs):
-        args = args or ()
-        kwargs = kwargs or {}
-        utc = self.utc
-        if not isinstance(args, (list, tuple)):
-            raise TypeError('task args must be a list or tuple')
-        if not isinstance(kwargs, Mapping):
-            raise TypeError('task keyword arguments must be a mapping')
-        if countdown:  # convert countdown to ETA
-            self._verify_seconds(countdown, 'countdown')
-            now = now or self.app.now()
-            eta = now + timedelta(seconds=countdown)
-        if isinstance(expires, numbers.Real):
-            self._verify_seconds(expires, 'expires')
-            now = now or self.app.now()
-            expires = now + timedelta(seconds=expires)
-        eta = eta and eta.isoformat()
-        expires = expires and expires.isoformat()
-
-        if callbacks:
-            callbacks = [utf8dict(callback) for callback in callbacks]
-        if errbacks:
-            errbacks = [utf8dict(errback) for errback in errbacks]
-        if chord:
-            chord = utf8dict(chord)
-
-        return task_message(
-            headers={},
-            properties={
-                'correlation_id': task_id,
-                'reply_to': reply_to or '',
-            },
-            body={
-                'task': name,
-                'id': task_id,
-                'args': args,
-                'kwargs': kwargs,
-                'group': group_id,
-                'group_index': group_index,
-                'retries': retries,
-                'eta': eta,
-                'expires': expires,
-                'utc': utc,
-                'callbacks': callbacks,
-                'errbacks': errbacks,
-                'timelimit': (time_limit, soft_time_limit),
-                'taskset': group_id,
-                'chord': chord,
-            },
-            sent_event={
-                'uuid': task_id,
-                'name': name,
-                'args': saferepr(args),
-                'kwargs': saferepr(kwargs),
-                'retries': retries,
-                'eta': eta,
-                'expires': expires,
-            } if create_sent_event else None,
-        )
-
-    def _verify_seconds(self, s, what):
-        if s < INT_MIN:
-            raise ValueError(f'{what} is out of range: {s!r}')
-        return s
-
-    def _create_task_sender(self):
-        default_retry = self.app.conf.task_publish_retry
-        default_policy = self.app.conf.task_publish_retry_policy
-        default_delivery_mode = self.app.conf.task_default_delivery_mode
-        default_queue = self.default_queue
-        queues = self.queues
-        send_before_publish = signals.before_task_publish.send
-        before_receivers = signals.before_task_publish.receivers
-        send_after_publish = signals.after_task_publish.send
-        after_receivers = signals.after_task_publish.receivers
-
-        send_task_sent = signals.task_sent.send   # XXX compat
-        sent_receivers = signals.task_sent.receivers
-
-        default_evd = self._event_dispatcher
-        default_exchange = self.default_exchange
-
-        default_rkey = self.app.conf.task_default_routing_key
-        default_serializer = self.app.conf.task_serializer
-        default_compressor = self.app.conf.result_compression
-
-        def send_task_message(producer, name, message,
-                              exchange=None, routing_key=None, queue=None,
-                              event_dispatcher=None,
-                              retry=None, retry_policy=None,
-                              serializer=None, delivery_mode=None,
-                              compression=None, declare=None,
-                              headers=None, exchange_type=None, **kwargs):
-            retry = default_retry if retry is None else retry
-            headers2, properties, body, sent_event = message
-            if headers:
-                headers2.update(headers)
-            if kwargs:
-                properties.update(kwargs)
-
-            qname = queue
-            if queue is None and exchange is None:
-                queue = default_queue
-            if queue is not None:
-                if isinstance(queue, str):
-                    qname, queue = queue, queues[queue]
-                else:
-                    qname = queue.name
-
-            if delivery_mode is None:
-                try:
-                    delivery_mode = queue.exchange.delivery_mode
-                except AttributeError:
-                    pass
-                delivery_mode = delivery_mode or default_delivery_mode
-
-            if exchange_type is None:
-                try:
-                    exchange_type = queue.exchange.type
-                except AttributeError:
-                    exchange_type = 'direct'
-
-            # convert to anon-exchange, when exchange not set and direct ex.
-            if (not exchange or not routing_key) and exchange_type == 'direct':
-                exchange, routing_key = '', qname
-            elif exchange is None:
-                # not topic exchange, and exchange not undefined
-                exchange = queue.exchange.name or default_exchange
-                routing_key = routing_key or queue.routing_key or default_rkey
-            if declare is None and queue and not isinstance(queue, Broadcast):
-                declare = [queue]
-
-            # merge default and custom policy
-            retry = default_retry if retry is None else retry
-            _rp = (dict(default_policy, **retry_policy) if retry_policy
-                   else default_policy)
-
-            if before_receivers:
-                send_before_publish(
-                    sender=name, body=body,
-                    exchange=exchange, routing_key=routing_key,
-                    declare=declare, headers=headers2,
-                    properties=properties, retry_policy=retry_policy,
-                )
-            ret = producer.publish(
-                body,
-                exchange=exchange,
-                routing_key=routing_key,
-                serializer=serializer or default_serializer,
-                compression=compression or default_compressor,
-                retry=retry, retry_policy=_rp,
-                delivery_mode=delivery_mode, declare=declare,
-                headers=headers2,
-                **properties
-            )
-            if after_receivers:
-                send_after_publish(sender=name, body=body, headers=headers2,
-                                   exchange=exchange, routing_key=routing_key)
-            if sent_receivers:  # XXX deprecated
-                if isinstance(body, tuple):  # protocol version 2
-                    send_task_sent(
-                        sender=name, task_id=headers2['id'], task=name,
-                        args=body[0], kwargs=body[1],
-                        eta=headers2['eta'], taskset=headers2['group'],
-                    )
-                else:  # protocol version 1
-                    send_task_sent(
-                        sender=name, task_id=body['id'], task=name,
-                        args=body['args'], kwargs=body['kwargs'],
-                        eta=body['eta'], taskset=body['taskset'],
-                    )
-            if sent_event:
-                evd = event_dispatcher or default_evd
-                exname = exchange
-                if isinstance(exname, Exchange):
-                    exname = exname.name
-                sent_event.update({
-                    'queue': qname,
-                    'exchange': exname,
-                    'routing_key': routing_key,
-                })
-                evd.publish('task-sent', sent_event,
-                            producer, retry=retry, retry_policy=retry_policy)
-            return ret
-        return send_task_message
-
-    @cached_property
-    def default_queue(self):
-        return self.queues[self.app.conf.task_default_queue]
-
-    @cached_property
-    def queues(self):
-        """Queue name⇒ declaration mapping."""
-        return self.Queues(self.app.conf.task_queues)
-
-    @queues.setter  # noqa
-    def queues(self, queues):
-        return self.Queues(queues)
-
-    @property
-    def routes(self):
-        if self._rtable is None:
-            self.flush_routes()
-        return self._rtable
-
-    @cached_property
-    def router(self):
-        return self.Router()
-
-    @router.setter
-    def router(self, value):
-        return value
-
-    @property
-    def producer_pool(self):
-        if self._producer_pool is None:
-            self._producer_pool = pools.producers[
-                self.app.connection_for_write()]
-            self._producer_pool.limit = self.app.pool.limit
-        return self._producer_pool
-    publisher_pool = producer_pool  # compat alias
-
-    @cached_property
-    def default_exchange(self):
-        return Exchange(self.app.conf.task_default_exchange,
-                        self.app.conf.task_default_exchange_type)
-
-    @cached_property
-    def utc(self):
-        return self.app.conf.enable_utc
-
-    @cached_property
-    def _event_dispatcher(self):
-        # We call Dispatcher.publish with a custom producer
-        # so don't need the diuspatcher to be enabled.
-        return self.app.events.Dispatcher(enabled=False)
-
-    def _handle_conf_update(self, *args, **kwargs):
-        if ('task_routes' in kwargs or 'task_routes' in args):
-            self.flush_routes()
-            self.router = self.Router()
-        return
+"""Sending/Receiving Messages (Kombu integration)."""
+import numbers
+from collections import namedtuple
+from collections.abc import Mapping
+from datetime import timedelta
+from weakref import WeakValueDictionary
+
+from kombu import Connection, Consumer, Exchange, Producer, Queue, pools
+from kombu.common import Broadcast
+from kombu.utils.functional import maybe_list
+from kombu.utils.objects import cached_property
+
+from celery import signals
+from celery.utils.nodenames import anon_nodename
+from celery.utils.saferepr import saferepr
+from celery.utils.text import indent as textindent
+from celery.utils.time import maybe_make_aware
+
+from . import routes as _routes
+
+__all__ = ('AMQP', 'Queues', 'task_message')
+
+#: earliest date supported by time.mktime.
+INT_MIN = -2147483648
+
+#: Human readable queue declaration.
+QUEUE_FORMAT = """
+.> {0.name:<16} exchange={0.exchange.name}({0.exchange.type}) \
+key={0.routing_key}
+"""
+
+task_message = namedtuple('task_message',
+                          ('headers', 'properties', 'body', 'sent_event'))
+
+
+def utf8dict(d, encoding='utf-8'):
+    return {k.decode(encoding) if isinstance(k, bytes) else k: v
+            for k, v in d.items()}
+
+
+class Queues(dict):
+    """Queue name⇒ declaration mapping.
+
+    Arguments:
+        queues (Iterable): Initial list/tuple or dict of queues.
+        create_missing (bool): By default any unknown queues will be
+            added automatically, but if this flag is disabled the occurrence
+            of unknown queues in `wanted` will raise :exc:`KeyError`.
+        max_priority (int): Default x-max-priority for queues with none set.
+    """
+
+    #: If set, this is a subset of queues to consume from.
+    #: The rest of the queues are then used for routing only.
+    _consume_from = None
+    queue_cls = Queue
+
+    def __init__(self, queues=None, default_exchange=None,
+                 create_missing=True, autoexchange=None,
+                 max_priority=None, default_routing_key=None):
+        dict.__init__(self)
+        self.aliases = WeakValueDictionary()
+        self.default_exchange = default_exchange
+        self.default_routing_key = default_routing_key
+        self.create_missing = create_missing
+        self.autoexchange = Exchange if autoexchange is None else autoexchange
+        self.max_priority = max_priority
+        if queues is not None and not isinstance(queues, Mapping):
+            queues = {q.name: q for q in queues}
+        queues = queues or {}
+        for name, q in queues.items():
+            if isinstance(q, self.queue_cls):
+                self.add(q)
+            else:
+                self.add_compat(name, **q)
+
+    def __getitem__(self, name):
+        try:
+            return self.aliases[name]
+        except KeyError:
+            return dict.__getitem__(self, name)
+
+    def __setitem__(self, name, queue):
+        if self.default_exchange and not queue.exchange:
+            queue.exchange = self.default_exchange
+        dict.__setitem__(self, name, queue)
+        if queue.alias:
+            self.aliases[queue.alias] = queue
+
+    def __missing__(self, name):
+        if self.create_missing:
+            return self.add(self.new_missing(name))
+        raise KeyError(name)
+
+    def add(self, queue, **kwargs):
+        """Add new queue.
+
+        The first argument can either be a :class:`kombu.Queue` instance,
+        or the name of a queue.  If the former the rest of the keyword
+        arguments are ignored, and options are simply taken from the queue
+        instance.
+
+        Arguments:
+            queue (kombu.Queue, str): Queue to add.
+            exchange (kombu.Exchange, str):
+                if queue is str, specifies exchange name.
+            routing_key (str): if queue is str, specifies binding key.
+            exchange_type (str): if queue is str, specifies type of exchange.
+            **options (Any): Additional declaration options used when
+                queue is a str.
+        """
+        if not isinstance(queue, self.queue_cls):
+            return self.add_compat(queue, **kwargs)
+        return self._add(queue)
+
+    def add_compat(self, name, **options):
+        # docs used to use binding_key as routing key
+        options.setdefault('routing_key', options.get('binding_key'))
+        if options['routing_key'] is None:
+            options['routing_key'] = name
+        return self._add(self.queue_cls.from_dict(name, **options))
+
+    def _add(self, queue):
+        if queue.exchange is None or queue.exchange.name == '':
+            queue.exchange = self.default_exchange
+        if not queue.routing_key:
+            queue.routing_key = self.default_routing_key
+        if self.max_priority is not None:
+            if queue.queue_arguments is None:
+                queue.queue_arguments = {}
+            self._set_max_priority(queue.queue_arguments)
+        self[queue.name] = queue
+        return queue
+
+    def _set_max_priority(self, args):
+        if 'x-max-priority' not in args and self.max_priority is not None:
+            return args.update({'x-max-priority': self.max_priority})
+
+    def format(self, indent=0, indent_first=True):
+        """Format routing table into string for log dumps."""
+        active = self.consume_from
+        if not active:
+            return ''
+        info = [QUEUE_FORMAT.strip().format(q)
+                for _, q in sorted(active.items())]
+        if indent_first:
+            return textindent('\n'.join(info), indent)
+        return info[0] + '\n' + textindent('\n'.join(info[1:]), indent)
+
+    def select_add(self, queue, **kwargs):
+        """Add new task queue that'll be consumed from.
+
+        The queue will be active even when a subset has been selected
+        using the :option:`celery worker -Q` option.
+        """
+        q = self.add(queue, **kwargs)
+        if self._consume_from is not None:
+            self._consume_from[q.name] = q
+        return q
+
+    def select(self, include):
+        """Select a subset of currently defined queues to consume from.
+
+        Arguments:
+            include (Sequence[str], str): Names of queues to consume from.
+        """
+        if include:
+            self._consume_from = {
+                name: self[name] for name in maybe_list(include)
+            }
+
+    def deselect(self, exclude):
+        """Deselect queues so that they won't be consumed from.
+
+        Arguments:
+            exclude (Sequence[str], str): Names of queues to avoid
+                consuming from.
+        """
+        if exclude:
+            exclude = maybe_list(exclude)
+            if self._consume_from is None:
+                # using all queues
+                return self.select(k for k in self if k not in exclude)
+            # using selection
+            for queue in exclude:
+                self._consume_from.pop(queue, None)
+
+    def new_missing(self, name):
+        return self.queue_cls(name, self.autoexchange(name), name)
+
+    @property
+    def consume_from(self):
+        if self._consume_from is not None:
+            return self._consume_from
+        return self
+
+
+class AMQP:
+    """App AMQP API: app.amqp."""
+
+    Connection = Connection
+    Consumer = Consumer
+    Producer = Producer
+
+    #: compat alias to Connection
+    BrokerConnection = Connection
+
+    queues_cls = Queues
+
+    #: Cached and prepared routing table.
+    _rtable = None
+
+    #: Underlying producer pool instance automatically
+    #: set by the :attr:`producer_pool`.
+    _producer_pool = None
+
+    # Exchange class/function used when defining automatic queues.
+    # For example, you can use ``autoexchange = lambda n: None`` to use the
+    # AMQP default exchange: a shortcut to bypass routing
+    # and instead send directly to the queue named in the routing key.
+    autoexchange = None
+
+    #: Max size of positional argument representation used for
+    #: logging purposes.
+    argsrepr_maxsize = 1024
+
+    #: Max size of keyword argument representation used for logging purposes.
+    kwargsrepr_maxsize = 1024
+
+    def __init__(self, app):
+        self.app = app
+        self.task_protocols = {
+            1: self.as_task_v1,
+            2: self.as_task_v2,
+        }
+        self.app._conf.bind_to(self._handle_conf_update)
+
+    @cached_property
+    def create_task_message(self):
+        return self.task_protocols[self.app.conf.task_protocol]
+
+    @cached_property
+    def send_task_message(self):
+        return self._create_task_sender()
+
+    def Queues(self, queues, create_missing=None,
+               autoexchange=None, max_priority=None):
+        # Create new :class:`Queues` instance, using queue defaults
+        # from the current configuration.
+        conf = self.app.conf
+        default_routing_key = conf.task_default_routing_key
+        if create_missing is None:
+            create_missing = conf.task_create_missing_queues
+        if max_priority is None:
+            max_priority = conf.task_queue_max_priority
+        if not queues and conf.task_default_queue:
+            queues = (self.queues_cls.queue_cls(
+                conf.task_default_queue,
+                exchange=self.default_exchange,
+                routing_key=default_routing_key),)
+        autoexchange = (self.autoexchange if autoexchange is None
+                        else autoexchange)
+        return self.queues_cls(
+            queues, self.default_exchange, create_missing,
+            autoexchange, max_priority, default_routing_key,
+        )
+
+    def Router(self, queues=None, create_missing=None):
+        """Return the current task router."""
+        return _routes.Router(self.routes, queues or self.queues,
+                              self.app.either('task_create_missing_queues',
+                                              create_missing), app=self.app)
+
+    def flush_routes(self):
+        self._rtable = _routes.prepare(self.app.conf.task_routes)
+
+    def TaskConsumer(self, channel, queues=None, accept=None, **kw):
+        if accept is None:
+            accept = self.app.conf.accept_content
+        return self.Consumer(
+            channel, accept=accept,
+            queues=queues or list(self.queues.consume_from.values()),
+            **kw
+        )
+
+    def as_task_v2(self, task_id, name, args=None, kwargs=None,
+                   countdown=None, eta=None, group_id=None, group_index=None,
+                   expires=None, retries=0, chord=None,
+                   callbacks=None, errbacks=None, reply_to=None,
+                   time_limit=None, soft_time_limit=None,
+                   create_sent_event=False, root_id=None, parent_id=None,
+                   shadow=None, chain=None, now=None, timezone=None,
+                   origin=None, argsrepr=None, kwargsrepr=None):
+        args = args or ()
+        kwargs = kwargs or {}
+        if not isinstance(args, (list, tuple)):
+            raise TypeError('task args must be a list or tuple')
+        if not isinstance(kwargs, Mapping):
+            raise TypeError('task keyword arguments must be a mapping')
+        if countdown:  # convert countdown to ETA
+            self._verify_seconds(countdown, 'countdown')
+            now = now or self.app.now()
+            timezone = timezone or self.app.timezone
+            eta = maybe_make_aware(
+                now + timedelta(seconds=countdown), tz=timezone,
+            )
+        if isinstance(expires, numbers.Real):
+            self._verify_seconds(expires, 'expires')
+            now = now or self.app.now()
+            timezone = timezone or self.app.timezone
+            expires = maybe_make_aware(
+                now + timedelta(seconds=expires), tz=timezone,
+            )
+        if not isinstance(eta, str):
+            eta = eta and eta.isoformat()
+        # If we retry a task `expires` will already be ISO8601-formatted.
+        if not isinstance(expires, str):
+            expires = expires and expires.isoformat()
+
+        if argsrepr is None:
+            argsrepr = saferepr(args, self.argsrepr_maxsize)
+        if kwargsrepr is None:
+            kwargsrepr = saferepr(kwargs, self.kwargsrepr_maxsize)
+
+        if callbacks:
+            callbacks = [utf8dict(callback) for callback in callbacks]
+        if errbacks:
+            errbacks = [utf8dict(errback) for errback in errbacks]
+        if chord:
+            chord = utf8dict(chord)
+
+        if not root_id:  # empty root_id defaults to task_id
+            root_id = task_id
+
+        return task_message(
+            headers={
+                'lang': 'py',
+                'task': name,
+                'id': task_id,
+                'shadow': shadow,
+                'eta': eta,
+                'expires': expires,
+                'group': group_id,
+                'group_index': group_index,
+                'retries': retries,
+                'timelimit': [time_limit, soft_time_limit],
+                'root_id': root_id,
+                'parent_id': parent_id,
+                'argsrepr': argsrepr,
+                'kwargsrepr': kwargsrepr,
+                'origin': origin or anon_nodename()
+            },
+            properties={
+                'correlation_id': task_id,
+                'reply_to': reply_to or '',
+            },
+            body=(
+                args, kwargs, {
+                    'callbacks': callbacks,
+                    'errbacks': errbacks,
+                    'chain': chain,
+                    'chord': chord,
+                },
+            ),
+            sent_event={
+                'uuid': task_id,
+                'root_id': root_id,
+                'parent_id': parent_id,
+                'name': name,
+                'args': argsrepr,
+                'kwargs': kwargsrepr,
+                'retries': retries,
+                'eta': eta,
+                'expires': expires,
+            } if create_sent_event else None,
+        )
+
+    def as_task_v1(self, task_id, name, args=None, kwargs=None,
+                   countdown=None, eta=None, group_id=None, group_index=None,
+                   expires=None, retries=0,
+                   chord=None, callbacks=None, errbacks=None, reply_to=None,
+                   time_limit=None, soft_time_limit=None,
+                   create_sent_event=False, root_id=None, parent_id=None,
+                   shadow=None, now=None, timezone=None,
+                   **compat_kwargs):
+        args = args or ()
+        kwargs = kwargs or {}
+        utc = self.utc
+        if not isinstance(args, (list, tuple)):
+            raise TypeError('task args must be a list or tuple')
+        if not isinstance(kwargs, Mapping):
+            raise TypeError('task keyword arguments must be a mapping')
+        if countdown:  # convert countdown to ETA
+            self._verify_seconds(countdown, 'countdown')
+            now = now or self.app.now()
+            eta = now + timedelta(seconds=countdown)
+        if isinstance(expires, numbers.Real):
+            self._verify_seconds(expires, 'expires')
+            now = now or self.app.now()
+            expires = now + timedelta(seconds=expires)
+        eta = eta and eta.isoformat()
+        expires = expires and expires.isoformat()
+
+        if callbacks:
+            callbacks = [utf8dict(callback) for callback in callbacks]
+        if errbacks:
+            errbacks = [utf8dict(errback) for errback in errbacks]
+        if chord:
+            chord = utf8dict(chord)
+
+        return task_message(
+            headers={},
+            properties={
+                'correlation_id': task_id,
+                'reply_to': reply_to or '',
+            },
+            body={
+                'task': name,
+                'id': task_id,
+                'args': args,
+                'kwargs': kwargs,
+                'group': group_id,
+                'group_index': group_index,
+                'retries': retries,
+                'eta': eta,
+                'expires': expires,
+                'utc': utc,
+                'callbacks': callbacks,
+                'errbacks': errbacks,
+                'timelimit': (time_limit, soft_time_limit),
+                'taskset': group_id,
+                'chord': chord,
+            },
+            sent_event={
+                'uuid': task_id,
+                'name': name,
+                'args': saferepr(args),
+                'kwargs': saferepr(kwargs),
+                'retries': retries,
+                'eta': eta,
+                'expires': expires,
+            } if create_sent_event else None,
+        )
+
+    def _verify_seconds(self, s, what):
+        if s < INT_MIN:
+            raise ValueError(f'{what} is out of range: {s!r}')
+        return s
+
+    def _create_task_sender(self):
+        default_retry = self.app.conf.task_publish_retry
+        default_policy = self.app.conf.task_publish_retry_policy
+        default_delivery_mode = self.app.conf.task_default_delivery_mode
+        default_queue = self.default_queue
+        queues = self.queues
+        send_before_publish = signals.before_task_publish.send
+        before_receivers = signals.before_task_publish.receivers
+        send_after_publish = signals.after_task_publish.send
+        after_receivers = signals.after_task_publish.receivers
+
+        send_task_sent = signals.task_sent.send   # XXX compat
+        sent_receivers = signals.task_sent.receivers
+
+        default_evd = self._event_dispatcher
+        default_exchange = self.default_exchange
+
+        default_rkey = self.app.conf.task_default_routing_key
+        default_serializer = self.app.conf.task_serializer
+        default_compressor = self.app.conf.result_compression
+
+        def send_task_message(producer, name, message,
+                              exchange=None, routing_key=None, queue=None,
+                              event_dispatcher=None,
+                              retry=None, retry_policy=None,
+                              serializer=None, delivery_mode=None,
+                              compression=None, declare=None,
+                              headers=None, exchange_type=None, **kwargs):
+            retry = default_retry if retry is None else retry
+            headers2, properties, body, sent_event = message
+            if headers:
+                headers2.update(headers)
+            if kwargs:
+                properties.update(kwargs)
+
+            qname = queue
+            if queue is None and exchange is None:
+                queue = default_queue
+            if queue is not None:
+                if isinstance(queue, str):
+                    qname, queue = queue, queues[queue]
+                else:
+                    qname = queue.name
+
+            if delivery_mode is None:
+                try:
+                    delivery_mode = queue.exchange.delivery_mode
+                except AttributeError:
+                    pass
+                delivery_mode = delivery_mode or default_delivery_mode
+
+            if exchange_type is None:
+                try:
+                    exchange_type = queue.exchange.type
+                except AttributeError:
+                    exchange_type = 'direct'
+
+            # convert to anon-exchange, when exchange not set and direct ex.
+            if (not exchange or not routing_key) and exchange_type == 'direct':
+                exchange, routing_key = '', qname
+            elif exchange is None:
+                # not topic exchange, and exchange not undefined
+                exchange = queue.exchange.name or default_exchange
+                routing_key = routing_key or queue.routing_key or default_rkey
+            if declare is None and queue and not isinstance(queue, Broadcast):
+                declare = [queue]
+
+            # merge default and custom policy
+            retry = default_retry if retry is None else retry
+            _rp = (dict(default_policy, **retry_policy) if retry_policy
+                   else default_policy)
+
+            if before_receivers:
+                send_before_publish(
+                    sender=name, body=body,
+                    exchange=exchange, routing_key=routing_key,
+                    declare=declare, headers=headers2,
+                    properties=properties, retry_policy=retry_policy,
+                )
+            ret = producer.publish(
+                body,
+                exchange=exchange,
+                routing_key=routing_key,
+                serializer=serializer or default_serializer,
+                compression=compression or default_compressor,
+                retry=retry, retry_policy=_rp,
+                delivery_mode=delivery_mode, declare=declare,
+                headers=headers2,
+                **properties
+            )
+            if after_receivers:
+                send_after_publish(sender=name, body=body, headers=headers2,
+                                   exchange=exchange, routing_key=routing_key)
+            if sent_receivers:  # XXX deprecated
+                if isinstance(body, tuple):  # protocol version 2
+                    send_task_sent(
+                        sender=name, task_id=headers2['id'], task=name,
+                        args=body[0], kwargs=body[1],
+                        eta=headers2['eta'], taskset=headers2['group'],
+                    )
+                else:  # protocol version 1
+                    send_task_sent(
+                        sender=name, task_id=body['id'], task=name,
+                        args=body['args'], kwargs=body['kwargs'],
+                        eta=body['eta'], taskset=body['taskset'],
+                    )
+            if sent_event:
+                evd = event_dispatcher or default_evd
+                exname = exchange
+                if isinstance(exname, Exchange):
+                    exname = exname.name
+                sent_event.update({
+                    'queue': qname,
+                    'exchange': exname,
+                    'routing_key': routing_key,
+                })
+                evd.publish('task-sent', sent_event,
+                            producer, retry=retry, retry_policy=retry_policy)
+            return ret
+        return send_task_message
+
+    @cached_property
+    def default_queue(self):
+        return self.queues[self.app.conf.task_default_queue]
+
+    @cached_property
+    def queues(self):
+        """Queue name⇒ declaration mapping."""
+        return self.Queues(self.app.conf.task_queues)
+
+    @queues.setter  # noqa
+    def queues(self, queues):
+        return self.Queues(queues)
+
+    @property
+    def routes(self):
+        if self._rtable is None:
+            self.flush_routes()
+        return self._rtable
+
+    @cached_property
+    def router(self):
+        return self.Router()
+
+    @router.setter
+    def router(self, value):
+        return value
+
+    @property
+    def producer_pool(self):
+        if self._producer_pool is None:
+            self._producer_pool = pools.producers[
+                self.app.connection_for_write()]
+            self._producer_pool.limit = self.app.pool.limit
+        return self._producer_pool
+    publisher_pool = producer_pool  # compat alias
+
+    @cached_property
+    def default_exchange(self):
+        return Exchange(self.app.conf.task_default_exchange,
+                        self.app.conf.task_default_exchange_type)
+
+    @cached_property
+    def utc(self):
+        return self.app.conf.enable_utc
+
+    @cached_property
+    def _event_dispatcher(self):
+        # We call Dispatcher.publish with a custom producer
+        # so don't need the diuspatcher to be enabled.
+        return self.app.events.Dispatcher(enabled=False)
+
+    def _handle_conf_update(self, *args, **kwargs):
+        if ('task_routes' in kwargs or 'task_routes' in args):
+            self.flush_routes()
+            self.router = self.Router()
+        return
```

## celery/app/backends.py

 * *Ordering differences only*

```diff
@@ -1,71 +1,71 @@
-"""Backend selection."""
-import sys
-import types
-
-from celery._state import current_app
-from celery.exceptions import ImproperlyConfigured, reraise
-from celery.utils.imports import load_extension_class_names, symbol_by_name
-
-__all__ = ('by_name', 'by_url')
-
-UNKNOWN_BACKEND = """
-Unknown result backend: {0!r}.  Did you spell that correctly? ({1!r})
-"""
-
-BACKEND_ALIASES = {
-    'amqp': 'celery.backends.amqp:AMQPBackend',
-    'rpc': 'celery.backends.rpc.RPCBackend',
-    'cache': 'celery.backends.cache:CacheBackend',
-    'redis': 'celery.backends.redis:RedisBackend',
-    'rediss': 'celery.backends.redis:RedisBackend',
-    'sentinel': 'celery.backends.redis:SentinelBackend',
-    'redis-cluster': 'celery.backends.redis:ClusterBackend',
-    'mongodb': 'celery.backends.mongodb:MongoBackend',
-    'db': 'celery.backends.database:DatabaseBackend',
-    'database': 'celery.backends.database:DatabaseBackend',
-    'elasticsearch': 'celery.backends.elasticsearch:ElasticsearchBackend',
-    'cassandra': 'celery.backends.cassandra:CassandraBackend',
-    'couchbase': 'celery.backends.couchbase:CouchbaseBackend',
-    'couchdb': 'celery.backends.couchdb:CouchBackend',
-    'cosmosdbsql': 'celery.backends.cosmosdbsql:CosmosDBSQLBackend',
-    'riak': 'celery.backends.riak:RiakBackend',
-    'file': 'celery.backends.filesystem:FilesystemBackend',
-    'disabled': 'celery.backends.base:DisabledBackend',
-    'consul': 'celery.backends.consul:ConsulBackend',
-    'dynamodb': 'celery.backends.dynamodb:DynamoDBBackend',
-    'azureblockblob': 'celery.backends.azureblockblob:AzureBlockBlobBackend',
-    'arangodb': 'celery.backends.arangodb:ArangoDbBackend',
-    's3': 'celery.backends.s3:S3Backend',
-}
-
-
-def by_name(backend=None, loader=None,
-            extension_namespace='celery.result_backends'):
-    """Get backend class by name/alias."""
-    backend = backend or 'disabled'
-    loader = loader or current_app.loader
-    aliases = dict(BACKEND_ALIASES, **loader.override_backends)
-    aliases.update(
-        load_extension_class_names(extension_namespace) or {})
-    try:
-        cls = symbol_by_name(backend, aliases)
-    except ValueError as exc:
-        reraise(ImproperlyConfigured, ImproperlyConfigured(
-            UNKNOWN_BACKEND.strip().format(backend, exc)), sys.exc_info()[2])
-    if isinstance(cls, types.ModuleType):
-        raise ImproperlyConfigured(UNKNOWN_BACKEND.strip().format(
-            backend, 'is a Python module, not a backend class.'))
-    return cls
-
-
-def by_url(backend=None, loader=None):
-    """Get backend class by URL."""
-    url = None
-    if backend and '://' in backend:
-        url = backend
-        scheme, _, _ = url.partition('://')
-        if '+' in scheme:
-            backend, url = url.split('+', 1)
-        else:
-            backend = scheme
-    return by_name(backend, loader), url
+"""Backend selection."""
+import sys
+import types
+
+from celery._state import current_app
+from celery.exceptions import ImproperlyConfigured, reraise
+from celery.utils.imports import load_extension_class_names, symbol_by_name
+
+__all__ = ('by_name', 'by_url')
+
+UNKNOWN_BACKEND = """
+Unknown result backend: {0!r}.  Did you spell that correctly? ({1!r})
+"""
+
+BACKEND_ALIASES = {
+    'amqp': 'celery.backends.amqp:AMQPBackend',
+    'rpc': 'celery.backends.rpc.RPCBackend',
+    'cache': 'celery.backends.cache:CacheBackend',
+    'redis': 'celery.backends.redis:RedisBackend',
+    'rediss': 'celery.backends.redis:RedisBackend',
+    'sentinel': 'celery.backends.redis:SentinelBackend',
+    'redis-cluster': 'celery.backends.redis:ClusterBackend',
+    'mongodb': 'celery.backends.mongodb:MongoBackend',
+    'db': 'celery.backends.database:DatabaseBackend',
+    'database': 'celery.backends.database:DatabaseBackend',
+    'elasticsearch': 'celery.backends.elasticsearch:ElasticsearchBackend',
+    'cassandra': 'celery.backends.cassandra:CassandraBackend',
+    'couchbase': 'celery.backends.couchbase:CouchbaseBackend',
+    'couchdb': 'celery.backends.couchdb:CouchBackend',
+    'cosmosdbsql': 'celery.backends.cosmosdbsql:CosmosDBSQLBackend',
+    'riak': 'celery.backends.riak:RiakBackend',
+    'file': 'celery.backends.filesystem:FilesystemBackend',
+    'disabled': 'celery.backends.base:DisabledBackend',
+    'consul': 'celery.backends.consul:ConsulBackend',
+    'dynamodb': 'celery.backends.dynamodb:DynamoDBBackend',
+    'azureblockblob': 'celery.backends.azureblockblob:AzureBlockBlobBackend',
+    'arangodb': 'celery.backends.arangodb:ArangoDbBackend',
+    's3': 'celery.backends.s3:S3Backend',
+}
+
+
+def by_name(backend=None, loader=None,
+            extension_namespace='celery.result_backends'):
+    """Get backend class by name/alias."""
+    backend = backend or 'disabled'
+    loader = loader or current_app.loader
+    aliases = dict(BACKEND_ALIASES, **loader.override_backends)
+    aliases.update(
+        load_extension_class_names(extension_namespace) or {})
+    try:
+        cls = symbol_by_name(backend, aliases)
+    except ValueError as exc:
+        reraise(ImproperlyConfigured, ImproperlyConfigured(
+            UNKNOWN_BACKEND.strip().format(backend, exc)), sys.exc_info()[2])
+    if isinstance(cls, types.ModuleType):
+        raise ImproperlyConfigured(UNKNOWN_BACKEND.strip().format(
+            backend, 'is a Python module, not a backend class.'))
+    return cls
+
+
+def by_url(backend=None, loader=None):
+    """Get backend class by URL."""
+    url = None
+    if backend and '://' in backend:
+        url = backend
+        scheme, _, _ = url.partition('://')
+        if '+' in scheme:
+            backend, url = url.split('+', 1)
+        else:
+            backend = scheme
+    return by_name(backend, loader), url
```

## celery/app/base.py

 * *Ordering differences only*

```diff
@@ -1,1540 +1,1540 @@
-"""Actual App instance implementation."""
-import inspect
-import os
-import sys
-import threading
-import warnings
-from collections import UserDict, defaultdict, deque
-from contextlib import asynccontextmanager
-from datetime import datetime
-from operator import attrgetter
-
-from click.exceptions import Exit
-from kombu import pools
-from kombu.clocks import LamportClock
-from kombu.common import oid_from
-from kombu.utils.compat import register_after_fork
-from kombu.utils.objects import cached_property
-from kombu.utils.uuid import uuid
-from vine import starpromise
-
-from celery import platforms, signals
-from celery._state import (_announce_app_finalized, _deregister_app,
-                           _register_app, _set_current_app, _task_stack,
-                           connect_on_app_finalize, get_current_app,
-                           get_current_worker_task, set_default_app)
-from celery.exceptions import AlwaysEagerIgnored, ImproperlyConfigured
-from celery.loaders import get_loader_cls
-from celery.local import PromiseProxy, maybe_evaluate
-from celery.utils import abstract
-from celery.utils.collections import AttributeDictMixin
-from celery.utils.dispatch import Signal
-from celery.utils.functional import first, head_from_fun, maybe_list
-from celery.utils.imports import gen_task_name, instantiate, symbol_by_name
-from celery.utils.log import get_logger
-from celery.utils.objects import (FallbackContext,
-                                  mro_lookup,
-                                  AsyncFallbackContext)
-from celery.utils.time import timezone, to_utc
-
-# Load all builtin tasks
-from . import builtins  # noqa
-from . import backends
-from .annotations import prepare as prepare_annotations
-from .autoretry import add_autoretry_behaviour
-from .defaults import DEFAULT_SECURITY_DIGEST, find_deprecated_settings
-from .registry import TaskRegistry
-from .utils import (AppPickler, Settings, _new_key_to_old, _old_key_to_new,
-                    _unpickle_app, _unpickle_app_v2, appstr, bugreport,
-                    detect_settings)
-
-__all__ = ('Celery',)
-
-logger = get_logger(__name__)
-
-BUILTIN_FIXUPS = {
-    'celery.fixups.django:fixup',
-}
-USING_EXECV = os.environ.get('FORKED_BY_MULTIPROCESSING')
-
-ERR_ENVVAR_NOT_SET = """
-The environment variable {0!r} is not set,
-and as such the configuration could not be loaded.
-
-Please set this variable and make sure it points to
-a valid configuration module.
-
-Example:
-    {0}="proj.celeryconfig"
-"""
-
-
-def app_has_custom(app, attr):
-    """Return true if app has customized method `attr`.
-
-    Note:
-        This is used for optimizations in cases where we know
-        how the default behavior works, but need to account
-        for someone using inheritance to override a method/property.
-    """
-    return mro_lookup(app.__class__, attr, stop={Celery, object},
-                      monkey_patched=[__name__])
-
-
-def _unpickle_appattr(reverse_name, args):
-    """Unpickle app."""
-    # Given an attribute name and a list of args, gets
-    # the attribute from the current app and calls it.
-    return get_current_app()._rgetattr(reverse_name)(*args)
-
-
-def _after_fork_cleanup_app(app):
-    # This is used with multiprocessing.register_after_fork,
-    # so need to be at module level.
-    try:
-        app._after_fork()
-    except Exception as exc:  # pylint: disable=broad-except
-        logger.info('after forker raised exception: %r', exc, exc_info=1)
-
-
-class PendingConfiguration(UserDict, AttributeDictMixin):
-    # `app.conf` will be of this type before being explicitly configured,
-    # meaning the app can keep any configuration set directly
-    # on `app.conf` before the `app.config_from_object` call.
-    #
-    # accessing any key will finalize the configuration,
-    # replacing `app.conf` with a concrete settings object.
-
-    callback = None
-    _data = None
-
-    def __init__(self, conf, callback):
-        object.__setattr__(self, '_data', conf)
-        object.__setattr__(self, 'callback', callback)
-
-    def __setitem__(self, key, value):
-        self._data[key] = value
-
-    def clear(self):
-        self._data.clear()
-
-    def update(self, *args, **kwargs):
-        self._data.update(*args, **kwargs)
-
-    def setdefault(self, *args, **kwargs):
-        return self._data.setdefault(*args, **kwargs)
-
-    def __contains__(self, key):
-        # XXX will not show finalized configuration
-        # setdefault will cause `key in d` to happen,
-        # so for setdefault to be lazy, so does contains.
-        return key in self._data
-
-    def __len__(self):
-        return len(self.data)
-
-    def __repr__(self):
-        return repr(self.data)
-
-    @cached_property
-    def data(self):
-        return self.callback()
-
-
-class Celery:
-    """Celery application.
-
-    Arguments:
-        main (str): Name of the main module if running as `__main__`.
-            This is used as the prefix for auto-generated task names.
-
-    Keyword Arguments:
-        broker (str): URL of the default broker used.
-        backend (Union[str, Type[celery.backends.base.Backend]]):
-            The result store backend class, or the name of the backend
-            class to use.
-
-            Default is the value of the :setting:`result_backend` setting.
-        autofinalize (bool): If set to False a :exc:`RuntimeError`
-            will be raised if the task registry or tasks are used before
-            the app is finalized.
-        set_as_current (bool):  Make this the global current app.
-        include (List[str]): List of modules every worker should import.
-
-        amqp (Union[str, Type[AMQP]]): AMQP object or class name.
-        events (Union[str, Type[celery.app.events.Events]]): Events object or
-            class name.
-        log (Union[str, Type[Logging]]): Log object or class name.
-        control (Union[str, Type[celery.app.control.Control]]): Control object
-            or class name.
-        tasks (Union[str, Type[TaskRegistry]]): A task registry, or the name of
-            a registry class.
-        fixups (List[str]): List of fix-up plug-ins (e.g., see
-            :mod:`celery.fixups.django`).
-        config_source (Union[str, class]): Take configuration from a class,
-            or object.  Attributes may include any settings described in
-            the documentation.
-        task_cls (Union[str, Type[celery.app.task.Task]]): base task class to
-            use. See :ref:`this section <custom-task-cls-app-wide>` for usage.
-    """
-
-    #: This is deprecated, use :meth:`reduce_keys` instead
-    Pickler = AppPickler
-
-    SYSTEM = platforms.SYSTEM
-    IS_macOS, IS_WINDOWS = platforms.IS_macOS, platforms.IS_WINDOWS
-
-    #: Name of the `__main__` module.  Required for standalone scripts.
-    #:
-    #: If set this will be used instead of `__main__` when automatically
-    #: generating task names.
-    main = None
-
-    #: Custom options for command-line programs.
-    #: See :ref:`extending-commandoptions`
-    user_options = None
-
-    #: Custom bootsteps to extend and modify the worker.
-    #: See :ref:`extending-bootsteps`.
-    steps = None
-
-    builtin_fixups = BUILTIN_FIXUPS
-
-    amqp_cls = 'celery.app.amqp:AMQP'
-    backend_cls = None
-    events_cls = 'celery.app.events:Events'
-    loader_cls = None
-    log_cls = 'celery.app.log:Logging'
-    control_cls = 'celery.app.control:Control'
-    task_cls = 'celery.app.task:Task'
-    registry_cls = 'celery.app.registry:TaskRegistry'
-
-    #: Thread local storage.
-    _local = None
-    _fixups = None
-    _pool = None
-    _conf = None
-    _after_fork_registered = False
-
-    #: Signal sent when app is loading configuration.
-    on_configure = None
-
-    #: Signal sent after app has prepared the configuration.
-    on_after_configure = None
-
-    #: Signal sent after app has been finalized.
-    on_after_finalize = None
-
-    #: Signal sent by every new process after fork.
-    on_after_fork = None
-
-    def __init__(self, main=None, loader=None, backend=None,
-                 amqp=None, events=None, log=None, control=None,
-                 set_as_current=True, tasks=None, broker=None, include=None,
-                 changes=None, config_source=None, fixups=None, task_cls=None,
-                 autofinalize=True, namespace=None, strict_typing=True,
-                 aio_control=None,
-                 **kwargs):
-
-        self._local = threading.local()
-
-        self.clock = LamportClock()
-        self.main = main
-        self.amqp_cls = amqp or self.amqp_cls
-        self.events_cls = events or self.events_cls
-        self.loader_cls = loader or self._get_default_loader()
-        self.log_cls = log or self.log_cls
-        self.control_cls = control or self.control_cls
-        self.aio_control_cls = aio_control or self.aio_control_cls
-        self.task_cls = task_cls or self.task_cls
-        self.set_as_current = set_as_current
-        self.registry_cls = symbol_by_name(self.registry_cls)
-        self.user_options = defaultdict(set)
-        self.steps = defaultdict(set)
-        self.autofinalize = autofinalize
-        self.namespace = namespace
-        self.strict_typing = strict_typing
-
-        self.configured = False
-        self._config_source = config_source
-        self._pending_defaults = deque()
-        self._pending_periodic_tasks = deque()
-
-        self.finalized = False
-        self._finalize_mutex = threading.Lock()
-        self._pending = deque()
-        self._tasks = tasks
-        if not isinstance(self._tasks, TaskRegistry):
-            self._tasks = self.registry_cls(self._tasks or {})
-
-        # If the class defines a custom __reduce_args__ we need to use
-        # the old way of pickling apps: pickling a list of
-        # args instead of the new way that pickles a dict of keywords.
-        self._using_v1_reduce = app_has_custom(self, '__reduce_args__')
-
-        # these options are moved to the config to
-        # simplify pickling of the app object.
-        self._preconf = changes or {}
-        self._preconf_set_by_auto = set()
-        self.__autoset('broker_url', broker)
-        self.__autoset('result_backend', backend)
-        self.__autoset('include', include)
-        self.__autoset('broker_use_ssl', kwargs.get('broker_use_ssl'))
-        self.__autoset('redis_backend_use_ssl', kwargs.get('redis_backend_use_ssl'))
-        self._conf = Settings(
-            PendingConfiguration(
-                self._preconf, self._finalize_pending_conf),
-            prefix=self.namespace,
-            keys=(_old_key_to_new, _new_key_to_old),
-        )
-
-        # - Apply fix-ups.
-        self.fixups = set(self.builtin_fixups) if fixups is None else fixups
-        # ...store fixup instances in _fixups to keep weakrefs alive.
-        self._fixups = [symbol_by_name(fixup)(self) for fixup in self.fixups]
-
-        if self.set_as_current:
-            self.set_current()
-
-        # Signals
-        if self.on_configure is None:
-            # used to be a method pre 4.0
-            self.on_configure = Signal(name='app.on_configure')
-        self.on_after_configure = Signal(
-            name='app.on_after_configure',
-            providing_args={'source'},
-        )
-        self.on_after_finalize = Signal(name='app.on_after_finalize')
-        self.on_after_fork = Signal(name='app.on_after_fork')
-
-        self.on_init()
-        _register_app(self)
-
-    def _get_default_loader(self):
-        # the --loader command-line argument sets the environment variable.
-        return (
-            os.environ.get('CELERY_LOADER') or
-            self.loader_cls or
-            'celery.loaders.app:AppLoader'
-        )
-
-    def on_init(self):
-        """Optional callback called at init."""
-
-    def __autoset(self, key, value):
-        if value:
-            self._preconf[key] = value
-            self._preconf_set_by_auto.add(key)
-
-    def set_current(self):
-        """Make this the current app for this thread."""
-        _set_current_app(self)
-
-    def set_default(self):
-        """Make this the default app for all threads."""
-        set_default_app(self)
-
-    def _ensure_after_fork(self):
-        if not self._after_fork_registered:
-            self._after_fork_registered = True
-            if register_after_fork is not None:
-                register_after_fork(self, _after_fork_cleanup_app)
-
-    def close(self):
-        """Clean up after the application.
-
-        Only necessary for dynamically created apps, and you should
-        probably use the :keyword:`with` statement instead.
-
-        Example:
-            >>> with Celery(set_as_current=False) as app:
-            ...     with app.connection_for_write() as conn:
-            ...         pass
-        """
-        self._pool = None
-        self._aio_pool = None
-        _deregister_app(self)
-
-    def start(self, argv=None):
-        from celery.bin.celery import celery
-
-        celery.params[0].default = self
-
-        try:
-            celery.main(args=argv, standalone_mode=False)
-        except Exit as e:
-            return e.exit_code
-        finally:
-            celery.params[0].default = None
-
-    def worker_main(self, argv=None):
-        if argv is None:
-            argv = sys.argv
-
-        if 'worker' not in argv:
-            raise ValueError(
-                "The worker sub-command must be specified in argv.\n"
-                "Use app.start() to programmatically start other commands."
-            )
-
-        self.start(argv=argv)
-
-    def task(self, *args, **opts):
-        """Decorator to create a task class out of any callable.
-
-        See :ref:`Task options<task-options>` for a list of the
-        arguments that can be passed to this decorator.
-
-        Examples:
-            .. code-block:: python
-
-                @app.task
-                def refresh_feed(url):
-                    store_feed(feedparser.parse(url))
-
-            with setting extra options:
-
-            .. code-block:: python
-
-                @app.task(exchange='feeds')
-                def refresh_feed(url):
-                    return store_feed(feedparser.parse(url))
-
-        Note:
-            App Binding: For custom apps the task decorator will return
-            a proxy object, so that the act of creating the task is not
-            performed until the task is used or the task registry is accessed.
-
-            If you're depending on binding to be deferred, then you must
-            not access any attributes on the returned object until the
-            application is fully set up (finalized).
-        """
-        if USING_EXECV and opts.get('lazy', True):
-            # When using execv the task in the original module will point to a
-            # different app, so doing things like 'add.request' will point to
-            # a different task instance.  This makes sure it will always use
-            # the task instance from the current app.
-            # Really need a better solution for this :(
-            from . import shared_task
-            return shared_task(*args, lazy=False, **opts)
-
-        def inner_create_task_cls(shared=True, filter=None, lazy=True, **opts):
-            _filt = filter
-
-            def _create_task_cls(fun):
-                if shared:
-                    def cons(app):
-                        return app._task_from_fun(fun, **opts)
-                    cons.__name__ = fun.__name__
-                    connect_on_app_finalize(cons)
-                if not lazy or self.finalized:
-                    ret = self._task_from_fun(fun, **opts)
-                else:
-                    # return a proxy object that evaluates on first use
-                    ret = PromiseProxy(self._task_from_fun, (fun,), opts,
-                                       __doc__=fun.__doc__)
-                    self._pending.append(ret)
-                if _filt:
-                    return _filt(ret)
-                return ret
-
-            return _create_task_cls
-
-        if len(args) == 1:
-            if callable(args[0]):
-                return inner_create_task_cls(**opts)(*args)
-            raise TypeError('argument 1 to @task() must be a callable')
-        if args:
-            raise TypeError(
-                '@task() takes exactly 1 argument ({} given)'.format(
-                    sum([len(args), len(opts)])))
-        return inner_create_task_cls(**opts)
-
-    def _task_from_fun(self, fun, name=None, base=None, bind=False, aio_variant=False, **options):
-        if not self.finalized and not self.autofinalize:
-            raise RuntimeError('Contract breach: app not finalized')
-        name = name or self.gen_task_name(fun.__name__, fun.__module__)
-        base = base or self.Task
-
-        if name not in self._tasks:
-            run = fun if bind else staticmethod(fun)
-            attrs = dict({
-                'app': self,
-                'name': name,
-                'run': run,
-                '_decorated': True,
-                '__doc__': fun.__doc__,
-                '__module__': fun.__module__,
-                '__annotations__': fun.__annotations__,
-                '__header__': staticmethod(head_from_fun(fun, bound=bind)),
-                '__wrapped__': run
-            }, **options)
-            task = type(fun.__name__, (base,), attrs)()
-            if aio_variant:
-                aio_task = type(fun.__name__ + '@aio', (self.AioTask,), attrs)()
-                task.aio = aio_task
-
-            # for some reason __qualname__ cannot be set in type()
-            # so we have to set it here.
-            try:
-                task.__qualname__ = fun.__qualname__
-                if aio_variant:
-                    aio_task.__qualname__ = fun.__qualname__
-            except AttributeError:
-                pass
-            self._tasks[task.name] = task
-            task.bind(self)  # connects task to this app
-            add_autoretry_behaviour(task, **options)
-            if aio_variant:
-                aio_task.bind(self)
-                add_autoretry_behaviour(aio_task, **options)
-        else:
-            task = self._tasks[name]
-        return task
-
-    def register_task(self, task):
-        """Utility for registering a task-based class.
-
-        Note:
-            This is here for compatibility with old Celery 1.0
-            style task classes, you should not need to use this for
-            new projects.
-        """
-        task = inspect.isclass(task) and task() or task
-        if not task.name:
-            task_cls = type(task)
-            task.name = self.gen_task_name(
-                task_cls.__name__, task_cls.__module__)
-        add_autoretry_behaviour(task)
-        self.tasks[task.name] = task
-        task._app = self
-        task.bind(self)
-        return task
-
-    def gen_task_name(self, name, module):
-        return gen_task_name(self, name, module)
-
-    def finalize(self, auto=False):
-        """Finalize the app.
-
-        This loads built-in tasks, evaluates pending task decorators,
-        reads configuration, etc.
-        """
-        with self._finalize_mutex:
-            if not self.finalized:
-                if auto and not self.autofinalize:
-                    raise RuntimeError('Contract breach: app not finalized')
-                self.finalized = True
-                _announce_app_finalized(self)
-
-                pending = self._pending
-                while pending:
-                    maybe_evaluate(pending.popleft())
-
-                for task in self._tasks.values():
-                    task.bind(self)
-
-                self.on_after_finalize.send(sender=self)
-
-    def add_defaults(self, fun):
-        """Add default configuration from dict ``d``.
-
-        If the argument is a callable function then it will be regarded
-        as a promise, and it won't be loaded until the configuration is
-        actually needed.
-
-        This method can be compared to:
-
-        .. code-block:: pycon
-
-            >>> celery.conf.update(d)
-
-        with a difference that 1) no copy will be made and 2) the dict will
-        not be transferred when the worker spawns child processes, so
-        it's important that the same configuration happens at import time
-        when pickle restores the object on the other side.
-        """
-        if not callable(fun):
-            d, fun = fun, lambda: d
-        if self.configured:
-            return self._conf.add_defaults(fun())
-        self._pending_defaults.append(fun)
-
-    def config_from_object(self, obj,
-                           silent=False, force=False, namespace=None):
-        """Read configuration from object.
-
-        Object is either an actual object or the name of a module to import.
-
-        Example:
-            >>> celery.config_from_object('myapp.celeryconfig')
-
-            >>> from myapp import celeryconfig
-            >>> celery.config_from_object(celeryconfig)
-
-        Arguments:
-            silent (bool): If true then import errors will be ignored.
-            force (bool): Force reading configuration immediately.
-                By default the configuration will be read only when required.
-        """
-        self._config_source = obj
-        self.namespace = namespace or self.namespace
-        if force or self.configured:
-            self._conf = None
-            if self.loader.config_from_object(obj, silent=silent):
-                return self.conf
-
-    def config_from_envvar(self, variable_name, silent=False, force=False):
-        """Read configuration from environment variable.
-
-        The value of the environment variable must be the name
-        of a module to import.
-
-        Example:
-            >>> os.environ['CELERY_CONFIG_MODULE'] = 'myapp.celeryconfig'
-            >>> celery.config_from_envvar('CELERY_CONFIG_MODULE')
-        """
-        module_name = os.environ.get(variable_name)
-        if not module_name:
-            if silent:
-                return False
-            raise ImproperlyConfigured(
-                ERR_ENVVAR_NOT_SET.strip().format(variable_name))
-        return self.config_from_object(module_name, silent=silent, force=force)
-
-    def config_from_cmdline(self, argv, namespace='celery'):
-        self._conf.update(
-            self.loader.cmdline_config_parser(argv, namespace)
-        )
-
-    def setup_security(self, allowed_serializers=None, key=None, cert=None,
-                       store=None, digest=DEFAULT_SECURITY_DIGEST,
-                       serializer='json'):
-        """Setup the message-signing serializer.
-
-        This will affect all application instances (a global operation).
-
-        Disables untrusted serializers and if configured to use the ``auth``
-        serializer will register the ``auth`` serializer with the provided
-        settings into the Kombu serializer registry.
-
-        Arguments:
-            allowed_serializers (Set[str]): List of serializer names, or
-                content_types that should be exempt from being disabled.
-            key (str): Name of private key file to use.
-                Defaults to the :setting:`security_key` setting.
-            cert (str): Name of certificate file to use.
-                Defaults to the :setting:`security_certificate` setting.
-            store (str): Directory containing certificates.
-                Defaults to the :setting:`security_cert_store` setting.
-            digest (str): Digest algorithm used when signing messages.
-                Default is ``sha256``.
-            serializer (str): Serializer used to encode messages after
-                they've been signed.  See :setting:`task_serializer` for
-                the serializers supported.  Default is ``json``.
-        """
-        from celery.security import setup_security
-        return setup_security(allowed_serializers, key, cert,
-                              store, digest, serializer, app=self)
-
-    def autodiscover_tasks(self, packages=None,
-                           related_name='tasks', force=False):
-        """Auto-discover task modules.
-
-        Searches a list of packages for a "tasks.py" module (or use
-        related_name argument).
-
-        If the name is empty, this will be delegated to fix-ups (e.g., Django).
-
-        For example if you have a directory layout like this:
-
-        .. code-block:: text
-
-            foo/__init__.py
-               tasks.py
-               models.py
-
-            bar/__init__.py
-                tasks.py
-                models.py
-
-            baz/__init__.py
-                models.py
-
-        Then calling ``app.autodiscover_tasks(['foo', 'bar', 'baz'])`` will
-        result in the modules ``foo.tasks`` and ``bar.tasks`` being imported.
-
-        Arguments:
-            packages (List[str]): List of packages to search.
-                This argument may also be a callable, in which case the
-                value returned is used (for lazy evaluation).
-            related_name (Optional[str]): The name of the module to find.  Defaults
-                to "tasks": meaning "look for 'module.tasks' for every
-                module in ``packages``.".  If ``None`` will only try to import
-                the package, i.e. "look for 'module'".
-            force (bool): By default this call is lazy so that the actual
-                auto-discovery won't happen until an application imports
-                the default modules.  Forcing will cause the auto-discovery
-                to happen immediately.
-        """
-        if force:
-            return self._autodiscover_tasks(packages, related_name)
-        signals.import_modules.connect(starpromise(
-            self._autodiscover_tasks, packages, related_name,
-        ), weak=False, sender=self)
-
-    def _autodiscover_tasks(self, packages, related_name, **kwargs):
-        if packages:
-            return self._autodiscover_tasks_from_names(packages, related_name)
-        return self._autodiscover_tasks_from_fixups(related_name)
-
-    def _autodiscover_tasks_from_names(self, packages, related_name):
-        # packages argument can be lazy
-        return self.loader.autodiscover_tasks(
-            packages() if callable(packages) else packages, related_name,
-        )
-
-    def _autodiscover_tasks_from_fixups(self, related_name):
-        return self._autodiscover_tasks_from_names([
-            pkg for fixup in self._fixups
-            if hasattr(fixup, 'autodiscover_tasks')
-            for pkg in fixup.autodiscover_tasks()
-        ], related_name=related_name)
-
-    def send_task(self, name, args=None, kwargs=None, countdown=None,
-                  eta=None, task_id=None, producer=None, connection=None,
-                  router=None, result_cls=None, expires=None,
-                  publisher=None, link=None, link_error=None,
-                  add_to_parent=True, group_id=None, group_index=None,
-                  retries=0, chord=None,
-                  reply_to=None, time_limit=None, soft_time_limit=None,
-                  root_id=None, parent_id=None, route_name=None,
-                  shadow=None, chain=None, task_type=None, **options):
-        """Send task by name.
-
-        Supports the same arguments as :meth:`@-Task.apply_async`.
-
-        Arguments:
-            name (str): Name of task to call (e.g., `"tasks.add"`).
-            result_cls (AsyncResult): Specify custom result class.
-        """
-        parent = have_parent = None
-        amqp = self.amqp
-        task_id = task_id or uuid()
-        producer = producer or publisher  # XXX compat
-        router = router or amqp.router
-        conf = self.conf
-        if conf.task_always_eager:  # pragma: no cover
-            warnings.warn(AlwaysEagerIgnored(
-                'task_always_eager has no effect on send_task',
-            ), stacklevel=2)
-
-        ignored_result = options.pop('ignore_result', False)
-        options = router.route(
-            options, route_name or name, args, kwargs, task_type)
-
-        if not root_id or not parent_id:
-            parent = self.current_worker_task
-            if parent:
-                if not root_id:
-                    root_id = parent.request.root_id or parent.request.id
-                if not parent_id:
-                    parent_id = parent.request.id
-
-                if conf.task_inherit_parent_priority:
-                    options.setdefault('priority',
-                                       parent.request.delivery_info.get('priority'))
-
-        message = amqp.create_task_message(
-            task_id, name, args, kwargs, countdown, eta, group_id, group_index,
-            expires, retries, chord,
-            maybe_list(link), maybe_list(link_error),
-            reply_to or self.thread_oid, time_limit, soft_time_limit,
-            self.conf.task_send_sent_event,
-            root_id, parent_id, shadow, chain,
-            argsrepr=options.get('argsrepr'),
-            kwargsrepr=options.get('kwargsrepr'),
-        )
-
-        if connection:
-            producer = amqp.Producer(connection, auto_declare=False)
-
-        with self.producer_or_acquire(producer) as P:
-            with P.connection._reraise_as_library_errors():
-                if not ignored_result:
-                    self.backend.on_task_call(P, task_id)
-                amqp.send_task_message(P, name, message, **options)
-        result = (result_cls or self.AsyncResult)(task_id)
-        # We avoid using the constructor since a custom result class
-        # can be used, in which case the constructor may still use
-        # the old signature.
-        result.ignored = ignored_result
-
-        if add_to_parent:
-            if not have_parent:
-                parent, have_parent = self.current_worker_task, True
-            if parent:
-                parent.add_trail(result)
-        return result
-
-    def connection_for_read(self, url=None, **kwargs):
-        """Establish connection used for consuming.
-
-        See Also:
-            :meth:`connection` for supported arguments.
-        """
-        return self._connection(url or self.conf.broker_read_url, **kwargs)
-
-    def connection_for_write(self, url=None, **kwargs):
-        """Establish connection used for producing.
-
-        See Also:
-            :meth:`connection` for supported arguments.
-        """
-        return self._connection(url or self.conf.broker_write_url, **kwargs)
-
-    def connection(self, hostname=None, userid=None, password=None,
-                   virtual_host=None, port=None, ssl=None,
-                   connect_timeout=None, transport=None,
-                   transport_options=None, heartbeat=None,
-                   login_method=None, failover_strategy=None, **kwargs):
-        """Establish a connection to the message broker.
-
-        Please use :meth:`connection_for_read` and
-        :meth:`connection_for_write` instead, to convey the intent
-        of use for this connection.
-
-        Arguments:
-            url: Either the URL or the hostname of the broker to use.
-            hostname (str): URL, Hostname/IP-address of the broker.
-                If a URL is used, then the other argument below will
-                be taken from the URL instead.
-            userid (str): Username to authenticate as.
-            password (str): Password to authenticate with
-            virtual_host (str): Virtual host to use (domain).
-            port (int): Port to connect to.
-            ssl (bool, Dict): Defaults to the :setting:`broker_use_ssl`
-                setting.
-            transport (str): defaults to the :setting:`broker_transport`
-                setting.
-            transport_options (Dict): Dictionary of transport specific options.
-            heartbeat (int): AMQP Heartbeat in seconds (``pyamqp`` only).
-            login_method (str): Custom login method to use (AMQP only).
-            failover_strategy (str, Callable): Custom failover strategy.
-            **kwargs: Additional arguments to :class:`kombu.Connection`.
-
-        Returns:
-            kombu.Connection: the lazy connection instance.
-        """
-        return self.connection_for_write(
-            hostname or self.conf.broker_write_url,
-            userid=userid, password=password,
-            virtual_host=virtual_host, port=port, ssl=ssl,
-            connect_timeout=connect_timeout, transport=transport,
-            transport_options=transport_options, heartbeat=heartbeat,
-            login_method=login_method, failover_strategy=failover_strategy,
-            **kwargs
-        )
-
-    def _connection(self, url, userid=None, password=None,
-                    virtual_host=None, port=None, ssl=None,
-                    connect_timeout=None, transport=None,
-                    transport_options=None, heartbeat=None,
-                    login_method=None, failover_strategy=None, **kwargs):
-        conf = self.conf
-        return self.amqp.Connection(
-            url,
-            userid or conf.broker_user,
-            password or conf.broker_password,
-            virtual_host or conf.broker_vhost,
-            port or conf.broker_port,
-            transport=transport or conf.broker_transport,
-            ssl=self.either('broker_use_ssl', ssl),
-            heartbeat=heartbeat,
-            login_method=login_method or conf.broker_login_method,
-            failover_strategy=(
-                failover_strategy or conf.broker_failover_strategy
-            ),
-            transport_options=dict(
-                conf.broker_transport_options, **transport_options or {}
-            ),
-            connect_timeout=self.either(
-                'broker_connection_timeout', connect_timeout
-            ),
-        )
-    broker_connection = connection
-
-    def _acquire_connection(self, pool=True):
-        """Helper for :meth:`connection_or_acquire`."""
-        if pool:
-            return self.pool.acquire(block=True)
-        return self.connection_for_write()
-
-    def connection_or_acquire(self, connection=None, pool=True, *_, **__):
-        """Context used to acquire a connection from the pool.
-
-        For use within a :keyword:`with` statement to get a connection
-        from the pool if one is not already provided.
-
-        Arguments:
-            connection (kombu.Connection): If not provided, a connection
-                will be acquired from the connection pool.
-        """
-        return FallbackContext(connection, self._acquire_connection, pool=pool)
-    default_connection = connection_or_acquire  # XXX compat
-
-    def producer_or_acquire(self, producer=None):
-        """Context used to acquire a producer from the pool.
-
-        For use within a :keyword:`with` statement to get a producer
-        from the pool if one is not already provided
-
-        Arguments:
-            producer (kombu.Producer): If not provided, a producer
-                will be acquired from the producer pool.
-        """
-        return FallbackContext(
-            producer, self.producer_pool.acquire, block=True,
-        )
-    default_producer = producer_or_acquire  # XXX compat
-
-    def prepare_config(self, c):
-        """Prepare configuration before it is merged with the defaults."""
-        return find_deprecated_settings(c)
-
-    def now(self):
-        """Return the current time and date as a datetime."""
-        now_in_utc = to_utc(datetime.utcnow())
-        return now_in_utc.astimezone(self.timezone)
-
-    def select_queues(self, queues=None):
-        """Select subset of queues.
-
-        Arguments:
-            queues (Sequence[str]): a list of queue names to keep.
-        """
-        return self.amqp.queues.select(queues)
-
-    def either(self, default_key, *defaults):
-        """Get key from configuration or use default values.
-
-        Fallback to the value of a configuration key if none of the
-        `*values` are true.
-        """
-        return first(None, [
-            first(None, defaults), starpromise(self.conf.get, default_key),
-        ])
-
-    def bugreport(self):
-        """Return information useful in bug reports."""
-        return bugreport(self)
-
-    def _get_backend(self):
-        backend, url = backends.by_url(
-            self.backend_cls or self.conf.result_backend,
-            self.loader)
-        return backend(app=self, url=url)
-
-    def _finalize_pending_conf(self):
-        """Get config value by key and finalize loading the configuration.
-
-        Note:
-            This is used by PendingConfiguration:
-                as soon as you access a key the configuration is read.
-        """
-        conf = self._conf = self._load_config()
-        return conf
-
-    def _load_config(self):
-        if isinstance(self.on_configure, Signal):
-            self.on_configure.send(sender=self)
-        else:
-            # used to be a method pre 4.0
-            self.on_configure()
-        if self._config_source:
-            self.loader.config_from_object(self._config_source)
-        self.configured = True
-        settings = detect_settings(
-            self.prepare_config(self.loader.conf), self._preconf,
-            ignore_keys=self._preconf_set_by_auto, prefix=self.namespace,
-        )
-        if self._conf is not None:
-            # replace in place, as someone may have referenced app.conf,
-            # done some changes, accessed a key, and then try to make more
-            # changes to the reference and not the finalized value.
-            self._conf.swap_with(settings)
-        else:
-            self._conf = settings
-
-        # load lazy config dict initializers.
-        pending_def = self._pending_defaults
-        while pending_def:
-            self._conf.add_defaults(maybe_evaluate(pending_def.popleft()()))
-
-        # load lazy periodic tasks
-        pending_beat = self._pending_periodic_tasks
-        while pending_beat:
-            self._add_periodic_task(*pending_beat.popleft())
-
-        self.on_after_configure.send(sender=self, source=self._conf)
-        return self._conf
-
-    def _after_fork(self):
-        self._pool = None
-        try:
-            self.__dict__['amqp']._producer_pool = None
-        except (AttributeError, KeyError):
-            pass
-        self.on_after_fork.send(sender=self)
-
-    def signature(self, *args, **kwargs):
-        """Return a new :class:`~celery.Signature` bound to this app."""
-        kwargs['app'] = self
-        return self._canvas.signature(*args, **kwargs)
-
-    def add_periodic_task(self, schedule, sig,
-                          args=(), kwargs=(), name=None, **opts):
-        key, entry = self._sig_to_periodic_task_entry(
-            schedule, sig, args, kwargs, name, **opts)
-        if self.configured:
-            self._add_periodic_task(key, entry)
-        else:
-            self._pending_periodic_tasks.append((key, entry))
-        return key
-
-    def _sig_to_periodic_task_entry(self, schedule, sig,
-                                    args=(), kwargs=None, name=None, **opts):
-        kwargs = {} if not kwargs else kwargs
-        sig = (sig.clone(args, kwargs)
-               if isinstance(sig, abstract.CallableSignature)
-               else self.signature(sig.name, args, kwargs))
-        return name or repr(sig), {
-            'schedule': schedule,
-            'task': sig.name,
-            'args': sig.args,
-            'kwargs': sig.kwargs,
-            'options': dict(sig.options, **opts),
-        }
-
-    def _add_periodic_task(self, key, entry):
-        self._conf.beat_schedule[key] = entry
-
-    def create_task_cls(self):
-        """Create a base task class bound to this app."""
-        return self.subclass_with_self(
-            self.task_cls, name='Task', attribute='_app',
-            keep_reduce=True, abstract=True,
-        )
-
-    def subclass_with_self(self, Class, name=None, attribute='app',
-                           reverse=None, keep_reduce=False, **kw):
-        """Subclass an app-compatible class.
-
-        App-compatible means that the class has a class attribute that
-        provides the default app it should use, for example:
-        ``class Foo: app = None``.
-
-        Arguments:
-            Class (type): The app-compatible class to subclass.
-            name (str): Custom name for the target class.
-            attribute (str): Name of the attribute holding the app,
-                Default is 'app'.
-            reverse (str): Reverse path to this object used for pickling
-                purposes. For example, to get ``app.AsyncResult``,
-                use ``"AsyncResult"``.
-            keep_reduce (bool): If enabled a custom ``__reduce__``
-                implementation won't be provided.
-        """
-        Class = symbol_by_name(Class)
-        reverse = reverse if reverse else Class.__name__
-
-        def __reduce__(self):
-            return _unpickle_appattr, (reverse, self.__reduce_args__())
-
-        attrs = dict(
-            {attribute: self},
-            __module__=Class.__module__,
-            __doc__=Class.__doc__,
-            **kw)
-        if not keep_reduce:
-            attrs['__reduce__'] = __reduce__
-
-        return type(name or Class.__name__, (Class,), attrs)
-
-    def _rgetattr(self, path):
-        return attrgetter(path)(self)
-
-    def __enter__(self):
-        return self
-
-    def __exit__(self, *exc_info):
-        self.close()
-
-    def __repr__(self):
-        return '<{} {}>'.format(type(self).__name__, appstr(self))
-
-    def __reduce__(self):
-        if self._using_v1_reduce:
-            return self.__reduce_v1__()
-        return (_unpickle_app_v2, (self.__class__, self.__reduce_keys__()))
-
-    def __reduce_v1__(self):
-        # Reduce only pickles the configuration changes,
-        # so the default configuration doesn't have to be passed
-        # between processes.
-        return (
-            _unpickle_app,
-            (self.__class__, self.Pickler) + self.__reduce_args__(),
-        )
-
-    def __reduce_keys__(self):
-        """Keyword arguments used to reconstruct the object when unpickling."""
-        return {
-            'main': self.main,
-            'changes':
-                self._conf.changes if self.configured else self._preconf,
-            'loader': self.loader_cls,
-            'backend': self.backend_cls,
-            'amqp': self.amqp_cls,
-            'events': self.events_cls,
-            'log': self.log_cls,
-            'control': self.control_cls,
-            'fixups': self.fixups,
-            'config_source': self._config_source,
-            'task_cls': self.task_cls,
-            'namespace': self.namespace,
-        }
-
-    def __reduce_args__(self):
-        """Deprecated method, please use :meth:`__reduce_keys__` instead."""
-        return (self.main, self._conf.changes if self.configured else {},
-                self.loader_cls, self.backend_cls, self.amqp_cls,
-                self.events_cls, self.log_cls, self.control_cls,
-                False, self._config_source)
-
-    @cached_property
-    def Worker(self):
-        """Worker application.
-
-        See Also:
-            :class:`~@Worker`.
-        """
-        return self.subclass_with_self('celery.apps.worker:Worker')
-
-    @cached_property
-    def WorkController(self, **kwargs):
-        """Embeddable worker.
-
-        See Also:
-            :class:`~@WorkController`.
-        """
-        return self.subclass_with_self('celery.worker:WorkController')
-
-    @cached_property
-    def Beat(self, **kwargs):
-        """:program:`celery beat` scheduler application.
-
-        See Also:
-            :class:`~@Beat`.
-        """
-        return self.subclass_with_self('celery.apps.beat:Beat')
-
-    @cached_property
-    def Task(self):
-        """Base task class for this app."""
-        return self.create_task_cls()
-
-    @cached_property
-    def annotations(self):
-        return prepare_annotations(self.conf.task_annotations)
-
-    @cached_property
-    def AsyncResult(self):
-        """Create new result instance.
-
-        See Also:
-            :class:`celery.result.AsyncResult`.
-        """
-        return self.subclass_with_self('celery.result:AsyncResult')
-
-    @cached_property
-    def ResultSet(self):
-        return self.subclass_with_self('celery.result:ResultSet')
-
-    @cached_property
-    def GroupResult(self):
-        """Create new group result instance.
-
-        See Also:
-            :class:`celery.result.GroupResult`.
-        """
-        return self.subclass_with_self('celery.result:GroupResult')
-
-    @property
-    def pool(self):
-        """Broker connection pool: :class:`~@pool`.
-
-        Note:
-            This attribute is not related to the workers concurrency pool.
-        """
-        if self._pool is None:
-            self._ensure_after_fork()
-            limit = self.conf.broker_pool_limit
-            pools.set_limit(limit)
-            self._pool = pools.connections[self.connection_for_write()]
-        return self._pool
-
-    @property
-    def current_task(self):
-        """Instance of task being executed, or :const:`None`."""
-        return _task_stack.top
-
-    @property
-    def current_worker_task(self):
-        """The task currently being executed by a worker or :const:`None`.
-
-        Differs from :data:`current_task` in that it's not affected
-        by tasks calling other tasks directly, or eagerly.
-        """
-        return get_current_worker_task()
-
-    @cached_property
-    def oid(self):
-        """Universally unique identifier for this app."""
-        # since 4.0: thread.get_ident() is not included when
-        # generating the process id.  This is due to how the RPC
-        # backend now dedicates a single thread to receive results,
-        # which would not work if each thread has a separate id.
-        return oid_from(self, threads=False)
-
-    @property
-    def thread_oid(self):
-        """Per-thread unique identifier for this app."""
-        try:
-            return self._local.oid
-        except AttributeError:
-            self._local.oid = new_oid = oid_from(self, threads=True)
-            return new_oid
-
-    @cached_property
-    def amqp(self):
-        """AMQP related functionality: :class:`~@amqp`."""
-        return instantiate(self.amqp_cls, app=self)
-
-    @property
-    def backend(self):
-        """Current backend instance."""
-        try:
-            return self._local.backend
-        except AttributeError:
-            self._local.backend = new_backend = self._get_backend()
-            return new_backend
-
-    @property
-    def conf(self):
-        """Current configuration."""
-        if self._conf is None:
-            self._conf = self._load_config()
-        return self._conf
-
-    @conf.setter
-    def conf(self, d):  # noqa
-        self._conf = d
-
-    @cached_property
-    def control(self):
-        """Remote control: :class:`~@control`."""
-        return instantiate(self.control_cls, app=self)
-
-    @cached_property
-    def events(self):
-        """Consuming and sending events: :class:`~@events`."""
-        return instantiate(self.events_cls, app=self)
-
-    @cached_property
-    def loader(self):
-        """Current loader instance."""
-        return get_loader_cls(self.loader_cls)(app=self)
-
-    @cached_property
-    def log(self):
-        """Logging: :class:`~@log`."""
-        return instantiate(self.log_cls, app=self)
-
-    @cached_property
-    def _canvas(self):
-        from celery import canvas
-        return canvas
-
-    @cached_property
-    def tasks(self):
-        """Task registry.
-
-        Warning:
-            Accessing this attribute will also auto-finalize the app.
-        """
-        self.finalize(auto=True)
-        return self._tasks
-
-    @property
-    def producer_pool(self):
-        return self.amqp.producer_pool
-
-    def uses_utc_timezone(self):
-        """Check if the application uses the UTC timezone."""
-        return self.timezone == timezone.utc
-
-    @cached_property
-    def timezone(self):
-        """Current timezone for this app.
-
-        This is a cached property taking the time zone from the
-        :setting:`timezone` setting.
-        """
-        conf = self.conf
-        if not conf.timezone:
-            if conf.enable_utc:
-                return timezone.utc
-            else:
-                return timezone.local
-        return timezone.get_timezone(conf.timezone)
-
-    # -----------------------------------------------------------------------------
-    # asyncio support
-    _aio_pool = None
-
-    aio_task_cls = 'celery.aio.task:AioTask'
-    aio_amqp_cls = 'celery.aio.amqp:AioAMQP'
-    aio_events_cls = 'celery.aio.events:Events'
-    aio_control_cls = 'celery.aio.control:Control'
-
-    def create_aio_task_cls(self):
-        """Create a base task class bound to this app."""
-        return self.subclass_with_self(
-            self.aio_task_cls, name='AioTask', attribute='_app',
-            keep_reduce=True, abstract=True,
-        )
-
-    @cached_property
-    def AioTask(self):
-        """Base aio-task class for this app."""
-        return self.create_aio_task_cls()
-
-    @cached_property
-    def aio_amqp(self):
-        """AMQP related functionality: :class:`~@aio_amqp`."""
-        return instantiate(self.aio_amqp_cls, app=self)
-
-    @cached_property
-    def aio_events(self):
-        """Consuming and sending events: :class:`~@events`."""
-        return instantiate(self.aio_events_cls, app=self)
-
-    @property
-    def aio_producer_pool(self):
-        return self.aio_amqp.producer_pool
-
-    @property
-    def aio_pool(self):
-        if self._aio_pool is None:
-            self._ensure_after_fork()
-            limit = self.conf.broker_pool_limit
-            pools.set_limit(limit)
-            self._aio_pool = pools.connections[self.aconnection_for_write()]
-        return self._aio_pool
-
-    @cached_property
-    def aio_control(self):
-        """Remote control: :class:`~@control`."""
-        return instantiate(self.aio_control_cls, app=self)
-
-    @cached_property
-    def AioAsyncResult(self):
-        return self.subclass_with_self('celery.aio.result:AsyncResult')
-
-    @cached_property
-    def AioGroupResult(self):
-        return self.subclass_with_self('celery.aio.result:GroupResult')
-
-    def aio_producer_or_acquire(self, producer=None):
-        """Context used to acquire a producer from the pool.
-
-        For use within a :keyword:`with` statement to get a producer
-        from the pool if one is not already provided
-
-        Arguments:
-            producer (kombu.Producer): If not provided, a producer
-                will be acquired from the producer pool.
-        """
-        return AsyncFallbackContext(
-            producer, self.aio_producer_pool.acquire, block=True,
-        )
-
-    @asynccontextmanager
-    async def _aio_acquire_connection(self, pool=True):
-        """Helper for :meth:`connection_or_acquire`."""
-        if pool:
-            async with self.aio_pool.acquire(block=True) as conn:
-                yield conn
-        else:
-            yield self.aconnection_for_write()
-
-    def aio_connection_or_acquire(self, connection=None, pool=True, *_, **__):
-        return AsyncFallbackContext(
-            connection, self._aio_acquire_connection, pool=pool
-        )
-
-    async def asend_task(self, name, args=None, kwargs=None, countdown=None,
-                  eta=None, task_id=None, producer=None, connection=None,
-                  router=None, result_cls=None, expires=None,
-                  publisher=None, link=None, link_error=None,
-                  add_to_parent=True, group_id=None, group_index=None,
-                  retries=0, chord=None,
-                  reply_to=None, time_limit=None, soft_time_limit=None,
-                  root_id=None, parent_id=None, route_name=None,
-                  shadow=None, chain=None, task_type=None, **options):
-        """Send task by name.
-
-        Supports the same arguments as :meth:`@-Task.apply_async`.
-
-        Arguments:
-            name (str): Name of task to call (e.g., `"tasks.add"`).
-            result_cls (AsyncResult): Specify custom result class.
-        """
-        parent = have_parent = None
-        amqp = self.aio_amqp
-        task_id = task_id or uuid()
-        producer = producer or publisher  # XXX compat
-        router = router or amqp.router
-        conf = self.conf
-        if conf.task_always_eager:  # pragma: no cover
-            warnings.warn(AlwaysEagerIgnored(
-                'task_always_eager has no effect on send_task',
-            ), stacklevel=2)
-
-        ignored_result = options.pop('ignore_result', False)
-        options = router.route(
-            options, route_name or name, args, kwargs, task_type)
-
-        if not root_id or not parent_id:
-            parent = self.current_worker_task
-            if parent:
-                if not root_id:
-                    root_id = parent.request.root_id or parent.request.id
-                if not parent_id:
-                    parent_id = parent.request.id
-
-                if conf.task_inherit_parent_priority:
-                    options.setdefault('priority',
-                                       parent.request.delivery_info.get('priority'))
-
-        message = amqp.create_task_message(
-            task_id, name, args, kwargs, countdown, eta, group_id, group_index,
-            expires, retries, chord,
-            maybe_list(link), maybe_list(link_error),
-            reply_to or self.thread_oid, time_limit, soft_time_limit,
-            self.conf.task_send_sent_event,
-            root_id, parent_id, shadow, chain,
-            argsrepr=options.get('argsrepr'),
-            kwargsrepr=options.get('kwargsrepr'),
-        )
-
-        if connection:
-            producer = amqp.Producer(connection, auto_declare=False)
-
-        async with self.aio_producer_or_acquire(producer) as P:
-            with (await P.connection)._reraise_as_library_errors():
-                if not ignored_result:
-                    await self.aio_backend.on_task_call(P, task_id)
-                await amqp.send_task_message(P, name, message, **options)
-        result = (result_cls or self.AsyncResult)(task_id)
-        # We avoid using the constructor since a custom result class
-        # can be used, in which case the constructor may still use
-        # the old signature.
-        result.ignored = ignored_result
-
-        if add_to_parent:
-            if not have_parent:
-                parent, have_parent = self.current_worker_task, True
-            if parent:
-                parent.add_trail(result)
-        return result
-
-    def aconnection_for_read(self, url=None, **kwargs):
-        """Establish connection used for consuming.
-
-        See Also:
-            :meth:`connection` for supported arguments.
-        """
-        return self._aconnection(url or self.conf.broker_read_url, **kwargs)
-
-    def aconnection_for_write(self, url=None, **kwargs):
-        """Establish connection used for producing.
-
-        See Also:
-            :meth:`connection` for supported arguments.
-        """
-        return self._aconnection(url or self.conf.broker_write_url, **kwargs)
-
-    def _aconnection(self, url, userid=None, password=None,
-                    virtual_host=None, port=None, ssl=None,
-                    connect_timeout=None, transport=None,
-                    transport_options=None, heartbeat=None,
-                    login_method=None, failover_strategy=None, **kwargs):
-        conf = self.conf
-        return self.aio_amqp.Connection(
-            url,
-            userid or conf.broker_user,
-            password or conf.broker_password,
-            virtual_host or conf.broker_vhost,
-            port or conf.broker_port,
-            transport=transport or conf.broker_transport,
-            ssl=self.either('broker_use_ssl', ssl),
-            heartbeat=heartbeat,
-            login_method=login_method or conf.broker_login_method,
-            failover_strategy=(
-                failover_strategy or conf.broker_failover_strategy
-            ),
-            transport_options=dict(
-                conf.broker_transport_options, **transport_options or {}
-            ),
-            connect_timeout=self.either(
-                'broker_connection_timeout', connect_timeout
-            ),
-        )
-
-    @property
-    def aio_backend(self):
-        """Current backend instance."""
-        try:
-            return self._local.aio_backend
-        except AttributeError:
-            override_backends = self.loader.override_backends
-            ori_redis = override_backends.pop('redis', None)
-            ori_rediss = override_backends.pop('rediss', None)
-            ori_sentinel = override_backends.pop('sentinel', None)
-            override_backends['redis'] = override_backends['rediss'] \
-                = 'celery.aio.backend:RedisBackend'
-            override_backends['sentinel'] = 'celery.aio.backend:SentinelBackend'
-            override_backends['redis-cluster'] = 'celery.aio.backend:ClusterBackend'
-            try:
-                self._local.aio_backend = new_backend = self._get_backend()
-            finally:
-                if ori_redis is not None:
-                    override_backends['redis'] = ori_rediss
-                else:
-                    override_backends.pop('redis', None)
-
-                if ori_rediss is not None:
-                    override_backends['rediss'] = ori_rediss
-                else:
-                    override_backends.pop('rediss', None)
-
-                if ori_sentinel is not None:
-                    override_backends['sentinel'] = ori_sentinel
-                else:
-                    override_backends.pop('sentinel', None)
-            return new_backend
-
-
-App = Celery  # noqa: E305 XXX compat
+"""Actual App instance implementation."""
+import inspect
+import os
+import sys
+import threading
+import warnings
+from collections import UserDict, defaultdict, deque
+from contextlib import asynccontextmanager
+from datetime import datetime
+from operator import attrgetter
+
+from click.exceptions import Exit
+from kombu import pools
+from kombu.clocks import LamportClock
+from kombu.common import oid_from
+from kombu.utils.compat import register_after_fork
+from kombu.utils.objects import cached_property
+from kombu.utils.uuid import uuid
+from vine import starpromise
+
+from celery import platforms, signals
+from celery._state import (_announce_app_finalized, _deregister_app,
+                           _register_app, _set_current_app, _task_stack,
+                           connect_on_app_finalize, get_current_app,
+                           get_current_worker_task, set_default_app)
+from celery.exceptions import AlwaysEagerIgnored, ImproperlyConfigured
+from celery.loaders import get_loader_cls
+from celery.local import PromiseProxy, maybe_evaluate
+from celery.utils import abstract
+from celery.utils.collections import AttributeDictMixin
+from celery.utils.dispatch import Signal
+from celery.utils.functional import first, head_from_fun, maybe_list
+from celery.utils.imports import gen_task_name, instantiate, symbol_by_name
+from celery.utils.log import get_logger
+from celery.utils.objects import (FallbackContext,
+                                  mro_lookup,
+                                  AsyncFallbackContext)
+from celery.utils.time import timezone, to_utc
+
+# Load all builtin tasks
+from . import builtins  # noqa
+from . import backends
+from .annotations import prepare as prepare_annotations
+from .autoretry import add_autoretry_behaviour
+from .defaults import DEFAULT_SECURITY_DIGEST, find_deprecated_settings
+from .registry import TaskRegistry
+from .utils import (AppPickler, Settings, _new_key_to_old, _old_key_to_new,
+                    _unpickle_app, _unpickle_app_v2, appstr, bugreport,
+                    detect_settings)
+
+__all__ = ('Celery',)
+
+logger = get_logger(__name__)
+
+BUILTIN_FIXUPS = {
+    'celery.fixups.django:fixup',
+}
+USING_EXECV = os.environ.get('FORKED_BY_MULTIPROCESSING')
+
+ERR_ENVVAR_NOT_SET = """
+The environment variable {0!r} is not set,
+and as such the configuration could not be loaded.
+
+Please set this variable and make sure it points to
+a valid configuration module.
+
+Example:
+    {0}="proj.celeryconfig"
+"""
+
+
+def app_has_custom(app, attr):
+    """Return true if app has customized method `attr`.
+
+    Note:
+        This is used for optimizations in cases where we know
+        how the default behavior works, but need to account
+        for someone using inheritance to override a method/property.
+    """
+    return mro_lookup(app.__class__, attr, stop={Celery, object},
+                      monkey_patched=[__name__])
+
+
+def _unpickle_appattr(reverse_name, args):
+    """Unpickle app."""
+    # Given an attribute name and a list of args, gets
+    # the attribute from the current app and calls it.
+    return get_current_app()._rgetattr(reverse_name)(*args)
+
+
+def _after_fork_cleanup_app(app):
+    # This is used with multiprocessing.register_after_fork,
+    # so need to be at module level.
+    try:
+        app._after_fork()
+    except Exception as exc:  # pylint: disable=broad-except
+        logger.info('after forker raised exception: %r', exc, exc_info=1)
+
+
+class PendingConfiguration(UserDict, AttributeDictMixin):
+    # `app.conf` will be of this type before being explicitly configured,
+    # meaning the app can keep any configuration set directly
+    # on `app.conf` before the `app.config_from_object` call.
+    #
+    # accessing any key will finalize the configuration,
+    # replacing `app.conf` with a concrete settings object.
+
+    callback = None
+    _data = None
+
+    def __init__(self, conf, callback):
+        object.__setattr__(self, '_data', conf)
+        object.__setattr__(self, 'callback', callback)
+
+    def __setitem__(self, key, value):
+        self._data[key] = value
+
+    def clear(self):
+        self._data.clear()
+
+    def update(self, *args, **kwargs):
+        self._data.update(*args, **kwargs)
+
+    def setdefault(self, *args, **kwargs):
+        return self._data.setdefault(*args, **kwargs)
+
+    def __contains__(self, key):
+        # XXX will not show finalized configuration
+        # setdefault will cause `key in d` to happen,
+        # so for setdefault to be lazy, so does contains.
+        return key in self._data
+
+    def __len__(self):
+        return len(self.data)
+
+    def __repr__(self):
+        return repr(self.data)
+
+    @cached_property
+    def data(self):
+        return self.callback()
+
+
+class Celery:
+    """Celery application.
+
+    Arguments:
+        main (str): Name of the main module if running as `__main__`.
+            This is used as the prefix for auto-generated task names.
+
+    Keyword Arguments:
+        broker (str): URL of the default broker used.
+        backend (Union[str, Type[celery.backends.base.Backend]]):
+            The result store backend class, or the name of the backend
+            class to use.
+
+            Default is the value of the :setting:`result_backend` setting.
+        autofinalize (bool): If set to False a :exc:`RuntimeError`
+            will be raised if the task registry or tasks are used before
+            the app is finalized.
+        set_as_current (bool):  Make this the global current app.
+        include (List[str]): List of modules every worker should import.
+
+        amqp (Union[str, Type[AMQP]]): AMQP object or class name.
+        events (Union[str, Type[celery.app.events.Events]]): Events object or
+            class name.
+        log (Union[str, Type[Logging]]): Log object or class name.
+        control (Union[str, Type[celery.app.control.Control]]): Control object
+            or class name.
+        tasks (Union[str, Type[TaskRegistry]]): A task registry, or the name of
+            a registry class.
+        fixups (List[str]): List of fix-up plug-ins (e.g., see
+            :mod:`celery.fixups.django`).
+        config_source (Union[str, class]): Take configuration from a class,
+            or object.  Attributes may include any settings described in
+            the documentation.
+        task_cls (Union[str, Type[celery.app.task.Task]]): base task class to
+            use. See :ref:`this section <custom-task-cls-app-wide>` for usage.
+    """
+
+    #: This is deprecated, use :meth:`reduce_keys` instead
+    Pickler = AppPickler
+
+    SYSTEM = platforms.SYSTEM
+    IS_macOS, IS_WINDOWS = platforms.IS_macOS, platforms.IS_WINDOWS
+
+    #: Name of the `__main__` module.  Required for standalone scripts.
+    #:
+    #: If set this will be used instead of `__main__` when automatically
+    #: generating task names.
+    main = None
+
+    #: Custom options for command-line programs.
+    #: See :ref:`extending-commandoptions`
+    user_options = None
+
+    #: Custom bootsteps to extend and modify the worker.
+    #: See :ref:`extending-bootsteps`.
+    steps = None
+
+    builtin_fixups = BUILTIN_FIXUPS
+
+    amqp_cls = 'celery.app.amqp:AMQP'
+    backend_cls = None
+    events_cls = 'celery.app.events:Events'
+    loader_cls = None
+    log_cls = 'celery.app.log:Logging'
+    control_cls = 'celery.app.control:Control'
+    task_cls = 'celery.app.task:Task'
+    registry_cls = 'celery.app.registry:TaskRegistry'
+
+    #: Thread local storage.
+    _local = None
+    _fixups = None
+    _pool = None
+    _conf = None
+    _after_fork_registered = False
+
+    #: Signal sent when app is loading configuration.
+    on_configure = None
+
+    #: Signal sent after app has prepared the configuration.
+    on_after_configure = None
+
+    #: Signal sent after app has been finalized.
+    on_after_finalize = None
+
+    #: Signal sent by every new process after fork.
+    on_after_fork = None
+
+    def __init__(self, main=None, loader=None, backend=None,
+                 amqp=None, events=None, log=None, control=None,
+                 set_as_current=True, tasks=None, broker=None, include=None,
+                 changes=None, config_source=None, fixups=None, task_cls=None,
+                 autofinalize=True, namespace=None, strict_typing=True,
+                 aio_control=None,
+                 **kwargs):
+
+        self._local = threading.local()
+
+        self.clock = LamportClock()
+        self.main = main
+        self.amqp_cls = amqp or self.amqp_cls
+        self.events_cls = events or self.events_cls
+        self.loader_cls = loader or self._get_default_loader()
+        self.log_cls = log or self.log_cls
+        self.control_cls = control or self.control_cls
+        self.aio_control_cls = aio_control or self.aio_control_cls
+        self.task_cls = task_cls or self.task_cls
+        self.set_as_current = set_as_current
+        self.registry_cls = symbol_by_name(self.registry_cls)
+        self.user_options = defaultdict(set)
+        self.steps = defaultdict(set)
+        self.autofinalize = autofinalize
+        self.namespace = namespace
+        self.strict_typing = strict_typing
+
+        self.configured = False
+        self._config_source = config_source
+        self._pending_defaults = deque()
+        self._pending_periodic_tasks = deque()
+
+        self.finalized = False
+        self._finalize_mutex = threading.Lock()
+        self._pending = deque()
+        self._tasks = tasks
+        if not isinstance(self._tasks, TaskRegistry):
+            self._tasks = self.registry_cls(self._tasks or {})
+
+        # If the class defines a custom __reduce_args__ we need to use
+        # the old way of pickling apps: pickling a list of
+        # args instead of the new way that pickles a dict of keywords.
+        self._using_v1_reduce = app_has_custom(self, '__reduce_args__')
+
+        # these options are moved to the config to
+        # simplify pickling of the app object.
+        self._preconf = changes or {}
+        self._preconf_set_by_auto = set()
+        self.__autoset('broker_url', broker)
+        self.__autoset('result_backend', backend)
+        self.__autoset('include', include)
+        self.__autoset('broker_use_ssl', kwargs.get('broker_use_ssl'))
+        self.__autoset('redis_backend_use_ssl', kwargs.get('redis_backend_use_ssl'))
+        self._conf = Settings(
+            PendingConfiguration(
+                self._preconf, self._finalize_pending_conf),
+            prefix=self.namespace,
+            keys=(_old_key_to_new, _new_key_to_old),
+        )
+
+        # - Apply fix-ups.
+        self.fixups = set(self.builtin_fixups) if fixups is None else fixups
+        # ...store fixup instances in _fixups to keep weakrefs alive.
+        self._fixups = [symbol_by_name(fixup)(self) for fixup in self.fixups]
+
+        if self.set_as_current:
+            self.set_current()
+
+        # Signals
+        if self.on_configure is None:
+            # used to be a method pre 4.0
+            self.on_configure = Signal(name='app.on_configure')
+        self.on_after_configure = Signal(
+            name='app.on_after_configure',
+            providing_args={'source'},
+        )
+        self.on_after_finalize = Signal(name='app.on_after_finalize')
+        self.on_after_fork = Signal(name='app.on_after_fork')
+
+        self.on_init()
+        _register_app(self)
+
+    def _get_default_loader(self):
+        # the --loader command-line argument sets the environment variable.
+        return (
+            os.environ.get('CELERY_LOADER') or
+            self.loader_cls or
+            'celery.loaders.app:AppLoader'
+        )
+
+    def on_init(self):
+        """Optional callback called at init."""
+
+    def __autoset(self, key, value):
+        if value:
+            self._preconf[key] = value
+            self._preconf_set_by_auto.add(key)
+
+    def set_current(self):
+        """Make this the current app for this thread."""
+        _set_current_app(self)
+
+    def set_default(self):
+        """Make this the default app for all threads."""
+        set_default_app(self)
+
+    def _ensure_after_fork(self):
+        if not self._after_fork_registered:
+            self._after_fork_registered = True
+            if register_after_fork is not None:
+                register_after_fork(self, _after_fork_cleanup_app)
+
+    def close(self):
+        """Clean up after the application.
+
+        Only necessary for dynamically created apps, and you should
+        probably use the :keyword:`with` statement instead.
+
+        Example:
+            >>> with Celery(set_as_current=False) as app:
+            ...     with app.connection_for_write() as conn:
+            ...         pass
+        """
+        self._pool = None
+        self._aio_pool = None
+        _deregister_app(self)
+
+    def start(self, argv=None):
+        from celery.bin.celery import celery
+
+        celery.params[0].default = self
+
+        try:
+            celery.main(args=argv, standalone_mode=False)
+        except Exit as e:
+            return e.exit_code
+        finally:
+            celery.params[0].default = None
+
+    def worker_main(self, argv=None):
+        if argv is None:
+            argv = sys.argv
+
+        if 'worker' not in argv:
+            raise ValueError(
+                "The worker sub-command must be specified in argv.\n"
+                "Use app.start() to programmatically start other commands."
+            )
+
+        self.start(argv=argv)
+
+    def task(self, *args, **opts):
+        """Decorator to create a task class out of any callable.
+
+        See :ref:`Task options<task-options>` for a list of the
+        arguments that can be passed to this decorator.
+
+        Examples:
+            .. code-block:: python
+
+                @app.task
+                def refresh_feed(url):
+                    store_feed(feedparser.parse(url))
+
+            with setting extra options:
+
+            .. code-block:: python
+
+                @app.task(exchange='feeds')
+                def refresh_feed(url):
+                    return store_feed(feedparser.parse(url))
+
+        Note:
+            App Binding: For custom apps the task decorator will return
+            a proxy object, so that the act of creating the task is not
+            performed until the task is used or the task registry is accessed.
+
+            If you're depending on binding to be deferred, then you must
+            not access any attributes on the returned object until the
+            application is fully set up (finalized).
+        """
+        if USING_EXECV and opts.get('lazy', True):
+            # When using execv the task in the original module will point to a
+            # different app, so doing things like 'add.request' will point to
+            # a different task instance.  This makes sure it will always use
+            # the task instance from the current app.
+            # Really need a better solution for this :(
+            from . import shared_task
+            return shared_task(*args, lazy=False, **opts)
+
+        def inner_create_task_cls(shared=True, filter=None, lazy=True, **opts):
+            _filt = filter
+
+            def _create_task_cls(fun):
+                if shared:
+                    def cons(app):
+                        return app._task_from_fun(fun, **opts)
+                    cons.__name__ = fun.__name__
+                    connect_on_app_finalize(cons)
+                if not lazy or self.finalized:
+                    ret = self._task_from_fun(fun, **opts)
+                else:
+                    # return a proxy object that evaluates on first use
+                    ret = PromiseProxy(self._task_from_fun, (fun,), opts,
+                                       __doc__=fun.__doc__)
+                    self._pending.append(ret)
+                if _filt:
+                    return _filt(ret)
+                return ret
+
+            return _create_task_cls
+
+        if len(args) == 1:
+            if callable(args[0]):
+                return inner_create_task_cls(**opts)(*args)
+            raise TypeError('argument 1 to @task() must be a callable')
+        if args:
+            raise TypeError(
+                '@task() takes exactly 1 argument ({} given)'.format(
+                    sum([len(args), len(opts)])))
+        return inner_create_task_cls(**opts)
+
+    def _task_from_fun(self, fun, name=None, base=None, bind=False, aio_variant=False, **options):
+        if not self.finalized and not self.autofinalize:
+            raise RuntimeError('Contract breach: app not finalized')
+        name = name or self.gen_task_name(fun.__name__, fun.__module__)
+        base = base or self.Task
+
+        if name not in self._tasks:
+            run = fun if bind else staticmethod(fun)
+            attrs = dict({
+                'app': self,
+                'name': name,
+                'run': run,
+                '_decorated': True,
+                '__doc__': fun.__doc__,
+                '__module__': fun.__module__,
+                '__annotations__': fun.__annotations__,
+                '__header__': staticmethod(head_from_fun(fun, bound=bind)),
+                '__wrapped__': run
+            }, **options)
+            task = type(fun.__name__, (base,), attrs)()
+            if aio_variant:
+                aio_task = type(fun.__name__ + '@aio', (self.AioTask,), attrs)()
+                task.aio = aio_task
+
+            # for some reason __qualname__ cannot be set in type()
+            # so we have to set it here.
+            try:
+                task.__qualname__ = fun.__qualname__
+                if aio_variant:
+                    aio_task.__qualname__ = fun.__qualname__
+            except AttributeError:
+                pass
+            self._tasks[task.name] = task
+            task.bind(self)  # connects task to this app
+            add_autoretry_behaviour(task, **options)
+            if aio_variant:
+                aio_task.bind(self)
+                add_autoretry_behaviour(aio_task, **options)
+        else:
+            task = self._tasks[name]
+        return task
+
+    def register_task(self, task):
+        """Utility for registering a task-based class.
+
+        Note:
+            This is here for compatibility with old Celery 1.0
+            style task classes, you should not need to use this for
+            new projects.
+        """
+        task = inspect.isclass(task) and task() or task
+        if not task.name:
+            task_cls = type(task)
+            task.name = self.gen_task_name(
+                task_cls.__name__, task_cls.__module__)
+        add_autoretry_behaviour(task)
+        self.tasks[task.name] = task
+        task._app = self
+        task.bind(self)
+        return task
+
+    def gen_task_name(self, name, module):
+        return gen_task_name(self, name, module)
+
+    def finalize(self, auto=False):
+        """Finalize the app.
+
+        This loads built-in tasks, evaluates pending task decorators,
+        reads configuration, etc.
+        """
+        with self._finalize_mutex:
+            if not self.finalized:
+                if auto and not self.autofinalize:
+                    raise RuntimeError('Contract breach: app not finalized')
+                self.finalized = True
+                _announce_app_finalized(self)
+
+                pending = self._pending
+                while pending:
+                    maybe_evaluate(pending.popleft())
+
+                for task in self._tasks.values():
+                    task.bind(self)
+
+                self.on_after_finalize.send(sender=self)
+
+    def add_defaults(self, fun):
+        """Add default configuration from dict ``d``.
+
+        If the argument is a callable function then it will be regarded
+        as a promise, and it won't be loaded until the configuration is
+        actually needed.
+
+        This method can be compared to:
+
+        .. code-block:: pycon
+
+            >>> celery.conf.update(d)
+
+        with a difference that 1) no copy will be made and 2) the dict will
+        not be transferred when the worker spawns child processes, so
+        it's important that the same configuration happens at import time
+        when pickle restores the object on the other side.
+        """
+        if not callable(fun):
+            d, fun = fun, lambda: d
+        if self.configured:
+            return self._conf.add_defaults(fun())
+        self._pending_defaults.append(fun)
+
+    def config_from_object(self, obj,
+                           silent=False, force=False, namespace=None):
+        """Read configuration from object.
+
+        Object is either an actual object or the name of a module to import.
+
+        Example:
+            >>> celery.config_from_object('myapp.celeryconfig')
+
+            >>> from myapp import celeryconfig
+            >>> celery.config_from_object(celeryconfig)
+
+        Arguments:
+            silent (bool): If true then import errors will be ignored.
+            force (bool): Force reading configuration immediately.
+                By default the configuration will be read only when required.
+        """
+        self._config_source = obj
+        self.namespace = namespace or self.namespace
+        if force or self.configured:
+            self._conf = None
+            if self.loader.config_from_object(obj, silent=silent):
+                return self.conf
+
+    def config_from_envvar(self, variable_name, silent=False, force=False):
+        """Read configuration from environment variable.
+
+        The value of the environment variable must be the name
+        of a module to import.
+
+        Example:
+            >>> os.environ['CELERY_CONFIG_MODULE'] = 'myapp.celeryconfig'
+            >>> celery.config_from_envvar('CELERY_CONFIG_MODULE')
+        """
+        module_name = os.environ.get(variable_name)
+        if not module_name:
+            if silent:
+                return False
+            raise ImproperlyConfigured(
+                ERR_ENVVAR_NOT_SET.strip().format(variable_name))
+        return self.config_from_object(module_name, silent=silent, force=force)
+
+    def config_from_cmdline(self, argv, namespace='celery'):
+        self._conf.update(
+            self.loader.cmdline_config_parser(argv, namespace)
+        )
+
+    def setup_security(self, allowed_serializers=None, key=None, cert=None,
+                       store=None, digest=DEFAULT_SECURITY_DIGEST,
+                       serializer='json'):
+        """Setup the message-signing serializer.
+
+        This will affect all application instances (a global operation).
+
+        Disables untrusted serializers and if configured to use the ``auth``
+        serializer will register the ``auth`` serializer with the provided
+        settings into the Kombu serializer registry.
+
+        Arguments:
+            allowed_serializers (Set[str]): List of serializer names, or
+                content_types that should be exempt from being disabled.
+            key (str): Name of private key file to use.
+                Defaults to the :setting:`security_key` setting.
+            cert (str): Name of certificate file to use.
+                Defaults to the :setting:`security_certificate` setting.
+            store (str): Directory containing certificates.
+                Defaults to the :setting:`security_cert_store` setting.
+            digest (str): Digest algorithm used when signing messages.
+                Default is ``sha256``.
+            serializer (str): Serializer used to encode messages after
+                they've been signed.  See :setting:`task_serializer` for
+                the serializers supported.  Default is ``json``.
+        """
+        from celery.security import setup_security
+        return setup_security(allowed_serializers, key, cert,
+                              store, digest, serializer, app=self)
+
+    def autodiscover_tasks(self, packages=None,
+                           related_name='tasks', force=False):
+        """Auto-discover task modules.
+
+        Searches a list of packages for a "tasks.py" module (or use
+        related_name argument).
+
+        If the name is empty, this will be delegated to fix-ups (e.g., Django).
+
+        For example if you have a directory layout like this:
+
+        .. code-block:: text
+
+            foo/__init__.py
+               tasks.py
+               models.py
+
+            bar/__init__.py
+                tasks.py
+                models.py
+
+            baz/__init__.py
+                models.py
+
+        Then calling ``app.autodiscover_tasks(['foo', 'bar', 'baz'])`` will
+        result in the modules ``foo.tasks`` and ``bar.tasks`` being imported.
+
+        Arguments:
+            packages (List[str]): List of packages to search.
+                This argument may also be a callable, in which case the
+                value returned is used (for lazy evaluation).
+            related_name (Optional[str]): The name of the module to find.  Defaults
+                to "tasks": meaning "look for 'module.tasks' for every
+                module in ``packages``.".  If ``None`` will only try to import
+                the package, i.e. "look for 'module'".
+            force (bool): By default this call is lazy so that the actual
+                auto-discovery won't happen until an application imports
+                the default modules.  Forcing will cause the auto-discovery
+                to happen immediately.
+        """
+        if force:
+            return self._autodiscover_tasks(packages, related_name)
+        signals.import_modules.connect(starpromise(
+            self._autodiscover_tasks, packages, related_name,
+        ), weak=False, sender=self)
+
+    def _autodiscover_tasks(self, packages, related_name, **kwargs):
+        if packages:
+            return self._autodiscover_tasks_from_names(packages, related_name)
+        return self._autodiscover_tasks_from_fixups(related_name)
+
+    def _autodiscover_tasks_from_names(self, packages, related_name):
+        # packages argument can be lazy
+        return self.loader.autodiscover_tasks(
+            packages() if callable(packages) else packages, related_name,
+        )
+
+    def _autodiscover_tasks_from_fixups(self, related_name):
+        return self._autodiscover_tasks_from_names([
+            pkg for fixup in self._fixups
+            if hasattr(fixup, 'autodiscover_tasks')
+            for pkg in fixup.autodiscover_tasks()
+        ], related_name=related_name)
+
+    def send_task(self, name, args=None, kwargs=None, countdown=None,
+                  eta=None, task_id=None, producer=None, connection=None,
+                  router=None, result_cls=None, expires=None,
+                  publisher=None, link=None, link_error=None,
+                  add_to_parent=True, group_id=None, group_index=None,
+                  retries=0, chord=None,
+                  reply_to=None, time_limit=None, soft_time_limit=None,
+                  root_id=None, parent_id=None, route_name=None,
+                  shadow=None, chain=None, task_type=None, **options):
+        """Send task by name.
+
+        Supports the same arguments as :meth:`@-Task.apply_async`.
+
+        Arguments:
+            name (str): Name of task to call (e.g., `"tasks.add"`).
+            result_cls (AsyncResult): Specify custom result class.
+        """
+        parent = have_parent = None
+        amqp = self.amqp
+        task_id = task_id or uuid()
+        producer = producer or publisher  # XXX compat
+        router = router or amqp.router
+        conf = self.conf
+        if conf.task_always_eager:  # pragma: no cover
+            warnings.warn(AlwaysEagerIgnored(
+                'task_always_eager has no effect on send_task',
+            ), stacklevel=2)
+
+        ignored_result = options.pop('ignore_result', False)
+        options = router.route(
+            options, route_name or name, args, kwargs, task_type)
+
+        if not root_id or not parent_id:
+            parent = self.current_worker_task
+            if parent:
+                if not root_id:
+                    root_id = parent.request.root_id or parent.request.id
+                if not parent_id:
+                    parent_id = parent.request.id
+
+                if conf.task_inherit_parent_priority:
+                    options.setdefault('priority',
+                                       parent.request.delivery_info.get('priority'))
+
+        message = amqp.create_task_message(
+            task_id, name, args, kwargs, countdown, eta, group_id, group_index,
+            expires, retries, chord,
+            maybe_list(link), maybe_list(link_error),
+            reply_to or self.thread_oid, time_limit, soft_time_limit,
+            self.conf.task_send_sent_event,
+            root_id, parent_id, shadow, chain,
+            argsrepr=options.get('argsrepr'),
+            kwargsrepr=options.get('kwargsrepr'),
+        )
+
+        if connection:
+            producer = amqp.Producer(connection, auto_declare=False)
+
+        with self.producer_or_acquire(producer) as P:
+            with P.connection._reraise_as_library_errors():
+                if not ignored_result:
+                    self.backend.on_task_call(P, task_id)
+                amqp.send_task_message(P, name, message, **options)
+        result = (result_cls or self.AsyncResult)(task_id)
+        # We avoid using the constructor since a custom result class
+        # can be used, in which case the constructor may still use
+        # the old signature.
+        result.ignored = ignored_result
+
+        if add_to_parent:
+            if not have_parent:
+                parent, have_parent = self.current_worker_task, True
+            if parent:
+                parent.add_trail(result)
+        return result
+
+    def connection_for_read(self, url=None, **kwargs):
+        """Establish connection used for consuming.
+
+        See Also:
+            :meth:`connection` for supported arguments.
+        """
+        return self._connection(url or self.conf.broker_read_url, **kwargs)
+
+    def connection_for_write(self, url=None, **kwargs):
+        """Establish connection used for producing.
+
+        See Also:
+            :meth:`connection` for supported arguments.
+        """
+        return self._connection(url or self.conf.broker_write_url, **kwargs)
+
+    def connection(self, hostname=None, userid=None, password=None,
+                   virtual_host=None, port=None, ssl=None,
+                   connect_timeout=None, transport=None,
+                   transport_options=None, heartbeat=None,
+                   login_method=None, failover_strategy=None, **kwargs):
+        """Establish a connection to the message broker.
+
+        Please use :meth:`connection_for_read` and
+        :meth:`connection_for_write` instead, to convey the intent
+        of use for this connection.
+
+        Arguments:
+            url: Either the URL or the hostname of the broker to use.
+            hostname (str): URL, Hostname/IP-address of the broker.
+                If a URL is used, then the other argument below will
+                be taken from the URL instead.
+            userid (str): Username to authenticate as.
+            password (str): Password to authenticate with
+            virtual_host (str): Virtual host to use (domain).
+            port (int): Port to connect to.
+            ssl (bool, Dict): Defaults to the :setting:`broker_use_ssl`
+                setting.
+            transport (str): defaults to the :setting:`broker_transport`
+                setting.
+            transport_options (Dict): Dictionary of transport specific options.
+            heartbeat (int): AMQP Heartbeat in seconds (``pyamqp`` only).
+            login_method (str): Custom login method to use (AMQP only).
+            failover_strategy (str, Callable): Custom failover strategy.
+            **kwargs: Additional arguments to :class:`kombu.Connection`.
+
+        Returns:
+            kombu.Connection: the lazy connection instance.
+        """
+        return self.connection_for_write(
+            hostname or self.conf.broker_write_url,
+            userid=userid, password=password,
+            virtual_host=virtual_host, port=port, ssl=ssl,
+            connect_timeout=connect_timeout, transport=transport,
+            transport_options=transport_options, heartbeat=heartbeat,
+            login_method=login_method, failover_strategy=failover_strategy,
+            **kwargs
+        )
+
+    def _connection(self, url, userid=None, password=None,
+                    virtual_host=None, port=None, ssl=None,
+                    connect_timeout=None, transport=None,
+                    transport_options=None, heartbeat=None,
+                    login_method=None, failover_strategy=None, **kwargs):
+        conf = self.conf
+        return self.amqp.Connection(
+            url,
+            userid or conf.broker_user,
+            password or conf.broker_password,
+            virtual_host or conf.broker_vhost,
+            port or conf.broker_port,
+            transport=transport or conf.broker_transport,
+            ssl=self.either('broker_use_ssl', ssl),
+            heartbeat=heartbeat,
+            login_method=login_method or conf.broker_login_method,
+            failover_strategy=(
+                failover_strategy or conf.broker_failover_strategy
+            ),
+            transport_options=dict(
+                conf.broker_transport_options, **transport_options or {}
+            ),
+            connect_timeout=self.either(
+                'broker_connection_timeout', connect_timeout
+            ),
+        )
+    broker_connection = connection
+
+    def _acquire_connection(self, pool=True):
+        """Helper for :meth:`connection_or_acquire`."""
+        if pool:
+            return self.pool.acquire(block=True)
+        return self.connection_for_write()
+
+    def connection_or_acquire(self, connection=None, pool=True, *_, **__):
+        """Context used to acquire a connection from the pool.
+
+        For use within a :keyword:`with` statement to get a connection
+        from the pool if one is not already provided.
+
+        Arguments:
+            connection (kombu.Connection): If not provided, a connection
+                will be acquired from the connection pool.
+        """
+        return FallbackContext(connection, self._acquire_connection, pool=pool)
+    default_connection = connection_or_acquire  # XXX compat
+
+    def producer_or_acquire(self, producer=None):
+        """Context used to acquire a producer from the pool.
+
+        For use within a :keyword:`with` statement to get a producer
+        from the pool if one is not already provided
+
+        Arguments:
+            producer (kombu.Producer): If not provided, a producer
+                will be acquired from the producer pool.
+        """
+        return FallbackContext(
+            producer, self.producer_pool.acquire, block=True,
+        )
+    default_producer = producer_or_acquire  # XXX compat
+
+    def prepare_config(self, c):
+        """Prepare configuration before it is merged with the defaults."""
+        return find_deprecated_settings(c)
+
+    def now(self):
+        """Return the current time and date as a datetime."""
+        now_in_utc = to_utc(datetime.utcnow())
+        return now_in_utc.astimezone(self.timezone)
+
+    def select_queues(self, queues=None):
+        """Select subset of queues.
+
+        Arguments:
+            queues (Sequence[str]): a list of queue names to keep.
+        """
+        return self.amqp.queues.select(queues)
+
+    def either(self, default_key, *defaults):
+        """Get key from configuration or use default values.
+
+        Fallback to the value of a configuration key if none of the
+        `*values` are true.
+        """
+        return first(None, [
+            first(None, defaults), starpromise(self.conf.get, default_key),
+        ])
+
+    def bugreport(self):
+        """Return information useful in bug reports."""
+        return bugreport(self)
+
+    def _get_backend(self):
+        backend, url = backends.by_url(
+            self.backend_cls or self.conf.result_backend,
+            self.loader)
+        return backend(app=self, url=url)
+
+    def _finalize_pending_conf(self):
+        """Get config value by key and finalize loading the configuration.
+
+        Note:
+            This is used by PendingConfiguration:
+                as soon as you access a key the configuration is read.
+        """
+        conf = self._conf = self._load_config()
+        return conf
+
+    def _load_config(self):
+        if isinstance(self.on_configure, Signal):
+            self.on_configure.send(sender=self)
+        else:
+            # used to be a method pre 4.0
+            self.on_configure()
+        if self._config_source:
+            self.loader.config_from_object(self._config_source)
+        self.configured = True
+        settings = detect_settings(
+            self.prepare_config(self.loader.conf), self._preconf,
+            ignore_keys=self._preconf_set_by_auto, prefix=self.namespace,
+        )
+        if self._conf is not None:
+            # replace in place, as someone may have referenced app.conf,
+            # done some changes, accessed a key, and then try to make more
+            # changes to the reference and not the finalized value.
+            self._conf.swap_with(settings)
+        else:
+            self._conf = settings
+
+        # load lazy config dict initializers.
+        pending_def = self._pending_defaults
+        while pending_def:
+            self._conf.add_defaults(maybe_evaluate(pending_def.popleft()()))
+
+        # load lazy periodic tasks
+        pending_beat = self._pending_periodic_tasks
+        while pending_beat:
+            self._add_periodic_task(*pending_beat.popleft())
+
+        self.on_after_configure.send(sender=self, source=self._conf)
+        return self._conf
+
+    def _after_fork(self):
+        self._pool = None
+        try:
+            self.__dict__['amqp']._producer_pool = None
+        except (AttributeError, KeyError):
+            pass
+        self.on_after_fork.send(sender=self)
+
+    def signature(self, *args, **kwargs):
+        """Return a new :class:`~celery.Signature` bound to this app."""
+        kwargs['app'] = self
+        return self._canvas.signature(*args, **kwargs)
+
+    def add_periodic_task(self, schedule, sig,
+                          args=(), kwargs=(), name=None, **opts):
+        key, entry = self._sig_to_periodic_task_entry(
+            schedule, sig, args, kwargs, name, **opts)
+        if self.configured:
+            self._add_periodic_task(key, entry)
+        else:
+            self._pending_periodic_tasks.append((key, entry))
+        return key
+
+    def _sig_to_periodic_task_entry(self, schedule, sig,
+                                    args=(), kwargs=None, name=None, **opts):
+        kwargs = {} if not kwargs else kwargs
+        sig = (sig.clone(args, kwargs)
+               if isinstance(sig, abstract.CallableSignature)
+               else self.signature(sig.name, args, kwargs))
+        return name or repr(sig), {
+            'schedule': schedule,
+            'task': sig.name,
+            'args': sig.args,
+            'kwargs': sig.kwargs,
+            'options': dict(sig.options, **opts),
+        }
+
+    def _add_periodic_task(self, key, entry):
+        self._conf.beat_schedule[key] = entry
+
+    def create_task_cls(self):
+        """Create a base task class bound to this app."""
+        return self.subclass_with_self(
+            self.task_cls, name='Task', attribute='_app',
+            keep_reduce=True, abstract=True,
+        )
+
+    def subclass_with_self(self, Class, name=None, attribute='app',
+                           reverse=None, keep_reduce=False, **kw):
+        """Subclass an app-compatible class.
+
+        App-compatible means that the class has a class attribute that
+        provides the default app it should use, for example:
+        ``class Foo: app = None``.
+
+        Arguments:
+            Class (type): The app-compatible class to subclass.
+            name (str): Custom name for the target class.
+            attribute (str): Name of the attribute holding the app,
+                Default is 'app'.
+            reverse (str): Reverse path to this object used for pickling
+                purposes. For example, to get ``app.AsyncResult``,
+                use ``"AsyncResult"``.
+            keep_reduce (bool): If enabled a custom ``__reduce__``
+                implementation won't be provided.
+        """
+        Class = symbol_by_name(Class)
+        reverse = reverse if reverse else Class.__name__
+
+        def __reduce__(self):
+            return _unpickle_appattr, (reverse, self.__reduce_args__())
+
+        attrs = dict(
+            {attribute: self},
+            __module__=Class.__module__,
+            __doc__=Class.__doc__,
+            **kw)
+        if not keep_reduce:
+            attrs['__reduce__'] = __reduce__
+
+        return type(name or Class.__name__, (Class,), attrs)
+
+    def _rgetattr(self, path):
+        return attrgetter(path)(self)
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, *exc_info):
+        self.close()
+
+    def __repr__(self):
+        return '<{} {}>'.format(type(self).__name__, appstr(self))
+
+    def __reduce__(self):
+        if self._using_v1_reduce:
+            return self.__reduce_v1__()
+        return (_unpickle_app_v2, (self.__class__, self.__reduce_keys__()))
+
+    def __reduce_v1__(self):
+        # Reduce only pickles the configuration changes,
+        # so the default configuration doesn't have to be passed
+        # between processes.
+        return (
+            _unpickle_app,
+            (self.__class__, self.Pickler) + self.__reduce_args__(),
+        )
+
+    def __reduce_keys__(self):
+        """Keyword arguments used to reconstruct the object when unpickling."""
+        return {
+            'main': self.main,
+            'changes':
+                self._conf.changes if self.configured else self._preconf,
+            'loader': self.loader_cls,
+            'backend': self.backend_cls,
+            'amqp': self.amqp_cls,
+            'events': self.events_cls,
+            'log': self.log_cls,
+            'control': self.control_cls,
+            'fixups': self.fixups,
+            'config_source': self._config_source,
+            'task_cls': self.task_cls,
+            'namespace': self.namespace,
+        }
+
+    def __reduce_args__(self):
+        """Deprecated method, please use :meth:`__reduce_keys__` instead."""
+        return (self.main, self._conf.changes if self.configured else {},
+                self.loader_cls, self.backend_cls, self.amqp_cls,
+                self.events_cls, self.log_cls, self.control_cls,
+                False, self._config_source)
+
+    @cached_property
+    def Worker(self):
+        """Worker application.
+
+        See Also:
+            :class:`~@Worker`.
+        """
+        return self.subclass_with_self('celery.apps.worker:Worker')
+
+    @cached_property
+    def WorkController(self, **kwargs):
+        """Embeddable worker.
+
+        See Also:
+            :class:`~@WorkController`.
+        """
+        return self.subclass_with_self('celery.worker:WorkController')
+
+    @cached_property
+    def Beat(self, **kwargs):
+        """:program:`celery beat` scheduler application.
+
+        See Also:
+            :class:`~@Beat`.
+        """
+        return self.subclass_with_self('celery.apps.beat:Beat')
+
+    @cached_property
+    def Task(self):
+        """Base task class for this app."""
+        return self.create_task_cls()
+
+    @cached_property
+    def annotations(self):
+        return prepare_annotations(self.conf.task_annotations)
+
+    @cached_property
+    def AsyncResult(self):
+        """Create new result instance.
+
+        See Also:
+            :class:`celery.result.AsyncResult`.
+        """
+        return self.subclass_with_self('celery.result:AsyncResult')
+
+    @cached_property
+    def ResultSet(self):
+        return self.subclass_with_self('celery.result:ResultSet')
+
+    @cached_property
+    def GroupResult(self):
+        """Create new group result instance.
+
+        See Also:
+            :class:`celery.result.GroupResult`.
+        """
+        return self.subclass_with_self('celery.result:GroupResult')
+
+    @property
+    def pool(self):
+        """Broker connection pool: :class:`~@pool`.
+
+        Note:
+            This attribute is not related to the workers concurrency pool.
+        """
+        if self._pool is None:
+            self._ensure_after_fork()
+            limit = self.conf.broker_pool_limit
+            pools.set_limit(limit)
+            self._pool = pools.connections[self.connection_for_write()]
+        return self._pool
+
+    @property
+    def current_task(self):
+        """Instance of task being executed, or :const:`None`."""
+        return _task_stack.top
+
+    @property
+    def current_worker_task(self):
+        """The task currently being executed by a worker or :const:`None`.
+
+        Differs from :data:`current_task` in that it's not affected
+        by tasks calling other tasks directly, or eagerly.
+        """
+        return get_current_worker_task()
+
+    @cached_property
+    def oid(self):
+        """Universally unique identifier for this app."""
+        # since 4.0: thread.get_ident() is not included when
+        # generating the process id.  This is due to how the RPC
+        # backend now dedicates a single thread to receive results,
+        # which would not work if each thread has a separate id.
+        return oid_from(self, threads=False)
+
+    @property
+    def thread_oid(self):
+        """Per-thread unique identifier for this app."""
+        try:
+            return self._local.oid
+        except AttributeError:
+            self._local.oid = new_oid = oid_from(self, threads=True)
+            return new_oid
+
+    @cached_property
+    def amqp(self):
+        """AMQP related functionality: :class:`~@amqp`."""
+        return instantiate(self.amqp_cls, app=self)
+
+    @property
+    def backend(self):
+        """Current backend instance."""
+        try:
+            return self._local.backend
+        except AttributeError:
+            self._local.backend = new_backend = self._get_backend()
+            return new_backend
+
+    @property
+    def conf(self):
+        """Current configuration."""
+        if self._conf is None:
+            self._conf = self._load_config()
+        return self._conf
+
+    @conf.setter
+    def conf(self, d):  # noqa
+        self._conf = d
+
+    @cached_property
+    def control(self):
+        """Remote control: :class:`~@control`."""
+        return instantiate(self.control_cls, app=self)
+
+    @cached_property
+    def events(self):
+        """Consuming and sending events: :class:`~@events`."""
+        return instantiate(self.events_cls, app=self)
+
+    @cached_property
+    def loader(self):
+        """Current loader instance."""
+        return get_loader_cls(self.loader_cls)(app=self)
+
+    @cached_property
+    def log(self):
+        """Logging: :class:`~@log`."""
+        return instantiate(self.log_cls, app=self)
+
+    @cached_property
+    def _canvas(self):
+        from celery import canvas
+        return canvas
+
+    @cached_property
+    def tasks(self):
+        """Task registry.
+
+        Warning:
+            Accessing this attribute will also auto-finalize the app.
+        """
+        self.finalize(auto=True)
+        return self._tasks
+
+    @property
+    def producer_pool(self):
+        return self.amqp.producer_pool
+
+    def uses_utc_timezone(self):
+        """Check if the application uses the UTC timezone."""
+        return self.timezone == timezone.utc
+
+    @cached_property
+    def timezone(self):
+        """Current timezone for this app.
+
+        This is a cached property taking the time zone from the
+        :setting:`timezone` setting.
+        """
+        conf = self.conf
+        if not conf.timezone:
+            if conf.enable_utc:
+                return timezone.utc
+            else:
+                return timezone.local
+        return timezone.get_timezone(conf.timezone)
+
+    # -----------------------------------------------------------------------------
+    # asyncio support
+    _aio_pool = None
+
+    aio_task_cls = 'celery.aio.task:AioTask'
+    aio_amqp_cls = 'celery.aio.amqp:AioAMQP'
+    aio_events_cls = 'celery.aio.events:Events'
+    aio_control_cls = 'celery.aio.control:Control'
+
+    def create_aio_task_cls(self):
+        """Create a base task class bound to this app."""
+        return self.subclass_with_self(
+            self.aio_task_cls, name='AioTask', attribute='_app',
+            keep_reduce=True, abstract=True,
+        )
+
+    @cached_property
+    def AioTask(self):
+        """Base aio-task class for this app."""
+        return self.create_aio_task_cls()
+
+    @cached_property
+    def aio_amqp(self):
+        """AMQP related functionality: :class:`~@aio_amqp`."""
+        return instantiate(self.aio_amqp_cls, app=self)
+
+    @cached_property
+    def aio_events(self):
+        """Consuming and sending events: :class:`~@events`."""
+        return instantiate(self.aio_events_cls, app=self)
+
+    @property
+    def aio_producer_pool(self):
+        return self.aio_amqp.producer_pool
+
+    @property
+    def aio_pool(self):
+        if self._aio_pool is None:
+            self._ensure_after_fork()
+            limit = self.conf.broker_pool_limit
+            pools.set_limit(limit)
+            self._aio_pool = pools.connections[self.aconnection_for_write()]
+        return self._aio_pool
+
+    @cached_property
+    def aio_control(self):
+        """Remote control: :class:`~@control`."""
+        return instantiate(self.aio_control_cls, app=self)
+
+    @cached_property
+    def AioAsyncResult(self):
+        return self.subclass_with_self('celery.aio.result:AsyncResult')
+
+    @cached_property
+    def AioGroupResult(self):
+        return self.subclass_with_self('celery.aio.result:GroupResult')
+
+    def aio_producer_or_acquire(self, producer=None):
+        """Context used to acquire a producer from the pool.
+
+        For use within a :keyword:`with` statement to get a producer
+        from the pool if one is not already provided
+
+        Arguments:
+            producer (kombu.Producer): If not provided, a producer
+                will be acquired from the producer pool.
+        """
+        return AsyncFallbackContext(
+            producer, self.aio_producer_pool.acquire, block=True,
+        )
+
+    @asynccontextmanager
+    async def _aio_acquire_connection(self, pool=True):
+        """Helper for :meth:`connection_or_acquire`."""
+        if pool:
+            async with self.aio_pool.acquire(block=True) as conn:
+                yield conn
+        else:
+            yield self.aconnection_for_write()
+
+    def aio_connection_or_acquire(self, connection=None, pool=True, *_, **__):
+        return AsyncFallbackContext(
+            connection, self._aio_acquire_connection, pool=pool
+        )
+
+    async def asend_task(self, name, args=None, kwargs=None, countdown=None,
+                  eta=None, task_id=None, producer=None, connection=None,
+                  router=None, result_cls=None, expires=None,
+                  publisher=None, link=None, link_error=None,
+                  add_to_parent=True, group_id=None, group_index=None,
+                  retries=0, chord=None,
+                  reply_to=None, time_limit=None, soft_time_limit=None,
+                  root_id=None, parent_id=None, route_name=None,
+                  shadow=None, chain=None, task_type=None, **options):
+        """Send task by name.
+
+        Supports the same arguments as :meth:`@-Task.apply_async`.
+
+        Arguments:
+            name (str): Name of task to call (e.g., `"tasks.add"`).
+            result_cls (AsyncResult): Specify custom result class.
+        """
+        parent = have_parent = None
+        amqp = self.aio_amqp
+        task_id = task_id or uuid()
+        producer = producer or publisher  # XXX compat
+        router = router or amqp.router
+        conf = self.conf
+        if conf.task_always_eager:  # pragma: no cover
+            warnings.warn(AlwaysEagerIgnored(
+                'task_always_eager has no effect on send_task',
+            ), stacklevel=2)
+
+        ignored_result = options.pop('ignore_result', False)
+        options = router.route(
+            options, route_name or name, args, kwargs, task_type)
+
+        if not root_id or not parent_id:
+            parent = self.current_worker_task
+            if parent:
+                if not root_id:
+                    root_id = parent.request.root_id or parent.request.id
+                if not parent_id:
+                    parent_id = parent.request.id
+
+                if conf.task_inherit_parent_priority:
+                    options.setdefault('priority',
+                                       parent.request.delivery_info.get('priority'))
+
+        message = amqp.create_task_message(
+            task_id, name, args, kwargs, countdown, eta, group_id, group_index,
+            expires, retries, chord,
+            maybe_list(link), maybe_list(link_error),
+            reply_to or self.thread_oid, time_limit, soft_time_limit,
+            self.conf.task_send_sent_event,
+            root_id, parent_id, shadow, chain,
+            argsrepr=options.get('argsrepr'),
+            kwargsrepr=options.get('kwargsrepr'),
+        )
+
+        if connection:
+            producer = amqp.Producer(connection, auto_declare=False)
+
+        async with self.aio_producer_or_acquire(producer) as P:
+            with (await P.connection)._reraise_as_library_errors():
+                if not ignored_result:
+                    await self.aio_backend.on_task_call(P, task_id)
+                await amqp.send_task_message(P, name, message, **options)
+        result = (result_cls or self.AsyncResult)(task_id)
+        # We avoid using the constructor since a custom result class
+        # can be used, in which case the constructor may still use
+        # the old signature.
+        result.ignored = ignored_result
+
+        if add_to_parent:
+            if not have_parent:
+                parent, have_parent = self.current_worker_task, True
+            if parent:
+                parent.add_trail(result)
+        return result
+
+    def aconnection_for_read(self, url=None, **kwargs):
+        """Establish connection used for consuming.
+
+        See Also:
+            :meth:`connection` for supported arguments.
+        """
+        return self._aconnection(url or self.conf.broker_read_url, **kwargs)
+
+    def aconnection_for_write(self, url=None, **kwargs):
+        """Establish connection used for producing.
+
+        See Also:
+            :meth:`connection` for supported arguments.
+        """
+        return self._aconnection(url or self.conf.broker_write_url, **kwargs)
+
+    def _aconnection(self, url, userid=None, password=None,
+                    virtual_host=None, port=None, ssl=None,
+                    connect_timeout=None, transport=None,
+                    transport_options=None, heartbeat=None,
+                    login_method=None, failover_strategy=None, **kwargs):
+        conf = self.conf
+        return self.aio_amqp.Connection(
+            url,
+            userid or conf.broker_user,
+            password or conf.broker_password,
+            virtual_host or conf.broker_vhost,
+            port or conf.broker_port,
+            transport=transport or conf.broker_transport,
+            ssl=self.either('broker_use_ssl', ssl),
+            heartbeat=heartbeat,
+            login_method=login_method or conf.broker_login_method,
+            failover_strategy=(
+                failover_strategy or conf.broker_failover_strategy
+            ),
+            transport_options=dict(
+                conf.broker_transport_options, **transport_options or {}
+            ),
+            connect_timeout=self.either(
+                'broker_connection_timeout', connect_timeout
+            ),
+        )
+
+    @property
+    def aio_backend(self):
+        """Current backend instance."""
+        try:
+            return self._local.aio_backend
+        except AttributeError:
+            override_backends = self.loader.override_backends
+            ori_redis = override_backends.pop('redis', None)
+            ori_rediss = override_backends.pop('rediss', None)
+            ori_sentinel = override_backends.pop('sentinel', None)
+            override_backends['redis'] = override_backends['rediss'] \
+                = 'celery.aio.backend:RedisBackend'
+            override_backends['sentinel'] = 'celery.aio.backend:SentinelBackend'
+            override_backends['redis-cluster'] = 'celery.aio.backend:ClusterBackend'
+            try:
+                self._local.aio_backend = new_backend = self._get_backend()
+            finally:
+                if ori_redis is not None:
+                    override_backends['redis'] = ori_rediss
+                else:
+                    override_backends.pop('redis', None)
+
+                if ori_rediss is not None:
+                    override_backends['rediss'] = ori_rediss
+                else:
+                    override_backends.pop('rediss', None)
+
+                if ori_sentinel is not None:
+                    override_backends['sentinel'] = ori_sentinel
+                else:
+                    override_backends.pop('sentinel', None)
+            return new_backend
+
+
+App = Celery  # noqa: E305 XXX compat
```

## celery/backends/redis.py

 * *Ordering differences only*

```diff
@@ -1,617 +1,617 @@
-"""Redis result store backend."""
-import time
-from contextlib import contextmanager
-from functools import partial
-from ssl import CERT_NONE, CERT_OPTIONAL, CERT_REQUIRED
-from urllib.parse import unquote
-
-from kombu.utils.functional import retry_over_time
-from kombu.utils.objects import cached_property
-from kombu.utils.url import _parse_url
-
-from celery import states
-from celery._state import task_join_will_block
-from celery.canvas import maybe_signature
-from celery.exceptions import ChordError, ImproperlyConfigured
-from celery.result import GroupResult, allow_join_result
-from celery.utils.functional import dictfilter
-from celery.utils.log import get_logger
-from celery.utils.time import humanize_seconds
-
-from .asynchronous import AsyncBackendMixin, BaseResultConsumer
-from .base import BaseKeyValueStoreBackend
-
-try:
-    import redis.connection
-    from kombu.transport.redis import get_redis_error_classes
-except ImportError:  # pragma: no cover
-    redis = None  # noqa
-    get_redis_error_classes = None  # noqa
-
-try:
-    import redis.sentinel
-    from redis.cluster import RedisCluster
-except ImportError:
-    pass
-
-__all__ = ('RedisBackend', 'SentinelBackend')
-
-E_REDIS_MISSING = """
-You need to install the redis library in order to use \
-the Redis result store backend.
-"""
-
-E_REDIS_SENTINEL_MISSING = """
-You need to install the redis library with support of \
-sentinel in order to use the Redis result store backend.
-"""
-
-W_REDIS_SSL_CERT_OPTIONAL = """
-Setting ssl_cert_reqs=CERT_OPTIONAL when connecting to redis means that \
-celery might not valdate the identity of the redis broker when connecting. \
-This leaves you vulnerable to man in the middle attacks.
-"""
-
-W_REDIS_SSL_CERT_NONE = """
-Setting ssl_cert_reqs=CERT_NONE when connecting to redis means that celery \
-will not valdate the identity of the redis broker when connecting. This \
-leaves you vulnerable to man in the middle attacks.
-"""
-
-E_REDIS_SSL_PARAMS_AND_SCHEME_MISMATCH = """
-SSL connection parameters have been provided but the specified URL scheme \
-is redis://. A Redis SSL connection URL should use the scheme rediss://.
-"""
-
-E_REDIS_SSL_CERT_REQS_MISSING_INVALID = """
-A rediss:// URL must have parameter ssl_cert_reqs and this must be set to \
-CERT_REQUIRED, CERT_OPTIONAL, or CERT_NONE
-"""
-
-E_LOST = 'Connection to Redis lost: Retry (%s/%s) %s.'
-
-E_RETRY_LIMIT_EXCEEDED = """
-Retry limit exceeded while trying to reconnect to the Celery redis result \
-store backend. The Celery application must be restarted.
-"""
-
-logger = get_logger(__name__)
-
-
-class ResultConsumer(BaseResultConsumer):
-    _pubsub = None
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._get_key_for_task = self.backend.get_key_for_task
-        self._decode_result = self.backend.decode_result
-        self._ensure = self.backend.ensure
-        self._connection_errors = self.backend.connection_errors
-        self.subscribed_to = set()
-
-    def on_after_fork(self):
-        try:
-            self.backend.client.connection_pool.reset()
-            if self._pubsub is not None:
-                self._pubsub.close()
-        except KeyError as e:
-            logger.warning(str(e))
-        super().on_after_fork()
-
-    def _reconnect_pubsub(self):
-        self._pubsub = None
-        self.backend.client.connection_pool.reset()
-        # task state might have changed when the connection was down so we
-        # retrieve meta for all subscribed tasks before going into pubsub mode
-        metas = self.backend.client.mget(self.subscribed_to)
-        metas = [meta for meta in metas if meta]
-        for meta in metas:
-            self.on_state_change(self._decode_result(meta), None)
-        self._pubsub = self.backend.client.pubsub(
-            ignore_subscribe_messages=True,
-        )
-        if self.subscribed_to:
-            self._pubsub.subscribe(*self.subscribed_to)
-
-    @contextmanager
-    def reconnect_on_error(self):
-        try:
-            yield
-        except self._connection_errors:
-            try:
-                self._ensure(self._reconnect_pubsub, ())
-            except self._connection_errors:
-                logger.critical(E_RETRY_LIMIT_EXCEEDED)
-                raise
-
-    def _maybe_cancel_ready_task(self, meta):
-        if meta['status'] in states.READY_STATES:
-            self.cancel_for(meta['task_id'])
-
-    def on_state_change(self, meta, message):
-        super().on_state_change(meta, message)
-        self._maybe_cancel_ready_task(meta)
-
-    def start(self, initial_task_id, **kwargs):
-        self._pubsub = self.backend.client.pubsub(
-            ignore_subscribe_messages=True,
-        )
-        self._consume_from(initial_task_id)
-
-    def on_wait_for_pending(self, result, **kwargs):
-        for meta in result._iter_meta(**kwargs):
-            if meta is not None:
-                self.on_state_change(meta, None)
-
-    def stop(self):
-        if self._pubsub is not None:
-            self._pubsub.close()
-
-    def drain_events(self, timeout=None):
-        if self._pubsub:
-            with self.reconnect_on_error():
-                message = self._pubsub.get_message(timeout=timeout)
-                if message and message['type'] == 'message':
-                    self.on_state_change(self._decode_result(message['data']), message)
-        elif timeout:
-            time.sleep(timeout)
-
-    def consume_from(self, task_id):
-        if self._pubsub is None:
-            return self.start(task_id)
-        self._consume_from(task_id)
-
-    def _consume_from(self, task_id):
-        key = self._get_key_for_task(task_id)
-        if key not in self.subscribed_to:
-            self.subscribed_to.add(key)
-            with self.reconnect_on_error():
-                self._pubsub.subscribe(key)
-
-    def cancel_for(self, task_id):
-        key = self._get_key_for_task(task_id)
-        self.subscribed_to.discard(key)
-        if self._pubsub:
-            with self.reconnect_on_error():
-                self._pubsub.unsubscribe(key)
-
-
-class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):
-    """Redis task result store.
-
-    It makes use of the following commands:
-    GET, MGET, DEL, INCRBY, EXPIRE, SET, SETEX
-    """
-
-    ResultConsumer = ResultConsumer
-
-    #: :pypi:`redis` client module.
-    redis = redis
-
-    #: Maximum number of connections in the pool.
-    max_connections = None
-
-    supports_autoexpire = True
-    supports_native_join = True
-
-    def __init__(self, host=None, port=None, db=None, password=None,
-                 max_connections=None, url=None,
-                 connection_pool=None, **kwargs):
-        super().__init__(expires_type=int, **kwargs)
-        _get = self.app.conf.get
-        if self.redis is None:
-            raise ImproperlyConfigured(E_REDIS_MISSING.strip())
-
-        if host and '://' in host:
-            url, host = host, None
-
-        self.max_connections = (
-            max_connections or
-            _get('redis_max_connections') or
-            self.max_connections)
-        self._ConnectionPool = connection_pool
-
-        socket_timeout = _get('redis_socket_timeout')
-        socket_connect_timeout = _get('redis_socket_connect_timeout')
-        retry_on_timeout = _get('redis_retry_on_timeout')
-        socket_keepalive = _get('redis_socket_keepalive')
-
-        self.connparams = {
-            'host': _get('redis_host') or 'localhost',
-            'port': _get('redis_port') or 6379,
-            'db': _get('redis_db') or 0,
-            'password': _get('redis_password'),
-            'max_connections': self.max_connections,
-            'socket_timeout': socket_timeout and float(socket_timeout),
-            'retry_on_timeout': retry_on_timeout or False,
-            'socket_connect_timeout':
-                socket_connect_timeout and float(socket_connect_timeout),
-        }
-
-        # absent in redis.connection.UnixDomainSocketConnection
-        if socket_keepalive:
-            self.connparams['socket_keepalive'] = socket_keepalive
-
-        # "redis_backend_use_ssl" must be a dict with the keys:
-        # 'ssl_cert_reqs', 'ssl_ca_certs', 'ssl_certfile', 'ssl_keyfile'
-        # (the same as "broker_use_ssl")
-        ssl = _get('redis_backend_use_ssl')
-        if ssl:
-            self.connparams.update(ssl)
-            self.connparams['connection_class'] = redis.SSLConnection
-
-        if url:
-            self.connparams = self._params_from_url(url, self.connparams)
-
-        # If we've received SSL parameters via query string or the
-        # redis_backend_use_ssl dict, check ssl_cert_reqs is valid. If set
-        # via query string ssl_cert_reqs will be a string so convert it here
-        if ('connection_class' in self.connparams and
-                self.connparams['connection_class'] is redis.SSLConnection):
-            ssl_cert_reqs_missing = 'MISSING'
-            ssl_string_to_constant = {'CERT_REQUIRED': CERT_REQUIRED,
-                                      'CERT_OPTIONAL': CERT_OPTIONAL,
-                                      'CERT_NONE': CERT_NONE,
-                                      'required': CERT_REQUIRED,
-                                      'optional': CERT_OPTIONAL,
-                                      'none': CERT_NONE}
-            ssl_cert_reqs = self.connparams.get('ssl_cert_reqs', ssl_cert_reqs_missing)
-            ssl_cert_reqs = ssl_string_to_constant.get(ssl_cert_reqs, ssl_cert_reqs)
-            if ssl_cert_reqs not in ssl_string_to_constant.values():
-                raise ValueError(E_REDIS_SSL_CERT_REQS_MISSING_INVALID)
-
-            if ssl_cert_reqs == CERT_OPTIONAL:
-                logger.warning(W_REDIS_SSL_CERT_OPTIONAL)
-            elif ssl_cert_reqs == CERT_NONE:
-                logger.warning(W_REDIS_SSL_CERT_NONE)
-            self.connparams['ssl_cert_reqs'] = ssl_cert_reqs
-
-        self.url = url
-
-        self.connection_errors, self.channel_errors = \
-            self.get_redis_error_classes()
-        self.result_consumer = self.ResultConsumer(
-            self, self.app, self.accept,
-            self._pending_results, self._pending_messages,
-        )
-
-    def get_redis_error_classes(self):
-        if get_redis_error_classes:
-            return get_redis_error_classes()
-        return ((), ())
-
-    def _params_from_url(self, url, defaults):
-        scheme, host, port, _, password, path, query = _parse_url(url)
-        connparams = dict(
-            defaults, **dictfilter({
-                'host': host, 'port': port, 'password': password,
-                'db': query.pop('virtual_host', None)})
-        )
-
-        if scheme == 'socket':
-            # use 'path' as path to the socket… in this case
-            # the database number should be given in 'query'
-            connparams.update({
-                'connection_class': self.redis.UnixDomainSocketConnection,
-                'path': '/' + path,
-            })
-            # host+port are invalid options when using this connection type.
-            connparams.pop('host', None)
-            connparams.pop('port', None)
-            connparams.pop('socket_connect_timeout')
-        else:
-            connparams['db'] = path
-
-        ssl_param_keys = ['ssl_ca_certs', 'ssl_certfile', 'ssl_keyfile',
-                          'ssl_cert_reqs']
-
-        if scheme == 'redis':
-            # If connparams or query string contain ssl params, raise error
-            if (any(key in connparams for key in ssl_param_keys) or
-                    any(key in query for key in ssl_param_keys)):
-                raise ValueError(E_REDIS_SSL_PARAMS_AND_SCHEME_MISMATCH)
-
-        if scheme == 'rediss':
-            connparams['connection_class'] = redis.SSLConnection
-            # The following parameters, if present in the URL, are encoded. We
-            # must add the decoded values to connparams.
-            for ssl_setting in ssl_param_keys:
-                ssl_val = query.pop(ssl_setting, None)
-                if ssl_val:
-                    connparams[ssl_setting] = unquote(ssl_val)
-
-        # db may be string and start with / like in kombu.
-        db = connparams.get('db') or 0
-        db = db.strip('/') if isinstance(db, str) else db
-        connparams['db'] = int(db)
-
-        for key, value in query.items():
-            if key in redis.connection.URL_QUERY_ARGUMENT_PARSERS:
-                query[key] = redis.connection.URL_QUERY_ARGUMENT_PARSERS[key](
-                    value
-                )
-
-        # Query parameters override other parameters
-        connparams.update(query)
-        return connparams
-
-    @cached_property
-    def retry_policy(self):
-        retry_policy = super().retry_policy
-        if "retry_policy" in self._transport_options:
-            retry_policy = retry_policy.copy()
-            retry_policy.update(self._transport_options['retry_policy'])
-
-        return retry_policy
-
-    def on_task_call(self, producer, task_id):
-        if not task_join_will_block():
-            self.result_consumer.consume_from(task_id)
-
-    def get(self, key):
-        return self.client.get(key)
-
-    def mget(self, keys):
-        return self.client.mget(keys)
-
-    def ensure(self, fun, args, **policy):
-        retry_policy = dict(self.retry_policy, **policy)
-        max_retries = retry_policy.get('max_retries')
-        return retry_over_time(
-            fun, self.connection_errors, args, {},
-            partial(self.on_connection_error, max_retries),
-            **retry_policy)
-
-    def on_connection_error(self, max_retries, exc, intervals, retries):
-        tts = next(intervals)
-        logger.error(
-            E_LOST.strip(),
-            retries, max_retries or 'Inf', humanize_seconds(tts, 'in '))
-        return tts
-
-    def set(self, key, value, **retry_policy):
-        return self.ensure(self._set, (key, value), **retry_policy)
-
-    def _set(self, key, value):
-        with self.client.pipeline() as pipe:
-            if self.expires:
-                pipe.setex(key, self.expires, value)
-            else:
-                pipe.set(key, value)
-            pipe.publish(key, value)
-            pipe.execute()
-
-    def forget(self, task_id):
-        super().forget(task_id)
-        self.result_consumer.cancel_for(task_id)
-
-    def delete(self, key):
-        self.client.delete(key)
-
-    def incr(self, key):
-        return self.client.incr(key)
-
-    def expire(self, key, value):
-        return self.client.expire(key, value)
-
-    def add_to_chord(self, group_id, result):
-        self.client.incr(self.get_key_for_group(group_id, '.t'), 1)
-
-    def _unpack_chord_result(self, tup, decode,
-                             EXCEPTION_STATES=states.EXCEPTION_STATES,
-                             PROPAGATE_STATES=states.PROPAGATE_STATES):
-        _, tid, state, retval = decode(tup)
-        if state in EXCEPTION_STATES:
-            retval = self.exception_to_python(retval)
-        if state in PROPAGATE_STATES:
-            raise ChordError(f'Dependency {tid} raised {retval!r}')
-        return retval
-
-    def apply_chord(self, header_result, body, **kwargs):
-        # If any of the child results of this chord are complex (ie. group
-        # results themselves), we need to save `header_result` to ensure that
-        # the expected structure is retained when we finish the chord and pass
-        # the results onward to the body in `on_chord_part_return()`. We don't
-        # do this is all cases to retain an optimisation in the common case
-        # where a chord header is comprised of simple result objects.
-        if any(isinstance(nr, GroupResult) for nr in header_result.results):
-            header_result.save(backend=self)
-
-    @cached_property
-    def _chord_zset(self):
-        return self._transport_options.get('result_chord_ordered', True)
-
-    @cached_property
-    def _transport_options(self):
-        return self.app.conf.get('result_backend_transport_options', {})
-
-    def on_chord_part_return(self, request, state, result,
-                             propagate=None, **kwargs):
-        app = self.app
-        tid, gid, group_index = request.id, request.group, request.group_index
-        if not gid or not tid:
-            return
-        if group_index is None:
-            group_index = '+inf'
-
-        client = self.client
-        jkey = self.get_key_for_group(gid, '.j')
-        tkey = self.get_key_for_group(gid, '.t')
-        result = self.encode_result(result, state)
-        encoded = self.encode([1, tid, state, result])
-        with client.pipeline() as pipe:
-            pipeline = (
-                pipe.zadd(jkey, {encoded: group_index}).zcount(jkey, "-inf", "+inf")
-                if self._chord_zset
-                else pipe.rpush(jkey, encoded).llen(jkey)
-            ).get(tkey)
-            if self.expires:
-                pipeline = pipeline \
-                    .expire(jkey, self.expires) \
-                    .expire(tkey, self.expires)
-
-            _, readycount, totaldiff = pipeline.execute()[:3]
-
-        totaldiff = int(totaldiff or 0)
-
-        try:
-            callback = maybe_signature(request.chord, app=app)
-            total = callback['chord_size'] + totaldiff
-            if readycount == total:
-                header_result = GroupResult.restore(gid)
-                if header_result is not None:
-                    # If we manage to restore a `GroupResult`, then it must
-                    # have been complex and saved by `apply_chord()` earlier.
-                    #
-                    # Before we can join the `GroupResult`, it needs to be
-                    # manually marked as ready to avoid blocking
-                    header_result.on_ready()
-                    # We'll `join()` it to get the results and ensure they are
-                    # structured as intended rather than the flattened version
-                    # we'd construct without any other information.
-                    join_func = (
-                        header_result.join_native
-                        if header_result.supports_native_join
-                        else header_result.join
-                    )
-                    with allow_join_result():
-                        resl = join_func(timeout=3.0, propagate=True)
-                else:
-                    # Otherwise simply extract and decode the results we
-                    # stashed along the way, which should be faster for large
-                    # numbers of simple results in the chord header.
-                    decode, unpack = self.decode, self._unpack_chord_result
-                    with client.pipeline() as pipe:
-                        if self._chord_zset:
-                            pipeline = pipe.zrange(jkey, 0, -1)
-                        else:
-                            pipeline = pipe.lrange(jkey, 0, total)
-                        resl, = pipeline.execute()
-                    resl = [unpack(tup, decode) for tup in resl]
-                try:
-                    callback.delay(resl)
-                except Exception as exc:  # pylint: disable=broad-except
-                    logger.exception(
-                        'Chord callback for %r raised: %r', request.group, exc)
-                    return self.chord_error_from_stack(
-                        callback,
-                        ChordError(f'Callback error: {exc!r}'),
-                    )
-                finally:
-                    with client.pipeline() as pipe:
-                        _, _ = pipe \
-                            .delete(jkey) \
-                            .delete(tkey) \
-                            .execute()
-        except ChordError as exc:
-            logger.exception('Chord %r raised: %r', request.group, exc)
-            return self.chord_error_from_stack(callback, exc)
-        except Exception as exc:  # pylint: disable=broad-except
-            logger.exception('Chord %r raised: %r', request.group, exc)
-            return self.chord_error_from_stack(
-                callback,
-                ChordError(f'Join error: {exc!r}'),
-            )
-
-    def _create_client(self, **params):
-        return self._get_client()(
-            connection_pool=self._get_pool(**params),
-        )
-
-    def _get_client(self):
-        return self.redis.StrictRedis
-
-    def _get_pool(self, **params):
-        return self.ConnectionPool(**params)
-
-    @property
-    def ConnectionPool(self):
-        if self._ConnectionPool is None:
-            self._ConnectionPool = self.redis.ConnectionPool
-        return self._ConnectionPool
-
-    @cached_property
-    def client(self):
-        return self._create_client(**self.connparams)
-
-    def __reduce__(self, args=(), kwargs=None):
-        kwargs = {} if not kwargs else kwargs
-        return super().__reduce__(
-            (self.url,), {'expires': self.expires},
-        )
-
-
-class SentinelBackend(RedisBackend):
-    """Redis sentinel task result store."""
-
-    sentinel = getattr(redis, "sentinel", None)
-
-    def __init__(self, *args, **kwargs):
-        if self.sentinel is None:
-            raise ImproperlyConfigured(E_REDIS_SENTINEL_MISSING.strip())
-
-        super().__init__(*args, **kwargs)
-
-    def _params_from_url(self, url, defaults):
-        # URL looks like sentinel://0.0.0.0:26347/3;sentinel://0.0.0.0:26348/3.
-        chunks = url.split(";")
-        connparams = dict(defaults, hosts=[])
-        for chunk in chunks:
-            data = super()._params_from_url(
-                url=chunk, defaults=defaults)
-            connparams['hosts'].append(data)
-        for param in ("host", "port", "db", "password"):
-            connparams.pop(param)
-
-        # Adding db/password in connparams to connect to the correct instance
-        for param in ("db", "password"):
-            if connparams['hosts'] and param in connparams['hosts'][0]:
-                connparams[param] = connparams['hosts'][0].get(param)
-        return connparams
-
-    def _get_sentinel_instance(self, **params):
-        connparams = params.copy()
-
-        hosts = connparams.pop("hosts")
-        min_other_sentinels = self._transport_options.get("min_other_sentinels", 0)
-        sentinel_kwargs = self._transport_options.get("sentinel_kwargs", {})
-
-        sentinel_instance = self.sentinel.Sentinel(
-            [(cp['host'], cp['port']) for cp in hosts],
-            min_other_sentinels=min_other_sentinels,
-            sentinel_kwargs=sentinel_kwargs,
-            **connparams)
-
-        return sentinel_instance
-
-    def _get_pool(self, **params):
-        sentinel_instance = self._get_sentinel_instance(**params)
-
-        master_name = self._transport_options.get("master_name", None)
-
-        return sentinel_instance.master_for(
-            service_name=master_name,
-            redis_class=self._get_client(),
-        ).connection_pool
-
-
-class ClusterBackend(RedisBackend):
-    """Redis task result store."""
-    redis = RedisCluster
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.connparams.pop('db', None)
-
-    def _set(self, key, value):
-        with self.client.pipeline() as pipe:
-            if self.expires:
-                pipe.setex(key, self.expires, value)
-            else:
-                pipe.set(key, value)
-            pipe.execute()
-        self.client.publish(key, value)
-
-    @cached_property
-    def client(self) -> RedisCluster:
-        return RedisCluster(**self.connparams)
+"""Redis result store backend."""
+import time
+from contextlib import contextmanager
+from functools import partial
+from ssl import CERT_NONE, CERT_OPTIONAL, CERT_REQUIRED
+from urllib.parse import unquote
+
+from kombu.utils.functional import retry_over_time
+from kombu.utils.objects import cached_property
+from kombu.utils.url import _parse_url
+
+from celery import states
+from celery._state import task_join_will_block
+from celery.canvas import maybe_signature
+from celery.exceptions import ChordError, ImproperlyConfigured
+from celery.result import GroupResult, allow_join_result
+from celery.utils.functional import dictfilter
+from celery.utils.log import get_logger
+from celery.utils.time import humanize_seconds
+
+from .asynchronous import AsyncBackendMixin, BaseResultConsumer
+from .base import BaseKeyValueStoreBackend
+
+try:
+    import redis.connection
+    from kombu.transport.redis import get_redis_error_classes
+except ImportError:  # pragma: no cover
+    redis = None  # noqa
+    get_redis_error_classes = None  # noqa
+
+try:
+    import redis.sentinel
+    from redis.cluster import RedisCluster
+except ImportError:
+    pass
+
+__all__ = ('RedisBackend', 'SentinelBackend')
+
+E_REDIS_MISSING = """
+You need to install the redis library in order to use \
+the Redis result store backend.
+"""
+
+E_REDIS_SENTINEL_MISSING = """
+You need to install the redis library with support of \
+sentinel in order to use the Redis result store backend.
+"""
+
+W_REDIS_SSL_CERT_OPTIONAL = """
+Setting ssl_cert_reqs=CERT_OPTIONAL when connecting to redis means that \
+celery might not valdate the identity of the redis broker when connecting. \
+This leaves you vulnerable to man in the middle attacks.
+"""
+
+W_REDIS_SSL_CERT_NONE = """
+Setting ssl_cert_reqs=CERT_NONE when connecting to redis means that celery \
+will not valdate the identity of the redis broker when connecting. This \
+leaves you vulnerable to man in the middle attacks.
+"""
+
+E_REDIS_SSL_PARAMS_AND_SCHEME_MISMATCH = """
+SSL connection parameters have been provided but the specified URL scheme \
+is redis://. A Redis SSL connection URL should use the scheme rediss://.
+"""
+
+E_REDIS_SSL_CERT_REQS_MISSING_INVALID = """
+A rediss:// URL must have parameter ssl_cert_reqs and this must be set to \
+CERT_REQUIRED, CERT_OPTIONAL, or CERT_NONE
+"""
+
+E_LOST = 'Connection to Redis lost: Retry (%s/%s) %s.'
+
+E_RETRY_LIMIT_EXCEEDED = """
+Retry limit exceeded while trying to reconnect to the Celery redis result \
+store backend. The Celery application must be restarted.
+"""
+
+logger = get_logger(__name__)
+
+
+class ResultConsumer(BaseResultConsumer):
+    _pubsub = None
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._get_key_for_task = self.backend.get_key_for_task
+        self._decode_result = self.backend.decode_result
+        self._ensure = self.backend.ensure
+        self._connection_errors = self.backend.connection_errors
+        self.subscribed_to = set()
+
+    def on_after_fork(self):
+        try:
+            self.backend.client.connection_pool.reset()
+            if self._pubsub is not None:
+                self._pubsub.close()
+        except KeyError as e:
+            logger.warning(str(e))
+        super().on_after_fork()
+
+    def _reconnect_pubsub(self):
+        self._pubsub = None
+        self.backend.client.connection_pool.reset()
+        # task state might have changed when the connection was down so we
+        # retrieve meta for all subscribed tasks before going into pubsub mode
+        metas = self.backend.client.mget(self.subscribed_to)
+        metas = [meta for meta in metas if meta]
+        for meta in metas:
+            self.on_state_change(self._decode_result(meta), None)
+        self._pubsub = self.backend.client.pubsub(
+            ignore_subscribe_messages=True,
+        )
+        if self.subscribed_to:
+            self._pubsub.subscribe(*self.subscribed_to)
+
+    @contextmanager
+    def reconnect_on_error(self):
+        try:
+            yield
+        except self._connection_errors:
+            try:
+                self._ensure(self._reconnect_pubsub, ())
+            except self._connection_errors:
+                logger.critical(E_RETRY_LIMIT_EXCEEDED)
+                raise
+
+    def _maybe_cancel_ready_task(self, meta):
+        if meta['status'] in states.READY_STATES:
+            self.cancel_for(meta['task_id'])
+
+    def on_state_change(self, meta, message):
+        super().on_state_change(meta, message)
+        self._maybe_cancel_ready_task(meta)
+
+    def start(self, initial_task_id, **kwargs):
+        self._pubsub = self.backend.client.pubsub(
+            ignore_subscribe_messages=True,
+        )
+        self._consume_from(initial_task_id)
+
+    def on_wait_for_pending(self, result, **kwargs):
+        for meta in result._iter_meta(**kwargs):
+            if meta is not None:
+                self.on_state_change(meta, None)
+
+    def stop(self):
+        if self._pubsub is not None:
+            self._pubsub.close()
+
+    def drain_events(self, timeout=None):
+        if self._pubsub:
+            with self.reconnect_on_error():
+                message = self._pubsub.get_message(timeout=timeout)
+                if message and message['type'] == 'message':
+                    self.on_state_change(self._decode_result(message['data']), message)
+        elif timeout:
+            time.sleep(timeout)
+
+    def consume_from(self, task_id):
+        if self._pubsub is None:
+            return self.start(task_id)
+        self._consume_from(task_id)
+
+    def _consume_from(self, task_id):
+        key = self._get_key_for_task(task_id)
+        if key not in self.subscribed_to:
+            self.subscribed_to.add(key)
+            with self.reconnect_on_error():
+                self._pubsub.subscribe(key)
+
+    def cancel_for(self, task_id):
+        key = self._get_key_for_task(task_id)
+        self.subscribed_to.discard(key)
+        if self._pubsub:
+            with self.reconnect_on_error():
+                self._pubsub.unsubscribe(key)
+
+
+class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):
+    """Redis task result store.
+
+    It makes use of the following commands:
+    GET, MGET, DEL, INCRBY, EXPIRE, SET, SETEX
+    """
+
+    ResultConsumer = ResultConsumer
+
+    #: :pypi:`redis` client module.
+    redis = redis
+
+    #: Maximum number of connections in the pool.
+    max_connections = None
+
+    supports_autoexpire = True
+    supports_native_join = True
+
+    def __init__(self, host=None, port=None, db=None, password=None,
+                 max_connections=None, url=None,
+                 connection_pool=None, **kwargs):
+        super().__init__(expires_type=int, **kwargs)
+        _get = self.app.conf.get
+        if self.redis is None:
+            raise ImproperlyConfigured(E_REDIS_MISSING.strip())
+
+        if host and '://' in host:
+            url, host = host, None
+
+        self.max_connections = (
+            max_connections or
+            _get('redis_max_connections') or
+            self.max_connections)
+        self._ConnectionPool = connection_pool
+
+        socket_timeout = _get('redis_socket_timeout')
+        socket_connect_timeout = _get('redis_socket_connect_timeout')
+        retry_on_timeout = _get('redis_retry_on_timeout')
+        socket_keepalive = _get('redis_socket_keepalive')
+
+        self.connparams = {
+            'host': _get('redis_host') or 'localhost',
+            'port': _get('redis_port') or 6379,
+            'db': _get('redis_db') or 0,
+            'password': _get('redis_password'),
+            'max_connections': self.max_connections,
+            'socket_timeout': socket_timeout and float(socket_timeout),
+            'retry_on_timeout': retry_on_timeout or False,
+            'socket_connect_timeout':
+                socket_connect_timeout and float(socket_connect_timeout),
+        }
+
+        # absent in redis.connection.UnixDomainSocketConnection
+        if socket_keepalive:
+            self.connparams['socket_keepalive'] = socket_keepalive
+
+        # "redis_backend_use_ssl" must be a dict with the keys:
+        # 'ssl_cert_reqs', 'ssl_ca_certs', 'ssl_certfile', 'ssl_keyfile'
+        # (the same as "broker_use_ssl")
+        ssl = _get('redis_backend_use_ssl')
+        if ssl:
+            self.connparams.update(ssl)
+            self.connparams['connection_class'] = redis.SSLConnection
+
+        if url:
+            self.connparams = self._params_from_url(url, self.connparams)
+
+        # If we've received SSL parameters via query string or the
+        # redis_backend_use_ssl dict, check ssl_cert_reqs is valid. If set
+        # via query string ssl_cert_reqs will be a string so convert it here
+        if ('connection_class' in self.connparams and
+                self.connparams['connection_class'] is redis.SSLConnection):
+            ssl_cert_reqs_missing = 'MISSING'
+            ssl_string_to_constant = {'CERT_REQUIRED': CERT_REQUIRED,
+                                      'CERT_OPTIONAL': CERT_OPTIONAL,
+                                      'CERT_NONE': CERT_NONE,
+                                      'required': CERT_REQUIRED,
+                                      'optional': CERT_OPTIONAL,
+                                      'none': CERT_NONE}
+            ssl_cert_reqs = self.connparams.get('ssl_cert_reqs', ssl_cert_reqs_missing)
+            ssl_cert_reqs = ssl_string_to_constant.get(ssl_cert_reqs, ssl_cert_reqs)
+            if ssl_cert_reqs not in ssl_string_to_constant.values():
+                raise ValueError(E_REDIS_SSL_CERT_REQS_MISSING_INVALID)
+
+            if ssl_cert_reqs == CERT_OPTIONAL:
+                logger.warning(W_REDIS_SSL_CERT_OPTIONAL)
+            elif ssl_cert_reqs == CERT_NONE:
+                logger.warning(W_REDIS_SSL_CERT_NONE)
+            self.connparams['ssl_cert_reqs'] = ssl_cert_reqs
+
+        self.url = url
+
+        self.connection_errors, self.channel_errors = \
+            self.get_redis_error_classes()
+        self.result_consumer = self.ResultConsumer(
+            self, self.app, self.accept,
+            self._pending_results, self._pending_messages,
+        )
+
+    def get_redis_error_classes(self):
+        if get_redis_error_classes:
+            return get_redis_error_classes()
+        return ((), ())
+
+    def _params_from_url(self, url, defaults):
+        scheme, host, port, _, password, path, query = _parse_url(url)
+        connparams = dict(
+            defaults, **dictfilter({
+                'host': host, 'port': port, 'password': password,
+                'db': query.pop('virtual_host', None)})
+        )
+
+        if scheme == 'socket':
+            # use 'path' as path to the socket… in this case
+            # the database number should be given in 'query'
+            connparams.update({
+                'connection_class': self.redis.UnixDomainSocketConnection,
+                'path': '/' + path,
+            })
+            # host+port are invalid options when using this connection type.
+            connparams.pop('host', None)
+            connparams.pop('port', None)
+            connparams.pop('socket_connect_timeout')
+        else:
+            connparams['db'] = path
+
+        ssl_param_keys = ['ssl_ca_certs', 'ssl_certfile', 'ssl_keyfile',
+                          'ssl_cert_reqs']
+
+        if scheme == 'redis':
+            # If connparams or query string contain ssl params, raise error
+            if (any(key in connparams for key in ssl_param_keys) or
+                    any(key in query for key in ssl_param_keys)):
+                raise ValueError(E_REDIS_SSL_PARAMS_AND_SCHEME_MISMATCH)
+
+        if scheme == 'rediss':
+            connparams['connection_class'] = redis.SSLConnection
+            # The following parameters, if present in the URL, are encoded. We
+            # must add the decoded values to connparams.
+            for ssl_setting in ssl_param_keys:
+                ssl_val = query.pop(ssl_setting, None)
+                if ssl_val:
+                    connparams[ssl_setting] = unquote(ssl_val)
+
+        # db may be string and start with / like in kombu.
+        db = connparams.get('db') or 0
+        db = db.strip('/') if isinstance(db, str) else db
+        connparams['db'] = int(db)
+
+        for key, value in query.items():
+            if key in redis.connection.URL_QUERY_ARGUMENT_PARSERS:
+                query[key] = redis.connection.URL_QUERY_ARGUMENT_PARSERS[key](
+                    value
+                )
+
+        # Query parameters override other parameters
+        connparams.update(query)
+        return connparams
+
+    @cached_property
+    def retry_policy(self):
+        retry_policy = super().retry_policy
+        if "retry_policy" in self._transport_options:
+            retry_policy = retry_policy.copy()
+            retry_policy.update(self._transport_options['retry_policy'])
+
+        return retry_policy
+
+    def on_task_call(self, producer, task_id):
+        if not task_join_will_block():
+            self.result_consumer.consume_from(task_id)
+
+    def get(self, key):
+        return self.client.get(key)
+
+    def mget(self, keys):
+        return self.client.mget(keys)
+
+    def ensure(self, fun, args, **policy):
+        retry_policy = dict(self.retry_policy, **policy)
+        max_retries = retry_policy.get('max_retries')
+        return retry_over_time(
+            fun, self.connection_errors, args, {},
+            partial(self.on_connection_error, max_retries),
+            **retry_policy)
+
+    def on_connection_error(self, max_retries, exc, intervals, retries):
+        tts = next(intervals)
+        logger.error(
+            E_LOST.strip(),
+            retries, max_retries or 'Inf', humanize_seconds(tts, 'in '))
+        return tts
+
+    def set(self, key, value, **retry_policy):
+        return self.ensure(self._set, (key, value), **retry_policy)
+
+    def _set(self, key, value):
+        with self.client.pipeline() as pipe:
+            if self.expires:
+                pipe.setex(key, self.expires, value)
+            else:
+                pipe.set(key, value)
+            pipe.publish(key, value)
+            pipe.execute()
+
+    def forget(self, task_id):
+        super().forget(task_id)
+        self.result_consumer.cancel_for(task_id)
+
+    def delete(self, key):
+        self.client.delete(key)
+
+    def incr(self, key):
+        return self.client.incr(key)
+
+    def expire(self, key, value):
+        return self.client.expire(key, value)
+
+    def add_to_chord(self, group_id, result):
+        self.client.incr(self.get_key_for_group(group_id, '.t'), 1)
+
+    def _unpack_chord_result(self, tup, decode,
+                             EXCEPTION_STATES=states.EXCEPTION_STATES,
+                             PROPAGATE_STATES=states.PROPAGATE_STATES):
+        _, tid, state, retval = decode(tup)
+        if state in EXCEPTION_STATES:
+            retval = self.exception_to_python(retval)
+        if state in PROPAGATE_STATES:
+            raise ChordError(f'Dependency {tid} raised {retval!r}')
+        return retval
+
+    def apply_chord(self, header_result, body, **kwargs):
+        # If any of the child results of this chord are complex (ie. group
+        # results themselves), we need to save `header_result` to ensure that
+        # the expected structure is retained when we finish the chord and pass
+        # the results onward to the body in `on_chord_part_return()`. We don't
+        # do this is all cases to retain an optimisation in the common case
+        # where a chord header is comprised of simple result objects.
+        if any(isinstance(nr, GroupResult) for nr in header_result.results):
+            header_result.save(backend=self)
+
+    @cached_property
+    def _chord_zset(self):
+        return self._transport_options.get('result_chord_ordered', True)
+
+    @cached_property
+    def _transport_options(self):
+        return self.app.conf.get('result_backend_transport_options', {})
+
+    def on_chord_part_return(self, request, state, result,
+                             propagate=None, **kwargs):
+        app = self.app
+        tid, gid, group_index = request.id, request.group, request.group_index
+        if not gid or not tid:
+            return
+        if group_index is None:
+            group_index = '+inf'
+
+        client = self.client
+        jkey = self.get_key_for_group(gid, '.j')
+        tkey = self.get_key_for_group(gid, '.t')
+        result = self.encode_result(result, state)
+        encoded = self.encode([1, tid, state, result])
+        with client.pipeline() as pipe:
+            pipeline = (
+                pipe.zadd(jkey, {encoded: group_index}).zcount(jkey, "-inf", "+inf")
+                if self._chord_zset
+                else pipe.rpush(jkey, encoded).llen(jkey)
+            ).get(tkey)
+            if self.expires:
+                pipeline = pipeline \
+                    .expire(jkey, self.expires) \
+                    .expire(tkey, self.expires)
+
+            _, readycount, totaldiff = pipeline.execute()[:3]
+
+        totaldiff = int(totaldiff or 0)
+
+        try:
+            callback = maybe_signature(request.chord, app=app)
+            total = callback['chord_size'] + totaldiff
+            if readycount == total:
+                header_result = GroupResult.restore(gid)
+                if header_result is not None:
+                    # If we manage to restore a `GroupResult`, then it must
+                    # have been complex and saved by `apply_chord()` earlier.
+                    #
+                    # Before we can join the `GroupResult`, it needs to be
+                    # manually marked as ready to avoid blocking
+                    header_result.on_ready()
+                    # We'll `join()` it to get the results and ensure they are
+                    # structured as intended rather than the flattened version
+                    # we'd construct without any other information.
+                    join_func = (
+                        header_result.join_native
+                        if header_result.supports_native_join
+                        else header_result.join
+                    )
+                    with allow_join_result():
+                        resl = join_func(timeout=3.0, propagate=True)
+                else:
+                    # Otherwise simply extract and decode the results we
+                    # stashed along the way, which should be faster for large
+                    # numbers of simple results in the chord header.
+                    decode, unpack = self.decode, self._unpack_chord_result
+                    with client.pipeline() as pipe:
+                        if self._chord_zset:
+                            pipeline = pipe.zrange(jkey, 0, -1)
+                        else:
+                            pipeline = pipe.lrange(jkey, 0, total)
+                        resl, = pipeline.execute()
+                    resl = [unpack(tup, decode) for tup in resl]
+                try:
+                    callback.delay(resl)
+                except Exception as exc:  # pylint: disable=broad-except
+                    logger.exception(
+                        'Chord callback for %r raised: %r', request.group, exc)
+                    return self.chord_error_from_stack(
+                        callback,
+                        ChordError(f'Callback error: {exc!r}'),
+                    )
+                finally:
+                    with client.pipeline() as pipe:
+                        _, _ = pipe \
+                            .delete(jkey) \
+                            .delete(tkey) \
+                            .execute()
+        except ChordError as exc:
+            logger.exception('Chord %r raised: %r', request.group, exc)
+            return self.chord_error_from_stack(callback, exc)
+        except Exception as exc:  # pylint: disable=broad-except
+            logger.exception('Chord %r raised: %r', request.group, exc)
+            return self.chord_error_from_stack(
+                callback,
+                ChordError(f'Join error: {exc!r}'),
+            )
+
+    def _create_client(self, **params):
+        return self._get_client()(
+            connection_pool=self._get_pool(**params),
+        )
+
+    def _get_client(self):
+        return self.redis.StrictRedis
+
+    def _get_pool(self, **params):
+        return self.ConnectionPool(**params)
+
+    @property
+    def ConnectionPool(self):
+        if self._ConnectionPool is None:
+            self._ConnectionPool = self.redis.ConnectionPool
+        return self._ConnectionPool
+
+    @cached_property
+    def client(self):
+        return self._create_client(**self.connparams)
+
+    def __reduce__(self, args=(), kwargs=None):
+        kwargs = {} if not kwargs else kwargs
+        return super().__reduce__(
+            (self.url,), {'expires': self.expires},
+        )
+
+
+class SentinelBackend(RedisBackend):
+    """Redis sentinel task result store."""
+
+    sentinel = getattr(redis, "sentinel", None)
+
+    def __init__(self, *args, **kwargs):
+        if self.sentinel is None:
+            raise ImproperlyConfigured(E_REDIS_SENTINEL_MISSING.strip())
+
+        super().__init__(*args, **kwargs)
+
+    def _params_from_url(self, url, defaults):
+        # URL looks like sentinel://0.0.0.0:26347/3;sentinel://0.0.0.0:26348/3.
+        chunks = url.split(";")
+        connparams = dict(defaults, hosts=[])
+        for chunk in chunks:
+            data = super()._params_from_url(
+                url=chunk, defaults=defaults)
+            connparams['hosts'].append(data)
+        for param in ("host", "port", "db", "password"):
+            connparams.pop(param)
+
+        # Adding db/password in connparams to connect to the correct instance
+        for param in ("db", "password"):
+            if connparams['hosts'] and param in connparams['hosts'][0]:
+                connparams[param] = connparams['hosts'][0].get(param)
+        return connparams
+
+    def _get_sentinel_instance(self, **params):
+        connparams = params.copy()
+
+        hosts = connparams.pop("hosts")
+        min_other_sentinels = self._transport_options.get("min_other_sentinels", 0)
+        sentinel_kwargs = self._transport_options.get("sentinel_kwargs", {})
+
+        sentinel_instance = self.sentinel.Sentinel(
+            [(cp['host'], cp['port']) for cp in hosts],
+            min_other_sentinels=min_other_sentinels,
+            sentinel_kwargs=sentinel_kwargs,
+            **connparams)
+
+        return sentinel_instance
+
+    def _get_pool(self, **params):
+        sentinel_instance = self._get_sentinel_instance(**params)
+
+        master_name = self._transport_options.get("master_name", None)
+
+        return sentinel_instance.master_for(
+            service_name=master_name,
+            redis_class=self._get_client(),
+        ).connection_pool
+
+
+class ClusterBackend(RedisBackend):
+    """Redis task result store."""
+    redis = RedisCluster
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.connparams.pop('db', None)
+
+    def _set(self, key, value):
+        with self.client.pipeline() as pipe:
+            if self.expires:
+                pipe.setex(key, self.expires, value)
+            else:
+                pipe.set(key, value)
+            pipe.execute()
+        self.client.publish(key, value)
+
+    @cached_property
+    def client(self) -> RedisCluster:
+        return RedisCluster(**self.connparams)
```

## celery/concurrency/asynpool.py

```diff
@@ -1,1520 +1,1497 @@
-"""Version of multiprocessing.Pool using Async I/O.
-
-.. note::
-
-    This module will be moved soon, so don't use it directly.
-
-This is a non-blocking version of :class:`multiprocessing.Pool`.
-
-This code deals with three major challenges:
-
-#. Starting up child processes and keeping them running.
-#. Sending jobs to the processes and receiving results back.
-#. Safely shutting down this system.
-"""
-import errno
-import gc
-import os
-import select
-import time
-import sys
-from collections import Counter, deque, namedtuple
-from io import BytesIO
-from numbers import Integral
-from pickle import HIGHEST_PROTOCOL
-from struct import pack, unpack, unpack_from
-from time import sleep
-from weakref import WeakValueDictionary, ref
-
-from billiard import pool as _pool
-from billiard.compat import buf_t, isblocking, setblocking
-from billiard.five import values
-from billiard.pool import (
-    ACK, NACK, RUN, TERMINATE, READY,
-    TASK, EX_OK, EX_FAILURE, EX_RECYCLE, MAXMEM_USED_FMT,
-    WorkersJoined, ExceptionInfo, MaybeEncodingError,
-    mem_rss,
-)
-from billiard.queues import _SimpleQueue
-from kombu.asynchronous import ERR, WRITE
-from kombu.serialization import pickle as _pickle
-from kombu.utils.eventio import SELECT_BAD_FD
-from kombu.utils.functional import fxrange
-from vine import promise
-
-from celery.utils.functional import noop
-from celery.utils.log import get_logger
-from celery.utils.psutil import process_memory_percent
-from celery.worker import state as worker_state
-
-# pylint: disable=redefined-outer-name
-# We cache globals and attribute lookups, so disable this warning.
-
-try:
-    from _billiard import read as __read__
-    readcanbuf = True
-
-except ImportError:  # pragma: no cover
-
-    def __read__(fd, buf, size, read=os.read):  # noqa
-        chunk = read(fd, size)
-        n = len(chunk)
-        if n != 0:
-            buf.write(chunk)
-        return n
-    readcanbuf = False  # noqa
-
-    def unpack_from(fmt, iobuf, unpack=unpack):  # noqa
-        return unpack(fmt, iobuf.getvalue())  # <-- BytesIO
-
-__all__ = ('AsynPool',)
-
-logger = get_logger(__name__)
-error, warning, info, debug = \
-    logger.error, logger.warning, logger.info, logger.debug
-
-UNAVAIL = frozenset({errno.EAGAIN, errno.EINTR})
-
-#: Constant sent by child process when started (ready to accept work)
-WORKER_UP = 15
-
-#: A process must've started before this timeout (in secs.) expires.
-PROC_ALIVE_TIMEOUT = 4.0
-
-SCHED_STRATEGY_FCFS = 1
-SCHED_STRATEGY_FAIR = 4
-
-SCHED_STRATEGIES = {
-    None: SCHED_STRATEGY_FAIR,
-    'default': SCHED_STRATEGY_FAIR,
-    'fast': SCHED_STRATEGY_FCFS,
-    'fcfs': SCHED_STRATEGY_FCFS,
-    'fair': SCHED_STRATEGY_FAIR,
-}
-SCHED_STRATEGY_TO_NAME = {v: k for k, v in SCHED_STRATEGIES.items()}
-
-Ack = namedtuple('Ack', ('id', 'fd', 'payload'))
-
-
-def gen_not_started(gen):
-    """Return true if generator is not started."""
-    # gi_frame is None when generator stopped.
-    return gen.gi_frame and gen.gi_frame.f_lasti == -1
-
-
-def _get_job_writer(job):
-    try:
-        writer = job._writer
-    except AttributeError:
-        pass
-    else:
-        return writer()  # is a weakref
-
-
-if hasattr(select, 'poll'):
-    def _select_imp(readers=None, writers=None, err=None, timeout=0,
-                    poll=select.poll, POLLIN=select.POLLIN,
-                    POLLOUT=select.POLLOUT, POLLERR=select.POLLERR):
-        poller = poll()
-        register = poller.register
-
-        if readers:
-            [register(fd, POLLIN) for fd in readers]
-        if writers:
-            [register(fd, POLLOUT) for fd in writers]
-        if err:
-            [register(fd, POLLERR) for fd in err]
-
-        R, W = set(), set()
-        timeout = 0 if timeout and timeout < 0 else round(timeout * 1e3)
-        events = poller.poll(timeout)
-        for fd, event in events:
-            if not isinstance(fd, Integral):
-                fd = fd.fileno()
-            if event & POLLIN:
-                R.add(fd)
-            if event & POLLOUT:
-                W.add(fd)
-            if event & POLLERR:
-                R.add(fd)
-        return R, W, 0
-else:
-    def _select_imp(readers=None, writers=None, err=None, timeout=0):
-        r, w, e = select.select(readers, writers, err, timeout)
-        if e:
-            r = list(set(r) | set(e))
-        return r, w, 0
-
-
-def _select(readers=None, writers=None, err=None, timeout=0,
-            poll=_select_imp):
-    """Simple wrapper to :class:`~select.select`, using :`~select.poll`.
-
-    Arguments:
-        readers (Set[Fd]): Set of reader fds to test if readable.
-        writers (Set[Fd]): Set of writer fds to test if writable.
-        err (Set[Fd]): Set of fds to test for error condition.
-
-    All fd sets passed must be mutable as this function
-    will remove non-working fds from them, this also means
-    the caller must make sure there are still fds in the sets
-    before calling us again.
-
-    Returns:
-        Tuple[Set, Set, Set]: of ``(readable, writable, again)``, where
-        ``readable`` is a set of fds that have data available for read,
-        ``writable`` is a set of fds that's ready to be written to
-        and ``again`` is a flag that if set means the caller must
-        throw away the result and call us again.
-    """
-    readers = set() if readers is None else readers
-    writers = set() if writers is None else writers
-    err = set() if err is None else err
-    try:
-        return poll(readers, writers, err, timeout)
-    except OSError as exc:
-        _errno = exc.errno
-
-        if _errno == errno.EINTR:
-            return set(), set(), 1
-        elif _errno in SELECT_BAD_FD:
-            for fd in readers | writers | err:
-                try:
-                    select.select([fd], [], [], 0)
-                except OSError as exc:
-                    _errno = exc.errno
-
-                    if _errno not in SELECT_BAD_FD:
-                        raise
-                    readers.discard(fd)
-                    writers.discard(fd)
-                    err.discard(fd)
-            return set(), set(), 1
-        else:
-            raise
-
-
-def iterate_file_descriptors_safely(fds_iter, source_data,
-                                    hub_method, *args, **kwargs):
-    """Apply hub method to fds in iter, remove from list if failure.
-
-    Some file descriptors may become stale through OS reasons
-    or possibly other reasons, so safely manage our lists of FDs.
-    :param fds_iter: the file descriptors to iterate and apply hub_method
-    :param source_data: data source to remove FD if it renders OSError
-    :param hub_method: the method to call with with each fd and kwargs
-    :*args to pass through to the hub_method;
-    with a special syntax string '*fd*' represents a substitution
-    for the current fd object in the iteration (for some callers).
-    :**kwargs to pass through to the hub method (no substitutions needed)
-    """
-    def _meta_fd_argument_maker():
-        # uses the current iterations value for fd
-        call_args = args
-        if "*fd*" in call_args:
-            call_args = [fd if arg == "*fd*" else arg for arg in args]
-        return call_args
-    # Track stale FDs for cleanup possibility
-    stale_fds = []
-    for fd in fds_iter:
-        # Handle using the correct arguments to the hub method
-        hub_args, hub_kwargs = _meta_fd_argument_maker(), kwargs
-        try:  # Call the hub method
-            hub_method(fd, *hub_args, **hub_kwargs)
-        except (OSError, FileNotFoundError):
-            logger.warning(
-                "Encountered OSError when accessing fd %s ",
-                fd, exc_info=True)
-            stale_fds.append(fd)  # take note of stale fd
-    # Remove now defunct fds from the managed list
-    if source_data:
-        for fd in stale_fds:
-            try:
-                if hasattr(source_data, 'remove'):
-                    source_data.remove(fd)
-                else:  # then not a list/set ... try dict
-                    source_data.pop(fd, None)
-            except ValueError:
-                logger.warning("ValueError trying to invalidate %s from %s",
-                               fd, source_data)
-
-
-class Worker(_pool.Worker):
-    """Pool worker process."""
-    def __init__(self, inq, outq, synq=None, initializer=None, initargs=(),
-                 maxtasks=None, sentinel=None, on_exit=None,
-                 sigprotection=True, wrap_exception=True,
-                 max_memory_per_child=None, on_ready_counter=None,
-                 status=None):
-        self.status = status
-        super().__init__(
-            inq, outq, synq, initializer, initargs,
-            maxtasks, sentinel, on_exit,
-            sigprotection, wrap_exception,
-            max_memory_per_child, on_ready_counter
-        )
-
-    def on_loop_start(self, pid):
-        # our version sends a WORKER_UP message when the process is ready
-        # to accept work, this will tell the parent that the inqueue fd
-        # is writable.
-        self.outq.put((WORKER_UP, (pid,)))
-
-    def workloop(self, debug=debug, now=time.monotonic, pid=None):
-        pid = pid or os.getpid()
-        put = self.outq.put
-        inqW_fd = self.inqW_fd
-        synqW_fd = self.synqW_fd
-        maxtasks = self.maxtasks
-        max_memory_per_child = self.max_memory_per_child or 0
-        prepare_result = self.prepare_result
-
-        wait_for_job = self.wait_for_job
-        _wait_for_syn = self.wait_for_syn
-
-        def _mark_idle():
-            if self.status is not None:
-                self.status.value = 0
-
-        def _mark_busy():
-            if self.status is not None:
-                self.status.value = 1
-
-        info = logger.info
-
-        def wait_for_syn(jid):
-            i = 0
-            while 1:
-                if i > 60:
-                    error('!!!WAIT FOR ACK TIMEOUT: job:%r fd:%r!!!',
-                          jid, self.synq._reader.fileno(), exc_info=1)
-                req = _wait_for_syn()
-                if req:
-                    type_, args = req
-                    if type_ == NACK:
-                        return False
-                    assert type_ == ACK
-                    return True
-                i += 1
-
-        completed = 0
-        try:
-            while maxtasks is None or (maxtasks and completed < maxtasks):
-                _mark_idle()
-                req = wait_for_job()
-                if req:
-                    type_, args_ = req
-                    assert type_ == TASK
-                    job, i, fun, args, kwargs = args_
-                    _mark_busy()
-                    info(f"Receive req[{args[1]}]")
-                    put((ACK, (job, i, now(), pid, synqW_fd)))
-                    if _wait_for_syn:
-                        confirm = wait_for_syn(job)
-                        if not confirm:
-                            continue  # received NACK
-                    try:
-                        result = (True, prepare_result(fun(*args, **kwargs)))
-                    except Exception:
-                        result = (False, ExceptionInfo())
-                    try:
-                        put((READY, (job, i, result, inqW_fd)))
-                    except Exception as exc:
-                        _, _, tb = sys.exc_info()
-                        try:
-                            wrapped = MaybeEncodingError(exc, result[1])
-                            einfo = ExceptionInfo((
-                                MaybeEncodingError, wrapped, tb,
-                            ))
-                            put((READY, (job, i, (False, einfo), inqW_fd)))
-                        finally:
-                            del tb
-                    completed += 1
-                    if max_memory_per_child > 0:
-                        used_kb = mem_rss()
-                        if used_kb <= 0:
-                            error('worker unable to determine memory usage')
-                        if used_kb > 0 and used_kb > max_memory_per_child:
-                            warning(MAXMEM_USED_FMT.format(
-                                used_kb, max_memory_per_child))
-                            return EX_RECYCLE
-
-            debug('worker exiting after %d tasks', completed)
-            if maxtasks:
-                return EX_RECYCLE if completed == maxtasks else EX_FAILURE
-            return EX_OK
-        finally:
-            # Before exiting the worker, we want to ensure that that all
-            # messages produced by the worker have been consumed by the main
-            # process. This prevents the worker being terminated prematurely
-            # and messages being lost.
-            self._ensure_messages_consumed(completed=completed)
-            _mark_idle()
-
-    def __reduce__(self):
-        return self.__class__, (
-            self.inq, self.outq, self.synq, self.initializer,
-            self.initargs, self.maxtasks, self._shutdown, self.on_exit,
-            self.sigprotection, self.wrap_exception, self.max_memory_per_child,
-            self.on_ready_counter
-        )
-
-
-class ResultHandler(_pool.ResultHandler):
-    """Handles messages from the pool processes."""
-
-    def __init__(self, *args, **kwargs):
-        self.fileno_to_outq = kwargs.pop('fileno_to_outq')
-        self.on_process_alive = kwargs.pop('on_process_alive')
-        super().__init__(*args, **kwargs)
-        # add our custom message handler
-        self.state_handlers[WORKER_UP] = self.on_process_alive
-
-    def _recv_message(self, add_reader, fd, callback,
-                      __read__=__read__, readcanbuf=readcanbuf,
-                      BytesIO=BytesIO, unpack_from=unpack_from,
-                      load=_pickle.load):
-        Hr = Br = 0
-        if readcanbuf:
-            buf = bytearray(4)
-            bufv = memoryview(buf)
-        else:
-            buf = bufv = BytesIO()
-        # header
-
-        while Hr < 4:
-            try:
-                n = __read__(
-                    fd, bufv[Hr:] if readcanbuf else bufv, 4 - Hr,
-                )
-            except OSError as exc:
-                if exc.errno not in UNAVAIL:
-                    raise
-                yield
-            else:
-                if n == 0:
-                    raise (OSError('End of file during message') if Hr
-                           else EOFError())
-                Hr += n
-
-        body_size, = unpack_from('>i', bufv)
-        if readcanbuf:
-            buf = bytearray(body_size)
-            bufv = memoryview(buf)
-        else:
-            buf = bufv = BytesIO()
-
-        while Br < body_size:
-            try:
-                n = __read__(
-                    fd, bufv[Br:] if readcanbuf else bufv, body_size - Br,
-                )
-            except OSError as exc:
-                if exc.errno not in UNAVAIL:
-                    raise
-                yield
-            else:
-                if n == 0:
-                    raise (OSError('End of file during message') if Br
-                           else EOFError())
-                Br += n
-        add_reader(fd, self.handle_event, fd)
-        if readcanbuf:
-            message = load(BytesIO(bufv))
-        else:
-            bufv.seek(0)
-            message = load(bufv)
-        if message:
-            callback(message)
-
-    def _make_process_result(self, hub):
-        """Coroutine reading messages from the pool processes."""
-        fileno_to_outq = self.fileno_to_outq
-        on_state_change = self.on_state_change
-        add_reader = hub.add_reader
-        remove_reader = hub.remove_reader
-        recv_message = self._recv_message
-
-        def on_result_readable(fileno):
-            try:
-                fileno_to_outq[fileno]
-            except KeyError:  # process gone
-                return remove_reader(fileno)
-            it = recv_message(add_reader, fileno, on_state_change)
-            try:
-                next(it)
-            except StopIteration:
-                pass
-            except (OSError, EOFError):
-                remove_reader(fileno)
-            else:
-                add_reader(fileno, it)
-        return on_result_readable
-
-    def register_with_event_loop(self, hub):
-        self.handle_event = self._make_process_result(hub)
-
-    def handle_event(self, *args):
-        # pylint: disable=method-hidden
-        #   register_with_event_loop overrides this
-        raise RuntimeError('Not registered with event loop')
-
-    def on_stop_not_started(self):
-        # This is always used, since we do not start any threads.
-        cache = self.cache
-        check_timeouts = self.check_timeouts
-        fileno_to_outq = self.fileno_to_outq
-        on_state_change = self.on_state_change
-        join_exited_workers = self.join_exited_workers
-
-        # flush the processes outqueues until they've all terminated.
-        outqueues = set(fileno_to_outq)
-        while cache and outqueues and self._state != TERMINATE:
-            if check_timeouts is not None:
-                # make sure tasks with a time limit will time out.
-                check_timeouts()
-            # cannot iterate and remove at the same time
-            pending_remove_fd = set()
-            for fd in outqueues:
-                iterate_file_descriptors_safely(
-                    [fd], self.fileno_to_outq, self._flush_outqueue,
-                    pending_remove_fd.add, fileno_to_outq, on_state_change
-                )
-                try:
-                    join_exited_workers(shutdown=True)
-                except WorkersJoined:
-                    debug('result handler: all workers terminated')
-                    return
-            outqueues.difference_update(pending_remove_fd)
-
-    def _flush_outqueue(self, fd, remove, process_index, on_state_change):
-        try:
-            proc = process_index[fd]
-        except KeyError:
-            # process already found terminated
-            # this means its outqueue has already been processed
-            # by the worker lost handler.
-            return remove(fd)
-
-        reader = proc.outq._reader
-        try:
-            setblocking(reader, 1)
-        except OSError:
-            return remove(fd)
-        try:
-            if reader.poll(0):
-                task = reader.recv()
-            else:
-                task = None
-                sleep(0.5)
-        except (OSError, EOFError):
-            return remove(fd)
-        else:
-            if task:
-                on_state_change(task)
-        finally:
-            try:
-                setblocking(reader, 0)
-            except OSError:
-                return remove(fd)
-
-
-class AsynPool(_pool.Pool):
-    """AsyncIO Pool (no threads)."""
-
-    ResultHandler = ResultHandler
-    Worker = Worker
-
-    def WorkerProcess(self, worker):
-        worker = super().WorkerProcess(worker)
-        worker.dead = False
-        return worker
-
-    def __init__(self, processes=None, synack=False,
-                 sched_strategy=None, proc_alive_timeout=None,
-                 *args, **kwargs):
-        self.sched_strategy = SCHED_STRATEGIES.get(sched_strategy,
-                                                   sched_strategy)
-        processes = self.cpu_count() if processes is None else processes
-        self.synack = synack
-        # create queue-pairs for all our processes in advance.
-        self._queues = {
-            self.create_process_queues(): None for _ in range(processes)
-        }
-
-        # inqueue fileno -> process mapping
-        self._fileno_to_inq = {}
-        # outqueue fileno -> process mapping
-        self._fileno_to_outq = {}
-        # synqueue fileno -> process mapping
-        self._fileno_to_synq = {}
-
-        # We keep track of processes that haven't yet
-        # sent a WORKER_UP message.  If a process fails to send
-        # this message within _proc_alive_timeout we terminate it
-        # and hope the next process will recover.
-        self._proc_alive_timeout = (
-            PROC_ALIVE_TIMEOUT if proc_alive_timeout is None
-            else proc_alive_timeout
-        )
-        self._waiting_to_start = set()
-
-        # denormalized set of all inqueues.
-        self._all_inqueues = set()
-
-        # Set of fds being written to (busy)
-        self._active_writes = set()
-
-        # Set of active co-routines currently writing jobs.
-        self._active_writers = set()
-
-        # Set of fds that are busy (executing task)
-        self._busy_workers = set()
-        self._mark_worker_as_available = self._busy_workers.discard
-
-        # Holds jobs waiting to be written to child processes.
-        self.outbound_buffer = deque()
-
-        self.write_stats = Counter()
-
-        super().__init__(processes, *args, **kwargs)
-
-        for proc in self._pool:
-            # create initial mappings, these will be updated
-            # as processes are recycled, or found lost elsewhere.
-            self._fileno_to_outq[proc.outqR_fd] = proc
-            self._fileno_to_synq[proc.synqW_fd] = proc
-
-        self.on_soft_timeout = getattr(
-            self._timeout_handler, 'on_soft_timeout', noop,
-        )
-        self.on_hard_timeout = getattr(
-            self._timeout_handler, 'on_hard_timeout', noop,
-        )
-        self._worker_status = {}
-
-    def _create_worker_process(self, i):
-        gc.collect()  # Issue #2927
-        sentinel = self._ctx.Event() if self.allow_restart else None
-        inq, outq, synq = self.get_process_queues()
-        on_ready_counter = self._ctx.Value('i')
-        status = self._ctx.Value('H')
-        w = self.WorkerProcess(self.Worker(
-            inq, outq, synq, self._initializer, self._initargs,
-            self._maxtasksperchild, sentinel, self._on_process_exit,
-            # Need to handle all signals if using the ipc semaphore,
-            # to make sure the semaphore is released.
-            sigprotection=self.threads,
-            wrap_exception=self._wrap_exception,
-            max_memory_per_child=self._max_memory_per_child,
-            on_ready_counter=on_ready_counter,
-            status=status
-        ))
-        self._pool.append(w)
-        self._process_register_queues(w, (inq, outq, synq))
-        w.name = w.name.replace('Process', 'PoolWorker')
-        w.daemon = True
-        w.index = i
-        w.start()
-        self._poolctrl[w.pid] = sentinel
-        self._on_ready_counters[w.pid] = on_ready_counter
-        self._worker_status[w.pid] = status
-        try:
-            w.base_mem_used = process_memory_percent(w.pid)
-        except Exception:   # noqa
-            w.base_mem_used = None
-        if self.on_process_up:
-            self.on_process_up(w)
-        return w
-
-    def _event_process_exit(self, hub, proc):
-        # This method is called whenever the process sentinel is readable.
-        self._untrack_child_process(proc, hub)
-        self.maintain_pool()
-
-    def _track_child_process(self, proc, hub):
-        """Helper method determines appropriate fd for process."""
-        try:
-            fd = proc._sentinel_poll
-        except AttributeError:
-            # we need to duplicate the fd here to carefully
-            # control when the fd is removed from the process table,
-            # as once the original fd is closed we cannot unregister
-            # the fd from epoll(7) anymore, causing a 100% CPU poll loop.
-            fd = proc._sentinel_poll = os.dup(proc._popen.sentinel)
-        # Safely call hub.add_reader for the determined fd
-        iterate_file_descriptors_safely(
-            [fd], None, hub.add_reader,
-            self._event_process_exit, hub, proc)
-
-    def _untrack_child_process(self, proc, hub):
-        if proc._sentinel_poll is not None:
-            fd, proc._sentinel_poll = proc._sentinel_poll, None
-            hub.remove(fd)
-            os.close(fd)
-
-    def register_with_event_loop(self, hub):
-        """Register the async pool with the current event loop."""
-        self._result_handler.register_with_event_loop(hub)
-        self.handle_result_event = self._result_handler.handle_event
-        self._create_timelimit_handlers(hub)
-        self._create_process_handlers(hub)
-        self._create_write_handlers(hub)
-
-        # Add handler for when a process exits (calls maintain_pool)
-        [self._track_child_process(w, hub) for w in self._pool]
-        # Handle_result_event is called whenever one of the
-        # result queues are readable.
-        iterate_file_descriptors_safely(
-            self._fileno_to_outq, self._fileno_to_outq, hub.add_reader,
-            self.handle_result_event, '*fd*')
-
-        # Timers include calling maintain_pool at a regular interval
-        # to be certain processes are restarted.
-        for handler, interval in self.timers.items():
-            hub.call_repeatedly(interval, handler)
-
-        hub.on_tick.add(self.on_poll_start)
-
-    def _create_timelimit_handlers(self, hub):
-        """Create handlers used to implement time limits."""
-        call_later = hub.call_later
-        trefs = self._tref_for_id = WeakValueDictionary()
-
-        def on_timeout_set(R, soft, hard):
-            if soft:
-                trefs[R._job] = call_later(
-                    soft, self._on_soft_timeout, R._job, soft, hard, hub,
-                )
-            elif hard:
-                trefs[R._job] = call_later(
-                    hard, self._on_hard_timeout, R._job,
-                )
-        self.on_timeout_set = on_timeout_set
-
-        def _discard_tref(job):
-            try:
-                tref = trefs.pop(job)
-                tref.cancel()
-                del tref
-            except (KeyError, AttributeError):
-                pass  # out of scope
-        self._discard_tref = _discard_tref
-
-        def on_timeout_cancel(R):
-            _discard_tref(R._job)
-        self.on_timeout_cancel = on_timeout_cancel
-
-    def _on_soft_timeout(self, job, soft, hard, hub):
-        # only used by async pool.
-        if hard:
-            self._tref_for_id[job] = hub.call_later(
-                hard - soft, self._on_hard_timeout, job,
-            )
-        try:
-            result = self._cache[job]
-        except KeyError:
-            pass  # job ready
-        else:
-            self.on_soft_timeout(result)
-        finally:
-            if not hard:
-                # remove tref
-                self._discard_tref(job)
-
-    def _on_hard_timeout(self, job):
-        # only used by async pool.
-        try:
-            result = self._cache[job]
-        except KeyError:
-            pass  # job ready
-        else:
-            self.on_hard_timeout(result)
-        finally:
-            # remove tref
-            self._discard_tref(job)
-
-    def on_job_ready(self, job, i, obj, inqW_fd):
-        self._mark_worker_as_available(inqW_fd)
-
-    def _create_process_handlers(self, hub):
-        """Create handlers called on process up/down, etc."""
-        add_reader, remove_reader, remove_writer = (
-            hub.add_reader, hub.remove_reader, hub.remove_writer,
-        )
-        cache = self._cache
-        all_inqueues = self._all_inqueues
-        fileno_to_inq = self._fileno_to_inq
-        fileno_to_outq = self._fileno_to_outq
-        fileno_to_synq = self._fileno_to_synq
-        busy_workers = self._busy_workers
-        handle_result_event = self.handle_result_event
-        process_flush_queues = self.process_flush_queues
-        waiting_to_start = self._waiting_to_start
-
-        def verify_process_alive(proc):
-            proc = proc()  # is a weakref
-            if (proc is not None and proc._is_alive() and
-                    proc in waiting_to_start):
-                assert proc.outqR_fd in fileno_to_outq
-                assert fileno_to_outq[proc.outqR_fd] is proc
-                assert proc.outqR_fd in hub.readers
-                error('Timed out waiting for UP message from %r', proc)
-                os.kill(proc.pid, 9)
-
-        def on_process_up(proc):
-            """Called when a process has started."""
-            # If we got the same fd as a previous process then we'll also
-            # receive jobs in the old buffer, so we need to reset the
-            # job._write_to and job._scheduled_for attributes used to recover
-            # message boundaries when processes exit.
-            infd = proc.inqW_fd
-            for job in cache.values():
-                if job._write_to and job._write_to.inqW_fd == infd:
-                    job._write_to = proc
-                if job._scheduled_for and job._scheduled_for.inqW_fd == infd:
-                    job._scheduled_for = proc
-            fileno_to_outq[proc.outqR_fd] = proc
-
-            # maintain_pool is called whenever a process exits.
-            self._track_child_process(proc, hub)
-
-            assert not isblocking(proc.outq._reader)
-
-            # handle_result_event is called when the processes outqueue is
-            # readable.
-            add_reader(proc.outqR_fd, handle_result_event, proc.outqR_fd)
-
-            waiting_to_start.add(proc)
-            hub.call_later(
-                self._proc_alive_timeout, verify_process_alive, ref(proc),
-            )
-
-        self.on_process_up = on_process_up
-
-        def _remove_from_index(obj, proc, index, remove_fun, callback=None):
-            # this remove the file descriptors for a process from
-            # the indices.  we have to make sure we don't overwrite
-            # another processes fds, as the fds may be reused.
-            try:
-                fd = obj.fileno()
-            except OSError:
-                return
-
-            try:
-                if index[fd] is proc:
-                    # fd hasn't been reused so we can remove it from index.
-                    index.pop(fd, None)
-            except KeyError:
-                pass
-            else:
-                remove_fun(fd)
-                if callback is not None:
-                    callback(fd)
-            return fd
-
-        def on_process_down(proc):
-            """Called when a worker process exits."""
-            if getattr(proc, 'dead', None):
-                return
-            process_flush_queues(proc)
-            _remove_from_index(
-                proc.outq._reader, proc, fileno_to_outq, remove_reader,
-            )
-            if proc.synq:
-                _remove_from_index(
-                    proc.synq._writer, proc, fileno_to_synq, remove_writer,
-                )
-            inq = _remove_from_index(
-                proc.inq._writer, proc, fileno_to_inq, remove_writer,
-                callback=all_inqueues.discard,
-            )
-            if inq:
-                busy_workers.discard(inq)
-            self._untrack_child_process(proc, hub)
-            waiting_to_start.discard(proc)
-            self._active_writes.discard(proc.inqW_fd)
-            remove_writer(proc.inq._writer)
-            remove_reader(proc.outq._reader)
-            if proc.synqR_fd:
-                remove_reader(proc.synq._reader)
-            if proc.synqW_fd:
-                self._active_writes.discard(proc.synqW_fd)
-                remove_reader(proc.synq._writer)
-        self.on_process_down = on_process_down
-
-    def _create_write_handlers(self, hub,
-                               pack=pack, dumps=_pickle.dumps,
-                               protocol=HIGHEST_PROTOCOL):
-        """Create handlers used to write data to child processes."""
-        fileno_to_inq = self._fileno_to_inq
-        fileno_to_synq = self._fileno_to_synq
-        outbound = self.outbound_buffer
-        pop_message = outbound.popleft
-        put_message = outbound.append
-        all_inqueues = self._all_inqueues
-        active_writes = self._active_writes
-        active_writers = self._active_writers
-        busy_workers = self._busy_workers
-        diff = all_inqueues.difference
-        add_writer = hub.add_writer
-        hub_add, hub_remove = hub.add, hub.remove
-        mark_write_fd_as_active = active_writes.add
-        mark_write_gen_as_active = active_writers.add
-        mark_worker_as_busy = busy_workers.add
-        write_generator_done = active_writers.discard
-        get_job = self._cache.__getitem__
-        write_stats = self.write_stats
-        is_fair_strategy = self.sched_strategy == SCHED_STRATEGY_FAIR
-        revoked_tasks = worker_state.revoked
-        getpid = os.getpid
-
-        precalc = {ACK: self._create_payload(ACK, (0,)),
-                   NACK: self._create_payload(NACK, (0,))}
-
-        def _put_back(job, _time=time.time):
-            # puts back at the end of the queue
-            if job._terminated is not None or \
-                    job.correlation_id in revoked_tasks:
-                if not job._accepted:
-                    job._ack(None, _time(), getpid(), None)
-                job._set_terminated(job._terminated)
-            else:
-                # XXX linear lookup, should find a better way,
-                # but this happens rarely and is here to protect against races.
-                if job not in outbound:
-                    outbound.appendleft(job)
-        self._put_back = _put_back
-
-        # called for every event loop iteration, and if there
-        # are messages pending this will schedule writing one message
-        # by registering the 'schedule_writes' function for all currently
-        # inactive inqueues (not already being written to)
-
-        # consolidate means the event loop will merge them
-        # and call the callback once with the list writable fds as
-        # argument.  Using this means we minimize the risk of having
-        # the same fd receive every task if the pipe read buffer is not
-        # full.
-
-        def on_poll_start():
-            # Determine which io descriptors are not busy
-            inactive = diff(active_writes)
-
-            # Determine hub_add vs hub_remove strategy conditional
-            if is_fair_strategy:
-                # outbound buffer present and idle workers exist
-                add_cond = outbound and len(busy_workers) < len(all_inqueues)
-            else:  # default is add when data exists in outbound buffer
-                add_cond = outbound
-
-            if add_cond:  # calling hub_add vs hub_remove
-                iterate_file_descriptors_safely(
-                    inactive, all_inqueues, hub_add,
-                    None, WRITE | ERR, consolidate=True)
-            else:
-                iterate_file_descriptors_safely(
-                    inactive, all_inqueues, hub_remove)
-        self.on_poll_start = on_poll_start
-
-        def on_inqueue_close(fd, proc):
-            # Makes sure the fd is removed from tracking when
-            # the connection is closed, this is essential as fds may be reused.
-            busy_workers.discard(fd)
-            try:
-                if fileno_to_inq[fd] is proc:
-                    fileno_to_inq.pop(fd, None)
-                    active_writes.discard(fd)
-                    all_inqueues.discard(fd)
-            except KeyError:
-                pass
-        self.on_inqueue_close = on_inqueue_close
-        self.hub_remove = hub_remove
-
-        def schedule_writes(ready_fds, total_write_count=None):
-            if not total_write_count:
-                total_write_count = [0]
-            # Schedule write operation to ready file descriptor.
-            # The file descriptor is writable, but that does not
-            # mean the process is currently reading from the socket.
-            # The socket is buffered so writable simply means that
-            # the buffer can accept at least 1 byte of data.
-
-            # This means we have to cycle between the ready fds.
-            # the first version used shuffle, but this version
-            # using `total_writes % ready_fds` is about 30% faster
-            # with many processes, and also leans more towards fairness
-            # in write stats when used with many processes
-            # [XXX On macOS, this may vary depending
-            # on event loop implementation (i.e, select/poll vs epoll), so
-            # have to test further]
-            num_ready = len(ready_fds)
-
-            for _ in range(num_ready):
-                ready_fd = ready_fds[total_write_count[0] % num_ready]
-                total_write_count[0] += 1
-                if ready_fd in active_writes:
-                    # already writing to this fd
-                    continue
-                if is_fair_strategy and ready_fd in busy_workers:
-                    # worker is already busy with another task
-                    continue
-                if ready_fd not in all_inqueues:
-                    hub_remove(ready_fd)
-                    continue
-                try:
-                    job = pop_message()
-                except IndexError:
-                    # no more messages, remove all inactive fds from the hub.
-                    # this is important since the fds are always writable
-                    # as long as there's 1 byte left in the buffer, and so
-                    # this may create a spinloop where the event loop
-                    # always wakes up.
-                    for inqfd in diff(active_writes):
-                        hub_remove(inqfd)
-                    break
-
-                else:
-                    if not job._accepted:  # job not accepted by another worker
-                        try:
-                            # keep track of what process the write operation
-                            # was scheduled for.
-                            proc = job._scheduled_for = fileno_to_inq[ready_fd]
-                            info(f"Schedule task[{job.correlation_id}] "
-                                 f"to <{proc.name}>")
-                        except KeyError:
-                            # write was scheduled for this fd but the process
-                            # has since exited and the message must be sent to
-                            # another process.
-                            put_message(job)
-                            continue
-                        cor = _write_job(proc, ready_fd, job)
-                        job._writer = ref(cor)
-                        mark_write_gen_as_active(cor)
-                        mark_write_fd_as_active(ready_fd)
-                        mark_worker_as_busy(ready_fd)
-
-                        # Try to write immediately, in case there's an error.
-                        try:
-                            next(cor)
-                        except StopIteration:
-                            pass
-                        except OSError as exc:
-                            if exc.errno != errno.EBADF:
-                                raise
-                        else:
-                            add_writer(ready_fd, cor)
-        hub.consolidate_callback = schedule_writes
-
-        def send_job(tup):
-            # Schedule writing job request for when one of the process
-            # inqueues are writable.
-            body = dumps(tup, protocol=protocol)
-            body_size = len(body)
-            header = pack('>I', body_size)
-            # index 1,0 is the job ID.
-            job = get_job(tup[1][0])
-            job._payload = buf_t(header), buf_t(body), body_size
-            put_message(job)
-        self._quick_put = send_job
-
-        def on_not_recovering(proc, fd, job, exc):
-            logger.exception(
-                'Process inqueue damaged: %r %r: %r', proc, proc.exitcode, exc)
-            if proc._is_alive():
-                proc.terminate()
-            hub.remove(fd)
-            self._put_back(job)
-
-        def _write_job(proc, fd, job):
-            # writes job to the worker process.
-            # Operation must complete if more than one byte of data
-            # was written.  If the broker connection is lost
-            # and no data was written the operation shall be canceled.
-            header, body, body_size = job._payload
-            errors = 0
-            try:
-                # job result keeps track of what process the job is sent to.
-                job._write_to = proc
-                send = proc.send_job_offset
-
-                Hw = Bw = 0
-                # write header
-                while Hw < 4:
-                    try:
-                        Hw += send(header, Hw)
-                    except Exception as exc:  # pylint: disable=broad-except
-                        if getattr(exc, 'errno', None) not in UNAVAIL:
-                            raise
-                        # suspend until more data
-                        errors += 1
-                        if errors > 100:
-                            on_not_recovering(proc, fd, job, exc)
-                            raise StopIteration()
-                        yield
-                    else:
-                        errors = 0
-
-                # write body
-                while Bw < body_size:
-                    try:
-                        Bw += send(body, Bw)
-                    except Exception as exc:  # pylint: disable=broad-except
-                        if getattr(exc, 'errno', None) not in UNAVAIL:
-                            raise
-                        # suspend until more data
-                        errors += 1
-                        if errors > 100:
-                            on_not_recovering(proc, fd, job, exc)
-                            raise StopIteration()
-                        yield
-                    else:
-                        errors = 0
-            finally:
-                hub_remove(fd)
-                write_stats[proc.index] += 1
-                # message written, so this fd is now available
-                active_writes.discard(fd)
-                write_generator_done(job._writer())  # is a weakref
-
-        def send_ack(response, pid, job, fd):
-            # Only used when synack is enabled.
-            # Schedule writing ack response for when the fd is writable.
-            msg = Ack(job, fd, precalc[response])
-            callback = promise(write_generator_done)
-            cor = _write_ack(fd, msg, callback=callback)
-            mark_write_gen_as_active(cor)
-            mark_write_fd_as_active(fd)
-            callback.args = (cor,)
-            add_writer(fd, cor)
-        self.send_ack = send_ack
-
-        def _write_ack(fd, ack, callback=None):
-            # writes ack back to the worker if synack enabled.
-            # this operation *MUST* complete, otherwise
-            # the worker process will hang waiting for the ack.
-            header, body, body_size = ack[2]
-            try:
-                try:
-                    proc = fileno_to_synq[fd]
-                except KeyError:
-                    # process died, we can safely discard the ack at this
-                    # point.
-                    raise StopIteration()
-                send = proc.send_syn_offset
-
-                Hw = Bw = 0
-                # write header
-                while Hw < 4:
-                    try:
-                        Hw += send(header, Hw)
-                    except Exception as exc:  # pylint: disable=broad-except
-                        if getattr(exc, 'errno', None) not in UNAVAIL:
-                            raise
-                        yield
-
-                # write body
-                while Bw < body_size:
-                    try:
-                        Bw += send(body, Bw)
-                    except Exception as exc:  # pylint: disable=broad-except
-                        if getattr(exc, 'errno', None) not in UNAVAIL:
-                            raise
-                        # suspend until more data
-                        yield
-            finally:
-                if callback:
-                    callback()
-                # message written, so this fd is now available
-                active_writes.discard(fd)
-
-    def flush(self):
-        if self._state == TERMINATE:
-            return
-        # cancel all tasks that haven't been accepted so that NACK is sent.
-        for job in self._cache.values():
-            if not job._accepted:
-                job._cancel()
-
-        # clear the outgoing buffer as the tasks will be redelivered by
-        # the broker anyway.
-        if self.outbound_buffer:
-            self.outbound_buffer.clear()
-
-        self.maintain_pool()
-
-        try:
-            # ...but we must continue writing the payloads we already started
-            # to keep message boundaries.
-            # The messages may be NACK'ed later if synack is enabled.
-            if self._state == RUN:
-                # flush outgoing buffers
-                intervals = fxrange(0.01, 0.1, 0.01, repeatlast=True)
-                owned_by = {}
-                for job in self._cache.values():
-                    writer = _get_job_writer(job)
-                    if writer is not None:
-                        owned_by[writer] = job
-
-                while self._active_writers:
-                    writers = list(self._active_writers)
-                    for gen in writers:
-                        if (gen.__name__ == '_write_job' and
-                                gen_not_started(gen)):
-                            # hasn't started writing the job so can
-                            # discard the task, but we must also remove
-                            # it from the Pool._cache.
-                            try:
-                                job = owned_by[gen]
-                            except KeyError:
-                                pass
-                            else:
-                                # removes from Pool._cache
-                                job.discard()
-                            self._active_writers.discard(gen)
-                        else:
-                            try:
-                                job = owned_by[gen]
-                            except KeyError:
-                                pass
-                            else:
-                                job_proc = job._write_to
-                                if job_proc._is_alive():
-                                    self._flush_writer(job_proc, gen)
-                    # workers may have exited in the meantime.
-                    self.maintain_pool()
-                    sleep(next(intervals))  # don't busyloop
-        finally:
-            self.outbound_buffer.clear()
-            self._active_writers.clear()
-            self._active_writes.clear()
-            self._busy_workers.clear()
-
-    def _flush_writer(self, proc, writer):
-        fds = {proc.inq._writer}
-        try:
-            while fds:
-                if not proc._is_alive():
-                    break  # process exited
-                readable, writable, again = _select(
-                    writers=fds, err=fds, timeout=0.5,
-                )
-                if not again and (writable or readable):
-                    try:
-                        next(writer)
-                    except (StopIteration, OSError, EOFError):
-                        break
-        finally:
-            self._active_writers.discard(writer)
-
-    def get_process_queues(self):
-        """Get queues for a new process.
-
-        Here we'll find an unused slot, as there should always
-        be one available when we start a new process.
-        """
-        return next(q for q, owner in self._queues.items()
-                    if owner is None)
-
-    def on_grow(self, n):
-        """Grow the pool by ``n`` proceses."""
-        diff = max(self._processes - len(self._queues), 0)
-        if diff:
-            self._queues.update({
-                self.create_process_queues(): None for _ in range(diff)
-            })
-
-    def on_shrink(self, n):
-        """Shrink the pool by ``n`` processes."""
-
-    @staticmethod
-    def sort_workers(worker):
-        try:
-            return process_memory_percent(worker.pid)
-        except Exception:   # noqa
-            # In case of any Exception, put it at last
-            return -1
-
-    # Called by pool.shrink only
-    def _iterinactive(self):
-        # Iter from large mem used to small
-        for worker in sorted(self._pool, key=self.sort_workers, reverse=True):
-            if not self._worker_active(worker):
-                yield worker
-
-    def _worker_active(self, worker):
-        for job in values(self._cache):
-            if (pid := worker.pid) in job.worker_pids():
-                return True
-            if (
-                (status := self._worker_status.get(pid)) is not None
-                and status.value == 1
-            ):
-                return True
-        return False
-
-    def create_process_queues(self):
-        """Create new in, out, etc. queues, returned as a tuple."""
-        # NOTE: Pipes must be set O_NONBLOCK at creation time (the original
-        # fd), otherwise it won't be possible to change the flags until
-        # there's an actual reader/writer on the other side.
-        inq = _SimpleQueue(wnonblock=True)
-        outq = _SimpleQueue(rnonblock=True)
-        synq = None
-        assert isblocking(inq._reader)
-        assert not isblocking(inq._writer)
-        assert not isblocking(outq._reader)
-        assert isblocking(outq._writer)
-        if self.synack:
-            synq = _SimpleQueue(wnonblock=True)
-            assert isblocking(synq._reader)
-            assert not isblocking(synq._writer)
-        return inq, outq, synq
-
-    def on_process_alive(self, pid):
-        """Called when receiving the :const:`WORKER_UP` message.
-
-        Marks the process as ready to receive work.
-        """
-        try:
-            proc = next(w for w in self._pool if w.pid == pid)
-        except StopIteration:
-            return logger.warning('process with pid=%s already exited', pid)
-        assert proc.inqW_fd not in self._fileno_to_inq
-        assert proc.inqW_fd not in self._all_inqueues
-        self._waiting_to_start.discard(proc)
-        self._fileno_to_inq[proc.inqW_fd] = proc
-        self._fileno_to_synq[proc.synqW_fd] = proc
-        self._all_inqueues.add(proc.inqW_fd)
-
-    def on_job_process_down(self, job, pid_gone):
-        """Called for each job when the process assigned to it exits."""
-        if job._write_to and not job._write_to._is_alive():
-            # job was partially written
-            self.on_partial_read(job, job._write_to)
-        elif job._scheduled_for and not job._scheduled_for._is_alive():
-            # job was only scheduled to be written to this process,
-            # but no data was sent so put it back on the outbound_buffer.
-            self._put_back(job)
-
-    def on_job_process_lost(self, job, pid, exitcode):
-        """Called when the process executing job' exits.
-
-        This happens when the process job'
-        was assigned to exited by mysterious means (error exitcodes and
-        signals).
-        """
-        self.mark_as_worker_lost(job, exitcode)
-
-    def human_write_stats(self):
-        if self.write_stats is None:
-            return 'N/A'
-        vals = list(self.write_stats.values())
-        total = sum(vals)
-
-        def per(v, total):
-            return f'{(float(v) / total) if v else 0:.2f}'
-
-        return {
-            'total': total,
-            'avg': per(total / len(self.write_stats) if total else 0, total),
-            'all': ', '.join(per(v, total) for v in vals),
-            'raw': ', '.join(map(str, vals)),
-            'strategy': SCHED_STRATEGY_TO_NAME.get(
-                self.sched_strategy, self.sched_strategy,
-            ),
-            'inqueues': {
-                'total': len(self._all_inqueues),
-                'active': len(self._active_writes),
-            }
-        }
-
-    def _process_cleanup_queues(self, proc):
-        """Called to clean up queues after process exit."""
-        if not proc.dead:
-            try:
-                self._queues[self._find_worker_queues(proc)] = None
-            except (KeyError, ValueError):
-                pass
-
-    @staticmethod
-    def _stop_task_handler(task_handler):
-        """Called at shutdown to tell processes that we're shutting down."""
-        for proc in task_handler.pool:
-            try:
-                setblocking(proc.inq._writer, 1)
-            except OSError:
-                pass
-            else:
-                try:
-                    proc.inq.put(None)
-                except OSError as exc:
-                    if exc.errno != errno.EBADF:
-                        raise
-
-    def create_result_handler(self):
-        return super().create_result_handler(
-            fileno_to_outq=self._fileno_to_outq,
-            on_process_alive=self.on_process_alive,
-        )
-
-    def _process_register_queues(self, proc, queues):
-        """Mark new ownership for ``queues`` to update fileno indices."""
-        assert queues in self._queues
-        b = len(self._queues)
-        self._queues[queues] = proc
-        assert b == len(self._queues)
-
-    def _find_worker_queues(self, proc):
-        """Find the queues owned by ``proc``."""
-        try:
-            return next(q for q, owner in self._queues.items()
-                        if owner == proc)
-        except StopIteration:
-            raise ValueError(proc)
-
-    def _setup_queues(self):
-        # this is only used by the original pool that used a shared
-        # queue for all processes.
-        self._quick_put = None
-
-        # these attributes are unused by this class, but we'll still
-        # have to initialize them for compatibility.
-        self._inqueue = self._outqueue = \
-            self._quick_get = self._poll_result = None
-
-    def process_flush_queues(self, proc):
-        """Flush all queues.
-
-        Including the outbound buffer, so that
-        all tasks that haven't been started will be discarded.
-
-        In Celery this is called whenever the transport connection is lost
-        (consumer restart), and when a process is terminated.
-        """
-        resq = proc.outq._reader
-        on_state_change = self._result_handler.on_state_change
-        fds = {resq}
-        while fds and not resq.closed and self._state != TERMINATE:
-            readable, _, _ = _select(fds, None, fds, timeout=0.01)
-            if readable:
-                try:
-                    task = resq.recv()
-                except (OSError, EOFError) as exc:
-                    _errno = getattr(exc, 'errno', None)
-                    if _errno == errno.EINTR:
-                        continue
-                    elif _errno == errno.EAGAIN:
-                        break
-                    elif _errno not in UNAVAIL:
-                        debug('got %r while flushing process %r',
-                              exc, proc, exc_info=1)
-                    break
-                else:
-                    if task is None:
-                        debug('got sentinel while flushing process %r', proc)
-                        break
-                    else:
-                        on_state_change(task)
-            else:
-                break
-
-    def on_partial_read(self, job, proc):
-        """Called when a job was partially written to exited child."""
-        # worker terminated by signal:
-        # we cannot reuse the sockets again, because we don't know if
-        # the process wrote/read anything frmo them, and if so we cannot
-        # restore the message boundaries.
-        if not job._accepted:
-            # job was not acked, so find another worker to send it to.
-            self._put_back(job)
-        writer = _get_job_writer(job)
-        if writer:
-            self._active_writers.discard(writer)
-            del writer
-
-        if not proc.dead:
-            proc.dead = True
-            # Replace queues to avoid reuse
-            before = len(self._queues)
-            try:
-                queues = self._find_worker_queues(proc)
-                if self.destroy_queues(queues, proc):
-                    self._queues[self.create_process_queues()] = None
-            except ValueError:
-                pass
-            assert len(self._queues) == before
-
-    def destroy_queues(self, queues, proc):
-        """Destroy queues that can no longer be used.
-
-        This way they can be replaced by new usable sockets.
-        """
-        assert not proc._is_alive()
-        self._waiting_to_start.discard(proc)
-        removed = 1
-        try:
-            self._queues.pop(queues)
-        except KeyError:
-            removed = 0
-        try:
-            self.on_inqueue_close(queues[0]._writer.fileno(), proc)
-        except OSError:
-            pass
-        for queue in queues:
-            if queue:
-                for sock in (queue._reader, queue._writer):
-                    if not sock.closed:
-                        self.hub_remove(sock)
-                        try:
-                            sock.close()
-                        except OSError:
-                            pass
-        return removed
-
-    def _create_payload(self, type_, args,
-                        dumps=_pickle.dumps, pack=pack,
-                        protocol=HIGHEST_PROTOCOL):
-        body = dumps((type_, args), protocol=protocol)
-        size = len(body)
-        header = pack('>I', size)
-        return header, body, size
-
-    @classmethod
-    def _set_result_sentinel(cls, _outqueue, _pool):
-        # unused
-        pass
-
-    def _help_stuff_finish_args(self):
-        # Pool._help_stuff_finished is a classmethod so we have to use this
-        # trick to modify the arguments passed to it.
-        return (self._pool,)
-
-    @classmethod
-    def _help_stuff_finish(cls, pool):
-        # pylint: disable=arguments-differ
-        debug(
-            'removing tasks from inqueue until task handler finished',
-        )
-        fileno_to_proc = {}
-        inqR = set()
-        for w in pool:
-            try:
-                fd = w.inq._reader.fileno()
-                inqR.add(fd)
-                fileno_to_proc[fd] = w
-            except OSError:
-                pass
-        while inqR:
-            readable, _, again = _select(inqR, timeout=0.5)
-            if again:
-                continue
-            if not readable:
-                break
-            for fd in readable:
-                fileno_to_proc[fd].inq._reader.recv()
-            sleep(0)
-
-    @property
-    def timers(self):
-        return {self.maintain_pool: 5.0}
+"""Version of multiprocessing.Pool using Async I/O.
+
+.. note::
+
+    This module will be moved soon, so don't use it directly.
+
+This is a non-blocking version of :class:`multiprocessing.Pool`.
+
+This code deals with three major challenges:
+
+#. Starting up child processes and keeping them running.
+#. Sending jobs to the processes and receiving results back.
+#. Safely shutting down this system.
+"""
+import errno
+import gc
+import os
+import select
+import time
+import sys
+from collections import Counter, deque, namedtuple
+from io import BytesIO
+from numbers import Integral
+from pickle import HIGHEST_PROTOCOL
+from struct import pack, unpack, unpack_from
+from time import sleep
+from weakref import WeakValueDictionary, ref
+
+from billiard import pool as _pool
+from billiard.compat import buf_t, isblocking, setblocking
+from billiard.five import values
+from billiard.pool import (
+    ACK, NACK, RUN, TERMINATE, READY,
+    TASK, EX_OK, EX_FAILURE, EX_RECYCLE, MAXMEM_USED_FMT,
+    WorkersJoined, ExceptionInfo, MaybeEncodingError,
+    mem_rss,
+)
+from billiard.queues import _SimpleQueue
+from kombu.asynchronous import ERR, WRITE
+from kombu.serialization import pickle as _pickle
+from kombu.utils.eventio import SELECT_BAD_FD
+from kombu.utils.functional import fxrange
+from vine import promise
+
+from celery.utils.functional import noop
+from celery.utils.log import get_logger
+from celery.utils.psutil import process_memory_percent
+from celery.worker import state as worker_state
+
+# pylint: disable=redefined-outer-name
+# We cache globals and attribute lookups, so disable this warning.
+
+try:
+    from _billiard import read as __read__
+    readcanbuf = True
+
+except ImportError:  # pragma: no cover
+
+    def __read__(fd, buf, size, read=os.read):  # noqa
+        chunk = read(fd, size)
+        n = len(chunk)
+        if n != 0:
+            buf.write(chunk)
+        return n
+    readcanbuf = False  # noqa
+
+    def unpack_from(fmt, iobuf, unpack=unpack):  # noqa
+        return unpack(fmt, iobuf.getvalue())  # <-- BytesIO
+
+__all__ = ('AsynPool',)
+
+logger = get_logger(__name__)
+error, warning, info, debug = \
+    logger.error, logger.warning, logger.info, logger.debug
+
+UNAVAIL = frozenset({errno.EAGAIN, errno.EINTR})
+
+#: Constant sent by child process when started (ready to accept work)
+WORKER_UP = 15
+
+#: A process must've started before this timeout (in secs.) expires.
+PROC_ALIVE_TIMEOUT = 4.0
+
+SCHED_STRATEGY_FCFS = 1
+SCHED_STRATEGY_FAIR = 4
+
+SCHED_STRATEGIES = {
+    None: SCHED_STRATEGY_FAIR,
+    'default': SCHED_STRATEGY_FAIR,
+    'fast': SCHED_STRATEGY_FCFS,
+    'fcfs': SCHED_STRATEGY_FCFS,
+    'fair': SCHED_STRATEGY_FAIR,
+}
+SCHED_STRATEGY_TO_NAME = {v: k for k, v in SCHED_STRATEGIES.items()}
+
+Ack = namedtuple('Ack', ('id', 'fd', 'payload'))
+
+
+def gen_not_started(gen):
+    """Return true if generator is not started."""
+    # gi_frame is None when generator stopped.
+    return gen.gi_frame and gen.gi_frame.f_lasti == -1
+
+
+def _get_job_writer(job):
+    try:
+        writer = job._writer
+    except AttributeError:
+        pass
+    else:
+        return writer()  # is a weakref
+
+
+if hasattr(select, 'poll'):
+    def _select_imp(readers=None, writers=None, err=None, timeout=0,
+                    poll=select.poll, POLLIN=select.POLLIN,
+                    POLLOUT=select.POLLOUT, POLLERR=select.POLLERR):
+        poller = poll()
+        register = poller.register
+
+        if readers:
+            [register(fd, POLLIN) for fd in readers]
+        if writers:
+            [register(fd, POLLOUT) for fd in writers]
+        if err:
+            [register(fd, POLLERR) for fd in err]
+
+        R, W = set(), set()
+        timeout = 0 if timeout and timeout < 0 else round(timeout * 1e3)
+        events = poller.poll(timeout)
+        for fd, event in events:
+            if not isinstance(fd, Integral):
+                fd = fd.fileno()
+            if event & POLLIN:
+                R.add(fd)
+            if event & POLLOUT:
+                W.add(fd)
+            if event & POLLERR:
+                R.add(fd)
+        return R, W, 0
+else:
+    def _select_imp(readers=None, writers=None, err=None, timeout=0):
+        r, w, e = select.select(readers, writers, err, timeout)
+        if e:
+            r = list(set(r) | set(e))
+        return r, w, 0
+
+
+def _select(readers=None, writers=None, err=None, timeout=0,
+            poll=_select_imp):
+    """Simple wrapper to :class:`~select.select`, using :`~select.poll`.
+
+    Arguments:
+        readers (Set[Fd]): Set of reader fds to test if readable.
+        writers (Set[Fd]): Set of writer fds to test if writable.
+        err (Set[Fd]): Set of fds to test for error condition.
+
+    All fd sets passed must be mutable as this function
+    will remove non-working fds from them, this also means
+    the caller must make sure there are still fds in the sets
+    before calling us again.
+
+    Returns:
+        Tuple[Set, Set, Set]: of ``(readable, writable, again)``, where
+        ``readable`` is a set of fds that have data available for read,
+        ``writable`` is a set of fds that's ready to be written to
+        and ``again`` is a flag that if set means the caller must
+        throw away the result and call us again.
+    """
+    readers = set() if readers is None else readers
+    writers = set() if writers is None else writers
+    err = set() if err is None else err
+    try:
+        return poll(readers, writers, err, timeout)
+    except OSError as exc:
+        _errno = exc.errno
+
+        if _errno == errno.EINTR:
+            return set(), set(), 1
+        elif _errno in SELECT_BAD_FD:
+            for fd in readers | writers | err:
+                try:
+                    select.select([fd], [], [], 0)
+                except OSError as exc:
+                    _errno = exc.errno
+
+                    if _errno not in SELECT_BAD_FD:
+                        raise
+                    readers.discard(fd)
+                    writers.discard(fd)
+                    err.discard(fd)
+            return set(), set(), 1
+        else:
+            raise
+
+
+def iterate_file_descriptors_safely(fds_iter, source_data,
+                                    hub_method, *args, **kwargs):
+    """Apply hub method to fds in iter, remove from list if failure.
+
+    Some file descriptors may become stale through OS reasons
+    or possibly other reasons, so safely manage our lists of FDs.
+    :param fds_iter: the file descriptors to iterate and apply hub_method
+    :param source_data: data source to remove FD if it renders OSError
+    :param hub_method: the method to call with with each fd and kwargs
+    :*args to pass through to the hub_method;
+    with a special syntax string '*fd*' represents a substitution
+    for the current fd object in the iteration (for some callers).
+    :**kwargs to pass through to the hub method (no substitutions needed)
+    """
+    def _meta_fd_argument_maker():
+        # uses the current iterations value for fd
+        call_args = args
+        if "*fd*" in call_args:
+            call_args = [fd if arg == "*fd*" else arg for arg in args]
+        return call_args
+    # Track stale FDs for cleanup possibility
+    stale_fds = []
+    for fd in fds_iter:
+        # Handle using the correct arguments to the hub method
+        hub_args, hub_kwargs = _meta_fd_argument_maker(), kwargs
+        try:  # Call the hub method
+            hub_method(fd, *hub_args, **hub_kwargs)
+        except (OSError, FileNotFoundError):
+            logger.warning(
+                "Encountered OSError when accessing fd %s ",
+                fd, exc_info=True)
+            stale_fds.append(fd)  # take note of stale fd
+    # Remove now defunct fds from the managed list
+    if source_data:
+        for fd in stale_fds:
+            try:
+                if hasattr(source_data, 'remove'):
+                    source_data.remove(fd)
+                else:  # then not a list/set ... try dict
+                    source_data.pop(fd, None)
+            except ValueError:
+                logger.warning("ValueError trying to invalidate %s from %s",
+                               fd, source_data)
+
+
+class Worker(_pool.Worker):
+    """Pool worker process."""
+    def __init__(self, inq, outq, synq=None, initializer=None, initargs=(),
+                 maxtasks=None, sentinel=None, on_exit=None,
+                 sigprotection=True, wrap_exception=True,
+                 max_memory_per_child=None, on_ready_counter=None,
+                 status=None):
+        self.status = status
+        super().__init__(
+            inq, outq, synq, initializer, initargs,
+            maxtasks, sentinel, on_exit,
+            sigprotection, wrap_exception,
+            max_memory_per_child, on_ready_counter
+        )
+
+    def on_loop_start(self, pid):
+        # our version sends a WORKER_UP message when the process is ready
+        # to accept work, this will tell the parent that the inqueue fd
+        # is writable.
+        self.outq.put((WORKER_UP, (pid,)))
+
+    def workloop(self, debug=debug, now=time.monotonic, pid=None):
+        pid = pid or os.getpid()
+        put = self.outq.put
+        inqW_fd = self.inqW_fd
+        synqW_fd = self.synqW_fd
+        maxtasks = self.maxtasks
+        max_memory_per_child = self.max_memory_per_child or 0
+        prepare_result = self.prepare_result
+
+        wait_for_job = self.wait_for_job
+        _wait_for_syn = self.wait_for_syn
+
+        def _mark_idle():
+            if self.status is not None:
+                self.status.value = 0
+
+        def _mark_busy():
+            if self.status is not None:
+                self.status.value = 1
+
+        def wait_for_syn(jid):
+            i = 0
+            while 1:
+                if i > 60:
+                    error('!!!WAIT FOR ACK TIMEOUT: job:%r fd:%r!!!',
+                          jid, self.synq._reader.fileno(), exc_info=1)
+                req = _wait_for_syn()
+                if req:
+                    type_, args = req
+                    if type_ == NACK:
+                        return False
+                    assert type_ == ACK
+                    return True
+                i += 1
+
+        completed = 0
+        try:
+            while maxtasks is None or (maxtasks and completed < maxtasks):
+                _mark_idle()
+                req = wait_for_job()
+                if req:
+                    type_, args_ = req
+                    assert type_ == TASK
+                    job, i, fun, args, kwargs = args_
+                    _mark_busy()
+                    info(f"Receive req[{args[1]}]")
+                    put((ACK, (job, i, now(), pid, synqW_fd)))
+                    if _wait_for_syn:
+                        confirm = wait_for_syn(job)
+                        if not confirm:
+                            continue  # received NACK
+                    try:
+                        result = (True, prepare_result(fun(*args, **kwargs)))
+                    except Exception:
+                        result = (False, ExceptionInfo())
+                    try:
+                        put((READY, (job, i, result, inqW_fd)))
+                    except Exception as exc:
+                        _, _, tb = sys.exc_info()
+                        try:
+                            wrapped = MaybeEncodingError(exc, result[1])
+                            einfo = ExceptionInfo((
+                                MaybeEncodingError, wrapped, tb,
+                            ))
+                            put((READY, (job, i, (False, einfo), inqW_fd)))
+                        finally:
+                            del tb
+                    completed += 1
+                    if max_memory_per_child > 0:
+                        used_kb = mem_rss()
+                        if used_kb <= 0:
+                            error('worker unable to determine memory usage')
+                        if used_kb > 0 and used_kb > max_memory_per_child:
+                            warning(MAXMEM_USED_FMT.format(
+                                used_kb, max_memory_per_child))
+                            return EX_RECYCLE
+
+            debug('worker exiting after %d tasks', completed)
+            if maxtasks:
+                return EX_RECYCLE if completed == maxtasks else EX_FAILURE
+            return EX_OK
+        finally:
+            # Before exiting the worker, we want to ensure that that all
+            # messages produced by the worker have been consumed by the main
+            # process. This prevents the worker being terminated prematurely
+            # and messages being lost.
+            self._ensure_messages_consumed(completed=completed)
+            _mark_idle()
+
+    def __reduce__(self):
+        return self.__class__, (
+            self.inq, self.outq, self.synq, self.initializer,
+            self.initargs, self.maxtasks, self._shutdown, self.on_exit,
+            self.sigprotection, self.wrap_exception, self.max_memory_per_child,
+            self.on_ready_counter, self.status
+        )
+
+
+class ResultHandler(_pool.ResultHandler):
+    """Handles messages from the pool processes."""
+
+    def __init__(self, *args, **kwargs):
+        self.fileno_to_outq = kwargs.pop('fileno_to_outq')
+        self.on_process_alive = kwargs.pop('on_process_alive')
+        super().__init__(*args, **kwargs)
+        # add our custom message handler
+        self.state_handlers[WORKER_UP] = self.on_process_alive
+
+    def _recv_message(self, add_reader, fd, callback,
+                      __read__=__read__, readcanbuf=readcanbuf,
+                      BytesIO=BytesIO, unpack_from=unpack_from,
+                      load=_pickle.load):
+        Hr = Br = 0
+        if readcanbuf:
+            buf = bytearray(4)
+            bufv = memoryview(buf)
+        else:
+            buf = bufv = BytesIO()
+        # header
+
+        while Hr < 4:
+            try:
+                n = __read__(
+                    fd, bufv[Hr:] if readcanbuf else bufv, 4 - Hr,
+                )
+            except OSError as exc:
+                if exc.errno not in UNAVAIL:
+                    raise
+                yield
+            else:
+                if n == 0:
+                    raise (OSError('End of file during message') if Hr
+                           else EOFError())
+                Hr += n
+
+        body_size, = unpack_from('>i', bufv)
+        if readcanbuf:
+            buf = bytearray(body_size)
+            bufv = memoryview(buf)
+        else:
+            buf = bufv = BytesIO()
+
+        while Br < body_size:
+            try:
+                n = __read__(
+                    fd, bufv[Br:] if readcanbuf else bufv, body_size - Br,
+                )
+            except OSError as exc:
+                if exc.errno not in UNAVAIL:
+                    raise
+                yield
+            else:
+                if n == 0:
+                    raise (OSError('End of file during message') if Br
+                           else EOFError())
+                Br += n
+        add_reader(fd, self.handle_event, fd)
+        if readcanbuf:
+            message = load(BytesIO(bufv))
+        else:
+            bufv.seek(0)
+            message = load(bufv)
+        if message:
+            callback(message)
+
+    def _make_process_result(self, hub):
+        """Coroutine reading messages from the pool processes."""
+        fileno_to_outq = self.fileno_to_outq
+        on_state_change = self.on_state_change
+        add_reader = hub.add_reader
+        remove_reader = hub.remove_reader
+        recv_message = self._recv_message
+
+        def on_result_readable(fileno):
+            try:
+                fileno_to_outq[fileno]
+            except KeyError:  # process gone
+                return remove_reader(fileno)
+            it = recv_message(add_reader, fileno, on_state_change)
+            try:
+                next(it)
+            except StopIteration:
+                pass
+            except (OSError, EOFError):
+                remove_reader(fileno)
+            else:
+                add_reader(fileno, it)
+        return on_result_readable
+
+    def register_with_event_loop(self, hub):
+        self.handle_event = self._make_process_result(hub)
+
+    def handle_event(self, *args):
+        # pylint: disable=method-hidden
+        #   register_with_event_loop overrides this
+        raise RuntimeError('Not registered with event loop')
+
+    def on_stop_not_started(self):
+        # This is always used, since we do not start any threads.
+        cache = self.cache
+        check_timeouts = self.check_timeouts
+        fileno_to_outq = self.fileno_to_outq
+        on_state_change = self.on_state_change
+        join_exited_workers = self.join_exited_workers
+
+        # flush the processes outqueues until they've all terminated.
+        outqueues = set(fileno_to_outq)
+        while cache and outqueues and self._state != TERMINATE:
+            if check_timeouts is not None:
+                # make sure tasks with a time limit will time out.
+                check_timeouts()
+            # cannot iterate and remove at the same time
+            pending_remove_fd = set()
+            for fd in outqueues:
+                iterate_file_descriptors_safely(
+                    [fd], self.fileno_to_outq, self._flush_outqueue,
+                    pending_remove_fd.add, fileno_to_outq, on_state_change
+                )
+                try:
+                    join_exited_workers(shutdown=True)
+                except WorkersJoined:
+                    debug('result handler: all workers terminated')
+                    return
+            outqueues.difference_update(pending_remove_fd)
+
+    def _flush_outqueue(self, fd, remove, process_index, on_state_change):
+        try:
+            proc = process_index[fd]
+        except KeyError:
+            # process already found terminated
+            # this means its outqueue has already been processed
+            # by the worker lost handler.
+            return remove(fd)
+
+        reader = proc.outq._reader
+        try:
+            setblocking(reader, 1)
+        except OSError:
+            return remove(fd)
+        try:
+            if reader.poll(0):
+                task = reader.recv()
+            else:
+                task = None
+                sleep(0.5)
+        except (OSError, EOFError):
+            return remove(fd)
+        else:
+            if task:
+                on_state_change(task)
+        finally:
+            try:
+                setblocking(reader, 0)
+            except OSError:
+                return remove(fd)
+
+
+class AsynPool(_pool.Pool):
+    """AsyncIO Pool (no threads)."""
+
+    ResultHandler = ResultHandler
+
+    def WorkerProcess(self, worker):
+        wproc = super().WorkerProcess(worker)
+        self._worker_status[wproc.pid] = worker.status
+        wproc.dead = False
+        return wproc
+
+    def __init__(self, processes=None, synack=False,
+                 sched_strategy=None, proc_alive_timeout=None,
+                 *args, **kwargs):
+        self.sched_strategy = SCHED_STRATEGIES.get(sched_strategy,
+                                                   sched_strategy)
+        processes = self.cpu_count() if processes is None else processes
+        self.synack = synack
+        # create queue-pairs for all our processes in advance.
+        self._queues = {
+            self.create_process_queues(): None for _ in range(processes)
+        }
+
+        # inqueue fileno -> process mapping
+        self._fileno_to_inq = {}
+        # outqueue fileno -> process mapping
+        self._fileno_to_outq = {}
+        # synqueue fileno -> process mapping
+        self._fileno_to_synq = {}
+
+        # We keep track of processes that haven't yet
+        # sent a WORKER_UP message.  If a process fails to send
+        # this message within _proc_alive_timeout we terminate it
+        # and hope the next process will recover.
+        self._proc_alive_timeout = (
+            PROC_ALIVE_TIMEOUT if proc_alive_timeout is None
+            else proc_alive_timeout
+        )
+        self._waiting_to_start = set()
+
+        # denormalized set of all inqueues.
+        self._all_inqueues = set()
+
+        # Set of fds being written to (busy)
+        self._active_writes = set()
+
+        # Set of active co-routines currently writing jobs.
+        self._active_writers = set()
+
+        # Set of fds that are busy (executing task)
+        self._busy_workers = set()
+        self._mark_worker_as_available = self._busy_workers.discard
+
+        # Holds jobs waiting to be written to child processes.
+        self.outbound_buffer = deque()
+
+        self.write_stats = Counter()
+
+        self._worker_status = {}
+
+        super().__init__(processes, *args, **kwargs)
+
+        for proc in self._pool:
+            # create initial mappings, these will be updated
+            # as processes are recycled, or found lost elsewhere.
+            self._fileno_to_outq[proc.outqR_fd] = proc
+            self._fileno_to_synq[proc.synqW_fd] = proc
+
+        self.on_soft_timeout = getattr(
+            self._timeout_handler, 'on_soft_timeout', noop,
+        )
+        self.on_hard_timeout = getattr(
+            self._timeout_handler, 'on_hard_timeout', noop,
+        )
+
+    def Worker(self, *args, **kwargs):  # noqa
+        return Worker(*args, **kwargs, status=self._ctx.Value('H'))
+
+    def _create_worker_process(self, i):
+        gc.collect()  # Issue #2927
+        w = super()._create_worker_process(i)
+        try:
+            w.base_mem_used = process_memory_percent(w.pid)
+        except Exception:   # noqa
+            w.base_mem_used = None
+        return w
+
+    def _event_process_exit(self, hub, proc):
+        # This method is called whenever the process sentinel is readable.
+        self._untrack_child_process(proc, hub)
+        self.maintain_pool()
+
+    def _track_child_process(self, proc, hub):
+        """Helper method determines appropriate fd for process."""
+        try:
+            fd = proc._sentinel_poll
+        except AttributeError:
+            # we need to duplicate the fd here to carefully
+            # control when the fd is removed from the process table,
+            # as once the original fd is closed we cannot unregister
+            # the fd from epoll(7) anymore, causing a 100% CPU poll loop.
+            fd = proc._sentinel_poll = os.dup(proc._popen.sentinel)
+        # Safely call hub.add_reader for the determined fd
+        iterate_file_descriptors_safely(
+            [fd], None, hub.add_reader,
+            self._event_process_exit, hub, proc)
+
+    def _untrack_child_process(self, proc, hub):
+        if proc._sentinel_poll is not None:
+            fd, proc._sentinel_poll = proc._sentinel_poll, None
+            hub.remove(fd)
+            os.close(fd)
+
+    def register_with_event_loop(self, hub):
+        """Register the async pool with the current event loop."""
+        self._result_handler.register_with_event_loop(hub)
+        self.handle_result_event = self._result_handler.handle_event
+        self._create_timelimit_handlers(hub)
+        self._create_process_handlers(hub)
+        self._create_write_handlers(hub)
+
+        # Add handler for when a process exits (calls maintain_pool)
+        [self._track_child_process(w, hub) for w in self._pool]
+        # Handle_result_event is called whenever one of the
+        # result queues are readable.
+        iterate_file_descriptors_safely(
+            self._fileno_to_outq, self._fileno_to_outq, hub.add_reader,
+            self.handle_result_event, '*fd*')
+
+        # Timers include calling maintain_pool at a regular interval
+        # to be certain processes are restarted.
+        for handler, interval in self.timers.items():
+            hub.call_repeatedly(interval, handler)
+
+        hub.on_tick.add(self.on_poll_start)
+
+    def _create_timelimit_handlers(self, hub):
+        """Create handlers used to implement time limits."""
+        call_later = hub.call_later
+        trefs = self._tref_for_id = WeakValueDictionary()
+
+        def on_timeout_set(R, soft, hard):
+            if soft:
+                trefs[R._job] = call_later(
+                    soft, self._on_soft_timeout, R._job, soft, hard, hub,
+                )
+            elif hard:
+                trefs[R._job] = call_later(
+                    hard, self._on_hard_timeout, R._job,
+                )
+        self.on_timeout_set = on_timeout_set
+
+        def _discard_tref(job):
+            try:
+                tref = trefs.pop(job)
+                tref.cancel()
+                del tref
+            except (KeyError, AttributeError):
+                pass  # out of scope
+        self._discard_tref = _discard_tref
+
+        def on_timeout_cancel(R):
+            _discard_tref(R._job)
+        self.on_timeout_cancel = on_timeout_cancel
+
+    def _on_soft_timeout(self, job, soft, hard, hub):
+        # only used by async pool.
+        if hard:
+            self._tref_for_id[job] = hub.call_later(
+                hard - soft, self._on_hard_timeout, job,
+            )
+        try:
+            result = self._cache[job]
+        except KeyError:
+            pass  # job ready
+        else:
+            self.on_soft_timeout(result)
+        finally:
+            if not hard:
+                # remove tref
+                self._discard_tref(job)
+
+    def _on_hard_timeout(self, job):
+        # only used by async pool.
+        try:
+            result = self._cache[job]
+        except KeyError:
+            pass  # job ready
+        else:
+            self.on_hard_timeout(result)
+        finally:
+            # remove tref
+            self._discard_tref(job)
+
+    def on_job_ready(self, job, i, obj, inqW_fd):
+        self._mark_worker_as_available(inqW_fd)
+
+    def _create_process_handlers(self, hub):
+        """Create handlers called on process up/down, etc."""
+        add_reader, remove_reader, remove_writer = (
+            hub.add_reader, hub.remove_reader, hub.remove_writer,
+        )
+        cache = self._cache
+        all_inqueues = self._all_inqueues
+        fileno_to_inq = self._fileno_to_inq
+        fileno_to_outq = self._fileno_to_outq
+        fileno_to_synq = self._fileno_to_synq
+        busy_workers = self._busy_workers
+        handle_result_event = self.handle_result_event
+        process_flush_queues = self.process_flush_queues
+        waiting_to_start = self._waiting_to_start
+
+        def verify_process_alive(proc):
+            proc = proc()  # is a weakref
+            if (proc is not None and proc._is_alive() and
+                    proc in waiting_to_start):
+                assert proc.outqR_fd in fileno_to_outq
+                assert fileno_to_outq[proc.outqR_fd] is proc
+                assert proc.outqR_fd in hub.readers
+                error('Timed out waiting for UP message from %r', proc)
+                os.kill(proc.pid, 9)
+
+        def on_process_up(proc):
+            """Called when a process has started."""
+            # If we got the same fd as a previous process then we'll also
+            # receive jobs in the old buffer, so we need to reset the
+            # job._write_to and job._scheduled_for attributes used to recover
+            # message boundaries when processes exit.
+            infd = proc.inqW_fd
+            for job in cache.values():
+                if job._write_to and job._write_to.inqW_fd == infd:
+                    job._write_to = proc
+                if job._scheduled_for and job._scheduled_for.inqW_fd == infd:
+                    job._scheduled_for = proc
+            fileno_to_outq[proc.outqR_fd] = proc
+
+            # maintain_pool is called whenever a process exits.
+            self._track_child_process(proc, hub)
+
+            assert not isblocking(proc.outq._reader)
+
+            # handle_result_event is called when the processes outqueue is
+            # readable.
+            add_reader(proc.outqR_fd, handle_result_event, proc.outqR_fd)
+
+            waiting_to_start.add(proc)
+            hub.call_later(
+                self._proc_alive_timeout, verify_process_alive, ref(proc),
+            )
+
+        self.on_process_up = on_process_up
+
+        def _remove_from_index(obj, proc, index, remove_fun, callback=None):
+            # this remove the file descriptors for a process from
+            # the indices.  we have to make sure we don't overwrite
+            # another processes fds, as the fds may be reused.
+            try:
+                fd = obj.fileno()
+            except OSError:
+                return
+
+            try:
+                if index[fd] is proc:
+                    # fd hasn't been reused so we can remove it from index.
+                    index.pop(fd, None)
+            except KeyError:
+                pass
+            else:
+                remove_fun(fd)
+                if callback is not None:
+                    callback(fd)
+            return fd
+
+        def on_process_down(proc):
+            """Called when a worker process exits."""
+            if getattr(proc, 'dead', None):
+                return
+            process_flush_queues(proc)
+            _remove_from_index(
+                proc.outq._reader, proc, fileno_to_outq, remove_reader,
+            )
+            if proc.synq:
+                _remove_from_index(
+                    proc.synq._writer, proc, fileno_to_synq, remove_writer,
+                )
+            inq = _remove_from_index(
+                proc.inq._writer, proc, fileno_to_inq, remove_writer,
+                callback=all_inqueues.discard,
+            )
+            if inq:
+                busy_workers.discard(inq)
+            self._untrack_child_process(proc, hub)
+            waiting_to_start.discard(proc)
+            self._active_writes.discard(proc.inqW_fd)
+            remove_writer(proc.inq._writer)
+            remove_reader(proc.outq._reader)
+            if proc.synqR_fd:
+                remove_reader(proc.synq._reader)
+            if proc.synqW_fd:
+                self._active_writes.discard(proc.synqW_fd)
+                remove_reader(proc.synq._writer)
+        self.on_process_down = on_process_down
+
+    def _create_write_handlers(self, hub,
+                               pack=pack, dumps=_pickle.dumps,
+                               protocol=HIGHEST_PROTOCOL):
+        """Create handlers used to write data to child processes."""
+        fileno_to_inq = self._fileno_to_inq
+        fileno_to_synq = self._fileno_to_synq
+        outbound = self.outbound_buffer
+        pop_message = outbound.popleft
+        put_message = outbound.append
+        all_inqueues = self._all_inqueues
+        active_writes = self._active_writes
+        active_writers = self._active_writers
+        busy_workers = self._busy_workers
+        diff = all_inqueues.difference
+        add_writer = hub.add_writer
+        hub_add, hub_remove = hub.add, hub.remove
+        mark_write_fd_as_active = active_writes.add
+        mark_write_gen_as_active = active_writers.add
+        mark_worker_as_busy = busy_workers.add
+        write_generator_done = active_writers.discard
+        get_job = self._cache.__getitem__
+        write_stats = self.write_stats
+        is_fair_strategy = self.sched_strategy == SCHED_STRATEGY_FAIR
+        revoked_tasks = worker_state.revoked
+        getpid = os.getpid
+
+        precalc = {ACK: self._create_payload(ACK, (0,)),
+                   NACK: self._create_payload(NACK, (0,))}
+
+        def _put_back(job, _time=time.time):
+            # puts back at the end of the queue
+            if job._terminated is not None or \
+                    job.correlation_id in revoked_tasks:
+                if not job._accepted:
+                    job._ack(None, _time(), getpid(), None)
+                job._set_terminated(job._terminated)
+            else:
+                # XXX linear lookup, should find a better way,
+                # but this happens rarely and is here to protect against races.
+                if job not in outbound:
+                    outbound.appendleft(job)
+        self._put_back = _put_back
+
+        # called for every event loop iteration, and if there
+        # are messages pending this will schedule writing one message
+        # by registering the 'schedule_writes' function for all currently
+        # inactive inqueues (not already being written to)
+
+        # consolidate means the event loop will merge them
+        # and call the callback once with the list writable fds as
+        # argument.  Using this means we minimize the risk of having
+        # the same fd receive every task if the pipe read buffer is not
+        # full.
+
+        def on_poll_start():
+            # Determine which io descriptors are not busy
+            inactive = diff(active_writes)
+
+            # Determine hub_add vs hub_remove strategy conditional
+            if is_fair_strategy:
+                # outbound buffer present and idle workers exist
+                add_cond = outbound and len(busy_workers) < len(all_inqueues)
+            else:  # default is add when data exists in outbound buffer
+                add_cond = outbound
+
+            if add_cond:  # calling hub_add vs hub_remove
+                iterate_file_descriptors_safely(
+                    inactive, all_inqueues, hub_add,
+                    None, WRITE | ERR, consolidate=True)
+            else:
+                iterate_file_descriptors_safely(
+                    inactive, all_inqueues, hub_remove)
+        self.on_poll_start = on_poll_start
+
+        def on_inqueue_close(fd, proc):
+            # Makes sure the fd is removed from tracking when
+            # the connection is closed, this is essential as fds may be reused.
+            busy_workers.discard(fd)
+            try:
+                if fileno_to_inq[fd] is proc:
+                    fileno_to_inq.pop(fd, None)
+                    active_writes.discard(fd)
+                    all_inqueues.discard(fd)
+            except KeyError:
+                pass
+        self.on_inqueue_close = on_inqueue_close
+        self.hub_remove = hub_remove
+
+        def schedule_writes(ready_fds, total_write_count=None):
+            if not total_write_count:
+                total_write_count = [0]
+            # Schedule write operation to ready file descriptor.
+            # The file descriptor is writable, but that does not
+            # mean the process is currently reading from the socket.
+            # The socket is buffered so writable simply means that
+            # the buffer can accept at least 1 byte of data.
+
+            # This means we have to cycle between the ready fds.
+            # the first version used shuffle, but this version
+            # using `total_writes % ready_fds` is about 30% faster
+            # with many processes, and also leans more towards fairness
+            # in write stats when used with many processes
+            # [XXX On macOS, this may vary depending
+            # on event loop implementation (i.e, select/poll vs epoll), so
+            # have to test further]
+            num_ready = len(ready_fds)
+
+            for _ in range(num_ready):
+                ready_fd = ready_fds[total_write_count[0] % num_ready]
+                total_write_count[0] += 1
+                if ready_fd in active_writes:
+                    # already writing to this fd
+                    continue
+                if is_fair_strategy and ready_fd in busy_workers:
+                    # worker is already busy with another task
+                    continue
+                if ready_fd not in all_inqueues:
+                    hub_remove(ready_fd)
+                    continue
+                try:
+                    job = pop_message()
+                except IndexError:
+                    # no more messages, remove all inactive fds from the hub.
+                    # this is important since the fds are always writable
+                    # as long as there's 1 byte left in the buffer, and so
+                    # this may create a spinloop where the event loop
+                    # always wakes up.
+                    for inqfd in diff(active_writes):
+                        hub_remove(inqfd)
+                    break
+
+                else:
+                    if not job._accepted:  # job not accepted by another worker
+                        try:
+                            # keep track of what process the write operation
+                            # was scheduled for.
+                            proc = job._scheduled_for = fileno_to_inq[ready_fd]
+                            info(f"Schedule task[{job.correlation_id}] "
+                                 f"to <{proc.name}>")
+                        except KeyError:
+                            # write was scheduled for this fd but the process
+                            # has since exited and the message must be sent to
+                            # another process.
+                            put_message(job)
+                            continue
+                        cor = _write_job(proc, ready_fd, job)
+                        job._writer = ref(cor)
+                        mark_write_gen_as_active(cor)
+                        mark_write_fd_as_active(ready_fd)
+                        mark_worker_as_busy(ready_fd)
+
+                        # Try to write immediately, in case there's an error.
+                        try:
+                            next(cor)
+                        except StopIteration:
+                            pass
+                        except OSError as exc:
+                            if exc.errno != errno.EBADF:
+                                raise
+                        else:
+                            add_writer(ready_fd, cor)
+        hub.consolidate_callback = schedule_writes
+
+        def send_job(tup):
+            # Schedule writing job request for when one of the process
+            # inqueues are writable.
+            body = dumps(tup, protocol=protocol)
+            body_size = len(body)
+            header = pack('>I', body_size)
+            # index 1,0 is the job ID.
+            job = get_job(tup[1][0])
+            job._payload = buf_t(header), buf_t(body), body_size
+            put_message(job)
+        self._quick_put = send_job
+
+        def on_not_recovering(proc, fd, job, exc):
+            logger.exception(
+                'Process inqueue damaged: %r %r: %r', proc, proc.exitcode, exc)
+            if proc._is_alive():
+                proc.terminate()
+            hub.remove(fd)
+            self._put_back(job)
+
+        def _write_job(proc, fd, job):
+            # writes job to the worker process.
+            # Operation must complete if more than one byte of data
+            # was written.  If the broker connection is lost
+            # and no data was written the operation shall be canceled.
+            header, body, body_size = job._payload
+            errors = 0
+            try:
+                # job result keeps track of what process the job is sent to.
+                job._write_to = proc
+                send = proc.send_job_offset
+
+                Hw = Bw = 0
+                # write header
+                while Hw < 4:
+                    try:
+                        Hw += send(header, Hw)
+                    except Exception as exc:  # pylint: disable=broad-except
+                        if getattr(exc, 'errno', None) not in UNAVAIL:
+                            raise
+                        # suspend until more data
+                        errors += 1
+                        if errors > 100:
+                            on_not_recovering(proc, fd, job, exc)
+                            raise StopIteration()
+                        yield
+                    else:
+                        errors = 0
+
+                # write body
+                while Bw < body_size:
+                    try:
+                        Bw += send(body, Bw)
+                    except Exception as exc:  # pylint: disable=broad-except
+                        if getattr(exc, 'errno', None) not in UNAVAIL:
+                            raise
+                        # suspend until more data
+                        errors += 1
+                        if errors > 100:
+                            on_not_recovering(proc, fd, job, exc)
+                            raise StopIteration()
+                        yield
+                    else:
+                        errors = 0
+            finally:
+                hub_remove(fd)
+                write_stats[proc.index] += 1
+                # message written, so this fd is now available
+                active_writes.discard(fd)
+                write_generator_done(job._writer())  # is a weakref
+
+        def send_ack(response, pid, job, fd):
+            # Only used when synack is enabled.
+            # Schedule writing ack response for when the fd is writable.
+            msg = Ack(job, fd, precalc[response])
+            callback = promise(write_generator_done)
+            cor = _write_ack(fd, msg, callback=callback)
+            mark_write_gen_as_active(cor)
+            mark_write_fd_as_active(fd)
+            callback.args = (cor,)
+            add_writer(fd, cor)
+        self.send_ack = send_ack
+
+        def _write_ack(fd, ack, callback=None):
+            # writes ack back to the worker if synack enabled.
+            # this operation *MUST* complete, otherwise
+            # the worker process will hang waiting for the ack.
+            header, body, body_size = ack[2]
+            try:
+                try:
+                    proc = fileno_to_synq[fd]
+                except KeyError:
+                    # process died, we can safely discard the ack at this
+                    # point.
+                    raise StopIteration()
+                send = proc.send_syn_offset
+
+                Hw = Bw = 0
+                # write header
+                while Hw < 4:
+                    try:
+                        Hw += send(header, Hw)
+                    except Exception as exc:  # pylint: disable=broad-except
+                        if getattr(exc, 'errno', None) not in UNAVAIL:
+                            raise
+                        yield
+
+                # write body
+                while Bw < body_size:
+                    try:
+                        Bw += send(body, Bw)
+                    except Exception as exc:  # pylint: disable=broad-except
+                        if getattr(exc, 'errno', None) not in UNAVAIL:
+                            raise
+                        # suspend until more data
+                        yield
+            finally:
+                if callback:
+                    callback()
+                # message written, so this fd is now available
+                active_writes.discard(fd)
+
+    def flush(self):
+        if self._state == TERMINATE:
+            return
+        # cancel all tasks that haven't been accepted so that NACK is sent.
+        for job in self._cache.values():
+            if not job._accepted:
+                job._cancel()
+
+        # clear the outgoing buffer as the tasks will be redelivered by
+        # the broker anyway.
+        if self.outbound_buffer:
+            self.outbound_buffer.clear()
+
+        self.maintain_pool()
+
+        try:
+            # ...but we must continue writing the payloads we already started
+            # to keep message boundaries.
+            # The messages may be NACK'ed later if synack is enabled.
+            if self._state == RUN:
+                # flush outgoing buffers
+                intervals = fxrange(0.01, 0.1, 0.01, repeatlast=True)
+                owned_by = {}
+                for job in self._cache.values():
+                    writer = _get_job_writer(job)
+                    if writer is not None:
+                        owned_by[writer] = job
+
+                while self._active_writers:
+                    writers = list(self._active_writers)
+                    for gen in writers:
+                        if (gen.__name__ == '_write_job' and
+                                gen_not_started(gen)):
+                            # hasn't started writing the job so can
+                            # discard the task, but we must also remove
+                            # it from the Pool._cache.
+                            try:
+                                job = owned_by[gen]
+                            except KeyError:
+                                pass
+                            else:
+                                # removes from Pool._cache
+                                job.discard()
+                            self._active_writers.discard(gen)
+                        else:
+                            try:
+                                job = owned_by[gen]
+                            except KeyError:
+                                pass
+                            else:
+                                job_proc = job._write_to
+                                if job_proc._is_alive():
+                                    self._flush_writer(job_proc, gen)
+                    # workers may have exited in the meantime.
+                    self.maintain_pool()
+                    sleep(next(intervals))  # don't busyloop
+        finally:
+            self.outbound_buffer.clear()
+            self._active_writers.clear()
+            self._active_writes.clear()
+            self._busy_workers.clear()
+
+    def _flush_writer(self, proc, writer):
+        fds = {proc.inq._writer}
+        try:
+            while fds:
+                if not proc._is_alive():
+                    break  # process exited
+                readable, writable, again = _select(
+                    writers=fds, err=fds, timeout=0.5,
+                )
+                if not again and (writable or readable):
+                    try:
+                        next(writer)
+                    except (StopIteration, OSError, EOFError):
+                        break
+        finally:
+            self._active_writers.discard(writer)
+
+    def get_process_queues(self):
+        """Get queues for a new process.
+
+        Here we'll find an unused slot, as there should always
+        be one available when we start a new process.
+        """
+        return next(q for q, owner in self._queues.items()
+                    if owner is None)
+
+    def on_grow(self, n):
+        """Grow the pool by ``n`` proceses."""
+        diff = max(self._processes - len(self._queues), 0)
+        if diff:
+            self._queues.update({
+                self.create_process_queues(): None for _ in range(diff)
+            })
+
+    def on_shrink(self, n):
+        """Shrink the pool by ``n`` processes."""
+
+    @staticmethod
+    def sort_workers(worker):
+        try:
+            return process_memory_percent(worker.pid)
+        except Exception:   # noqa
+            # In case of any Exception, put it at last
+            return -1
+
+    # Called by pool.shrink only
+    def _iterinactive(self):
+        # Iter from large mem used to small
+        for worker in sorted(self._pool, key=self.sort_workers, reverse=True):
+            if not self._worker_active(worker):
+                yield worker
+
+    def _worker_active(self, worker):
+        for job in values(self._cache):
+            if (pid := worker.pid) in job.worker_pids():
+                return True
+            if (
+                (status := self._worker_status.get(pid)) is not None
+                and status.value == 1
+            ):
+                return True
+        return False
+
+    def create_process_queues(self):
+        """Create new in, out, etc. queues, returned as a tuple."""
+        # NOTE: Pipes must be set O_NONBLOCK at creation time (the original
+        # fd), otherwise it won't be possible to change the flags until
+        # there's an actual reader/writer on the other side.
+        inq = _SimpleQueue(wnonblock=True)
+        outq = _SimpleQueue(rnonblock=True)
+        synq = None
+        assert isblocking(inq._reader)
+        assert not isblocking(inq._writer)
+        assert not isblocking(outq._reader)
+        assert isblocking(outq._writer)
+        if self.synack:
+            synq = _SimpleQueue(wnonblock=True)
+            assert isblocking(synq._reader)
+            assert not isblocking(synq._writer)
+        return inq, outq, synq
+
+    def on_process_alive(self, pid):
+        """Called when receiving the :const:`WORKER_UP` message.
+
+        Marks the process as ready to receive work.
+        """
+        try:
+            proc = next(w for w in self._pool if w.pid == pid)
+        except StopIteration:
+            return logger.warning('process with pid=%s already exited', pid)
+        assert proc.inqW_fd not in self._fileno_to_inq
+        assert proc.inqW_fd not in self._all_inqueues
+        self._waiting_to_start.discard(proc)
+        self._fileno_to_inq[proc.inqW_fd] = proc
+        self._fileno_to_synq[proc.synqW_fd] = proc
+        self._all_inqueues.add(proc.inqW_fd)
+
+    def on_job_process_down(self, job, pid_gone):
+        """Called for each job when the process assigned to it exits."""
+        if job._write_to and not job._write_to._is_alive():
+            # job was partially written
+            self.on_partial_read(job, job._write_to)
+        elif job._scheduled_for and not job._scheduled_for._is_alive():
+            # job was only scheduled to be written to this process,
+            # but no data was sent so put it back on the outbound_buffer.
+            self._put_back(job)
+
+    def on_job_process_lost(self, job, pid, exitcode):
+        """Called when the process executing job' exits.
+
+        This happens when the process job'
+        was assigned to exited by mysterious means (error exitcodes and
+        signals).
+        """
+        self.mark_as_worker_lost(job, exitcode)
+
+    def human_write_stats(self):
+        if self.write_stats is None:
+            return 'N/A'
+        vals = list(self.write_stats.values())
+        total = sum(vals)
+
+        def per(v, total):
+            return f'{(float(v) / total) if v else 0:.2f}'
+
+        return {
+            'total': total,
+            'avg': per(total / len(self.write_stats) if total else 0, total),
+            'all': ', '.join(per(v, total) for v in vals),
+            'raw': ', '.join(map(str, vals)),
+            'strategy': SCHED_STRATEGY_TO_NAME.get(
+                self.sched_strategy, self.sched_strategy,
+            ),
+            'inqueues': {
+                'total': len(self._all_inqueues),
+                'active': len(self._active_writes),
+            }
+        }
+
+    def _process_cleanup_queues(self, proc):
+        """Called to clean up queues after process exit."""
+        if not proc.dead:
+            try:
+                self._queues[self._find_worker_queues(proc)] = None
+            except (KeyError, ValueError):
+                pass
+
+    @staticmethod
+    def _stop_task_handler(task_handler):
+        """Called at shutdown to tell processes that we're shutting down."""
+        for proc in task_handler.pool:
+            try:
+                setblocking(proc.inq._writer, 1)
+            except OSError:
+                pass
+            else:
+                try:
+                    proc.inq.put(None)
+                except OSError as exc:
+                    if exc.errno != errno.EBADF:
+                        raise
+
+    def create_result_handler(self):
+        return super().create_result_handler(
+            fileno_to_outq=self._fileno_to_outq,
+            on_process_alive=self.on_process_alive,
+        )
+
+    def _process_register_queues(self, proc, queues):
+        """Mark new ownership for ``queues`` to update fileno indices."""
+        assert queues in self._queues
+        b = len(self._queues)
+        self._queues[queues] = proc
+        assert b == len(self._queues)
+
+    def _find_worker_queues(self, proc):
+        """Find the queues owned by ``proc``."""
+        try:
+            return next(q for q, owner in self._queues.items()
+                        if owner == proc)
+        except StopIteration:
+            raise ValueError(proc)
+
+    def _setup_queues(self):
+        # this is only used by the original pool that used a shared
+        # queue for all processes.
+        self._quick_put = None
+
+        # these attributes are unused by this class, but we'll still
+        # have to initialize them for compatibility.
+        self._inqueue = self._outqueue = \
+            self._quick_get = self._poll_result = None
+
+    def process_flush_queues(self, proc):
+        """Flush all queues.
+
+        Including the outbound buffer, so that
+        all tasks that haven't been started will be discarded.
+
+        In Celery this is called whenever the transport connection is lost
+        (consumer restart), and when a process is terminated.
+        """
+        resq = proc.outq._reader
+        on_state_change = self._result_handler.on_state_change
+        fds = {resq}
+        while fds and not resq.closed and self._state != TERMINATE:
+            readable, _, _ = _select(fds, None, fds, timeout=0.01)
+            if readable:
+                try:
+                    task = resq.recv()
+                except (OSError, EOFError) as exc:
+                    _errno = getattr(exc, 'errno', None)
+                    if _errno == errno.EINTR:
+                        continue
+                    elif _errno == errno.EAGAIN:
+                        break
+                    elif _errno not in UNAVAIL:
+                        debug('got %r while flushing process %r',
+                              exc, proc, exc_info=1)
+                    break
+                else:
+                    if task is None:
+                        debug('got sentinel while flushing process %r', proc)
+                        break
+                    else:
+                        on_state_change(task)
+            else:
+                break
+
+    def on_partial_read(self, job, proc):
+        """Called when a job was partially written to exited child."""
+        # worker terminated by signal:
+        # we cannot reuse the sockets again, because we don't know if
+        # the process wrote/read anything frmo them, and if so we cannot
+        # restore the message boundaries.
+        if not job._accepted:
+            # job was not acked, so find another worker to send it to.
+            self._put_back(job)
+        writer = _get_job_writer(job)
+        if writer:
+            self._active_writers.discard(writer)
+            del writer
+
+        if not proc.dead:
+            proc.dead = True
+            # Replace queues to avoid reuse
+            before = len(self._queues)
+            try:
+                queues = self._find_worker_queues(proc)
+                if self.destroy_queues(queues, proc):
+                    self._queues[self.create_process_queues()] = None
+            except ValueError:
+                pass
+            assert len(self._queues) == before
+
+    def destroy_queues(self, queues, proc):
+        """Destroy queues that can no longer be used.
+
+        This way they can be replaced by new usable sockets.
+        """
+        assert not proc._is_alive()
+        self._waiting_to_start.discard(proc)
+        removed = 1
+        try:
+            self._queues.pop(queues)
+        except KeyError:
+            removed = 0
+        try:
+            self.on_inqueue_close(queues[0]._writer.fileno(), proc)
+        except OSError:
+            pass
+        for queue in queues:
+            if queue:
+                for sock in (queue._reader, queue._writer):
+                    if not sock.closed:
+                        self.hub_remove(sock)
+                        try:
+                            sock.close()
+                        except OSError:
+                            pass
+        return removed
+
+    def _create_payload(self, type_, args,
+                        dumps=_pickle.dumps, pack=pack,
+                        protocol=HIGHEST_PROTOCOL):
+        body = dumps((type_, args), protocol=protocol)
+        size = len(body)
+        header = pack('>I', size)
+        return header, body, size
+
+    @classmethod
+    def _set_result_sentinel(cls, _outqueue, _pool):
+        # unused
+        pass
+
+    def _help_stuff_finish_args(self):
+        # Pool._help_stuff_finished is a classmethod so we have to use this
+        # trick to modify the arguments passed to it.
+        return (self._pool,)
+
+    @classmethod
+    def _help_stuff_finish(cls, pool):
+        # pylint: disable=arguments-differ
+        debug(
+            'removing tasks from inqueue until task handler finished',
+        )
+        fileno_to_proc = {}
+        inqR = set()
+        for w in pool:
+            try:
+                fd = w.inq._reader.fileno()
+                inqR.add(fd)
+                fileno_to_proc[fd] = w
+            except OSError:
+                pass
+        while inqR:
+            readable, _, again = _select(inqR, timeout=0.5)
+            if again:
+                continue
+            if not readable:
+                break
+            for fd in readable:
+                fileno_to_proc[fd].inq._reader.recv()
+            sleep(0)
+
+    @property
+    def timers(self):
+        return {self.maintain_pool: 5.0}
```

## celery/events/event.py

 * *Ordering differences only*

```diff
@@ -1,63 +1,63 @@
-"""Creating events, and event exchange definition."""
-import time
-from copy import copy
-
-from kombu import Exchange
-
-__all__ = (
-    'Event', 'event_exchange', 'get_exchange', 'group_from',
-)
-
-EVENT_EXCHANGE_NAME = 'celeryev'
-#: Exchange used to send events on.
-#: Note: Use :func:`get_exchange` instead, as the type of
-#: exchange will vary depending on the broker connection.
-event_exchange = Exchange(EVENT_EXCHANGE_NAME, type='topic')
-
-
-def Event(type, _fields=None, __dict__=dict, __now__=time.time, **fields):
-    """Create an event.
-
-    Notes:
-        An event is simply a dictionary: the only required field is ``type``.
-        A ``timestamp`` field will be set to the current time if not provided.
-    """
-    event = __dict__(_fields, **fields) if _fields else fields
-    if 'timestamp' not in event:
-        event.update(timestamp=__now__(), type=type)
-    else:
-        event['type'] = type
-    return event
-
-
-def group_from(type):
-    """Get the group part of an event type name.
-
-    Example:
-        >>> group_from('task-sent')
-        'task'
-
-        >>> group_from('custom-my-event')
-        'custom'
-    """
-    return type.split('-', 1)[0]
-
-
-def get_exchange(conn, name=EVENT_EXCHANGE_NAME):
-    """Get exchange used for sending events.
-
-    Arguments:
-        conn (kombu.Connection): Connection used for sending/receiving events.
-        name (str): Name of the exchange. Default is ``celeryev``.
-
-    Note:
-        The event type changes if Redis is used as the transport
-        (from topic -> fanout).
-    """
-    ex = copy(event_exchange)
-    if conn.transport.driver_type in ('redis', 'redis-cluster'):
-        # quick hack for Issue #436
-        ex.type = 'fanout'
-    if name != ex.name:
-        ex.name = name
-    return ex
+"""Creating events, and event exchange definition."""
+import time
+from copy import copy
+
+from kombu import Exchange
+
+__all__ = (
+    'Event', 'event_exchange', 'get_exchange', 'group_from',
+)
+
+EVENT_EXCHANGE_NAME = 'celeryev'
+#: Exchange used to send events on.
+#: Note: Use :func:`get_exchange` instead, as the type of
+#: exchange will vary depending on the broker connection.
+event_exchange = Exchange(EVENT_EXCHANGE_NAME, type='topic')
+
+
+def Event(type, _fields=None, __dict__=dict, __now__=time.time, **fields):
+    """Create an event.
+
+    Notes:
+        An event is simply a dictionary: the only required field is ``type``.
+        A ``timestamp`` field will be set to the current time if not provided.
+    """
+    event = __dict__(_fields, **fields) if _fields else fields
+    if 'timestamp' not in event:
+        event.update(timestamp=__now__(), type=type)
+    else:
+        event['type'] = type
+    return event
+
+
+def group_from(type):
+    """Get the group part of an event type name.
+
+    Example:
+        >>> group_from('task-sent')
+        'task'
+
+        >>> group_from('custom-my-event')
+        'custom'
+    """
+    return type.split('-', 1)[0]
+
+
+def get_exchange(conn, name=EVENT_EXCHANGE_NAME):
+    """Get exchange used for sending events.
+
+    Arguments:
+        conn (kombu.Connection): Connection used for sending/receiving events.
+        name (str): Name of the exchange. Default is ``celeryev``.
+
+    Note:
+        The event type changes if Redis is used as the transport
+        (from topic -> fanout).
+    """
+    ex = copy(event_exchange)
+    if conn.transport.driver_type in ('redis', 'redis-cluster'):
+        # quick hack for Issue #436
+        ex.type = 'fanout'
+    if name != ex.name:
+        ex.name = name
+    return ex
```

## celery/patches/__init__.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-
-
-def apply_patches():
-    from . import kombu
-    from . import redis
-
-    kombu.apply_patch()
-    redis.apply_patch()
+
+
+def apply_patches():
+    from . import kombu
+    from . import redis
+
+    kombu.apply_patch()
+    redis.apply_patch()
```

## celery/patches/kombu/__init__.py

 * *Ordering differences only*

```diff
@@ -1,9 +1,9 @@
-__all__ = ('apply_patch', )
-
-
-def apply_patch():
-    from kombu import transport
-
-    transport.TRANSPORT_ALIASES.update({
-      'redis-cluster': 'celery.patches.kombu.redis_cluster:Transport',
-    })
+__all__ = ('apply_patch', )
+
+
+def apply_patch():
+    from kombu import transport
+
+    transport.TRANSPORT_ALIASES.update({
+      'redis-cluster': 'celery.patches.kombu.redis_cluster:Transport',
+    })
```

## celery/patches/kombu/redis.py

 * *Ordering differences only*

```diff
@@ -1,1096 +1,1096 @@
-import asyncio
-import contextlib
-import socket
-import functools
-import warnings
-
-from itertools import count
-from contextlib import asynccontextmanager
-from time import monotonic, time
-from typing import *
-
-import redis
-import redis.asyncio as aioredis
-import numbers
-
-from redis.asyncio import (
-    sentinel,
-    cluster
-)
-from redis import exceptions
-from kombu.transport.redis import (
-    Transport as SyncTransport,
-    Channel as SyncChannel,
-    MultiChannelPoller as SyncMultiChannelPoller,
-    QoS as SyncQoS,
-    PrefixedStrictRedis,
-    error_classes_t,
-    virtual,
-    dumps,
-    loads,
-    cycle_by_name,
-    bytes_to_str,
-    InconsistencyError,
-    VersionMismatch,
-    Empty,
-    MutexHeld,
-
-    crit,
-    warn,
-    _parse_url,
-)
-from kombu.transport.virtual.base import (
-    UNDELIVERABLE_FMT,
-    queue_declare_ok_t,
-    uuid,
-    ChannelError,
-    UndeliverableWarning,
-    FairCycle,
-)
-from redis.exceptions import MovedError
-
-from .exchange import STANDARD_EXCHANGE_TYPES
-from .utils import is_async_callable
-from celery.patches.redis import ClusterConnectionPool
-
-
-@asynccontextmanager
-async def Mutex(client: aioredis.Redis, name, expire):
-    """Acquire redis lock in non blocking way.
-
-    Raise MutexHeld if not successful.
-    """
-    lock = client.lock(name, timeout=expire)
-    lock_acquired = False
-    try:
-        lock_acquired = await lock.acquire(blocking=False)
-        if lock_acquired:
-            yield
-        else:
-            raise MutexHeld()
-    finally:
-        if lock_acquired:
-            try:
-                await lock.release()
-            except exceptions.LockNotOwnedError:
-                # when lock is expired
-                pass
-
-
-class QoS(SyncQoS):
-    @asynccontextmanager
-    async def pipe_or_acquire(self, pipe=None, client=None):
-        if pipe:
-            yield pipe
-        else:
-            with self.channel.conn_or_acquire(client) as client:
-                async with client.pipeline() as pipe:
-                    yield pipe
-
-    async def _remove_from_indices(self, delivery_tag, pipe=None):
-        async with self.pipe_or_acquire(pipe) as pipe:
-            return pipe.zrem(self.unacked_index_key, delivery_tag) \
-                       .hdel(self.unacked_key, delivery_tag)
-
-    async def append(self, message, delivery_tag):
-        delivery = message.delivery_info
-        EX, RK = delivery['exchange'], delivery['routing_key']
-        zadd_args = [{delivery_tag: time()}]
-
-        async with self.pipe_or_acquire() as pipe:
-            await (pipe
-                .zadd(self.unacked_index_key, *zadd_args)
-                .hset(self.unacked_key, delivery_tag,
-                      dumps([message._raw, EX, RK]))
-                .execute()
-            )
-            if self._dirty:
-                self._flush()
-            self._quick_append(delivery_tag, message)
-
-    async def restore_unacked(self, client=None):
-        with self.channel.conn_or_acquire(client) as client:
-            for tag in self._delivered:
-                await self.restore_by_tag(tag, client=client)
-        self._delivered.clear()
-
-    async def ack(self, delivery_tag):
-        pipe = await self._remove_from_indices(delivery_tag)
-        await pipe.execute()
-        self._quick_ack(delivery_tag)
-
-    async def reject(self, delivery_tag, requeue=False):
-        if requeue:
-            await self.restore_by_tag(delivery_tag, leftmost=True)
-        await self.ack(delivery_tag)
-
-    async def restore_visible(self, start=0, num=10, interval=10):
-        self._vrestore_count += 1
-        if (self._vrestore_count - 1) % interval:
-            return
-        with self.channel.conn_or_acquire() as client:
-            ceil = time() - self.visibility_timeout
-            try:
-                async with Mutex(client, self.unacked_mutex_key,
-                           self.unacked_mutex_expire):
-                    visible = await client.zrevrangebyscore(
-                        self.unacked_index_key, ceil, 0,
-                        start=num and start, num=num, withscores=True)
-                    for tag, score in visible or []:
-                        await self.restore_by_tag(tag, client)
-            except MutexHeld:
-                pass
-
-    async def restore_by_tag(self, tag, client=None, leftmost=False):
-
-        async def restore_transaction(pipe):
-            p = pipe.hget(self.unacked_key, tag)
-            pipe.multi()
-            pipe = await self._remove_from_indices(tag, pipe)
-            if p:
-                M, EX, RK = loads(bytes_to_str(p))  # json is unicode
-                await self.channel._do_restore_message(M, EX, RK, pipe, leftmost)
-
-        with self.channel.conn_or_acquire(client) as client:
-            await client.transaction(restore_transaction, self.unacked_key)
-
-
-class AsyncFairCycle(FairCycle):
-    async def get(self, callback, **kwargs):
-        """Get from next resource."""
-        for tried in count(0):  # for infinity
-            resource = self._next()
-            try:
-                return await self.fun(resource, callback, **kwargs)
-            except self.predicate:
-                # reraise when retries exchausted.
-                if tried >= len(self.resources) - 1:
-                    raise
-
-
-def get_redis_error_classes():
-    # This exception suddenly changed name between redis-py versions
-    if hasattr(exceptions, 'InvalidData'):
-        DataError = exceptions.InvalidData
-    else:
-        DataError = exceptions.DataError
-
-    return error_classes_t(
-        (virtual.Transport.connection_errors + (
-            InconsistencyError,
-            socket.error,
-            IOError,
-            OSError,
-            exceptions.ConnectionError,
-            exceptions.AuthenticationError,
-            exceptions.TimeoutError,
-            RuntimeError,  # uvloop might raise this
-        )),
-        (virtual.Transport.channel_errors + (
-            DataError,
-            exceptions.InvalidResponse,
-            exceptions.ResponseError)),
-    )
-
-
-class Channel(SyncChannel):
-    QoS = QoS
-    connection_class = aioredis.Connection
-    connection_class_ssl = aioredis.SSLConnection
-    exchange_types = dict(STANDARD_EXCHANGE_TYPES)
-
-    if TYPE_CHECKING:
-        from redis.asyncio.client import PubSub
-        @property
-        def subclient(self) -> PubSub: ...  # noqa
-
-    def __init__(self, *args, **kwargs):
-        virtual.Channel.__init__(self, *args, **kwargs)
-
-        if not self.ack_emulation:  # disable visibility timeout
-            self.QoS = virtual.QoS
-
-        self._queue_cycle = cycle_by_name(self.queue_order_strategy)()
-        self.Client = self._get_client()
-        self.ResponseError = self._get_response_error()
-        self.active_fanout_queues = set()
-        self.auto_delete_queues = set()
-        self._fanout_to_queue = {}
-        self.handlers = {'BRPOP': self._brpop_read, 'LISTEN': self._receive}
-
-        if self.fanout_prefix:
-            if isinstance(self.fanout_prefix, str):
-                self.keyprefix_fanout = self.fanout_prefix
-        else:
-            # previous versions did not set a fanout, so cannot enable
-            # by default.
-            self.keyprefix_fanout = ''
-
-        self.connection.cycle.add(self)  # add to channel poller.
-        # copy errors, in case channel closed but threads still
-        # are still waiting for data.
-        self.connection_errors = self.connection.connection_errors
-        self._conn_evaled = False
-
-        self._brpop_task: asyncio.Task = None
-        self._listen_task: asyncio.Task = None
-
-        # if register_after_fork is not None:
-        #     register_after_fork(self, _after_fork_cleanup_channel)
-
-    async def ping(self):
-        return await self.client.ping()
-
-    async def _after_fork(self):
-        await self._disconnect_pools()
-
-    async def _disconnect_pools(self):
-        pool = self._pool
-        async_pool = self._async_pool
-        is_same_pool = pool is async_pool
-
-        self._async_pool = self._pool = None
-
-        if pool is not None:
-            await pool.disconnect()
-
-        if not is_same_pool and async_pool is not None:
-            await async_pool.disconnect()
-
-    async def _do_restore_message(self, payload, exchange, routing_key,
-                            pipe, leftmost=False):
-        try:
-            try:
-                payload['headers']['redelivered'] = True
-                payload['properties']['delivery_info']['redelivered'] = True
-            except KeyError:
-                pass
-            for queue in await self._lookup(exchange, routing_key):
-                await (pipe.lpush if leftmost else pipe.rpush)(
-                    queue, dumps(payload),
-                )
-        except Exception:
-            crit('Could not restore message: %r', payload, exc_info=True)
-
-    async def _restore(self, message, leftmost=False):
-        if not self.ack_emulation:
-            delivery_info = message.delivery_info
-            message = message.serializable()
-            message['redelivered'] = True
-            for queue in await self._lookup(
-                delivery_info['exchange'], delivery_info['routing_key']
-            ):
-                await self._put(queue, message)
-            return
-
-        tag = message.delivery_tag
-
-        async def restore_transaction(pipe):
-            P = await pipe.hget(self.unacked_key, tag)
-            pipe.multi()
-            await pipe.hdel(self.unacked_key, tag)
-            if P:
-                M, EX, RK = loads(bytes_to_str(P))  # json is unicode
-                await self._do_restore_message(M, EX, RK, pipe, leftmost)
-
-        with self.conn_or_acquire() as client:
-            await client.transaction(restore_transaction, self.unacked_key)
-
-    async def _restore_at_beginning(self, message):
-        return await self._restore(message, leftmost=True)
-
-    def _get_response_error(self):
-        return exceptions.ResponseError
-
-    async def _get(self, queue):
-        with self.conn_or_acquire() as client:
-            for pri in self.priority_steps:
-                item = await client.rpop(self._q_for_pri(queue, pri))
-                if item:
-                    return loads(bytes_to_str(item))
-            raise Empty()
-
-    async def _size(self, queue):
-        with self.conn_or_acquire() as client:
-            async with client.pipeline() as pipe:
-                for pri in self.priority_steps:
-                    pipe = pipe.llen(self._q_for_pri(queue, pri))
-                sizes = await pipe.execute()
-        return sum(size for size in sizes
-                   if isinstance(size, numbers.Integral))
-
-    async def _put(self, queue, message, **kwargs):
-        """Deliver message."""
-        pri = self._get_message_priority(message, reverse=False)
-
-        with self.conn_or_acquire() as client:
-            await client.lpush(self._q_for_pri(queue, pri), dumps(message))
-
-    async def _put_fanout(self, exchange, message, routing_key, **kwargs):
-        """Deliver fanout message."""
-        with self.conn_or_acquire() as client:
-            await client.publish(
-                self._get_publish_topic(exchange, routing_key),
-                dumps(message),
-            )
-
-    async def _queue_bind(self, exchange, routing_key, pattern, queue):
-        if self.typeof(exchange).type == 'fanout':
-            # Mark exchange as fanout.
-            self._fanout_queues[queue] = (
-                exchange, routing_key.replace('#', '*'),
-            )
-        with self.conn_or_acquire() as client:
-            await client.sadd(
-                self.keyprefix_queue % (exchange,),
-                self.sep.join([routing_key or '', pattern or '', queue or ''])
-            )
-
-    async def _delete(self, queue, exchange, routing_key, pattern, *args, **kwargs):
-        self.auto_delete_queues.discard(queue)
-        with self.conn_or_acquire(client=kwargs.get('client')) as client:
-            await client.srem(self.keyprefix_queue % (exchange,),
-                        self.sep.join([routing_key or '',
-                                       pattern or '',
-                                       queue or '']))
-            async with client.pipeline() as pipe:
-                for pri in self.priority_steps:
-                    pipe = pipe.delete(self._q_for_pri(queue, pri))
-                await pipe.execute()
-
-    async def _has_queue(self, queue, **kwargs):
-        with self.conn_or_acquire() as client:
-            async with client.pipeline() as pipe:
-                for pri in self.priority_steps:
-                    pipe = pipe.exists(self._q_for_pri(queue, pri))
-                return any(await pipe.execute())
-
-    async def get_table(self, exchange):
-        key = self.keyprefix_queue % exchange
-        with self.conn_or_acquire() as client:
-            values = await client.smembers(key)
-            if not values:
-                # table does not exists since all queues bound to the exchange
-                # were deleted. We need just return empty list.
-                return []
-            return [tuple(bytes_to_str(val).split(self.sep)) for val in values]
-
-    async def _purge(self, queue):
-        with self.conn_or_acquire() as client:
-            async with client.pipeline() as pipe:
-                for pri in self.priority_steps:
-                    priq = self._q_for_pri(queue, pri)
-                    pipe = pipe.llen(priq).delete(priq)
-                sizes = await pipe.execute()
-                return sum(sizes[::2])
-
-    async def close(self):
-        self._closing = True
-        if not self.closed:
-            # remove from channel poller.
-            self.connection.cycle.discard(self)
-
-            # delete fanout bindings
-            client = self.__dict__.get('client')  # only if property cached
-            if client is not None:
-                for queue in self._fanout_queues:
-                    if queue in self.auto_delete_queues:
-                        await self.queue_delete(queue, client=client)
-            await self._disconnect_pools()
-            await self._close_clients()
-
-            # --------------------------------------
-            # virtual.base::Channel
-            self.closed = True
-
-            for consumer in list(self._consumers):
-                self.basic_cancel(consumer)  # todo maybe await
-            if self._qos:
-                self._qos.restore_unacked_once()
-            if self._cycle is not None:
-                self._cycle.close()
-                self._cycle = None
-            if self.connection is not None:
-                self.connection.close_channel(self)  # todo maybe await
-        self.exchange_types = None
-
-    async def _close_clients(self):
-        for attr in 'client', 'subclient':
-            try:
-                client = self.__dict__[attr]
-                connection, client.connection = client.connection, None
-                await connection.disconnect()
-            except (KeyError, AttributeError, self.ResponseError):
-                pass
-
-    def _connparams(self, asynchronous=False):
-        params = super(Channel, self)._connparams(asynchronous)
-        channel = self
-
-        if asynchronous:
-            class Connection(self.connection_class):
-                async def disconnect(self, nowait: bool = False):
-                    await super().disconnect(nowait=nowait)
-                    channel._on_connection_disconnect(self)
-
-            params['connection_class'] = Connection
-        return params
-
-    def _get_pool(self, asynchronous=True):
-        params = self._connparams(asynchronous=True)
-        self.keyprefix_fanout = self.keyprefix_fanout.format(db=params['db'])
-        return aioredis.ConnectionPool(**params)
-
-    def _get_client(self):
-        if getattr(redis, 'VERSION', (1, 0)) < (4, 1, 0):
-            raise VersionMismatch(
-                'Redis transport requires redis versions 4.1.0 or later. '
-                f'You have {redis.__version__}')
-
-        if self.global_keyprefix:
-            return functools.partial(
-                PrefixedStrictRedis,
-                global_keyprefix=self.global_keyprefix,
-            )
-        return aioredis.StrictRedis
-
-    # -----------------------------------------------------------------------------
-    # virtual.base::Channel method overload
-    async def queue_delete(self, queue, if_unused=False, if_empty=False, **kwargs):
-        """Delete queue."""
-        if if_empty and self._size(queue):
-            return
-        for exchange, routing_key, args in self.state.queue_bindings(queue):
-            meta = self.typeof(exchange).prepare_bind(
-                queue, exchange, routing_key, args,
-            )
-            await self._delete(queue, exchange, *meta, **kwargs)
-        self.state.queue_bindings_delete(queue)
-
-    async def after_reply_message_received(self, queue):
-        await self.queue_delete(queue)
-
-    async def queue_bind(self, queue, exchange=None, routing_key='',
-                   arguments=None, **kwargs):
-        """Bind `queue` to `exchange` with `routing key`."""
-        exchange = exchange or 'amq.direct'
-        if self.state.has_binding(queue, exchange, routing_key):
-            return
-        # Add binding:
-        self.state.binding_declare(queue, exchange, routing_key, arguments)
-        # Update exchange's routing table:
-        table = self.state.exchanges[exchange].setdefault('table', [])
-        meta = self.typeof(exchange).prepare_bind(
-            queue, exchange, routing_key, arguments,
-        )
-        table.append(meta)
-        if self.supports_fanout:
-            await self._queue_bind(exchange, *meta)
-
-    async def queue_unbind(self, queue, exchange=None, routing_key='',
-                     arguments=None, **kwargs):
-        # Remove queue binding:
-        self.state.binding_delete(queue, exchange, routing_key)
-        try:
-            table = await self.get_table(exchange)
-        except KeyError:
-            return
-        binding_meta = self.typeof(exchange).prepare_bind(
-            queue, exchange, routing_key, arguments,
-        )
-        # TODO: the complexity of this operation is O(number of bindings).
-        # Should be optimized.  Modifying table in place.
-        table[:] = [meta for meta in table if meta != binding_meta]
-
-    async def list_bindings(self):
-        return ((queue, exchange, rkey)
-                for exchange in self.state.exchanges
-                for rkey, pattern, queue in await self.get_table(exchange))
-
-    async def queue_declare(self, queue=None, passive=False, **kwargs):
-        """Declare queue."""
-        queue = queue or 'amq.gen-%s' % uuid()
-        if passive and not (await self._has_queue(queue, **kwargs)):
-            raise ChannelError(
-                'NOT_FOUND - no queue {!r} in vhost {!r}'.format(
-                    queue, self.connection.client.virtual_host or '/'),
-                (50, 10), 'Channel.queue_declare', '404',
-            )
-        else:
-            self._new_queue(queue, **kwargs)
-        return queue_declare_ok_t(queue, await self._size(queue), 0)
-
-    async def queue_purge(self, queue, **kwargs):
-        """Remove all ready messages from queue."""
-        return await self._purge(queue)
-
-    async def basic_publish(self, message, exchange, routing_key, **kwargs):
-        """Publish message."""
-        self._inplace_augment_message(message, exchange, routing_key)
-        if exchange:
-            return await self.typeof(exchange).deliver(
-                message, exchange, routing_key, **kwargs
-            )
-        # anon exchange: routing_key is the destination queue
-        return await self._put(routing_key, message, **kwargs)
-
-    async def basic_get(self, queue, no_ack=False, **kwargs):
-        """Get message by direct access (synchronous)."""
-        try:
-            message = self.Message((await self._get(queue)), channel=self)
-            if not no_ack:
-                self.qos.append(message, message.delivery_tag)
-            return message
-        except Empty:
-            pass
-
-    async def _lookup(self, exchange, routing_key, default=None):
-        """Find all queues matching `routing_key` for the given `exchange`.
-
-        Returns:
-            str: queue name -- must return the string `default`
-                if no queues matched.
-        """
-        if default is None:
-            default = self.deadletter_queue
-        if not exchange:  # anon exchange
-            return [routing_key or default]
-
-        try:
-            R = self.typeof(exchange).lookup(
-                await self.get_table(exchange),
-                exchange, routing_key, default,
-            )
-        except KeyError:
-            R = []
-
-        if not R and default is not None:
-            warnings.warn(UndeliverableWarning(UNDELIVERABLE_FMT.format(
-                exchange=exchange, routing_key=routing_key)),
-            )
-            self._new_queue(default)
-            R = [default]
-        return R
-
-    async def __aenter__(self):
-        return self
-
-    async def __aexit__(self, exc_type, exc_val, exc_tb):
-        await self.close()
-
-    def __enter__(self):
-        raise NotImplementedError
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        raise NotImplementedError
-
-    def _reset_cycle(self):
-        self._cycle = AsyncFairCycle(
-            self._get_and_deliver, self._active_queues, Empty)
-
-    async def _get_and_deliver(self, queue, callback):
-        message = await self._get(queue)
-        callback(message, queue)
-
-    # -----------------------------------------------------------------------------
-    # exchange
-    async def exchange_delete(self, exchange, if_unused=False, nowait=False):
-        """Delete `exchange` and all its bindings."""
-        for rkey, _, queue in (await self.get_table(exchange)):
-            await self.queue_delete(queue, if_unused=True, if_empty=True)
-        self.state.exchanges.pop(exchange, None)
-
-    # -----------------------------------------------------------------------------
-    # consumer
-    async def _brpop_start(self, timeout=1):
-        queues = self._queue_cycle.consume(len(self.active_queues))
-        if not queues:
-            return
-
-        keys = [self._q_for_pri(queue, pri) for pri in self.priority_steps
-                for queue in queues] + [timeout or 0]
-
-        command_args = ['BRPOP', *keys]
-        if self.global_keyprefix:
-            command_args = self.client._prefix_args(command_args)
-
-        async def brpop(conn):
-            await conn.send_command(*command_args)
-            return await self._brpop_read_impl()
-
-        self._in_poll = True
-        self._brpop_task = task = asyncio.create_task(
-            brpop(self.client.connection))
-
-        await asyncio.sleep(0)
-        return task
-
-    async def _brpop_read_impl(self, **options):
-        try:
-            dest__item = await self.client.parse_response(
-                self.client.connection,
-                'BRPOP',
-                **options
-            )
-        except self.connection_errors:
-            # if there's a ConnectionError, disconnect so the next
-            # iteration will reconnect automatically.
-            await self.client.connection.disconnect()
-            raise
-        if dest__item:
-            dest, item = dest__item
-            dest = bytes_to_str(dest).rsplit(self.sep, 1)[0]
-            self._queue_cycle.rotate(dest)
-            self.connection._deliver(loads(bytes_to_str(item)), dest)
-            return True
-        else:
-            raise Empty()
-
-    def _brpop_read(self, **options):
-        try:
-            return self._brpop_task.result()
-        finally:
-            self._brpop_task = None
-            self._in_poll = None
-
-    async def _subscribe(self):
-        keys = [self._get_subscribe_topic(queue)
-                for queue in self.active_fanout_queues]
-        if not keys:
-            return
-        c = self.subclient
-        if not c.connection.is_connected:
-            await c.connection.connect()
-        self._in_listen = c.connection
-        await c.psubscribe(keys)
-        self._listen_task = t = asyncio.create_task(c.parse_response())
-        await asyncio.sleep(0)
-        return t
-
-    async def _unsubscribe_from(self, queue):
-        topic = self._get_subscribe_topic(queue)
-        c = self.subclient
-        if c.connection and c.connection.is_connected:
-            await c.unsubscribe([topic])
-
-    def _receive_one(self, c):
-        try:
-            response = self._listen_task.result()
-        except self.connection_errors:
-            self._in_listen = None
-            raise
-        finally:
-            self._listen_task = None
-        if isinstance(response, (list, tuple)):
-            payload = self._handle_message(c, response)
-            if bytes_to_str(payload['type']).endswith('message'):
-                channel = bytes_to_str(payload['channel'])
-                if payload['data']:
-                    if channel[0] == '/':
-                        _, _, channel = channel.partition('.')
-                    try:
-                        message = loads(bytes_to_str(payload['data']))
-                    except (TypeError, ValueError):
-                        warn('Cannot process event on channel %r: %s',
-                             channel, repr(payload)[:4096], exc_info=1)
-                        raise Empty()
-                    exchange = channel.split('/', 1)[0]
-                    self.connection._deliver(
-                        message, self._fanout_to_queue[exchange])
-                    return True
-
-
-class MultiChannelPoller(SyncMultiChannelPoller):
-    def __init__(self):
-        self._channels = set()
-        self._pending_tasks = set()
-
-    @staticmethod
-    async def ensure_connection(client):
-        conn = getattr(client, 'connection', None)
-        if conn is None:
-            client.connection = await client.connection_pool.get_connection('_')
-        elif not conn.is_connected:
-            await client.connection.connect()
-
-    async def _register_BRPOP(self, channel):
-        """Enable BRPOP mode for channel."""
-        await self.ensure_connection(channel.client)
-
-        if not channel._in_poll:  # send BRPOP
-            task = await channel._brpop_start()
-            task.channel = channel
-            task.cmd = 'BRPOP'
-            self._pending_tasks.add(task)
-
-    async def _register_LISTEN(self, channel):
-        """Enable LISTEN mode for channel."""
-        await self.ensure_connection(channel.client)
-
-        if not channel._in_listen:
-            task = await channel._subscribe()  # send SUBSCR
-            task.channel = channel
-            task.cmd = 'LISTEN'
-            self._pending_tasks.add(task)
-
-    async def poll(self, timeout):
-        if not self._pending_tasks:
-            return
-
-        done, pending = await asyncio.wait(
-            self._pending_tasks,
-            timeout=timeout,
-            return_when=asyncio.FIRST_COMPLETED
-        )
-        for task in done:
-            self._pending_tasks.discard(task)
-        if not done:
-            return
-        else:
-            return done.pop()
-
-    @staticmethod
-    async def handle_task(task):
-        chan = task.channel  # noqa
-        type = task.cmd  # noqa
-
-        if chan.qos.can_consume():
-            hdlr = chan.handlers[type]
-            if is_async_callable(hdlr):
-                await hdlr()
-            else:
-                hdlr()
-
-        return True
-
-    async def get(self, callback, timeout=None):
-        self._in_protected_read = True
-        try:
-            for channel in self._channels:
-                if channel.active_queues:           # BRPOP mode?
-                    if channel.qos.can_consume():
-                        await self._register_BRPOP(channel)
-                if channel.active_fanout_queues:    # LISTEN mode?
-                    await self._register_LISTEN(channel)
-
-            if (
-                (done_task := await self.poll(timeout))
-                and (await self.handle_task(done_task))
-            ):
-                return
-            # - no new data, so try to restore messages.
-            # - reset active redis commands.
-            await self.maybe_restore_messages()
-            raise Empty()
-        finally:
-            self._in_protected_read = False
-            while self.after_read:
-                try:
-                    fun = self.after_read.pop()
-                except KeyError:
-                    break
-                else:
-                    fun()
-
-    async def maybe_restore_messages(self):
-        for channel in self._channels:
-            if channel.active_queues:
-                # only need to do this once, as they are not local to channel.
-                return await channel.qos.restore_visible(
-                    num=channel.unacked_restore_limit,
-                )
-
-
-class Transport(SyncTransport):
-    Channel = Channel
-    connection_errors, channel_errors = get_redis_error_classes()
-    Poller = MultiChannelPoller
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-
-        # All channels share the same poller.
-        self.cycle = self.Poller()
-
-    async def establish_connection(self):
-        channel: Channel = self.create_channel(self)
-        await channel.ping()
-        self._avail_channels.append(channel)
-        return self     # for drain events
-
-    async def close_connection(self, connection):
-        self.cycle.close()
-        for chan_list in self._avail_channels, self.channels:
-            while chan_list:
-                try:
-                    channel = chan_list.pop()
-                except LookupError:  # pragma: no cover
-                    pass
-                else:
-                    await channel.close()
-
-    async def drain_events(self, connection, timeout=None):
-        time_start = monotonic()
-        get = self.cycle.get
-        polling_interval = self.polling_interval
-        if timeout and polling_interval and polling_interval > timeout:
-            polling_interval = timeout
-        while 1:
-            try:
-                await get(self._deliver, timeout=timeout)
-            except Empty:
-                if timeout is not None and monotonic() - time_start >= timeout:
-                    raise socket.timeout()
-                if polling_interval is not None:
-                    await asyncio.sleep(polling_interval)
-            else:
-                break
-
-
-class SentinelManagedSSLConnection(
-        sentinel.SentinelManagedConnection,
-        aioredis.SSLConnection):
-    pass
-
-
-class SentinelChannel(Channel):
-    from_transport_options = Channel.from_transport_options + (
-        'master_name',
-        'min_other_sentinels',
-        'sentinel_kwargs')
-
-    connection_class = sentinel.SentinelManagedConnection
-    connection_class_ssl = SentinelManagedSSLConnection
-
-    def _sentinel_managed_pool(self, asynchronous=False):
-        connparams = self._connparams(asynchronous)
-
-        additional_params = connparams.copy()
-
-        additional_params.pop('host', None)
-        additional_params.pop('port', None)
-
-        sentinels = []
-        for url in self.connection.client.alt:
-            url = _parse_url(url)
-            if url.scheme == 'sentinel':
-                port = url.port or self.connection.default_port
-                sentinels.append((url.hostname, port))
-
-        # Fallback for when only one sentinel is provided.
-        if not sentinels:
-            sentinels.append((connparams['host'], connparams['port']))
-
-        sentinel_inst = sentinel.Sentinel(
-            sentinels,
-            min_other_sentinels=getattr(self, 'min_other_sentinels', 0),
-            sentinel_kwargs=getattr(self, 'sentinel_kwargs', None),
-            **additional_params)
-
-        master_name = getattr(self, 'master_name', None)
-
-        if master_name is None:
-            raise ValueError(
-                "'master_name' transport option must be specified."
-            )
-
-        return sentinel_inst.master_for(
-            master_name,
-            self.Client,
-        ).connection_pool
-
-    def _get_pool(self, asynchronous=False):
-        return self._sentinel_managed_pool(asynchronous)
-
-
-class SentinelTransport(Transport):
-    """Redis Sentinel Transport."""
-
-    default_port = 26379
-    Channel = SentinelChannel
-
-
-class ClusterPoller(MultiChannelPoller):
-    @staticmethod
-    async def ensure_connection(client: cluster.RedisCluster):
-        if client._initialize:
-            await client.initialize()
-
-    async def _register_BRPOP(self, channel):
-        """Enable BRPOP mode for channel."""
-
-        if not channel._in_poll:  # send BRPOP
-            task = await channel._brpop_start()
-            task.channel = channel
-            task.cmd = 'BRPOP'
-            self._pending_tasks.add(task)
-
-
-async def wait_util_first_complete(*coros):
-    futs = map(asyncio.ensure_future, coros)
-    done, pending = await asyncio.wait(
-        futs, return_when=asyncio.FIRST_COMPLETED)
-    for fut in pending:
-        assert isinstance(fut, asyncio.Future)
-        try:
-            fut.cancel()
-        except Exception:  # noqa
-            pass
-    return done.pop()
-
-
-class ClusterQoS(QoS):
-    async def restore_by_tag(self, tag, client=None, leftmost=False):
-        assert isinstance(client, cluster.RedisCluster)
-        node = client.get_node_from_key(self.unacked_key)
-        redis_cli = aioredis.Redis(
-            connection_pool=ClusterConnectionPool(client, node))
-        return await super().restore_by_tag(tag, redis_cli, leftmost)
-
-
-class ClusterChannleMixin:
-    socket_keepalive = True
-
-    namespace = '{celery}'
-    keyprefix_queue = '/{namespace}/_kombu/binding%s'
-    keyprefix_fanout = '/{namespace}/_kombu/fanout.'
-    unacked_key = '/{namespace}/_kombu/unacked'
-    unacked_index_key = '/{namespace}/_kombu/unacked_index'
-    unacked_mutex_key = '/{namespace}/_kombu/unacked_mutex'
-
-    min_priority = 0
-    max_priority = 0
-    priority_steps = [min_priority]
-
-    def _patch_options(self, options):
-        namespace = options.get('namespace', self.namespace)
-        keys = [
-            'keyprefix_queue',
-            'keyprefix_fanout',
-            'unacked_key',
-            'unacked_index_key',
-            'unacked_mutex_key',
-        ]
-
-        for key in keys:
-            if key not in options:
-                value = options.get(key, getattr(self, key))
-                options[key] = value.format(namespace=namespace)
-
-    def _get_pool(self, asynchronous=False):
-        raise NotImplementedError
-
-
-class ClusterChannel(ClusterChannleMixin, Channel):
-    QoS = ClusterQoS
-    connection_class = cluster.Connection
-
-    from_transport_options = Channel.from_transport_options + (
-        'namespace',
-        'keyprefix_queue',
-        'keyprefix_fanout',
-    )
-    client: cluster.RedisCluster
-
-    def __init__(self, conn, *args, **kwargs):
-        options = conn.client.transport_options
-        self._patch_options(options)
-        super().__init__(conn, *args, **kwargs)
-        self._default_client = self._create_client()
-
-    @contextlib.contextmanager
-    def conn_or_acquire(self, client=None):
-        if client:
-            yield client
-        else:
-            yield self._default_client
-
-    def _get_client(self):
-        return cluster.RedisCluster
-
-    def _create_client(self, asynchronous=False):
-        params = self._connparams(asynchronous=False)
-        params.pop('db', None)
-        params.pop('connection_class', None)
-        return self.Client(**params)
-
-    async def _brpop_read_impl(
-        self,
-        node: cluster.ClusterNode,
-        conn: cluster.Connection,
-        **options
-    ):
-        client = self.client
-        try:
-            dest__item = await node.parse_response(conn, 'BRPOP', **options)
-        except self.connection_errors:
-            await self.client.close()
-            raise Empty()
-        except MovedError as err:
-            # copied from rediscluster/client.py
-            client.reinitialize_counter += 1
-            if (
-                client.reinitialize_steps
-                and client.reinitialize_counter % client.reinitialize_steps == 0
-            ):
-                await client.close()
-                # Reset the counter
-                client.reinitialize_counter = 0
-            else:
-                client.nodes_manager._moved_exception = err
-            raise Empty()
-
-        if dest__item:
-            dest, item = dest__item
-            dest = bytes_to_str(dest).rsplit(self.sep, 1)[0]
-            self._queue_cycle.rotate(dest)
-            self.connection._deliver(loads(bytes_to_str(item)), dest)
-            return True
-
-    async def _brpop_start(self, timeout=1):
-        queues = self._queue_cycle.consume(len(self.active_queues))
-        if not queues:
-            return
-
-        cli = self.client
-        node_to_keys = {}
-
-        for key in queues:
-            node = cli.get_node_from_key(key)
-            node_to_keys.setdefault(node.name, []).append(key)
-
-        async def brpop(node: cluster.ClusterNode, keys):
-            conn = None
-            try:
-                conn = node.acquire_connection()
-                await conn.send_command('BRPOP', *keys)
-                return await self._brpop_read_impl(node, conn)
-            finally: # noqa
-                if conn is not None:
-                    node._free.append(conn)
-
-        subtasks = []
-
-        for node_name, keys in node_to_keys.items():
-            node = cli.get_node(node_name=node_name)
-            subtasks.append(brpop(node, keys + [timeout]))
-
-        self._in_poll = True
-        self._brpop_task = task = asyncio.create_task(
-            wait_util_first_complete(*subtasks))
-
-        await asyncio.sleep(0)
-        return task
-
-    async def close(self):
-        await super().close()
-        await self._default_client.close()
-
-
-class ClusterTransport(Transport):
-    default_port = 30001
-    Channel = ClusterChannel
-
-    driver_type = 'redis-cluster'
-    driver_name = driver_type
-    Poller = ClusterPoller
+import asyncio
+import contextlib
+import socket
+import functools
+import warnings
+
+from itertools import count
+from contextlib import asynccontextmanager
+from time import monotonic, time
+from typing import *
+
+import redis
+import redis.asyncio as aioredis
+import numbers
+
+from redis.asyncio import (
+    sentinel,
+    cluster
+)
+from redis import exceptions
+from kombu.transport.redis import (
+    Transport as SyncTransport,
+    Channel as SyncChannel,
+    MultiChannelPoller as SyncMultiChannelPoller,
+    QoS as SyncQoS,
+    PrefixedStrictRedis,
+    error_classes_t,
+    virtual,
+    dumps,
+    loads,
+    cycle_by_name,
+    bytes_to_str,
+    InconsistencyError,
+    VersionMismatch,
+    Empty,
+    MutexHeld,
+
+    crit,
+    warn,
+    _parse_url,
+)
+from kombu.transport.virtual.base import (
+    UNDELIVERABLE_FMT,
+    queue_declare_ok_t,
+    uuid,
+    ChannelError,
+    UndeliverableWarning,
+    FairCycle,
+)
+from redis.exceptions import MovedError
+
+from .exchange import STANDARD_EXCHANGE_TYPES
+from .utils import is_async_callable
+from celery.patches.redis import ClusterConnectionPool
+
+
+@asynccontextmanager
+async def Mutex(client: aioredis.Redis, name, expire):
+    """Acquire redis lock in non blocking way.
+
+    Raise MutexHeld if not successful.
+    """
+    lock = client.lock(name, timeout=expire)
+    lock_acquired = False
+    try:
+        lock_acquired = await lock.acquire(blocking=False)
+        if lock_acquired:
+            yield
+        else:
+            raise MutexHeld()
+    finally:
+        if lock_acquired:
+            try:
+                await lock.release()
+            except exceptions.LockNotOwnedError:
+                # when lock is expired
+                pass
+
+
+class QoS(SyncQoS):
+    @asynccontextmanager
+    async def pipe_or_acquire(self, pipe=None, client=None):
+        if pipe:
+            yield pipe
+        else:
+            with self.channel.conn_or_acquire(client) as client:
+                async with client.pipeline() as pipe:
+                    yield pipe
+
+    async def _remove_from_indices(self, delivery_tag, pipe=None):
+        async with self.pipe_or_acquire(pipe) as pipe:
+            return pipe.zrem(self.unacked_index_key, delivery_tag) \
+                       .hdel(self.unacked_key, delivery_tag)
+
+    async def append(self, message, delivery_tag):
+        delivery = message.delivery_info
+        EX, RK = delivery['exchange'], delivery['routing_key']
+        zadd_args = [{delivery_tag: time()}]
+
+        async with self.pipe_or_acquire() as pipe:
+            await (pipe
+                .zadd(self.unacked_index_key, *zadd_args)
+                .hset(self.unacked_key, delivery_tag,
+                      dumps([message._raw, EX, RK]))
+                .execute()
+            )
+            if self._dirty:
+                self._flush()
+            self._quick_append(delivery_tag, message)
+
+    async def restore_unacked(self, client=None):
+        with self.channel.conn_or_acquire(client) as client:
+            for tag in self._delivered:
+                await self.restore_by_tag(tag, client=client)
+        self._delivered.clear()
+
+    async def ack(self, delivery_tag):
+        pipe = await self._remove_from_indices(delivery_tag)
+        await pipe.execute()
+        self._quick_ack(delivery_tag)
+
+    async def reject(self, delivery_tag, requeue=False):
+        if requeue:
+            await self.restore_by_tag(delivery_tag, leftmost=True)
+        await self.ack(delivery_tag)
+
+    async def restore_visible(self, start=0, num=10, interval=10):
+        self._vrestore_count += 1
+        if (self._vrestore_count - 1) % interval:
+            return
+        with self.channel.conn_or_acquire() as client:
+            ceil = time() - self.visibility_timeout
+            try:
+                async with Mutex(client, self.unacked_mutex_key,
+                           self.unacked_mutex_expire):
+                    visible = await client.zrevrangebyscore(
+                        self.unacked_index_key, ceil, 0,
+                        start=num and start, num=num, withscores=True)
+                    for tag, score in visible or []:
+                        await self.restore_by_tag(tag, client)
+            except MutexHeld:
+                pass
+
+    async def restore_by_tag(self, tag, client=None, leftmost=False):
+
+        async def restore_transaction(pipe):
+            p = pipe.hget(self.unacked_key, tag)
+            pipe.multi()
+            pipe = await self._remove_from_indices(tag, pipe)
+            if p:
+                M, EX, RK = loads(bytes_to_str(p))  # json is unicode
+                await self.channel._do_restore_message(M, EX, RK, pipe, leftmost)
+
+        with self.channel.conn_or_acquire(client) as client:
+            await client.transaction(restore_transaction, self.unacked_key)
+
+
+class AsyncFairCycle(FairCycle):
+    async def get(self, callback, **kwargs):
+        """Get from next resource."""
+        for tried in count(0):  # for infinity
+            resource = self._next()
+            try:
+                return await self.fun(resource, callback, **kwargs)
+            except self.predicate:
+                # reraise when retries exchausted.
+                if tried >= len(self.resources) - 1:
+                    raise
+
+
+def get_redis_error_classes():
+    # This exception suddenly changed name between redis-py versions
+    if hasattr(exceptions, 'InvalidData'):
+        DataError = exceptions.InvalidData
+    else:
+        DataError = exceptions.DataError
+
+    return error_classes_t(
+        (virtual.Transport.connection_errors + (
+            InconsistencyError,
+            socket.error,
+            IOError,
+            OSError,
+            exceptions.ConnectionError,
+            exceptions.AuthenticationError,
+            exceptions.TimeoutError,
+            RuntimeError,  # uvloop might raise this
+        )),
+        (virtual.Transport.channel_errors + (
+            DataError,
+            exceptions.InvalidResponse,
+            exceptions.ResponseError)),
+    )
+
+
+class Channel(SyncChannel):
+    QoS = QoS
+    connection_class = aioredis.Connection
+    connection_class_ssl = aioredis.SSLConnection
+    exchange_types = dict(STANDARD_EXCHANGE_TYPES)
+
+    if TYPE_CHECKING:
+        from redis.asyncio.client import PubSub
+        @property
+        def subclient(self) -> PubSub: ...  # noqa
+
+    def __init__(self, *args, **kwargs):
+        virtual.Channel.__init__(self, *args, **kwargs)
+
+        if not self.ack_emulation:  # disable visibility timeout
+            self.QoS = virtual.QoS
+
+        self._queue_cycle = cycle_by_name(self.queue_order_strategy)()
+        self.Client = self._get_client()
+        self.ResponseError = self._get_response_error()
+        self.active_fanout_queues = set()
+        self.auto_delete_queues = set()
+        self._fanout_to_queue = {}
+        self.handlers = {'BRPOP': self._brpop_read, 'LISTEN': self._receive}
+
+        if self.fanout_prefix:
+            if isinstance(self.fanout_prefix, str):
+                self.keyprefix_fanout = self.fanout_prefix
+        else:
+            # previous versions did not set a fanout, so cannot enable
+            # by default.
+            self.keyprefix_fanout = ''
+
+        self.connection.cycle.add(self)  # add to channel poller.
+        # copy errors, in case channel closed but threads still
+        # are still waiting for data.
+        self.connection_errors = self.connection.connection_errors
+        self._conn_evaled = False
+
+        self._brpop_task: asyncio.Task = None
+        self._listen_task: asyncio.Task = None
+
+        # if register_after_fork is not None:
+        #     register_after_fork(self, _after_fork_cleanup_channel)
+
+    async def ping(self):
+        return await self.client.ping()
+
+    async def _after_fork(self):
+        await self._disconnect_pools()
+
+    async def _disconnect_pools(self):
+        pool = self._pool
+        async_pool = self._async_pool
+        is_same_pool = pool is async_pool
+
+        self._async_pool = self._pool = None
+
+        if pool is not None:
+            await pool.disconnect()
+
+        if not is_same_pool and async_pool is not None:
+            await async_pool.disconnect()
+
+    async def _do_restore_message(self, payload, exchange, routing_key,
+                            pipe, leftmost=False):
+        try:
+            try:
+                payload['headers']['redelivered'] = True
+                payload['properties']['delivery_info']['redelivered'] = True
+            except KeyError:
+                pass
+            for queue in await self._lookup(exchange, routing_key):
+                await (pipe.lpush if leftmost else pipe.rpush)(
+                    queue, dumps(payload),
+                )
+        except Exception:
+            crit('Could not restore message: %r', payload, exc_info=True)
+
+    async def _restore(self, message, leftmost=False):
+        if not self.ack_emulation:
+            delivery_info = message.delivery_info
+            message = message.serializable()
+            message['redelivered'] = True
+            for queue in await self._lookup(
+                delivery_info['exchange'], delivery_info['routing_key']
+            ):
+                await self._put(queue, message)
+            return
+
+        tag = message.delivery_tag
+
+        async def restore_transaction(pipe):
+            P = await pipe.hget(self.unacked_key, tag)
+            pipe.multi()
+            await pipe.hdel(self.unacked_key, tag)
+            if P:
+                M, EX, RK = loads(bytes_to_str(P))  # json is unicode
+                await self._do_restore_message(M, EX, RK, pipe, leftmost)
+
+        with self.conn_or_acquire() as client:
+            await client.transaction(restore_transaction, self.unacked_key)
+
+    async def _restore_at_beginning(self, message):
+        return await self._restore(message, leftmost=True)
+
+    def _get_response_error(self):
+        return exceptions.ResponseError
+
+    async def _get(self, queue):
+        with self.conn_or_acquire() as client:
+            for pri in self.priority_steps:
+                item = await client.rpop(self._q_for_pri(queue, pri))
+                if item:
+                    return loads(bytes_to_str(item))
+            raise Empty()
+
+    async def _size(self, queue):
+        with self.conn_or_acquire() as client:
+            async with client.pipeline() as pipe:
+                for pri in self.priority_steps:
+                    pipe = pipe.llen(self._q_for_pri(queue, pri))
+                sizes = await pipe.execute()
+        return sum(size for size in sizes
+                   if isinstance(size, numbers.Integral))
+
+    async def _put(self, queue, message, **kwargs):
+        """Deliver message."""
+        pri = self._get_message_priority(message, reverse=False)
+
+        with self.conn_or_acquire() as client:
+            await client.lpush(self._q_for_pri(queue, pri), dumps(message))
+
+    async def _put_fanout(self, exchange, message, routing_key, **kwargs):
+        """Deliver fanout message."""
+        with self.conn_or_acquire() as client:
+            await client.publish(
+                self._get_publish_topic(exchange, routing_key),
+                dumps(message),
+            )
+
+    async def _queue_bind(self, exchange, routing_key, pattern, queue):
+        if self.typeof(exchange).type == 'fanout':
+            # Mark exchange as fanout.
+            self._fanout_queues[queue] = (
+                exchange, routing_key.replace('#', '*'),
+            )
+        with self.conn_or_acquire() as client:
+            await client.sadd(
+                self.keyprefix_queue % (exchange,),
+                self.sep.join([routing_key or '', pattern or '', queue or ''])
+            )
+
+    async def _delete(self, queue, exchange, routing_key, pattern, *args, **kwargs):
+        self.auto_delete_queues.discard(queue)
+        with self.conn_or_acquire(client=kwargs.get('client')) as client:
+            await client.srem(self.keyprefix_queue % (exchange,),
+                        self.sep.join([routing_key or '',
+                                       pattern or '',
+                                       queue or '']))
+            async with client.pipeline() as pipe:
+                for pri in self.priority_steps:
+                    pipe = pipe.delete(self._q_for_pri(queue, pri))
+                await pipe.execute()
+
+    async def _has_queue(self, queue, **kwargs):
+        with self.conn_or_acquire() as client:
+            async with client.pipeline() as pipe:
+                for pri in self.priority_steps:
+                    pipe = pipe.exists(self._q_for_pri(queue, pri))
+                return any(await pipe.execute())
+
+    async def get_table(self, exchange):
+        key = self.keyprefix_queue % exchange
+        with self.conn_or_acquire() as client:
+            values = await client.smembers(key)
+            if not values:
+                # table does not exists since all queues bound to the exchange
+                # were deleted. We need just return empty list.
+                return []
+            return [tuple(bytes_to_str(val).split(self.sep)) for val in values]
+
+    async def _purge(self, queue):
+        with self.conn_or_acquire() as client:
+            async with client.pipeline() as pipe:
+                for pri in self.priority_steps:
+                    priq = self._q_for_pri(queue, pri)
+                    pipe = pipe.llen(priq).delete(priq)
+                sizes = await pipe.execute()
+                return sum(sizes[::2])
+
+    async def close(self):
+        self._closing = True
+        if not self.closed:
+            # remove from channel poller.
+            self.connection.cycle.discard(self)
+
+            # delete fanout bindings
+            client = self.__dict__.get('client')  # only if property cached
+            if client is not None:
+                for queue in self._fanout_queues:
+                    if queue in self.auto_delete_queues:
+                        await self.queue_delete(queue, client=client)
+            await self._disconnect_pools()
+            await self._close_clients()
+
+            # --------------------------------------
+            # virtual.base::Channel
+            self.closed = True
+
+            for consumer in list(self._consumers):
+                self.basic_cancel(consumer)  # todo maybe await
+            if self._qos:
+                self._qos.restore_unacked_once()
+            if self._cycle is not None:
+                self._cycle.close()
+                self._cycle = None
+            if self.connection is not None:
+                self.connection.close_channel(self)  # todo maybe await
+        self.exchange_types = None
+
+    async def _close_clients(self):
+        for attr in 'client', 'subclient':
+            try:
+                client = self.__dict__[attr]
+                connection, client.connection = client.connection, None
+                await connection.disconnect()
+            except (KeyError, AttributeError, self.ResponseError):
+                pass
+
+    def _connparams(self, asynchronous=False):
+        params = super(Channel, self)._connparams(asynchronous)
+        channel = self
+
+        if asynchronous:
+            class Connection(self.connection_class):
+                async def disconnect(self, nowait: bool = False):
+                    await super().disconnect(nowait=nowait)
+                    channel._on_connection_disconnect(self)
+
+            params['connection_class'] = Connection
+        return params
+
+    def _get_pool(self, asynchronous=True):
+        params = self._connparams(asynchronous=True)
+        self.keyprefix_fanout = self.keyprefix_fanout.format(db=params['db'])
+        return aioredis.ConnectionPool(**params)
+
+    def _get_client(self):
+        if getattr(redis, 'VERSION', (1, 0)) < (4, 1, 0):
+            raise VersionMismatch(
+                'Redis transport requires redis versions 4.1.0 or later. '
+                f'You have {redis.__version__}')
+
+        if self.global_keyprefix:
+            return functools.partial(
+                PrefixedStrictRedis,
+                global_keyprefix=self.global_keyprefix,
+            )
+        return aioredis.StrictRedis
+
+    # -----------------------------------------------------------------------------
+    # virtual.base::Channel method overload
+    async def queue_delete(self, queue, if_unused=False, if_empty=False, **kwargs):
+        """Delete queue."""
+        if if_empty and self._size(queue):
+            return
+        for exchange, routing_key, args in self.state.queue_bindings(queue):
+            meta = self.typeof(exchange).prepare_bind(
+                queue, exchange, routing_key, args,
+            )
+            await self._delete(queue, exchange, *meta, **kwargs)
+        self.state.queue_bindings_delete(queue)
+
+    async def after_reply_message_received(self, queue):
+        await self.queue_delete(queue)
+
+    async def queue_bind(self, queue, exchange=None, routing_key='',
+                   arguments=None, **kwargs):
+        """Bind `queue` to `exchange` with `routing key`."""
+        exchange = exchange or 'amq.direct'
+        if self.state.has_binding(queue, exchange, routing_key):
+            return
+        # Add binding:
+        self.state.binding_declare(queue, exchange, routing_key, arguments)
+        # Update exchange's routing table:
+        table = self.state.exchanges[exchange].setdefault('table', [])
+        meta = self.typeof(exchange).prepare_bind(
+            queue, exchange, routing_key, arguments,
+        )
+        table.append(meta)
+        if self.supports_fanout:
+            await self._queue_bind(exchange, *meta)
+
+    async def queue_unbind(self, queue, exchange=None, routing_key='',
+                     arguments=None, **kwargs):
+        # Remove queue binding:
+        self.state.binding_delete(queue, exchange, routing_key)
+        try:
+            table = await self.get_table(exchange)
+        except KeyError:
+            return
+        binding_meta = self.typeof(exchange).prepare_bind(
+            queue, exchange, routing_key, arguments,
+        )
+        # TODO: the complexity of this operation is O(number of bindings).
+        # Should be optimized.  Modifying table in place.
+        table[:] = [meta for meta in table if meta != binding_meta]
+
+    async def list_bindings(self):
+        return ((queue, exchange, rkey)
+                for exchange in self.state.exchanges
+                for rkey, pattern, queue in await self.get_table(exchange))
+
+    async def queue_declare(self, queue=None, passive=False, **kwargs):
+        """Declare queue."""
+        queue = queue or 'amq.gen-%s' % uuid()
+        if passive and not (await self._has_queue(queue, **kwargs)):
+            raise ChannelError(
+                'NOT_FOUND - no queue {!r} in vhost {!r}'.format(
+                    queue, self.connection.client.virtual_host or '/'),
+                (50, 10), 'Channel.queue_declare', '404',
+            )
+        else:
+            self._new_queue(queue, **kwargs)
+        return queue_declare_ok_t(queue, await self._size(queue), 0)
+
+    async def queue_purge(self, queue, **kwargs):
+        """Remove all ready messages from queue."""
+        return await self._purge(queue)
+
+    async def basic_publish(self, message, exchange, routing_key, **kwargs):
+        """Publish message."""
+        self._inplace_augment_message(message, exchange, routing_key)
+        if exchange:
+            return await self.typeof(exchange).deliver(
+                message, exchange, routing_key, **kwargs
+            )
+        # anon exchange: routing_key is the destination queue
+        return await self._put(routing_key, message, **kwargs)
+
+    async def basic_get(self, queue, no_ack=False, **kwargs):
+        """Get message by direct access (synchronous)."""
+        try:
+            message = self.Message((await self._get(queue)), channel=self)
+            if not no_ack:
+                self.qos.append(message, message.delivery_tag)
+            return message
+        except Empty:
+            pass
+
+    async def _lookup(self, exchange, routing_key, default=None):
+        """Find all queues matching `routing_key` for the given `exchange`.
+
+        Returns:
+            str: queue name -- must return the string `default`
+                if no queues matched.
+        """
+        if default is None:
+            default = self.deadletter_queue
+        if not exchange:  # anon exchange
+            return [routing_key or default]
+
+        try:
+            R = self.typeof(exchange).lookup(
+                await self.get_table(exchange),
+                exchange, routing_key, default,
+            )
+        except KeyError:
+            R = []
+
+        if not R and default is not None:
+            warnings.warn(UndeliverableWarning(UNDELIVERABLE_FMT.format(
+                exchange=exchange, routing_key=routing_key)),
+            )
+            self._new_queue(default)
+            R = [default]
+        return R
+
+    async def __aenter__(self):
+        return self
+
+    async def __aexit__(self, exc_type, exc_val, exc_tb):
+        await self.close()
+
+    def __enter__(self):
+        raise NotImplementedError
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        raise NotImplementedError
+
+    def _reset_cycle(self):
+        self._cycle = AsyncFairCycle(
+            self._get_and_deliver, self._active_queues, Empty)
+
+    async def _get_and_deliver(self, queue, callback):
+        message = await self._get(queue)
+        callback(message, queue)
+
+    # -----------------------------------------------------------------------------
+    # exchange
+    async def exchange_delete(self, exchange, if_unused=False, nowait=False):
+        """Delete `exchange` and all its bindings."""
+        for rkey, _, queue in (await self.get_table(exchange)):
+            await self.queue_delete(queue, if_unused=True, if_empty=True)
+        self.state.exchanges.pop(exchange, None)
+
+    # -----------------------------------------------------------------------------
+    # consumer
+    async def _brpop_start(self, timeout=1):
+        queues = self._queue_cycle.consume(len(self.active_queues))
+        if not queues:
+            return
+
+        keys = [self._q_for_pri(queue, pri) for pri in self.priority_steps
+                for queue in queues] + [timeout or 0]
+
+        command_args = ['BRPOP', *keys]
+        if self.global_keyprefix:
+            command_args = self.client._prefix_args(command_args)
+
+        async def brpop(conn):
+            await conn.send_command(*command_args)
+            return await self._brpop_read_impl()
+
+        self._in_poll = True
+        self._brpop_task = task = asyncio.create_task(
+            brpop(self.client.connection))
+
+        await asyncio.sleep(0)
+        return task
+
+    async def _brpop_read_impl(self, **options):
+        try:
+            dest__item = await self.client.parse_response(
+                self.client.connection,
+                'BRPOP',
+                **options
+            )
+        except self.connection_errors:
+            # if there's a ConnectionError, disconnect so the next
+            # iteration will reconnect automatically.
+            await self.client.connection.disconnect()
+            raise
+        if dest__item:
+            dest, item = dest__item
+            dest = bytes_to_str(dest).rsplit(self.sep, 1)[0]
+            self._queue_cycle.rotate(dest)
+            self.connection._deliver(loads(bytes_to_str(item)), dest)
+            return True
+        else:
+            raise Empty()
+
+    def _brpop_read(self, **options):
+        try:
+            return self._brpop_task.result()
+        finally:
+            self._brpop_task = None
+            self._in_poll = None
+
+    async def _subscribe(self):
+        keys = [self._get_subscribe_topic(queue)
+                for queue in self.active_fanout_queues]
+        if not keys:
+            return
+        c = self.subclient
+        if not c.connection.is_connected:
+            await c.connection.connect()
+        self._in_listen = c.connection
+        await c.psubscribe(keys)
+        self._listen_task = t = asyncio.create_task(c.parse_response())
+        await asyncio.sleep(0)
+        return t
+
+    async def _unsubscribe_from(self, queue):
+        topic = self._get_subscribe_topic(queue)
+        c = self.subclient
+        if c.connection and c.connection.is_connected:
+            await c.unsubscribe([topic])
+
+    def _receive_one(self, c):
+        try:
+            response = self._listen_task.result()
+        except self.connection_errors:
+            self._in_listen = None
+            raise
+        finally:
+            self._listen_task = None
+        if isinstance(response, (list, tuple)):
+            payload = self._handle_message(c, response)
+            if bytes_to_str(payload['type']).endswith('message'):
+                channel = bytes_to_str(payload['channel'])
+                if payload['data']:
+                    if channel[0] == '/':
+                        _, _, channel = channel.partition('.')
+                    try:
+                        message = loads(bytes_to_str(payload['data']))
+                    except (TypeError, ValueError):
+                        warn('Cannot process event on channel %r: %s',
+                             channel, repr(payload)[:4096], exc_info=1)
+                        raise Empty()
+                    exchange = channel.split('/', 1)[0]
+                    self.connection._deliver(
+                        message, self._fanout_to_queue[exchange])
+                    return True
+
+
+class MultiChannelPoller(SyncMultiChannelPoller):
+    def __init__(self):
+        self._channels = set()
+        self._pending_tasks = set()
+
+    @staticmethod
+    async def ensure_connection(client):
+        conn = getattr(client, 'connection', None)
+        if conn is None:
+            client.connection = await client.connection_pool.get_connection('_')
+        elif not conn.is_connected:
+            await client.connection.connect()
+
+    async def _register_BRPOP(self, channel):
+        """Enable BRPOP mode for channel."""
+        await self.ensure_connection(channel.client)
+
+        if not channel._in_poll:  # send BRPOP
+            task = await channel._brpop_start()
+            task.channel = channel
+            task.cmd = 'BRPOP'
+            self._pending_tasks.add(task)
+
+    async def _register_LISTEN(self, channel):
+        """Enable LISTEN mode for channel."""
+        await self.ensure_connection(channel.client)
+
+        if not channel._in_listen:
+            task = await channel._subscribe()  # send SUBSCR
+            task.channel = channel
+            task.cmd = 'LISTEN'
+            self._pending_tasks.add(task)
+
+    async def poll(self, timeout):
+        if not self._pending_tasks:
+            return
+
+        done, pending = await asyncio.wait(
+            self._pending_tasks,
+            timeout=timeout,
+            return_when=asyncio.FIRST_COMPLETED
+        )
+        for task in done:
+            self._pending_tasks.discard(task)
+        if not done:
+            return
+        else:
+            return done.pop()
+
+    @staticmethod
+    async def handle_task(task):
+        chan = task.channel  # noqa
+        type = task.cmd  # noqa
+
+        if chan.qos.can_consume():
+            hdlr = chan.handlers[type]
+            if is_async_callable(hdlr):
+                await hdlr()
+            else:
+                hdlr()
+
+        return True
+
+    async def get(self, callback, timeout=None):
+        self._in_protected_read = True
+        try:
+            for channel in self._channels:
+                if channel.active_queues:           # BRPOP mode?
+                    if channel.qos.can_consume():
+                        await self._register_BRPOP(channel)
+                if channel.active_fanout_queues:    # LISTEN mode?
+                    await self._register_LISTEN(channel)
+
+            if (
+                (done_task := await self.poll(timeout))
+                and (await self.handle_task(done_task))
+            ):
+                return
+            # - no new data, so try to restore messages.
+            # - reset active redis commands.
+            await self.maybe_restore_messages()
+            raise Empty()
+        finally:
+            self._in_protected_read = False
+            while self.after_read:
+                try:
+                    fun = self.after_read.pop()
+                except KeyError:
+                    break
+                else:
+                    fun()
+
+    async def maybe_restore_messages(self):
+        for channel in self._channels:
+            if channel.active_queues:
+                # only need to do this once, as they are not local to channel.
+                return await channel.qos.restore_visible(
+                    num=channel.unacked_restore_limit,
+                )
+
+
+class Transport(SyncTransport):
+    Channel = Channel
+    connection_errors, channel_errors = get_redis_error_classes()
+    Poller = MultiChannelPoller
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+        # All channels share the same poller.
+        self.cycle = self.Poller()
+
+    async def establish_connection(self):
+        channel: Channel = self.create_channel(self)
+        await channel.ping()
+        self._avail_channels.append(channel)
+        return self     # for drain events
+
+    async def close_connection(self, connection):
+        self.cycle.close()
+        for chan_list in self._avail_channels, self.channels:
+            while chan_list:
+                try:
+                    channel = chan_list.pop()
+                except LookupError:  # pragma: no cover
+                    pass
+                else:
+                    await channel.close()
+
+    async def drain_events(self, connection, timeout=None):
+        time_start = monotonic()
+        get = self.cycle.get
+        polling_interval = self.polling_interval
+        if timeout and polling_interval and polling_interval > timeout:
+            polling_interval = timeout
+        while 1:
+            try:
+                await get(self._deliver, timeout=timeout)
+            except Empty:
+                if timeout is not None and monotonic() - time_start >= timeout:
+                    raise socket.timeout()
+                if polling_interval is not None:
+                    await asyncio.sleep(polling_interval)
+            else:
+                break
+
+
+class SentinelManagedSSLConnection(
+        sentinel.SentinelManagedConnection,
+        aioredis.SSLConnection):
+    pass
+
+
+class SentinelChannel(Channel):
+    from_transport_options = Channel.from_transport_options + (
+        'master_name',
+        'min_other_sentinels',
+        'sentinel_kwargs')
+
+    connection_class = sentinel.SentinelManagedConnection
+    connection_class_ssl = SentinelManagedSSLConnection
+
+    def _sentinel_managed_pool(self, asynchronous=False):
+        connparams = self._connparams(asynchronous)
+
+        additional_params = connparams.copy()
+
+        additional_params.pop('host', None)
+        additional_params.pop('port', None)
+
+        sentinels = []
+        for url in self.connection.client.alt:
+            url = _parse_url(url)
+            if url.scheme == 'sentinel':
+                port = url.port or self.connection.default_port
+                sentinels.append((url.hostname, port))
+
+        # Fallback for when only one sentinel is provided.
+        if not sentinels:
+            sentinels.append((connparams['host'], connparams['port']))
+
+        sentinel_inst = sentinel.Sentinel(
+            sentinels,
+            min_other_sentinels=getattr(self, 'min_other_sentinels', 0),
+            sentinel_kwargs=getattr(self, 'sentinel_kwargs', None),
+            **additional_params)
+
+        master_name = getattr(self, 'master_name', None)
+
+        if master_name is None:
+            raise ValueError(
+                "'master_name' transport option must be specified."
+            )
+
+        return sentinel_inst.master_for(
+            master_name,
+            self.Client,
+        ).connection_pool
+
+    def _get_pool(self, asynchronous=False):
+        return self._sentinel_managed_pool(asynchronous)
+
+
+class SentinelTransport(Transport):
+    """Redis Sentinel Transport."""
+
+    default_port = 26379
+    Channel = SentinelChannel
+
+
+class ClusterPoller(MultiChannelPoller):
+    @staticmethod
+    async def ensure_connection(client: cluster.RedisCluster):
+        if client._initialize:
+            await client.initialize()
+
+    async def _register_BRPOP(self, channel):
+        """Enable BRPOP mode for channel."""
+
+        if not channel._in_poll:  # send BRPOP
+            task = await channel._brpop_start()
+            task.channel = channel
+            task.cmd = 'BRPOP'
+            self._pending_tasks.add(task)
+
+
+async def wait_util_first_complete(*coros):
+    futs = map(asyncio.ensure_future, coros)
+    done, pending = await asyncio.wait(
+        futs, return_when=asyncio.FIRST_COMPLETED)
+    for fut in pending:
+        assert isinstance(fut, asyncio.Future)
+        try:
+            fut.cancel()
+        except Exception:  # noqa
+            pass
+    return done.pop()
+
+
+class ClusterQoS(QoS):
+    async def restore_by_tag(self, tag, client=None, leftmost=False):
+        assert isinstance(client, cluster.RedisCluster)
+        node = client.get_node_from_key(self.unacked_key)
+        redis_cli = aioredis.Redis(
+            connection_pool=ClusterConnectionPool(client, node))
+        return await super().restore_by_tag(tag, redis_cli, leftmost)
+
+
+class ClusterChannleMixin:
+    socket_keepalive = True
+
+    namespace = '{celery}'
+    keyprefix_queue = '/{namespace}/_kombu/binding%s'
+    keyprefix_fanout = '/{namespace}/_kombu/fanout.'
+    unacked_key = '/{namespace}/_kombu/unacked'
+    unacked_index_key = '/{namespace}/_kombu/unacked_index'
+    unacked_mutex_key = '/{namespace}/_kombu/unacked_mutex'
+
+    min_priority = 0
+    max_priority = 0
+    priority_steps = [min_priority]
+
+    def _patch_options(self, options):
+        namespace = options.get('namespace', self.namespace)
+        keys = [
+            'keyprefix_queue',
+            'keyprefix_fanout',
+            'unacked_key',
+            'unacked_index_key',
+            'unacked_mutex_key',
+        ]
+
+        for key in keys:
+            if key not in options:
+                value = options.get(key, getattr(self, key))
+                options[key] = value.format(namespace=namespace)
+
+    def _get_pool(self, asynchronous=False):
+        raise NotImplementedError
+
+
+class ClusterChannel(ClusterChannleMixin, Channel):
+    QoS = ClusterQoS
+    connection_class = cluster.Connection
+
+    from_transport_options = Channel.from_transport_options + (
+        'namespace',
+        'keyprefix_queue',
+        'keyprefix_fanout',
+    )
+    client: cluster.RedisCluster
+
+    def __init__(self, conn, *args, **kwargs):
+        options = conn.client.transport_options
+        self._patch_options(options)
+        super().__init__(conn, *args, **kwargs)
+        self._default_client = self._create_client()
+
+    @contextlib.contextmanager
+    def conn_or_acquire(self, client=None):
+        if client:
+            yield client
+        else:
+            yield self._default_client
+
+    def _get_client(self):
+        return cluster.RedisCluster
+
+    def _create_client(self, asynchronous=False):
+        params = self._connparams(asynchronous=False)
+        params.pop('db', None)
+        params.pop('connection_class', None)
+        return self.Client(**params)
+
+    async def _brpop_read_impl(
+        self,
+        node: cluster.ClusterNode,
+        conn: cluster.Connection,
+        **options
+    ):
+        client = self.client
+        try:
+            dest__item = await node.parse_response(conn, 'BRPOP', **options)
+        except self.connection_errors:
+            await self.client.close()
+            raise Empty()
+        except MovedError as err:
+            # copied from rediscluster/client.py
+            client.reinitialize_counter += 1
+            if (
+                client.reinitialize_steps
+                and client.reinitialize_counter % client.reinitialize_steps == 0
+            ):
+                await client.close()
+                # Reset the counter
+                client.reinitialize_counter = 0
+            else:
+                client.nodes_manager._moved_exception = err
+            raise Empty()
+
+        if dest__item:
+            dest, item = dest__item
+            dest = bytes_to_str(dest).rsplit(self.sep, 1)[0]
+            self._queue_cycle.rotate(dest)
+            self.connection._deliver(loads(bytes_to_str(item)), dest)
+            return True
+
+    async def _brpop_start(self, timeout=1):
+        queues = self._queue_cycle.consume(len(self.active_queues))
+        if not queues:
+            return
+
+        cli = self.client
+        node_to_keys = {}
+
+        for key in queues:
+            node = cli.get_node_from_key(key)
+            node_to_keys.setdefault(node.name, []).append(key)
+
+        async def brpop(node: cluster.ClusterNode, keys):
+            conn = None
+            try:
+                conn = node.acquire_connection()
+                await conn.send_command('BRPOP', *keys)
+                return await self._brpop_read_impl(node, conn)
+            finally: # noqa
+                if conn is not None:
+                    node._free.append(conn)
+
+        subtasks = []
+
+        for node_name, keys in node_to_keys.items():
+            node = cli.get_node(node_name=node_name)
+            subtasks.append(brpop(node, keys + [timeout]))
+
+        self._in_poll = True
+        self._brpop_task = task = asyncio.create_task(
+            wait_util_first_complete(*subtasks))
+
+        await asyncio.sleep(0)
+        return task
+
+    async def close(self):
+        await super().close()
+        await self._default_client.close()
+
+
+class ClusterTransport(Transport):
+    default_port = 30001
+    Channel = ClusterChannel
+
+    driver_type = 'redis-cluster'
+    driver_name = driver_type
+    Poller = ClusterPoller
```

## celery/patches/kombu/redis_cluster.py

 * *Ordering differences only*

```diff
@@ -1,292 +1,292 @@
-from contextlib import contextmanager
-from queue import Empty
-from typing import NamedTuple, List
-
-from kombu.transport import virtual
-from kombu.transport.redis import (
-    Channel as RedisChannel,
-    MultiChannelPoller,
-    QoS as RedisQoS,
-    Transport as RedisTransport,
-)
-from kombu.utils.encoding import bytes_to_str
-from kombu.utils.eventio import READ, ERR
-from kombu.utils.json import loads
-from redis import cluster
-from redis import exceptions
-from redis.cluster import (
-    RedisCluster, ClusterPubSub,
-)
-from redis.connection import Connection
-
-from .redis import ClusterChannleMixin
-
-
-class QoS(RedisQoS):
-    def restore_by_tag(self, tag, client=None, leftmost=False):
-        assert isinstance(client, RedisCluster)
-        node = client.get_node_from_key(self.unacked_key)
-        return super().restore_by_tag(
-            tag, client.get_redis_connection(node), leftmost)
-
-
-class RedisNodeClient(NamedTuple):
-    client: RedisCluster
-    node: str
-    conn: Connection
-
-    def parse_response(self, cmd, **options):
-        node = self.client.get_node(node_name=self.node)
-        redis_node = node.redis_connection
-        return redis_node.parse_response(self.conn, cmd, **options)
-
-
-class PubSubNodeClient(RedisNodeClient):
-    client: ClusterPubSub
-
-
-class ClusterPoller(MultiChannelPoller):
-
-    def _register(self, channel, client: RedisNodeClient, type):
-        ident = (channel, client, type)
-
-        if ident in self._chan_to_sock:
-            self._unregister(*ident)
-
-        if client.conn._sock is None:
-            client.conn.connect()
-
-        sock = client.conn._sock
-        self._fd_to_chan[sock.fileno()] = (channel, client, type)
-        self._chan_to_sock[ident] = sock
-        self.poller.register(sock, self.eventflags)
-
-    def _unregister(self, channel, client, type):
-        sock = self._chan_to_sock[(channel, client, type)]
-        self.poller.unregister(sock)
-
-    def _register_BRPOP(self, channel):
-        for cli in self._get_clis_for_client(
-            channel.client, channel
-        ):
-            ident = (channel, cli, 'BRPOP')
-
-            if cli.conn._sock is None or ident not in self._chan_to_sock:
-                channel._in_poll = False
-                self._register(*ident)
-
-        if not channel._in_poll:  # send BRPOP
-            channel._brpop_start()
-
-    def _register_LISTEN(self, channel):
-        for cli in self._get_clis_for_subclient(
-            channel.subclient, channel
-        ):
-            ident = (channel, cli, 'LISTEN')
-
-            if cli.conn._sock is None or ident not in self._chan_to_sock:
-                channel._in_listen = False
-                self._register(*ident)
-
-        if not channel._in_listen:  # subscribe
-            channel._subscribe()
-
-    def _find_existed_clis(self, target_cls):
-        chan_to_sock = [
-            item for item in self._chan_to_sock or []
-            if item[1].client.__class__ is target_cls
-        ]
-
-        if chan_to_sock:
-            return [cli for _, cli, _ in chan_to_sock]
-
-    def _get_clis_for_client(
-        self,
-        client: RedisCluster,
-        channel
-    ) -> List[RedisNodeClient]:
-        if exist_clis := self._find_existed_clis(RedisCluster):
-            return exist_clis
-
-        clis = []
-        for key in channel.active_queues:
-            node = client.get_node_from_key(key)
-            redis_conn = client.get_redis_connection(node)
-            conn = redis_conn.connection_pool.get_connection('_')
-            clis.append(RedisNodeClient(
-                client=client, node=node.name, conn=conn))
-        return clis
-
-    def _get_clis_for_subclient(
-        self,
-        client: ClusterPubSub,
-        channel
-    ) -> List[PubSubNodeClient]:
-        if exist_clis := self._find_existed_clis(ClusterPubSub):
-            return exist_clis
-
-        clis = []
-        for queue in channel.active_fanout_queues:
-            key = channel._get_subscribe_topic(queue)
-            node = client.cluster.get_node_from_key(key)
-            client.set_pubsub_node(client.cluster, node=node)
-            redis_conn = client.get_redis_connection()
-            conn = redis_conn.connection_pool.get_connection('_')
-            clis.append(PubSubNodeClient(
-                client=client, node=node.name, conn=conn))
-        return clis
-
-    def handle_event(self, fileno, event):
-        if event & READ:
-            return self.on_readable(fileno), self
-        elif event & ERR:
-            chan, cli, cmd = self._fd_to_chan[fileno]
-            chan._poll_error(cmd, client=cli)
-
-    def on_readable(self, fileno):
-        try:
-            chan, cli, cmd = self._fd_to_chan[fileno]
-        except KeyError:
-            return
-
-        if chan.qos.can_consume():
-            return chan.handlers[cmd](client=cli)
-
-
-class Channel(ClusterChannleMixin, RedisChannel):
-    QoS = QoS
-    connection_class = Connection
-
-    from_transport_options = RedisChannel.from_transport_options + (
-        'namespace',
-        'keyprefix_queue',
-        'keyprefix_fanout',
-    )
-    client: cluster.RedisCluster
-
-    def __init__(self, conn, *args, **kwargs):
-        options = conn.client.transport_options
-        self._patch_options(options)
-        super().__init__(conn, *args, **kwargs)
-        self.client.info()
-        self.connection_errors += (
-            exceptions.ClusterError,
-        )
-
-    @contextmanager
-    def conn_or_acquire(self, client=None):
-        if client:
-            yield client
-        else:
-            yield self.client
-
-    def _get_client(self):
-        return cluster.RedisCluster
-
-    def _create_client(self, asynchronous=False):
-        params = self._connparams(asynchronous=asynchronous)
-        params.pop('db', None)
-        params.pop('connection_class', None)
-        return self.Client(**params)
-
-    def _receive(self, **options):
-        self.subclient.connection = options['client'].conn
-        return super()._receive()
-
-    def _brpop_start(self, timeout=1):
-        queues = self._queue_cycle.consume(len(self.active_queues))
-        if not queues:
-            return
-
-        self._in_poll = True
-        timeout = timeout or 0
-        cli = self.client
-        node_to_keys = {}
-
-        for key in queues:
-            node = cli.get_node_from_key(key)
-            node_to_keys.setdefault(node.name, []).append(key)
-
-        for chan, client, cmd in self.connection.cycle._chan_to_sock:
-            expected = (self, cli, 'BRPOP')
-            keys = node_to_keys.get(client.node)
-
-            if keys and (chan, client.client, cmd) == expected:
-                for key in keys:
-                    client.conn.send_command('BRPOP', key, timeout)
-
-    def _brpop_read(self, client: RedisNodeClient, **options):
-        try:
-            conn = client.conn
-            cli = client.client
-
-            try:
-                resp = client.parse_response('BRPOP', **options)
-            except self.connection_errors:
-                conn.disconnect()
-                raise Empty()
-            except exceptions.MovedError as err:
-                # copied from rediscluster/client.py
-                cli.reinitialize_counter += 1
-                if cli._should_reinitialized():
-                    cli.nodes_manager.initialize()
-                    # Reset the counter
-                    cli.reinitialize_counter = 0
-                else:
-                    cli.nodes_manager.update_moved_exception(err)
-                raise Empty()
-
-            if resp:
-                dest, item = resp
-                dest = bytes_to_str(dest).rsplit(self.sep, 1)[0]
-                self._queue_cycle.rotate(dest)
-                self.connection._deliver(loads(bytes_to_str(item)), dest)
-                return True
-        finally:
-            self._in_poll = False
-
-    def _subscribe(self):
-        keys = [self._get_subscribe_topic(queue)
-                for queue in self.active_fanout_queues]
-        if not keys:
-            return
-
-        self._in_listen = True
-        cli = self.subclient
-        node_to_keys = {}
-
-        for key in keys:
-            node = cli.cluster.get_node_from_key(key)
-            node_to_keys.setdefault(node.name, []).append(key)
-
-        for chan, client, cmd in self.connection.cycle._chan_to_sock:
-            expected = (self, cli, 'LISTEN')
-            keys = node_to_keys.get(client.node)
-
-            cli.connection = client.conn
-            if keys and (chan, client.client, cmd) == expected:
-                for key in keys:
-                    cli.psubscribe(key)
-
-    def _poll_error(self, cmd, **options):
-        cli: RedisNodeClient = options['client']
-
-        if cmd == 'BRPOP':
-            cli.parse_response(cmd)
-
-
-class Transport(RedisTransport):
-
-    Channel = Channel
-
-    driver_type = 'redis-cluster'
-    driver_name = driver_type
-
-    implements = virtual.Transport.implements.extend(
-        asynchronous=True,
-        exchange_type=frozenset(['direct', 'fanout'])
-    )
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.cycle = ClusterPoller()
+from contextlib import contextmanager
+from queue import Empty
+from typing import NamedTuple, List
+
+from kombu.transport import virtual
+from kombu.transport.redis import (
+    Channel as RedisChannel,
+    MultiChannelPoller,
+    QoS as RedisQoS,
+    Transport as RedisTransport,
+)
+from kombu.utils.encoding import bytes_to_str
+from kombu.utils.eventio import READ, ERR
+from kombu.utils.json import loads
+from redis import cluster
+from redis import exceptions
+from redis.cluster import (
+    RedisCluster, ClusterPubSub,
+)
+from redis.connection import Connection
+
+from .redis import ClusterChannleMixin
+
+
+class QoS(RedisQoS):
+    def restore_by_tag(self, tag, client=None, leftmost=False):
+        assert isinstance(client, RedisCluster)
+        node = client.get_node_from_key(self.unacked_key)
+        return super().restore_by_tag(
+            tag, client.get_redis_connection(node), leftmost)
+
+
+class RedisNodeClient(NamedTuple):
+    client: RedisCluster
+    node: str
+    conn: Connection
+
+    def parse_response(self, cmd, **options):
+        node = self.client.get_node(node_name=self.node)
+        redis_node = node.redis_connection
+        return redis_node.parse_response(self.conn, cmd, **options)
+
+
+class PubSubNodeClient(RedisNodeClient):
+    client: ClusterPubSub
+
+
+class ClusterPoller(MultiChannelPoller):
+
+    def _register(self, channel, client: RedisNodeClient, type):
+        ident = (channel, client, type)
+
+        if ident in self._chan_to_sock:
+            self._unregister(*ident)
+
+        if client.conn._sock is None:
+            client.conn.connect()
+
+        sock = client.conn._sock
+        self._fd_to_chan[sock.fileno()] = (channel, client, type)
+        self._chan_to_sock[ident] = sock
+        self.poller.register(sock, self.eventflags)
+
+    def _unregister(self, channel, client, type):
+        sock = self._chan_to_sock[(channel, client, type)]
+        self.poller.unregister(sock)
+
+    def _register_BRPOP(self, channel):
+        for cli in self._get_clis_for_client(
+            channel.client, channel
+        ):
+            ident = (channel, cli, 'BRPOP')
+
+            if cli.conn._sock is None or ident not in self._chan_to_sock:
+                channel._in_poll = False
+                self._register(*ident)
+
+        if not channel._in_poll:  # send BRPOP
+            channel._brpop_start()
+
+    def _register_LISTEN(self, channel):
+        for cli in self._get_clis_for_subclient(
+            channel.subclient, channel
+        ):
+            ident = (channel, cli, 'LISTEN')
+
+            if cli.conn._sock is None or ident not in self._chan_to_sock:
+                channel._in_listen = False
+                self._register(*ident)
+
+        if not channel._in_listen:  # subscribe
+            channel._subscribe()
+
+    def _find_existed_clis(self, target_cls):
+        chan_to_sock = [
+            item for item in self._chan_to_sock or []
+            if item[1].client.__class__ is target_cls
+        ]
+
+        if chan_to_sock:
+            return [cli for _, cli, _ in chan_to_sock]
+
+    def _get_clis_for_client(
+        self,
+        client: RedisCluster,
+        channel
+    ) -> List[RedisNodeClient]:
+        if exist_clis := self._find_existed_clis(RedisCluster):
+            return exist_clis
+
+        clis = []
+        for key in channel.active_queues:
+            node = client.get_node_from_key(key)
+            redis_conn = client.get_redis_connection(node)
+            conn = redis_conn.connection_pool.get_connection('_')
+            clis.append(RedisNodeClient(
+                client=client, node=node.name, conn=conn))
+        return clis
+
+    def _get_clis_for_subclient(
+        self,
+        client: ClusterPubSub,
+        channel
+    ) -> List[PubSubNodeClient]:
+        if exist_clis := self._find_existed_clis(ClusterPubSub):
+            return exist_clis
+
+        clis = []
+        for queue in channel.active_fanout_queues:
+            key = channel._get_subscribe_topic(queue)
+            node = client.cluster.get_node_from_key(key)
+            client.set_pubsub_node(client.cluster, node=node)
+            redis_conn = client.get_redis_connection()
+            conn = redis_conn.connection_pool.get_connection('_')
+            clis.append(PubSubNodeClient(
+                client=client, node=node.name, conn=conn))
+        return clis
+
+    def handle_event(self, fileno, event):
+        if event & READ:
+            return self.on_readable(fileno), self
+        elif event & ERR:
+            chan, cli, cmd = self._fd_to_chan[fileno]
+            chan._poll_error(cmd, client=cli)
+
+    def on_readable(self, fileno):
+        try:
+            chan, cli, cmd = self._fd_to_chan[fileno]
+        except KeyError:
+            return
+
+        if chan.qos.can_consume():
+            return chan.handlers[cmd](client=cli)
+
+
+class Channel(ClusterChannleMixin, RedisChannel):
+    QoS = QoS
+    connection_class = Connection
+
+    from_transport_options = RedisChannel.from_transport_options + (
+        'namespace',
+        'keyprefix_queue',
+        'keyprefix_fanout',
+    )
+    client: cluster.RedisCluster
+
+    def __init__(self, conn, *args, **kwargs):
+        options = conn.client.transport_options
+        self._patch_options(options)
+        super().__init__(conn, *args, **kwargs)
+        self.client.info()
+        self.connection_errors += (
+            exceptions.ClusterError,
+        )
+
+    @contextmanager
+    def conn_or_acquire(self, client=None):
+        if client:
+            yield client
+        else:
+            yield self.client
+
+    def _get_client(self):
+        return cluster.RedisCluster
+
+    def _create_client(self, asynchronous=False):
+        params = self._connparams(asynchronous=asynchronous)
+        params.pop('db', None)
+        params.pop('connection_class', None)
+        return self.Client(**params)
+
+    def _receive(self, **options):
+        self.subclient.connection = options['client'].conn
+        return super()._receive()
+
+    def _brpop_start(self, timeout=1):
+        queues = self._queue_cycle.consume(len(self.active_queues))
+        if not queues:
+            return
+
+        self._in_poll = True
+        timeout = timeout or 0
+        cli = self.client
+        node_to_keys = {}
+
+        for key in queues:
+            node = cli.get_node_from_key(key)
+            node_to_keys.setdefault(node.name, []).append(key)
+
+        for chan, client, cmd in self.connection.cycle._chan_to_sock:
+            expected = (self, cli, 'BRPOP')
+            keys = node_to_keys.get(client.node)
+
+            if keys and (chan, client.client, cmd) == expected:
+                for key in keys:
+                    client.conn.send_command('BRPOP', key, timeout)
+
+    def _brpop_read(self, client: RedisNodeClient, **options):
+        try:
+            conn = client.conn
+            cli = client.client
+
+            try:
+                resp = client.parse_response('BRPOP', **options)
+            except self.connection_errors:
+                conn.disconnect()
+                raise Empty()
+            except exceptions.MovedError as err:
+                # copied from rediscluster/client.py
+                cli.reinitialize_counter += 1
+                if cli._should_reinitialized():
+                    cli.nodes_manager.initialize()
+                    # Reset the counter
+                    cli.reinitialize_counter = 0
+                else:
+                    cli.nodes_manager.update_moved_exception(err)
+                raise Empty()
+
+            if resp:
+                dest, item = resp
+                dest = bytes_to_str(dest).rsplit(self.sep, 1)[0]
+                self._queue_cycle.rotate(dest)
+                self.connection._deliver(loads(bytes_to_str(item)), dest)
+                return True
+        finally:
+            self._in_poll = False
+
+    def _subscribe(self):
+        keys = [self._get_subscribe_topic(queue)
+                for queue in self.active_fanout_queues]
+        if not keys:
+            return
+
+        self._in_listen = True
+        cli = self.subclient
+        node_to_keys = {}
+
+        for key in keys:
+            node = cli.cluster.get_node_from_key(key)
+            node_to_keys.setdefault(node.name, []).append(key)
+
+        for chan, client, cmd in self.connection.cycle._chan_to_sock:
+            expected = (self, cli, 'LISTEN')
+            keys = node_to_keys.get(client.node)
+
+            cli.connection = client.conn
+            if keys and (chan, client.client, cmd) == expected:
+                for key in keys:
+                    cli.psubscribe(key)
+
+    def _poll_error(self, cmd, **options):
+        cli: RedisNodeClient = options['client']
+
+        if cmd == 'BRPOP':
+            cli.parse_response(cmd)
+
+
+class Transport(RedisTransport):
+
+    Channel = Channel
+
+    driver_type = 'redis-cluster'
+    driver_name = driver_type
+
+    implements = virtual.Transport.implements.extend(
+        asynchronous=True,
+        exchange_type=frozenset(['direct', 'fanout'])
+    )
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.cycle = ClusterPoller()
```

## celery/patches/redis/__init__.py

 * *Ordering differences only*

```diff
@@ -1,141 +1,141 @@
-from typing import Optional
-
-import redis.asyncio as aioredis
-
-from redis.asyncio.cluster import (
-    RedisCluster,
-    ClusterNode,
-)
-from redis.asyncio.client import PubSub, Lock
-from redis.commands.core import AsyncPubSubCommands
-
-
-__all__ = ('apply_patch', )
-
-
-class ClusterConnectionPool(aioredis.ConnectionPool):
-    """
-    A connection pool wraps ClusterNode to
-    mimic redis.asyncio.ConnectionPool
-    """
-
-    def __init__(
-        self,
-        cluster: RedisCluster,
-        node: Optional[ClusterNode] = None,
-        connection_class=aioredis.Connection,
-        max_connections: Optional[int] = None,
-        **connection_kwargs,
-    ):
-        super().__init__(
-            connection_class, max_connections, **connection_kwargs)
-        self.node = node
-        self.cluster = cluster
-
-    async def get_connection(self, command_name, *keys, **options):
-        if self.node is None:
-            self.node = self.cluster.get_random_node()
-        return self.node.acquire_connection()
-
-    def get_encoder(self):
-        return self.cluster.get_encoder()
-
-    def make_connection(self):
-        if self.node is None:
-            self.node = self.cluster.get_random_node()
-        node = self.node
-        return node.connection_class(**node.connection_kwargs)
-
-    async def release(self, connection: aioredis.Connection):
-        self.node._free.append(connection)
-
-    async def disconnect(self, inuse_connections: bool = True):
-        await self.node.disconnect()
-
-
-class ClusterPubSub(PubSub):
-    cluster: RedisCluster
-
-    def __init__(self, redis_cluster, **kwargs):
-        """
-        When a pubsub instance is created without specifying a node, a single
-        node will be transparently chosen for the pubsub connection on the
-        first command execution. The node will be determined by:
-         1. Hashing the channel name in the request to find its keyslot
-         2. Selecting a node that handles the keyslot: If read_from_replicas is
-            set to true, a replica can be selected.
-
-        :type redis_cluster: RedisCluster
-        """
-        self.node = None
-        self.cluster = redis_cluster
-        super().__init__(
-            **kwargs, connection_pool=None,
-            encoder=redis_cluster.encoder
-        )
-
-    async def execute_command(self, *args, **kwargs):
-        if self.connection is None:
-            if self.connection_pool is None:
-                if self.cluster._initialize:
-                    await self.cluster.initialize()
-
-                if len(args) > 1:
-                    # Hash the first channel and get one of the nodes holding
-                    # this slot
-                    channel = args[1]
-                    node = self.cluster.get_node_from_key(
-                        channel, self.cluster.read_from_replicas)
-                else:
-                    # Get a random node
-                    node = self.cluster.get_random_node()
-                self.node = node
-                self.connection_pool = ClusterConnectionPool(
-                    self.cluster, node=node)
-            self.connection = await self.connection_pool.get_connection('_')
-            # register a callback that re-subscribes to any channels we
-            # were listening to when we were disconnected
-            self.connection.register_connect_callback(self.on_connect)
-        connection = self.connection
-        await self._execute(connection, connection.send_command, *args)
-
-
-class PatchedCluster(RedisCluster, AsyncPubSubCommands):
-    def __init__(self, *args, **kwargs):
-        kwargs.pop('retry_on_timeout', None)
-        max_conns = kwargs.pop('max_connections', None)
-        if max_conns is not None:
-            kwargs['max_connections'] = max_conns
-        super().__init__(*args, **kwargs)
-        self.connection_pool = ClusterConnectionPool(self)
-
-    def pubsub(self,  **kwargs):
-        return ClusterPubSub(self, **kwargs)
-
-    async def publish(self, channel, message, **kwargs):
-        async with self.pubsub() as pubsub:
-            return await pubsub.execute_command(
-                'PUBLISH', channel, message, **kwargs)
-
-    async def mget(self, keys, *args):
-        res = [None] * len(keys)
-        key_groups = {}
-
-        for idx, key in enumerate(keys):
-            group = key_groups.setdefault(
-                self.keyslot(key), ([], [])
-            )
-            group[0].append(key)
-            group[1].append(idx)
-
-        for gkeys, gidx in key_groups.values():
-            gres = await super().mget(gkeys, *args)
-            for i, r in enumerate(gres):
-                res[gidx[i]] = r
-        return res
-
-
-def apply_patch():
-    from redis.asyncio import cluster
-
-    cluster.RedisCluster = PatchedCluster
+from typing import Optional
+
+import redis.asyncio as aioredis
+
+from redis.asyncio.cluster import (
+    RedisCluster,
+    ClusterNode,
+)
+from redis.asyncio.client import PubSub, Lock
+from redis.commands.core import AsyncPubSubCommands
+
+
+__all__ = ('apply_patch', )
+
+
+class ClusterConnectionPool(aioredis.ConnectionPool):
+    """
+    A connection pool wraps ClusterNode to
+    mimic redis.asyncio.ConnectionPool
+    """
+
+    def __init__(
+        self,
+        cluster: RedisCluster,
+        node: Optional[ClusterNode] = None,
+        connection_class=aioredis.Connection,
+        max_connections: Optional[int] = None,
+        **connection_kwargs,
+    ):
+        super().__init__(
+            connection_class, max_connections, **connection_kwargs)
+        self.node = node
+        self.cluster = cluster
+
+    async def get_connection(self, command_name, *keys, **options):
+        if self.node is None:
+            self.node = self.cluster.get_random_node()
+        return self.node.acquire_connection()
+
+    def get_encoder(self):
+        return self.cluster.get_encoder()
+
+    def make_connection(self):
+        if self.node is None:
+            self.node = self.cluster.get_random_node()
+        node = self.node
+        return node.connection_class(**node.connection_kwargs)
+
+    async def release(self, connection: aioredis.Connection):
+        self.node._free.append(connection)
+
+    async def disconnect(self, inuse_connections: bool = True):
+        await self.node.disconnect()
+
+
+class ClusterPubSub(PubSub):
+    cluster: RedisCluster
+
+    def __init__(self, redis_cluster, **kwargs):
+        """
+        When a pubsub instance is created without specifying a node, a single
+        node will be transparently chosen for the pubsub connection on the
+        first command execution. The node will be determined by:
+         1. Hashing the channel name in the request to find its keyslot
+         2. Selecting a node that handles the keyslot: If read_from_replicas is
+            set to true, a replica can be selected.
+
+        :type redis_cluster: RedisCluster
+        """
+        self.node = None
+        self.cluster = redis_cluster
+        super().__init__(
+            **kwargs, connection_pool=None,
+            encoder=redis_cluster.encoder
+        )
+
+    async def execute_command(self, *args, **kwargs):
+        if self.connection is None:
+            if self.connection_pool is None:
+                if self.cluster._initialize:
+                    await self.cluster.initialize()
+
+                if len(args) > 1:
+                    # Hash the first channel and get one of the nodes holding
+                    # this slot
+                    channel = args[1]
+                    node = self.cluster.get_node_from_key(
+                        channel, self.cluster.read_from_replicas)
+                else:
+                    # Get a random node
+                    node = self.cluster.get_random_node()
+                self.node = node
+                self.connection_pool = ClusterConnectionPool(
+                    self.cluster, node=node)
+            self.connection = await self.connection_pool.get_connection('_')
+            # register a callback that re-subscribes to any channels we
+            # were listening to when we were disconnected
+            self.connection.register_connect_callback(self.on_connect)
+        connection = self.connection
+        await self._execute(connection, connection.send_command, *args)
+
+
+class PatchedCluster(RedisCluster, AsyncPubSubCommands):
+    def __init__(self, *args, **kwargs):
+        kwargs.pop('retry_on_timeout', None)
+        max_conns = kwargs.pop('max_connections', None)
+        if max_conns is not None:
+            kwargs['max_connections'] = max_conns
+        super().__init__(*args, **kwargs)
+        self.connection_pool = ClusterConnectionPool(self)
+
+    def pubsub(self,  **kwargs):
+        return ClusterPubSub(self, **kwargs)
+
+    async def publish(self, channel, message, **kwargs):
+        async with self.pubsub() as pubsub:
+            return await pubsub.execute_command(
+                'PUBLISH', channel, message, **kwargs)
+
+    async def mget(self, keys, *args):
+        res = [None] * len(keys)
+        key_groups = {}
+
+        for idx, key in enumerate(keys):
+            group = key_groups.setdefault(
+                self.keyslot(key), ([], [])
+            )
+            group[0].append(key)
+            group[1].append(idx)
+
+        for gkeys, gidx in key_groups.values():
+            gres = await super().mget(gkeys, *args)
+            for i, r in enumerate(gres):
+                res[gidx[i]] = r
+        return res
+
+
+def apply_patch():
+    from redis.asyncio import cluster
+
+    cluster.RedisCluster = PatchedCluster
```

## celery/worker/worker.py

 * *Ordering differences only*

```diff
@@ -1,412 +1,412 @@
-"""WorkController can be used to instantiate in-process workers.
-
-The command-line interface for the worker is in :mod:`celery.bin.worker`,
-while the worker program is in :mod:`celery.apps.worker`.
-
-The worker program is responsible for adding signal handlers,
-setting up logging, etc.  This is a bare-bones worker without
-global side-effects (i.e., except for the global state stored in
-:mod:`celery.worker.state`).
-
-The worker consists of several components, all managed by bootsteps
-(mod:`celery.bootsteps`).
-"""
-
-import os
-import sys
-from datetime import datetime
-
-from billiard import cpu_count
-from kombu.utils.compat import detect_environment
-
-from celery import bootsteps
-from celery import concurrency as _concurrency
-from celery import signals
-from celery.bootsteps import RUN, TERMINATE
-from celery.exceptions import (ImproperlyConfigured, TaskRevokedError,
-                               WorkerTerminate)
-from celery.platforms import EX_FAILURE, create_pidlock
-from celery.utils.imports import reload_from_cwd
-from celery.utils.log import mlevel
-from celery.utils.log import worker_logger as logger
-from celery.utils.nodenames import default_nodename, worker_direct
-from celery.utils.text import str_to_list
-from celery.utils.threads import default_socket_timeout
-
-from . import state
-
-try:
-    import resource
-except ImportError:  # pragma: no cover
-    resource = None  # noqa
-
-
-__all__ = ('WorkController',)
-
-#: Default socket timeout at shutdown.
-SHUTDOWN_SOCKET_TIMEOUT = 5.0
-
-SELECT_UNKNOWN_QUEUE = """
-Trying to select queue subset of {0!r}, but queue {1} isn't
-defined in the `task_queues` setting.
-
-If you want to automatically declare unknown queues you can
-enable the `task_create_missing_queues` setting.
-"""
-
-DESELECT_UNKNOWN_QUEUE = """
-Trying to deselect queue subset of {0!r}, but queue {1} isn't
-defined in the `task_queues` setting.
-"""
-
-
-class WorkController:
-    """Unmanaged worker instance."""
-
-    app = None
-
-    pidlock = None
-    blueprint = None
-    pool = None
-    semaphore = None
-
-    #: contains the exit code if a :exc:`SystemExit` event is handled.
-    exitcode = None
-
-    class Blueprint(bootsteps.Blueprint):
-        """Worker bootstep blueprint."""
-
-        name = 'Worker'
-        default_steps = {
-            'celery.worker.components:Hub',
-            'celery.worker.components:Pool',
-            'celery.worker.components:Beat',
-            'celery.worker.components:Timer',
-            'celery.worker.components:StateDB',
-            'celery.worker.components:Consumer',
-            'celery.worker.autoscale:WorkerComponent',
-        }
-
-    def __init__(self, app=None, hostname=None, **kwargs):
-        self.app = app or self.app
-        self.hostname = default_nodename(hostname)
-        self.startup_time = datetime.utcnow()
-        self.app.loader.init_worker()
-        self.on_before_init(**kwargs)
-        self.setup_defaults(**kwargs)
-        self.on_after_init(**kwargs)
-
-        self.setup_instance(**self.prepare_args(**kwargs))
-
-    def setup_instance(self, queues=None, ready_callback=None, pidfile=None,
-                       include=None, use_eventloop=None, exclude_queues=None,
-                       **kwargs):
-        self.pidfile = pidfile
-        self.setup_queues(queues, exclude_queues)
-        self.setup_includes(str_to_list(include))
-
-        # Set default concurrency
-        if not self.concurrency:
-            try:
-                self.concurrency = cpu_count()
-            except NotImplementedError:
-                self.concurrency = 2
-
-        # Options
-        self.loglevel = mlevel(self.loglevel)
-        self.ready_callback = ready_callback or self.on_consumer_ready
-
-        # this connection won't establish, only used for params
-        self._conninfo = self.app.connection_for_read()
-        self.use_eventloop = (
-            self.should_use_eventloop() if use_eventloop is None
-            else use_eventloop
-        )
-        self.options = kwargs
-
-        signals.worker_init.send(sender=self)
-
-        # Initialize bootsteps
-        self.pool_cls = _concurrency.get_implementation(self.pool_cls)
-        self.steps = []
-        self.on_init_blueprint()
-        self.blueprint = self.Blueprint(
-            steps=self.app.steps['worker'],
-            on_start=self.on_start,
-            on_close=self.on_close,
-            on_stopped=self.on_stopped,
-        )
-        self.blueprint.apply(self, **kwargs)
-
-    def on_init_blueprint(self):
-        pass
-
-    def on_before_init(self, **kwargs):
-        pass
-
-    def on_after_init(self, **kwargs):
-        pass
-
-    def on_start(self):
-        if self.pidfile:
-            self.pidlock = create_pidlock(self.pidfile)
-
-    def on_consumer_ready(self, consumer):
-        pass
-
-    def on_close(self):
-        self.app.loader.shutdown_worker()
-
-    def on_stopped(self):
-        self.timer.stop()
-        self.consumer.shutdown()
-
-        if self.pidlock:
-            self.pidlock.release()
-
-    def setup_queues(self, include, exclude=None):
-        include = str_to_list(include)
-        exclude = str_to_list(exclude)
-        try:
-            self.app.amqp.queues.select(include)
-        except KeyError as exc:
-            raise ImproperlyConfigured(
-                SELECT_UNKNOWN_QUEUE.strip().format(include, exc))
-        try:
-            self.app.amqp.queues.deselect(exclude)
-        except KeyError as exc:
-            raise ImproperlyConfigured(
-                DESELECT_UNKNOWN_QUEUE.strip().format(exclude, exc))
-        if self.app.conf.worker_direct:
-            self.app.amqp.queues.select_add(worker_direct(self.hostname))
-
-    def setup_includes(self, includes):
-        # Update celery_include to have all known task modules, so that we
-        # ensure all task modules are imported in case an execv happens.
-        prev = tuple(self.app.conf.include)
-        if includes:
-            prev += tuple(includes)
-            [self.app.loader.import_task_module(m) for m in includes]
-        self.include = includes
-        task_modules = {task.__class__.__module__
-                        for task in self.app.tasks.values()}
-        self.app.conf.include = tuple(set(prev) | task_modules)
-
-    def prepare_args(self, **kwargs):
-        return kwargs
-
-    def _send_worker_shutdown(self):
-        signals.worker_shutdown.send(sender=self)
-
-    def start(self):
-        try:
-            self.blueprint.start(self)
-        except WorkerTerminate:
-            self.terminate()
-        except Exception as exc:
-            logger.critical('Unrecoverable error: %r', exc, exc_info=True)
-            self.stop(exitcode=EX_FAILURE)
-        except SystemExit as exc:
-            self.stop(exitcode=exc.code)
-        except KeyboardInterrupt:
-            self.stop(exitcode=EX_FAILURE)
-
-    def register_with_event_loop(self, hub):
-        self.blueprint.send_all(
-            self, 'register_with_event_loop', args=(hub,),
-            description='hub.register',
-        )
-
-    def _process_task_sem(self, req):
-        logger.info(
-            f"Acquire worker(idle:{self.semaphore.value}) for task[{req.id}]")
-        return self._quick_acquire(self._process_task, req)
-
-    def _process_task(self, req):
-        """Process task by sending it to the pool of workers."""
-        try:
-            req.execute_using_pool(self.pool)
-        except TaskRevokedError:
-            try:
-                self._quick_release()   # Issue 877
-            except AttributeError:
-                pass
-
-    def signal_consumer_close(self):
-        try:
-            self.consumer.close()
-        except AttributeError:
-            pass
-
-    def should_use_eventloop(self):
-        return (detect_environment() == 'default' and
-                self._conninfo.transport.implements.asynchronous and
-                not self.app.IS_WINDOWS)
-
-    def stop(self, in_sighandler=False, exitcode=None):
-        """Graceful shutdown of the worker server."""
-        if exitcode is not None:
-            self.exitcode = exitcode
-        if self.blueprint.state == RUN:
-            self.signal_consumer_close()
-            if not in_sighandler or self.pool.signal_safe:
-                self._shutdown(warm=True)
-        self._send_worker_shutdown()
-
-    def terminate(self, in_sighandler=False):
-        """Not so graceful shutdown of the worker server."""
-        if self.blueprint.state != TERMINATE:
-            self.signal_consumer_close()
-            if not in_sighandler or self.pool.signal_safe:
-                self._shutdown(warm=False)
-
-    def _shutdown(self, warm=True):
-        # if blueprint does not exist it means that we had an
-        # error before the bootsteps could be initialized.
-        if self.blueprint is not None:
-            with default_socket_timeout(SHUTDOWN_SOCKET_TIMEOUT):  # Issue 975
-                self.blueprint.stop(self, terminate=not warm)
-                self.blueprint.join()
-
-    def reload(self, modules=None, reload=False, reloader=None):
-        list(self._reload_modules(
-            modules, force_reload=reload, reloader=reloader))
-
-        if self.consumer:
-            self.consumer.update_strategies()
-            self.consumer.reset_rate_limits()
-        try:
-            self.pool.restart()
-        except NotImplementedError:
-            pass
-
-    def _reload_modules(self, modules=None, **kwargs):
-        return (
-            self._maybe_reload_module(m, **kwargs)
-            for m in set(self.app.loader.task_modules
-                         if modules is None else (modules or ()))
-        )
-
-    def _maybe_reload_module(self, module, force_reload=False, reloader=None):
-        if module not in sys.modules:
-            logger.debug('importing module %s', module)
-            return self.app.loader.import_from_cwd(module)
-        elif force_reload:
-            logger.debug('reloading module %s', module)
-            return reload_from_cwd(sys.modules[module], reloader)
-
-    def info(self):
-        uptime = datetime.utcnow() - self.startup_time
-        return {'total': self.state.total_count,
-                'pid': os.getpid(),
-                'clock': str(self.app.clock),
-                'uptime': round(uptime.total_seconds())}
-
-    def rusage(self):
-        if resource is None:
-            raise NotImplementedError('rusage not supported by this platform')
-        s = resource.getrusage(resource.RUSAGE_SELF)
-        return {
-            'utime': s.ru_utime,
-            'stime': s.ru_stime,
-            'maxrss': s.ru_maxrss,
-            'ixrss': s.ru_ixrss,
-            'idrss': s.ru_idrss,
-            'isrss': s.ru_isrss,
-            'minflt': s.ru_minflt,
-            'majflt': s.ru_majflt,
-            'nswap': s.ru_nswap,
-            'inblock': s.ru_inblock,
-            'oublock': s.ru_oublock,
-            'msgsnd': s.ru_msgsnd,
-            'msgrcv': s.ru_msgrcv,
-            'nsignals': s.ru_nsignals,
-            'nvcsw': s.ru_nvcsw,
-            'nivcsw': s.ru_nivcsw,
-        }
-
-    def stats(self):
-        info = self.info()
-        info.update(self.blueprint.info(self))
-        info.update(self.consumer.blueprint.info(self.consumer))
-        try:
-            info['rusage'] = self.rusage()
-        except NotImplementedError:
-            info['rusage'] = 'N/A'
-        return info
-
-    def __repr__(self):
-        """``repr(worker)``."""
-        return '<Worker: {self.hostname} ({state})>'.format(
-            self=self,
-            state=self.blueprint.human_state() if self.blueprint else 'INIT',
-        )
-
-    def __str__(self):
-        """``str(worker) == worker.hostname``."""
-        return self.hostname
-
-    @property
-    def state(self):
-        return state
-
-    def setup_defaults(self, concurrency=None, loglevel='WARN', logfile=None,
-                       task_events=None, pool=None, consumer_cls=None,
-                       timer_cls=None, timer_precision=None,
-                       autoscaler_cls=None,
-                       pool_putlocks=None,
-                       pool_restarts=None,
-                       optimization=None, O=None,  # O maps to -O=fair
-                       statedb=None,
-                       time_limit=None,
-                       soft_time_limit=None,
-                       scheduler=None,
-                       pool_cls=None,              # XXX use pool
-                       state_db=None,              # XXX use statedb
-                       task_time_limit=None,       # XXX use time_limit
-                       task_soft_time_limit=None,  # XXX use soft_time_limit
-                       scheduler_cls=None,         # XXX use scheduler
-                       schedule_filename=None,
-                       max_tasks_per_child=None,
-                       prefetch_multiplier=None, disable_rate_limits=None,
-                       worker_lost_wait=None,
-                       max_memory_per_child=None, **_kw):
-        either = self.app.either
-        self.loglevel = loglevel
-        self.logfile = logfile
-
-        self.concurrency = either('worker_concurrency', concurrency)
-        self.task_events = either('worker_send_task_events', task_events)
-        self.pool_cls = either('worker_pool', pool, pool_cls)
-        self.consumer_cls = either('worker_consumer', consumer_cls)
-        self.timer_cls = either('worker_timer', timer_cls)
-        self.timer_precision = either(
-            'worker_timer_precision', timer_precision,
-        )
-        self.optimization = optimization or O
-        self.autoscaler_cls = either('worker_autoscaler', autoscaler_cls)
-        self.pool_putlocks = either('worker_pool_putlocks', pool_putlocks)
-        self.pool_restarts = either('worker_pool_restarts', pool_restarts)
-        self.statedb = either('worker_state_db', statedb, state_db)
-        self.schedule_filename = either(
-            'beat_schedule_filename', schedule_filename,
-        )
-        self.scheduler = either('beat_scheduler', scheduler, scheduler_cls)
-        self.time_limit = either(
-            'task_time_limit', time_limit, task_time_limit)
-        self.soft_time_limit = either(
-            'task_soft_time_limit', soft_time_limit, task_soft_time_limit,
-        )
-        self.max_tasks_per_child = either(
-            'worker_max_tasks_per_child', max_tasks_per_child,
-        )
-        self.max_memory_per_child = either(
-            'worker_max_memory_per_child', max_memory_per_child,
-        )
-        self.prefetch_multiplier = int(either(
-            'worker_prefetch_multiplier', prefetch_multiplier,
-        ))
-        self.disable_rate_limits = either(
-            'worker_disable_rate_limits', disable_rate_limits,
-        )
-        self.worker_lost_wait = either('worker_lost_wait', worker_lost_wait)
+"""WorkController can be used to instantiate in-process workers.
+
+The command-line interface for the worker is in :mod:`celery.bin.worker`,
+while the worker program is in :mod:`celery.apps.worker`.
+
+The worker program is responsible for adding signal handlers,
+setting up logging, etc.  This is a bare-bones worker without
+global side-effects (i.e., except for the global state stored in
+:mod:`celery.worker.state`).
+
+The worker consists of several components, all managed by bootsteps
+(mod:`celery.bootsteps`).
+"""
+
+import os
+import sys
+from datetime import datetime
+
+from billiard import cpu_count
+from kombu.utils.compat import detect_environment
+
+from celery import bootsteps
+from celery import concurrency as _concurrency
+from celery import signals
+from celery.bootsteps import RUN, TERMINATE
+from celery.exceptions import (ImproperlyConfigured, TaskRevokedError,
+                               WorkerTerminate)
+from celery.platforms import EX_FAILURE, create_pidlock
+from celery.utils.imports import reload_from_cwd
+from celery.utils.log import mlevel
+from celery.utils.log import worker_logger as logger
+from celery.utils.nodenames import default_nodename, worker_direct
+from celery.utils.text import str_to_list
+from celery.utils.threads import default_socket_timeout
+
+from . import state
+
+try:
+    import resource
+except ImportError:  # pragma: no cover
+    resource = None  # noqa
+
+
+__all__ = ('WorkController',)
+
+#: Default socket timeout at shutdown.
+SHUTDOWN_SOCKET_TIMEOUT = 5.0
+
+SELECT_UNKNOWN_QUEUE = """
+Trying to select queue subset of {0!r}, but queue {1} isn't
+defined in the `task_queues` setting.
+
+If you want to automatically declare unknown queues you can
+enable the `task_create_missing_queues` setting.
+"""
+
+DESELECT_UNKNOWN_QUEUE = """
+Trying to deselect queue subset of {0!r}, but queue {1} isn't
+defined in the `task_queues` setting.
+"""
+
+
+class WorkController:
+    """Unmanaged worker instance."""
+
+    app = None
+
+    pidlock = None
+    blueprint = None
+    pool = None
+    semaphore = None
+
+    #: contains the exit code if a :exc:`SystemExit` event is handled.
+    exitcode = None
+
+    class Blueprint(bootsteps.Blueprint):
+        """Worker bootstep blueprint."""
+
+        name = 'Worker'
+        default_steps = {
+            'celery.worker.components:Hub',
+            'celery.worker.components:Pool',
+            'celery.worker.components:Beat',
+            'celery.worker.components:Timer',
+            'celery.worker.components:StateDB',
+            'celery.worker.components:Consumer',
+            'celery.worker.autoscale:WorkerComponent',
+        }
+
+    def __init__(self, app=None, hostname=None, **kwargs):
+        self.app = app or self.app
+        self.hostname = default_nodename(hostname)
+        self.startup_time = datetime.utcnow()
+        self.app.loader.init_worker()
+        self.on_before_init(**kwargs)
+        self.setup_defaults(**kwargs)
+        self.on_after_init(**kwargs)
+
+        self.setup_instance(**self.prepare_args(**kwargs))
+
+    def setup_instance(self, queues=None, ready_callback=None, pidfile=None,
+                       include=None, use_eventloop=None, exclude_queues=None,
+                       **kwargs):
+        self.pidfile = pidfile
+        self.setup_queues(queues, exclude_queues)
+        self.setup_includes(str_to_list(include))
+
+        # Set default concurrency
+        if not self.concurrency:
+            try:
+                self.concurrency = cpu_count()
+            except NotImplementedError:
+                self.concurrency = 2
+
+        # Options
+        self.loglevel = mlevel(self.loglevel)
+        self.ready_callback = ready_callback or self.on_consumer_ready
+
+        # this connection won't establish, only used for params
+        self._conninfo = self.app.connection_for_read()
+        self.use_eventloop = (
+            self.should_use_eventloop() if use_eventloop is None
+            else use_eventloop
+        )
+        self.options = kwargs
+
+        signals.worker_init.send(sender=self)
+
+        # Initialize bootsteps
+        self.pool_cls = _concurrency.get_implementation(self.pool_cls)
+        self.steps = []
+        self.on_init_blueprint()
+        self.blueprint = self.Blueprint(
+            steps=self.app.steps['worker'],
+            on_start=self.on_start,
+            on_close=self.on_close,
+            on_stopped=self.on_stopped,
+        )
+        self.blueprint.apply(self, **kwargs)
+
+    def on_init_blueprint(self):
+        pass
+
+    def on_before_init(self, **kwargs):
+        pass
+
+    def on_after_init(self, **kwargs):
+        pass
+
+    def on_start(self):
+        if self.pidfile:
+            self.pidlock = create_pidlock(self.pidfile)
+
+    def on_consumer_ready(self, consumer):
+        pass
+
+    def on_close(self):
+        self.app.loader.shutdown_worker()
+
+    def on_stopped(self):
+        self.timer.stop()
+        self.consumer.shutdown()
+
+        if self.pidlock:
+            self.pidlock.release()
+
+    def setup_queues(self, include, exclude=None):
+        include = str_to_list(include)
+        exclude = str_to_list(exclude)
+        try:
+            self.app.amqp.queues.select(include)
+        except KeyError as exc:
+            raise ImproperlyConfigured(
+                SELECT_UNKNOWN_QUEUE.strip().format(include, exc))
+        try:
+            self.app.amqp.queues.deselect(exclude)
+        except KeyError as exc:
+            raise ImproperlyConfigured(
+                DESELECT_UNKNOWN_QUEUE.strip().format(exclude, exc))
+        if self.app.conf.worker_direct:
+            self.app.amqp.queues.select_add(worker_direct(self.hostname))
+
+    def setup_includes(self, includes):
+        # Update celery_include to have all known task modules, so that we
+        # ensure all task modules are imported in case an execv happens.
+        prev = tuple(self.app.conf.include)
+        if includes:
+            prev += tuple(includes)
+            [self.app.loader.import_task_module(m) for m in includes]
+        self.include = includes
+        task_modules = {task.__class__.__module__
+                        for task in self.app.tasks.values()}
+        self.app.conf.include = tuple(set(prev) | task_modules)
+
+    def prepare_args(self, **kwargs):
+        return kwargs
+
+    def _send_worker_shutdown(self):
+        signals.worker_shutdown.send(sender=self)
+
+    def start(self):
+        try:
+            self.blueprint.start(self)
+        except WorkerTerminate:
+            self.terminate()
+        except Exception as exc:
+            logger.critical('Unrecoverable error: %r', exc, exc_info=True)
+            self.stop(exitcode=EX_FAILURE)
+        except SystemExit as exc:
+            self.stop(exitcode=exc.code)
+        except KeyboardInterrupt:
+            self.stop(exitcode=EX_FAILURE)
+
+    def register_with_event_loop(self, hub):
+        self.blueprint.send_all(
+            self, 'register_with_event_loop', args=(hub,),
+            description='hub.register',
+        )
+
+    def _process_task_sem(self, req):
+        logger.info(
+            f"Acquire worker(idle:{self.semaphore.value}) for task[{req.id}]")
+        return self._quick_acquire(self._process_task, req)
+
+    def _process_task(self, req):
+        """Process task by sending it to the pool of workers."""
+        try:
+            req.execute_using_pool(self.pool)
+        except TaskRevokedError:
+            try:
+                self._quick_release()   # Issue 877
+            except AttributeError:
+                pass
+
+    def signal_consumer_close(self):
+        try:
+            self.consumer.close()
+        except AttributeError:
+            pass
+
+    def should_use_eventloop(self):
+        return (detect_environment() == 'default' and
+                self._conninfo.transport.implements.asynchronous and
+                not self.app.IS_WINDOWS)
+
+    def stop(self, in_sighandler=False, exitcode=None):
+        """Graceful shutdown of the worker server."""
+        if exitcode is not None:
+            self.exitcode = exitcode
+        if self.blueprint.state == RUN:
+            self.signal_consumer_close()
+            if not in_sighandler or self.pool.signal_safe:
+                self._shutdown(warm=True)
+        self._send_worker_shutdown()
+
+    def terminate(self, in_sighandler=False):
+        """Not so graceful shutdown of the worker server."""
+        if self.blueprint.state != TERMINATE:
+            self.signal_consumer_close()
+            if not in_sighandler or self.pool.signal_safe:
+                self._shutdown(warm=False)
+
+    def _shutdown(self, warm=True):
+        # if blueprint does not exist it means that we had an
+        # error before the bootsteps could be initialized.
+        if self.blueprint is not None:
+            with default_socket_timeout(SHUTDOWN_SOCKET_TIMEOUT):  # Issue 975
+                self.blueprint.stop(self, terminate=not warm)
+                self.blueprint.join()
+
+    def reload(self, modules=None, reload=False, reloader=None):
+        list(self._reload_modules(
+            modules, force_reload=reload, reloader=reloader))
+
+        if self.consumer:
+            self.consumer.update_strategies()
+            self.consumer.reset_rate_limits()
+        try:
+            self.pool.restart()
+        except NotImplementedError:
+            pass
+
+    def _reload_modules(self, modules=None, **kwargs):
+        return (
+            self._maybe_reload_module(m, **kwargs)
+            for m in set(self.app.loader.task_modules
+                         if modules is None else (modules or ()))
+        )
+
+    def _maybe_reload_module(self, module, force_reload=False, reloader=None):
+        if module not in sys.modules:
+            logger.debug('importing module %s', module)
+            return self.app.loader.import_from_cwd(module)
+        elif force_reload:
+            logger.debug('reloading module %s', module)
+            return reload_from_cwd(sys.modules[module], reloader)
+
+    def info(self):
+        uptime = datetime.utcnow() - self.startup_time
+        return {'total': self.state.total_count,
+                'pid': os.getpid(),
+                'clock': str(self.app.clock),
+                'uptime': round(uptime.total_seconds())}
+
+    def rusage(self):
+        if resource is None:
+            raise NotImplementedError('rusage not supported by this platform')
+        s = resource.getrusage(resource.RUSAGE_SELF)
+        return {
+            'utime': s.ru_utime,
+            'stime': s.ru_stime,
+            'maxrss': s.ru_maxrss,
+            'ixrss': s.ru_ixrss,
+            'idrss': s.ru_idrss,
+            'isrss': s.ru_isrss,
+            'minflt': s.ru_minflt,
+            'majflt': s.ru_majflt,
+            'nswap': s.ru_nswap,
+            'inblock': s.ru_inblock,
+            'oublock': s.ru_oublock,
+            'msgsnd': s.ru_msgsnd,
+            'msgrcv': s.ru_msgrcv,
+            'nsignals': s.ru_nsignals,
+            'nvcsw': s.ru_nvcsw,
+            'nivcsw': s.ru_nivcsw,
+        }
+
+    def stats(self):
+        info = self.info()
+        info.update(self.blueprint.info(self))
+        info.update(self.consumer.blueprint.info(self.consumer))
+        try:
+            info['rusage'] = self.rusage()
+        except NotImplementedError:
+            info['rusage'] = 'N/A'
+        return info
+
+    def __repr__(self):
+        """``repr(worker)``."""
+        return '<Worker: {self.hostname} ({state})>'.format(
+            self=self,
+            state=self.blueprint.human_state() if self.blueprint else 'INIT',
+        )
+
+    def __str__(self):
+        """``str(worker) == worker.hostname``."""
+        return self.hostname
+
+    @property
+    def state(self):
+        return state
+
+    def setup_defaults(self, concurrency=None, loglevel='WARN', logfile=None,
+                       task_events=None, pool=None, consumer_cls=None,
+                       timer_cls=None, timer_precision=None,
+                       autoscaler_cls=None,
+                       pool_putlocks=None,
+                       pool_restarts=None,
+                       optimization=None, O=None,  # O maps to -O=fair
+                       statedb=None,
+                       time_limit=None,
+                       soft_time_limit=None,
+                       scheduler=None,
+                       pool_cls=None,              # XXX use pool
+                       state_db=None,              # XXX use statedb
+                       task_time_limit=None,       # XXX use time_limit
+                       task_soft_time_limit=None,  # XXX use soft_time_limit
+                       scheduler_cls=None,         # XXX use scheduler
+                       schedule_filename=None,
+                       max_tasks_per_child=None,
+                       prefetch_multiplier=None, disable_rate_limits=None,
+                       worker_lost_wait=None,
+                       max_memory_per_child=None, **_kw):
+        either = self.app.either
+        self.loglevel = loglevel
+        self.logfile = logfile
+
+        self.concurrency = either('worker_concurrency', concurrency)
+        self.task_events = either('worker_send_task_events', task_events)
+        self.pool_cls = either('worker_pool', pool, pool_cls)
+        self.consumer_cls = either('worker_consumer', consumer_cls)
+        self.timer_cls = either('worker_timer', timer_cls)
+        self.timer_precision = either(
+            'worker_timer_precision', timer_precision,
+        )
+        self.optimization = optimization or O
+        self.autoscaler_cls = either('worker_autoscaler', autoscaler_cls)
+        self.pool_putlocks = either('worker_pool_putlocks', pool_putlocks)
+        self.pool_restarts = either('worker_pool_restarts', pool_restarts)
+        self.statedb = either('worker_state_db', statedb, state_db)
+        self.schedule_filename = either(
+            'beat_schedule_filename', schedule_filename,
+        )
+        self.scheduler = either('beat_scheduler', scheduler, scheduler_cls)
+        self.time_limit = either(
+            'task_time_limit', time_limit, task_time_limit)
+        self.soft_time_limit = either(
+            'task_soft_time_limit', soft_time_limit, task_soft_time_limit,
+        )
+        self.max_tasks_per_child = either(
+            'worker_max_tasks_per_child', max_tasks_per_child,
+        )
+        self.max_memory_per_child = either(
+            'worker_max_memory_per_child', max_memory_per_child,
+        )
+        self.prefetch_multiplier = int(either(
+            'worker_prefetch_multiplier', prefetch_multiplier,
+        ))
+        self.disable_rate_limits = either(
+            'worker_disable_rate_limits', disable_rate_limits,
+        )
+        self.worker_lost_wait = either('worker_lost_wait', worker_lost_wait)
```

## Comparing `deepfos_celery-1.1.8.dist-info/LICENSE` & `deepfos_celery-1.1.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `deepfos_celery-1.1.8.dist-info/METADATA` & `deepfos_celery-1.1.9.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deepfos-celery
-Version: 1.1.8
+Version: 1.1.9
 Summary: Distributed Task Queue.
 Home-page: http://celeryproject.org
 Author: DeepFOS
 Author-email: python@deepfinance.com
 License: BSD
 Project-URL: Documentation, http://docs.celeryproject.org/en/latest/index.html
 Project-URL: Code, https://gitee.com/python-development-team/deepfos-celery/
@@ -105,15 +105,15 @@
 Provides-Extra: zstd
 Requires-Dist: zstandard ; extra == 'zstd'
 
 .. image:: https://docs.celeryq.dev/en/latest/_images/celery-banner-small.png
 
 |license| |wheel| |pyversion|
 
-:Version: 1.1.8
+:Version: 1.1.9
 :Web: http://celeryproject.org/
 :Download: https://pypi.org/project/deepfos-celery/
 :Source: https://gitee.com/python-development-team/deepfos-celery/
 :Keywords: task, queue, job, async, rabbitmq, amqp, redis,
   python, distributed, actors
 
 What is this?
```

## Comparing `deepfos_celery-1.1.8.dist-info/RECORD` & `deepfos_celery-1.1.9.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,35 +1,35 @@
-celery/__init__.py,sha256=7dn1sEyYeEksOdmC-8H6ypcSbU8_7xUITm-eeqbVJp8,6078
+celery/__init__.py,sha256=ZNY5PvarWXmsAQiftsRitVFWsqIdA1jwobWNJ1NVXTc,6254
 celery/__main__.py,sha256=-2sIMgK0_-22t6i19645vA-CfBFvHnzIuZ8les4AmdE,401
 celery/_state.py,sha256=bGNKTgAdtn2VIkePeCzXBfKkMiJz3iqDlYVupM1uA1M,5049
 celery/beat.py,sha256=lj3tHtFM_mfqiODqi_An0NwWqnJqt2TFuzKLFAgArSM,24135
 celery/bootsteps.py,sha256=ddrxGDRCc2ra2ug0X5NBEI3fqPjkSlNRiYSXg2vGCq0,12297
 celery/canvas.py,sha256=A15hHG_DD3nymp6dOV4PX5W8-nMPUFSGmzuEUhdIJSw,58315
 celery/exceptions.py,sha256=ayeUePwMOfUyURpfaGgnY9lLf4NQzstM-I1d7N8N6dE,8601
 celery/local.py,sha256=OW4OUr9A-dbH-syk1ZbfzXBz-taMc4msms-rTV5ObXU,17016
 celery/platforms.py,sha256=YaxSDQ0iN7OljGAe3l5mSzPjAu1qOW6qBL4kTOWLBtc,24279
 celery/result.py,sha256=iz50QTqMt8MNjz5_-aGvPAlzZTjakltAV7Ht5yJL36E,34176
 celery/schedules.py,sha256=JoarLbaT-n7NbmFNiQH-hMsfAdslctTCbiivwU_fL_o,29259
 celery/signals.py,sha256=CgnwpZUNMZZqKIKOrCjsDnmxCSHSNgRXIjZFZ0uqlt4,4273
 celery/states.py,sha256=iIBdd02UPwDKTIYm9MGlIgHGBmrr9AzGgvbeBkPgSyU,3260
 celery/aio/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-celery/aio/amqp.py,sha256=pVguSRzz82pUeys5adFBS1Xm3jvgn5KaGnuNqeKHwME,6304
-celery/aio/backend.py,sha256=1G9fCTonAegaR8xFEZatZ7PeftdVHc7sTUe31JCuh9o,24479
+celery/aio/amqp.py,sha256=owQKkjGLXLiglFVQPU6-vrfjA-yr1Gnks_WrOF-h3mo,6461
+celery/aio/backend.py,sha256=Z5LcZTltcd9ABP52VlKSigSsB3m_BV4pcXh7_FQgZ3o,25158
 celery/aio/canvas.py,sha256=zJesq2emDEkJ6ae-FPo9gXXdN6a-4roysPH2g3Gu3MI,3008
 celery/aio/control.py,sha256=Pdyk3E2TcD7c-YRiBCdWGUfA_Q2EXCvx3vEE9i57wYY,8918
 celery/aio/dispatcher.py,sha256=EtASVKNXmw-fnEFiguyxyk3cq0xS1iui_zHHMW_ZFNE,6242
 celery/aio/events.py,sha256=Taqf0exjNddP5ZvtKm73z_coOd_QXbRHSShnUUCFOFI,145
 celery/aio/result.py,sha256=61SLSmC0eKnIcMNKAzaB6oqrpP11y7vozPLxx4v2ilI,13301
 celery/aio/task.py,sha256=SQjQzNnnhFEbTRkKHtw531-4yWcHGAxAVO5eX6UQHeE,3017
 celery/app/__init__.py,sha256=iadhB_pmHYrlqaOMge-aO2CLQKxlG18yHaNYtiHL-3Y,2459
-celery/app/amqp.py,sha256=y_p966eX5wVMcezEEtTlj_lnIR-Whwe21OvSYP4s-4w,23519
+celery/app/amqp.py,sha256=ln1zBOxDHKhBHBAwbMfjfDHU0g_VjVD-4fElFYqZN4o,24144
 celery/app/annotations.py,sha256=93zuKNCE7pcMD3K5tM5HMeVCQ5lfJR_0htFpottgOeU,1445
 celery/app/autoretry.py,sha256=kyBQXI75-96GNbu1uNfU135SgoaEI_0w0G4VjLsfHdE,2288
-celery/app/backends.py,sha256=VcsDY2VLRLl0u2BrPT2vjQ9yCWaDDFy3LRzri12Q_l0,2826
-celery/app/base.py,sha256=DSaS0Owklup-qLD9zRYmS8T_QMU9LNnbHPW513YgVYw,56868
+celery/app/backends.py,sha256=QHiVWGGhXG-gnwsGz7FavYcoLc3HPgOgl5ThgMHKY3A,2897
+celery/app/base.py,sha256=orv6vYqrQgutCkoISd05O5kzhqNdVov193She_V1PC8,58408
 celery/app/builtins.py,sha256=gnOyE07M8zgxatTmb0D0vKztx1sQZaRi_hO_d-FLNUs,6673
 celery/app/control.py,sha256=ccv6LWR2ZlgFfppzN8lvUuJjBglUb2UmQkPGxUqUxbA,16748
 celery/app/defaults.py,sha256=6Ms7p-cmqYTKPTiH2bUzTaOE2n6a0_DmH3YXrS2iJos,14654
 celery/app/events.py,sha256=9ZyjdhUVvrt6xLdOMOVTPN7gjydLWQGNr4hvFoProuA,1326
 celery/app/log.py,sha256=boRudy2dgGU0DQVO_Oe34TayDAxXtAZeIIq83D-obUM,9169
 celery/app/registry.py,sha256=RdwOWprajMrg4fHMeRbcrH5VgHMNRCks34xhroXZCPA,2002
 celery/app/routes.py,sha256=5pakyI78LMjxm3eERgsREAUSlx73houst5qmRY7MVB8,4685
@@ -51,15 +51,15 @@
 celery/backends/cosmosdbsql.py,sha256=iyTV3M0xVllzNrMMHTWHHJoEbU0W0wHtjzuBzlf-1Wc,6847
 celery/backends/couchbase.py,sha256=xRw4mbgjfyNtfd1X1mox3cPbcR4-emeJ6Re56F-X0Gk,3173
 celery/backends/couchdb.py,sha256=tbOCHbZTmi5RppnCAUjqygM3gCUr82NfuKR5eVNCRuI,2919
 celery/backends/dynamodb.py,sha256=cNfi97bZxyfFR3pwQpueJQBbZmSWq3I9NpLgGyGPLq8,17262
 celery/backends/elasticsearch.py,sha256=99Hf8o8lCWgFHBvN-cu5ARR2cSBWrywBKzOBc1K_djs,8469
 celery/backends/filesystem.py,sha256=gF1NIw6QyzFRbqHSKesMN6a355ZElUCc83baVfdEI5g,2925
 celery/backends/mongodb.py,sha256=UEzW5n6L-ilt9h6Q7enYULv0KyWdXFeY1X2nLlPOCdY,11189
-celery/backends/redis.py,sha256=fQmCP0Sxd_Nrg0oDREu1X_6sv8gOHC_QfX8pzLxha0o,23218
+celery/backends/redis.py,sha256=9d7jBEdWKV-OypJBxprX6yLO_E2zyZbikREKkSoVZpU,23835
 celery/backends/rpc.py,sha256=Pfzjpz7znOfmHRERuQfOlTW-entAsl803oc1-EWpnTY,12077
 celery/backends/s3.py,sha256=MUL4-bEHCcTL53XXyb020zyLYTr44DDjOh6BXtkp9lQ,2752
 celery/backends/database/__init__.py,sha256=ukNFAyO1PK8F9bm9S9VzDydCJVgAB3N-pSLufYyuQig,7771
 celery/backends/database/models.py,sha256=_6WZMv53x8I1iBRCa4hY35LaBUeLIZJzDusjvS-8aAg,3351
 celery/backends/database/session.py,sha256=9a0HgErjuQBxRsitUnUDbnT5wYfVi1RKgclUS-Dx45c,2866
 celery/bin/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 celery/bin/amqp.py,sha256=m4im2XnjeNCVSAJFj-4i3IN5UNNCvXRizbyxg1v5Bc8,9960
@@ -76,15 +76,15 @@
 celery/bin/multi.py,sha256=AoChXbxsS0Q0v8gh7t2Tzow6LfJFlIxj2R0P3LM8VBY,15363
 celery/bin/purge.py,sha256=WudoOOROcIh3yFVb2vHyZ9pRkrpKoebE_gFJz4rXQxw,2578
 celery/bin/result.py,sha256=TK2k84Mqh1nQ3XIOT_EN3Y4Y3h5ZcJIH8MEU-YIgJIE,1007
 celery/bin/shell.py,sha256=HxSoyFIpwEY9vp6bt6_5XDN4R2hfm0bVO7Ska2Y7R60,4809
 celery/bin/upgrade.py,sha256=EeqwiD0X_BRzoZY3uIlmgwTmMswZbh9os6rU7n6rYb8,3072
 celery/bin/worker.py,sha256=SWJCJjiMxx8W34I_NUHZuAMUkI44bXzlwTfPDhDeY8g,11751
 celery/concurrency/__init__.py,sha256=b1JLNOS2vl7Q6MxjRHx8JWx7KC5Vto_D7knYHAQyB3c,852
-celery/concurrency/asynpool.py,sha256=v79ER45ecA9pt62Svb88m_S4x4fde8UwjpgDzJ0SFuo,57584
+celery/concurrency/asynpool.py,sha256=PG4yWAB75vZhQz8c6XQ9VwMHQOJip9rGBAiF1oBX9DY,58104
 celery/concurrency/base.py,sha256=MXjG1Aft74cXG5c7VL1afmDA68ZBz0HqunhRSD55JVE,4373
 celery/concurrency/eventlet.py,sha256=lXSf5bT3F2aOMWKfupt7jBCIPJGEjbEpX-ucKETe22Y,4156
 celery/concurrency/gevent.py,sha256=M5emMhoGweaAjOCw-caO96rPrV_yLSLyBTNi6bAV08c,3415
 celery/concurrency/prefork.py,sha256=i5mvs88YCXS35KDEpzsAw1peZ_8hLxEfvxJJfeUqDt0,5931
 celery/concurrency/solo.py,sha256=WcOi0h4sJlE7WbR7DbHwZiHRE1-jlwMPW_4ENckQFKk,693
 celery/concurrency/thread.py,sha256=n-7ayKpyM80nRIQcxUVJo1yZLezI4n5IjWkfkXqTA6g,1248
 celery/contrib/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
@@ -99,36 +99,36 @@
 celery/contrib/testing/mocks.py,sha256=tqWzr_NscWpCZhogLr3K1h8Fi4WBkBKeX_R8sf-8dlQ,3394
 celery/contrib/testing/tasks.py,sha256=pJM3aabw7udcppz4QNeUg1-6nlnbklrT-hP5JXmL-gM,208
 celery/contrib/testing/worker.py,sha256=yLGHTiYzKdW0wupmBEDVuUQ9GlADotBnw91A7-7zDcQ,5040
 celery/events/__init__.py,sha256=9d2cviCw5zIsZ3AvQJkx77HPTlxmVIahRR7Qa54nQnU,477
 celery/events/cursesmon.py,sha256=pDPV1roUP08zo_Io6-8oP0Qr1KSs1eO1Pp6sB3A43fA,18050
 celery/events/dispatcher.py,sha256=7b3-3d_6ukvRNajyfiHMX1YvoWNIzaB6zS3-zEUQhG4,8987
 celery/events/dumper.py,sha256=7zOVmAVfG2HXW79Fuvpo_0C2cjztTzgIXnaiUc4NL8c,3116
-celery/events/event.py,sha256=vEk4YUOkNxCFb2OMdC2pj9_mGlJx1bL_ORyAru2os6M,1755
+celery/events/event.py,sha256=p8ea0IdqTDNUMwPBpSNjt7Je_EzUWHnzwQH9IQP-5i4,1818
 celery/events/receiver.py,sha256=7dVvezYkBQOtyI-rH77-5QDJztPLB933VF7NgmezSuU,4998
 celery/events/snapshot.py,sha256=oTLYH8t2muolBucP5UN4GGNuK81C1nz9uUIPcAt_t-g,3274
 celery/events/state.py,sha256=D2tWItcs3i47Rj8W_eTGPhDbuJgWykuBIqHocvIKKoQ,25773
 celery/fixups/__init__.py,sha256=7ctNaKHiOa2fVePcdKPU9J-_bQ0k1jFHaoZlCHXY0vU,14
 celery/fixups/django.py,sha256=8YLxkFOPIFV_4RYNtEbk3Bc-MJkJo42mqHnv3UdH4EU,6596
 celery/loaders/__init__.py,sha256=LnRTWk8pz2r7BUj2VUJiBstPjSBwCP0gUDRkbchGW24,490
 celery/loaders/app.py,sha256=xqRpRDJkGmTW21N_7zx5F4Na-GCTbNs6Q6tGfInnZnU,199
 celery/loaders/base.py,sha256=vImYhKI3NfS97gQ9pIMmVZAa2sPeqm7EA8nNAfLZhcY,8731
 celery/loaders/default.py,sha256=TZq6zR4tg_20sVJAuSwSBLVRHRyfevHkHhUYrNRYkTU,1520
-celery/patches/__init__.py,sha256=4zhi_LJMm8t9LP5-oMGeXIjMvKR1uK1ffxtlqsd7DnU,120
-celery/patches/kombu/__init__.py,sha256=SNgsLnjMuOsg3KuBlunfkZr66RvUbMYJ3fZvtvyOcqI,201
+celery/patches/__init__.py,sha256=E6BQK4HOpm438EV-7Dqwwq0dvJSOVPLqLaat4lz7OO0,128
+celery/patches/kombu/__init__.py,sha256=wacopPfUm9nh-xT6lMOWQ1hKnYzAJT4hjtoRPufYSG4,210
 celery/patches/kombu/common.py,sha256=kP4XyOSvrRqR2llwfkfB60onaVzEHr3CDwZ-2ooyOjA,6656
 celery/patches/kombu/connection.py,sha256=halFvCY9HyIrO2paoar9rkWD1SZvK5DN0q_lBN2vo6c,12680
 celery/patches/kombu/entity.py,sha256=bigkRE44bLfh2fIlNeQ3j6Hq_78o6BNU0XW1G4guj_A,4396
 celery/patches/kombu/exchange.py,sha256=rAK2NYH7np1ppAhe5XME83jqQWUEwke0w4oiobD_0lY,2702
 celery/patches/kombu/messaging.py,sha256=olyZV7Ca8XGRLXRJX-sk-x0EkmIi4PM2BRqR7i8BbKM,8775
 celery/patches/kombu/pools.py,sha256=xo1AuQaclZzL1lzg-Ob5Z8aEdoRBrAJ0G__qSYqrQqo,1991
-celery/patches/kombu/redis.py,sha256=JhyqISbkaNWFr9Kl_4Z__QUD8zUj6z86GylQ5-lPSXo,37932
-celery/patches/kombu/redis_cluster.py,sha256=5DAryjBulBJkaqN9dPlI_xC_THyolGZ8VyQsQ1iY3es,9057
+celery/patches/kombu/redis.py,sha256=3uxsJQNplnScGXS8nPnf_PkhxfJUWrj9u35ihD_HNOQ,39028
+celery/patches/kombu/redis_cluster.py,sha256=_Ryg4MbhmS0KXnbF7YOPfbqJNjJCNoOnK7WG9-2PDxs,9349
 celery/patches/kombu/utils.py,sha256=lND9fEpwUdsI5Mx8udWlKYlwsDth8cnRNUVSs9NzOew,6623
-celery/patches/redis/__init__.py,sha256=VHDX8WZoN2Q3xQrrvskbpJrHAfSFjvAsYLw9Zlaxx0U,4729
+celery/patches/redis/__init__.py,sha256=XdJM7sixJytOIVCCM4Oaz8MhWEuFB0jDNrJ443YZKi0,4870
 celery/security/__init__.py,sha256=SCOoCLegztDzK7rQsLeuF0QBtyyZagChR1S7QDqDIDU,2278
 celery/security/certificate.py,sha256=VzBbZucYY0uXdqvFTN8jPLdg47Nc5rZTZDw4b0afs4w,2993
 celery/security/key.py,sha256=AP-8kBfqaT8a6IpZsf2qALTCVB36RsoqN7e555VF6mw,1042
 celery/security/serialization.py,sha256=bOPi_ugy9EhmneLjjALvt-ZrwFCtPqQwH7f4zcqdq3I,4206
 celery/security/utils.py,sha256=VJuWxLZFKXQXzlBczuxo94wXWSULnXwbO_5ul_hwse0,845
 celery/utils/__init__.py,sha256=lIJjBxvXCspC-ib-XasdEPlB0xAQc16P0eOPb0gWsL0,935
 celery/utils/abstract.py,sha256=xN2Qr-TEp12P8AYO6WigxFr5p8kJPUUb0f5UX3FtHjI,2874
@@ -161,24 +161,24 @@
 celery/worker/control.py,sha256=2M9ed8Ss_ueQdpFa1HBWnPm-Z9yab0nQcBa85e4APzs,16771
 celery/worker/heartbeat.py,sha256=sTV_d0RB9M6zsXIvLZ7VU6teUfX3IK1ITynDpxMS298,2107
 celery/worker/loops.py,sha256=zLKqw_AXKIT77pBbtUa7u6n8iWhVf3cy7WLIi7M10DI,3857
 celery/worker/pidbox.py,sha256=LcQsKDkd8Z93nQxk0SOLulB8GLEfIjPkN-J0pGk7dfM,3630
 celery/worker/request.py,sha256=i_qEhH2JWTECV20nkS86Nx53pcCZhGzWSLS1MTghXJM,22670
 celery/worker/state.py,sha256=S3aX8c2xep7hb668phDm9ykOwvW9D6D6l3D8m0-VQoU,7639
 celery/worker/strategy.py,sha256=bra8zu80V7a4uLnBW2Aa37Pg_8do_8lPVPmljR_B1mQ,6893
-celery/worker/worker.py,sha256=WMh9ilMHigrleKypQjzUHbgS5XefKKQhvfxT916uuPg,14621
+celery/worker/worker.py,sha256=spk9QU1AXISZ4f9f5vCblTjCjglMr64EX8zgKS3oyfo,15033
 celery/worker/consumer/__init__.py,sha256=yKaGZtBzYKADZMzbSq14_AUYpT4QAY9nRRCf73DDhqc,391
 celery/worker/consumer/agent.py,sha256=bThS8ZVeuybAyqNe8jmdN6RgaJhDq0llewosGrO85-c,525
 celery/worker/consumer/connection.py,sha256=a7g23wmzevkEiMjjjD8Kt4scihf_NgkpR4gcuksys9M,1026
 celery/worker/consumer/consumer.py,sha256=mUqmZVp04As92dJwXXauuzysulCw3sPijMWjoTFAbFw,21497
 celery/worker/consumer/control.py,sha256=0NiJ9P-AHdv134mXkgRgU9hfhdJ_P7HKb7z9A4Xqa2Q,946
 celery/worker/consumer/events.py,sha256=FgDwbV0Jbj9aWPbV3KAUtsXZq4JvZEfrWfnrYgvkMgo,2054
 celery/worker/consumer/gossip.py,sha256=g-WJL2rr_q9aM_SaTUrQlPj2ONf8vHs2LvmyRQtDMEU,6833
 celery/worker/consumer/heart.py,sha256=IenkkliKk6sAk2a1NfYyh-doNDlmFWGRiaJd5e8ALpI,930
 celery/worker/consumer/mingle.py,sha256=UG8K6sXF1KUJXNiJ4eMHUMIg4_7K1tDWqYRNfd9Nz9k,2519
 celery/worker/consumer/tasks.py,sha256=lYjn-Mt2ZulQyO1lwSoMRzCKeWd8f955H_VfGG0-cyg,1959
-deepfos_celery-1.1.8.dist-info/LICENSE,sha256=OPfyaTUqhatgAqmGqGPC73aDk2xXbtiejt3gvKB6yMc,2631
-deepfos_celery-1.1.8.dist-info/METADATA,sha256=NuvKAOtoUN1RPHHC4OExLqVgR0YcCY00qUA_r9xmbwY,6651
-deepfos_celery-1.1.8.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-deepfos_celery-1.1.8.dist-info/entry_points.txt,sha256=y-9XhDI_sXAtYn8ayrWDhRcZmiY4tsmIoa5braB_WVA,49
-deepfos_celery-1.1.8.dist-info/top_level.txt,sha256=sQQ-a5HNsZIi2A8DiKQnB1HODFMfmrzIAZIE8t_XiOA,7
-deepfos_celery-1.1.8.dist-info/RECORD,,
+deepfos_celery-1.1.9.dist-info/LICENSE,sha256=OPfyaTUqhatgAqmGqGPC73aDk2xXbtiejt3gvKB6yMc,2631
+deepfos_celery-1.1.9.dist-info/METADATA,sha256=WqR4wADRF4oLHYCiglwD2I3nEqsq1bqsYlvHZGMarqk,6651
+deepfos_celery-1.1.9.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+deepfos_celery-1.1.9.dist-info/entry_points.txt,sha256=y-9XhDI_sXAtYn8ayrWDhRcZmiY4tsmIoa5braB_WVA,49
+deepfos_celery-1.1.9.dist-info/top_level.txt,sha256=sQQ-a5HNsZIi2A8DiKQnB1HODFMfmrzIAZIE8t_XiOA,7
+deepfos_celery-1.1.9.dist-info/RECORD,,
```

