# Comparing `tmp/gaea_operator-1.2.0.3-py3-none-any.whl.zip` & `tmp/gaea_operator-1.2.0.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,94 +1,94 @@
-Zip file size: 126376 bytes, number of entries: 92
--rw-r--r--  2.0 unx      131 b- defN 24-Mar-30 06:27 gaea_operator/__init__.py
--rw-r--r--  2.0 unx      131 b- defN 24-Mar-30 06:27 gaea_operator/components/__init__.py
--rw-r--r--  2.0 unx      131 b- defN 24-Mar-30 06:27 gaea_operator/components/eval/__init__.py
--rw-r--r--  2.0 unx     4104 b- defN 24-Mar-30 06:27 gaea_operator/components/eval/ppyoloe_plus.py
--rw-r--r--  2.0 unx     4149 b- defN 24-Mar-30 06:27 gaea_operator/components/eval/resnet.py
--rw-r--r--  2.0 unx     2956 b- defN 24-Mar-30 06:27 gaea_operator/components/inference/__init__.py
--rw-r--r--  2.0 unx     4022 b- defN 24-Mar-30 06:27 gaea_operator/components/inference/inference.py
--rw-r--r--  2.0 unx     3524 b- defN 24-Mar-30 06:27 gaea_operator/components/package/__init__.py
--rw-r--r--  2.0 unx     8066 b- defN 24-Mar-30 06:27 gaea_operator/components/package/package.py
--rw-r--r--  2.0 unx      131 b- defN 24-Mar-30 06:27 gaea_operator/components/train/__init__.py
--rw-r--r--  2.0 unx     8345 b- defN 24-Mar-30 06:27 gaea_operator/components/train/ppyoloe_plus.py
--rw-r--r--  2.0 unx     7370 b- defN 24-Mar-30 06:27 gaea_operator/components/train/resnet.py
--rw-r--r--  2.0 unx      131 b- defN 24-Mar-30 06:27 gaea_operator/components/transform/__init__.py
--rw-r--r--  2.0 unx     6066 b- defN 24-Mar-30 06:27 gaea_operator/components/transform/ppyoloe_plus.py
--rw-r--r--  2.0 unx     6202 b- defN 24-Mar-30 06:27 gaea_operator/components/transform/resnet.py
--rw-r--r--  2.0 unx     3323 b- defN 24-Mar-30 06:27 gaea_operator/components/transform_eval/__init__.py
--rw-r--r--  2.0 unx     7486 b- defN 24-Mar-30 06:27 gaea_operator/components/transform_eval/transform_eval.py
--rw-r--r--  2.0 unx      411 b- defN 24-Mar-30 06:27 gaea_operator/config/__init__.py
--rw-r--r--  2.0 unx     4883 b- defN 24-Mar-30 06:27 gaea_operator/config/config.py
--rw-r--r--  2.0 unx    13881 b- defN 24-Mar-30 06:27 gaea_operator/config/generate_transform_config.py
--rw-r--r--  2.0 unx    11921 b- defN 24-Mar-30 06:27 gaea_operator/config/modify_package_files.py
--rw-r--r--  2.0 unx     3626 b- defN 24-Mar-30 06:27 gaea_operator/config/update_parse.py
--rw-r--r--  2.0 unx    16689 b- defN 24-Mar-30 06:27 gaea_operator/config/update_pbtxt.py
--rw-r--r--  2.0 unx      130 b- defN 24-Mar-30 06:27 gaea_operator/config/ppyoloe_plus/__init__.py
--rw-r--r--  2.0 unx     4058 b- defN 24-Mar-30 06:27 gaea_operator/config/ppyoloe_plus/ppyoloeplus_config.py
--rw-r--r--  2.0 unx      130 b- defN 24-Mar-30 06:27 gaea_operator/config/ppyoloe_plus/template/__init__.py
--rw-r--r--  2.0 unx    12561 b- defN 24-Mar-30 06:27 gaea_operator/config/ppyoloe_plus/template/modify_train_parameter.py
--rw-r--r--  2.0 unx     4582 b- defN 24-Mar-30 06:27 gaea_operator/config/ppyoloe_plus/template/parameter.yaml
--rw-r--r--  2.0 unx     4666 b- defN 24-Mar-30 06:27 gaea_operator/config/ppyoloe_plus/template/parameter_c.yaml
--rw-r--r--  2.0 unx      130 b- defN 24-Mar-30 06:27 gaea_operator/config/resnet/__init__.py
--rw-r--r--  2.0 unx     4283 b- defN 24-Mar-30 06:27 gaea_operator/config/resnet/resnet_config.py
--rw-r--r--  2.0 unx    11330 b- defN 24-Mar-30 06:27 gaea_operator/config/resnet/template/modify_train_parameter.py
--rw-r--r--  2.0 unx     2315 b- defN 24-Mar-30 06:27 gaea_operator/config/resnet/template/parameter.yaml
--rw-r--r--  2.0 unx      260 b- defN 24-Mar-30 06:27 gaea_operator/dataset/__init__.py
--rw-r--r--  2.0 unx     3924 b- defN 24-Mar-30 06:27 gaea_operator/dataset/cityscape_dataset.py
--rw-r--r--  2.0 unx     4870 b- defN 24-Mar-30 06:27 gaea_operator/dataset/coco_dataset.py
--rw-r--r--  2.0 unx     4835 b- defN 24-Mar-30 06:27 gaea_operator/dataset/dataset.py
--rw-r--r--  2.0 unx     4095 b- defN 24-Mar-30 06:27 gaea_operator/dataset/imagenet_dataset.py
--rw-r--r--  2.0 unx      584 b- defN 24-Mar-30 06:27 gaea_operator/metric/__init__.py
--rw-r--r--  2.0 unx     5461 b- defN 24-Mar-30 06:27 gaea_operator/metric/metric.py
--rw-r--r--  2.0 unx      433 b- defN 24-Mar-30 06:27 gaea_operator/metric/analysis/__init__.py
--rw-r--r--  2.0 unx    14048 b- defN 24-Mar-30 06:33 gaea_operator/metric/analysis/eval_metric_analysis.py
--rw-r--r--  2.0 unx     5987 b- defN 24-Mar-30 06:27 gaea_operator/metric/analysis/inference_metric_analysis.py
--rw-r--r--  2.0 unx     8902 b- defN 24-Mar-30 06:27 gaea_operator/metric/analysis/label_statistics_metric_analysis.py
--rw-r--r--  2.0 unx      737 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/__init__.py
--rw-r--r--  2.0 unx     3195 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/check.py
--rw-r--r--  2.0 unx     1923 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/metric.py
--rw-r--r--  2.0 unx      701 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/image/__init__.py
--rw-r--r--  2.0 unx     6960 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/image/accuracy.py
--rw-r--r--  2.0 unx     2571 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/image/average_precision.py
--rw-r--r--  2.0 unx     3939 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/image/confusion_matrix.py
--rw-r--r--  2.0 unx    21177 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/image/mean_ap.py
--rw-r--r--  2.0 unx     6128 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/image/mean_iou.py
--rw-r--r--  2.0 unx    19509 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/image/precision_recall_curve.py
--rw-r--r--  2.0 unx    11271 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/image/precision_recall_f1score.py
--rw-r--r--  2.0 unx      289 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/tabular/__init__.py
--rw-r--r--  2.0 unx     1207 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/tabular/count_statistic.py
--rw-r--r--  2.0 unx     2185 b- defN 24-Mar-30 06:27 gaea_operator/metric/operator/tabular/histogram_statistic.py
--rw-r--r--  2.0 unx     4864 b- defN 24-Mar-30 06:27 gaea_operator/metric/schema/object_detection.yaml
--rw-r--r--  2.0 unx      131 b- defN 24-Mar-30 06:27 gaea_operator/metric/types/__init__.py
--rw-r--r--  2.0 unx     1173 b- defN 24-Mar-30 06:27 gaea_operator/metric/types/image_classification_metric.py
--rw-r--r--  2.0 unx     4874 b- defN 24-Mar-30 06:27 gaea_operator/metric/types/metric.py
--rw-r--r--  2.0 unx     2362 b- defN 24-Mar-30 06:27 gaea_operator/metric/types/object_detection_metric.py
--rw-r--r--  2.0 unx     1244 b- defN 24-Mar-30 06:27 gaea_operator/metric/types/semantic_segmentation_metric.py
--rw-r--r--  2.0 unx      214 b- defN 24-Mar-30 06:27 gaea_operator/model/__init__.py
--rw-r--r--  2.0 unx     1424 b- defN 24-Mar-30 06:27 gaea_operator/model/model.py
--rw-r--r--  2.0 unx      181 b- defN 24-Mar-30 06:27 gaea_operator/trainer/__init__.py
--rw-r--r--  2.0 unx     4310 b- defN 24-Mar-30 06:27 gaea_operator/trainer/trainer.py
--rw-r--r--  2.0 unx      187 b- defN 24-Mar-30 06:27 gaea_operator/transform/__init__.py
--rw-r--r--  2.0 unx     3295 b- defN 24-Mar-30 06:27 gaea_operator/transform/cvt_copy_model.py
--rw-r--r--  2.0 unx      776 b- defN 24-Mar-30 06:27 gaea_operator/transform/transform.py
--rw-r--r--  2.0 unx     1788 b- defN 24-Mar-30 06:27 gaea_operator/utils/__init__.py
--rw-r--r--  2.0 unx     5077 b- defN 24-Mar-30 06:27 gaea_operator/utils/accelerator.py
--rw-r--r--  2.0 unx     2709 b- defN 24-Mar-30 06:27 gaea_operator/utils/compress.py
--rw-r--r--  2.0 unx      426 b- defN 24-Mar-30 06:27 gaea_operator/utils/consts.py
--rw-r--r--  2.0 unx     1948 b- defN 24-Mar-30 06:27 gaea_operator/utils/file.py
--rw-r--r--  2.0 unx     1533 b- defN 24-Mar-30 06:27 gaea_operator/utils/import_module.py
--rw-r--r--  2.0 unx     4919 b- defN 24-Mar-30 06:27 gaea_operator/utils/model_template.py
--rw-r--r--  2.0 unx     1680 b- defN 24-Mar-30 06:27 gaea_operator/utils/registry.py
--rw-r--r--  2.0 unx     1000 b- defN 24-Mar-30 06:27 gaea_operator/utils/tensor.py
--rw-r--r--  2.0 unx      301 b- defN 24-Mar-30 06:27 gaea_operator/utils/time.py
--rw-r--r--  2.0 unx    11330 b- defN 24-Mar-30 06:27 gaea_operator-1.2.0.3.data/data/classify.config/modify_train_parameter.py
--rw-r--r--  2.0 unx     2315 b- defN 24-Mar-30 06:27 gaea_operator-1.2.0.3.data/data/classify.config/parameter.yaml
--rw-r--r--  2.0 unx      130 b- defN 24-Mar-30 06:27 gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/__init__.py
--rw-r--r--  2.0 unx    12561 b- defN 24-Mar-30 06:27 gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/modify_train_parameter.py
--rw-r--r--  2.0 unx     4582 b- defN 24-Mar-30 06:27 gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/parameter.yaml
--rw-r--r--  2.0 unx     4666 b- defN 24-Mar-30 06:27 gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/parameter_c.yaml
--rw-r--r--  2.0 unx     4864 b- defN 24-Mar-30 06:27 gaea_operator-1.2.0.3.data/data/schema/object_detection.yaml
--rw-r--r--  2.0 unx     2091 b- defN 24-Mar-30 06:51 gaea_operator-1.2.0.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-30 06:51 gaea_operator-1.2.0.3.dist-info/WHEEL
--rw-r--r--  2.0 unx       14 b- defN 24-Mar-30 06:51 gaea_operator-1.2.0.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     9314 b- defN 24-Mar-30 06:51 gaea_operator-1.2.0.3.dist-info/RECORD
-92 files, 395430 bytes uncompressed, 111082 bytes compressed:  71.9%
+Zip file size: 126661 bytes, number of entries: 92
+-rw-r--r--  2.0 unx      131 b- defN 24-Apr-02 08:59 gaea_operator/__init__.py
+-rw-r--r--  2.0 unx      131 b- defN 24-Apr-02 08:59 gaea_operator/components/__init__.py
+-rw-r--r--  2.0 unx      131 b- defN 24-Apr-02 08:59 gaea_operator/components/eval/__init__.py
+-rw-r--r--  2.0 unx     4268 b- defN 24-Apr-02 08:59 gaea_operator/components/eval/ppyoloe_plus.py
+-rw-r--r--  2.0 unx     4153 b- defN 24-Apr-02 08:59 gaea_operator/components/eval/resnet.py
+-rw-r--r--  2.0 unx     2956 b- defN 24-Apr-02 08:59 gaea_operator/components/inference/__init__.py
+-rw-r--r--  2.0 unx     4034 b- defN 24-Apr-02 08:59 gaea_operator/components/inference/inference.py
+-rw-r--r--  2.0 unx     3555 b- defN 24-Apr-02 08:59 gaea_operator/components/package/__init__.py
+-rw-r--r--  2.0 unx     8375 b- defN 24-Apr-02 08:59 gaea_operator/components/package/package.py
+-rw-r--r--  2.0 unx      131 b- defN 24-Apr-02 08:59 gaea_operator/components/train/__init__.py
+-rw-r--r--  2.0 unx     8365 b- defN 24-Apr-02 08:59 gaea_operator/components/train/ppyoloe_plus.py
+-rw-r--r--  2.0 unx     7390 b- defN 24-Apr-02 08:59 gaea_operator/components/train/resnet.py
+-rw-r--r--  2.0 unx      131 b- defN 24-Apr-02 08:59 gaea_operator/components/transform/__init__.py
+-rw-r--r--  2.0 unx     6078 b- defN 24-Apr-02 08:59 gaea_operator/components/transform/ppyoloe_plus.py
+-rw-r--r--  2.0 unx     6214 b- defN 24-Apr-02 08:59 gaea_operator/components/transform/resnet.py
+-rw-r--r--  2.0 unx     3559 b- defN 24-Apr-02 08:59 gaea_operator/components/transform_eval/__init__.py
+-rw-r--r--  2.0 unx     7658 b- defN 24-Apr-02 08:59 gaea_operator/components/transform_eval/transform_eval.py
+-rw-r--r--  2.0 unx      411 b- defN 24-Apr-02 08:59 gaea_operator/config/__init__.py
+-rw-r--r--  2.0 unx     4883 b- defN 24-Apr-02 08:59 gaea_operator/config/config.py
+-rw-r--r--  2.0 unx    14048 b- defN 24-Apr-02 08:59 gaea_operator/config/generate_transform_config.py
+-rw-r--r--  2.0 unx    11878 b- defN 24-Apr-02 08:59 gaea_operator/config/modify_package_files.py
+-rw-r--r--  2.0 unx     3642 b- defN 24-Apr-02 08:59 gaea_operator/config/update_parse.py
+-rw-r--r--  2.0 unx    16721 b- defN 24-Apr-02 08:59 gaea_operator/config/update_pbtxt.py
+-rw-r--r--  2.0 unx      130 b- defN 24-Apr-02 08:59 gaea_operator/config/ppyoloe_plus/__init__.py
+-rw-r--r--  2.0 unx     4058 b- defN 24-Apr-02 08:59 gaea_operator/config/ppyoloe_plus/ppyoloeplus_config.py
+-rw-r--r--  2.0 unx      130 b- defN 24-Apr-02 08:59 gaea_operator/config/ppyoloe_plus/template/__init__.py
+-rw-r--r--  2.0 unx    12621 b- defN 24-Apr-02 08:59 gaea_operator/config/ppyoloe_plus/template/modify_train_parameter.py
+-rw-r--r--  2.0 unx     4582 b- defN 24-Apr-02 08:59 gaea_operator/config/ppyoloe_plus/template/parameter.yaml
+-rw-r--r--  2.0 unx     4666 b- defN 24-Apr-02 08:59 gaea_operator/config/ppyoloe_plus/template/parameter_c.yaml
+-rw-r--r--  2.0 unx      130 b- defN 24-Apr-02 08:59 gaea_operator/config/resnet/__init__.py
+-rw-r--r--  2.0 unx     4283 b- defN 24-Apr-02 08:59 gaea_operator/config/resnet/resnet_config.py
+-rw-r--r--  2.0 unx    11390 b- defN 24-Apr-02 08:59 gaea_operator/config/resnet/template/modify_train_parameter.py
+-rw-r--r--  2.0 unx     2315 b- defN 24-Apr-02 08:59 gaea_operator/config/resnet/template/parameter.yaml
+-rw-r--r--  2.0 unx      260 b- defN 24-Apr-02 08:59 gaea_operator/dataset/__init__.py
+-rw-r--r--  2.0 unx     3969 b- defN 24-Apr-02 08:59 gaea_operator/dataset/cityscape_dataset.py
+-rw-r--r--  2.0 unx     5061 b- defN 24-Apr-02 08:59 gaea_operator/dataset/coco_dataset.py
+-rw-r--r--  2.0 unx     4855 b- defN 24-Apr-02 08:59 gaea_operator/dataset/dataset.py
+-rw-r--r--  2.0 unx     3912 b- defN 24-Apr-02 08:59 gaea_operator/dataset/imagenet_dataset.py
+-rw-r--r--  2.0 unx      584 b- defN 24-Apr-02 08:59 gaea_operator/metric/__init__.py
+-rw-r--r--  2.0 unx     5613 b- defN 24-Apr-02 08:59 gaea_operator/metric/metric.py
+-rw-r--r--  2.0 unx      433 b- defN 24-Apr-02 08:59 gaea_operator/metric/analysis/__init__.py
+-rw-r--r--  2.0 unx    14048 b- defN 24-Apr-02 08:59 gaea_operator/metric/analysis/eval_metric_analysis.py
+-rw-r--r--  2.0 unx     5987 b- defN 24-Apr-02 08:59 gaea_operator/metric/analysis/inference_metric_analysis.py
+-rw-r--r--  2.0 unx     8902 b- defN 24-Apr-02 08:59 gaea_operator/metric/analysis/label_statistics_metric_analysis.py
+-rw-r--r--  2.0 unx      737 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/__init__.py
+-rw-r--r--  2.0 unx     3195 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/check.py
+-rw-r--r--  2.0 unx     1923 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/metric.py
+-rw-r--r--  2.0 unx      701 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/image/__init__.py
+-rw-r--r--  2.0 unx     6960 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/image/accuracy.py
+-rw-r--r--  2.0 unx     2571 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/image/average_precision.py
+-rw-r--r--  2.0 unx     3939 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/image/confusion_matrix.py
+-rw-r--r--  2.0 unx    21177 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/image/mean_ap.py
+-rw-r--r--  2.0 unx     6128 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/image/mean_iou.py
+-rw-r--r--  2.0 unx    19509 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/image/precision_recall_curve.py
+-rw-r--r--  2.0 unx    11271 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/image/precision_recall_f1score.py
+-rw-r--r--  2.0 unx      289 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/tabular/__init__.py
+-rw-r--r--  2.0 unx     1207 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/tabular/count_statistic.py
+-rw-r--r--  2.0 unx     2185 b- defN 24-Apr-02 08:59 gaea_operator/metric/operator/tabular/histogram_statistic.py
+-rw-r--r--  2.0 unx     4864 b- defN 24-Apr-02 08:59 gaea_operator/metric/schema/object_detection.yaml
+-rw-r--r--  2.0 unx      131 b- defN 24-Apr-02 08:59 gaea_operator/metric/types/__init__.py
+-rw-r--r--  2.0 unx     1173 b- defN 24-Apr-02 08:59 gaea_operator/metric/types/image_classification_metric.py
+-rw-r--r--  2.0 unx     4874 b- defN 24-Apr-02 08:59 gaea_operator/metric/types/metric.py
+-rw-r--r--  2.0 unx     2362 b- defN 24-Apr-02 08:59 gaea_operator/metric/types/object_detection_metric.py
+-rw-r--r--  2.0 unx     1244 b- defN 24-Apr-02 08:59 gaea_operator/metric/types/semantic_segmentation_metric.py
+-rw-r--r--  2.0 unx      214 b- defN 24-Apr-02 08:59 gaea_operator/model/__init__.py
+-rw-r--r--  2.0 unx     1436 b- defN 24-Apr-02 08:59 gaea_operator/model/model.py
+-rw-r--r--  2.0 unx      181 b- defN 24-Apr-02 08:59 gaea_operator/trainer/__init__.py
+-rw-r--r--  2.0 unx     4330 b- defN 24-Apr-02 08:59 gaea_operator/trainer/trainer.py
+-rw-r--r--  2.0 unx      187 b- defN 24-Apr-02 08:59 gaea_operator/transform/__init__.py
+-rw-r--r--  2.0 unx     3307 b- defN 24-Apr-02 08:59 gaea_operator/transform/cvt_copy_model.py
+-rw-r--r--  2.0 unx      776 b- defN 24-Apr-02 08:59 gaea_operator/transform/transform.py
+-rw-r--r--  2.0 unx     1788 b- defN 24-Apr-02 08:59 gaea_operator/utils/__init__.py
+-rw-r--r--  2.0 unx     5053 b- defN 24-Apr-02 08:59 gaea_operator/utils/accelerator.py
+-rw-r--r--  2.0 unx     2717 b- defN 24-Apr-02 08:59 gaea_operator/utils/compress.py
+-rw-r--r--  2.0 unx      426 b- defN 24-Apr-02 08:59 gaea_operator/utils/consts.py
+-rw-r--r--  2.0 unx     1948 b- defN 24-Apr-02 08:59 gaea_operator/utils/file.py
+-rw-r--r--  2.0 unx     1533 b- defN 24-Apr-02 08:59 gaea_operator/utils/import_module.py
+-rw-r--r--  2.0 unx     4919 b- defN 24-Apr-02 08:59 gaea_operator/utils/model_template.py
+-rw-r--r--  2.0 unx     1680 b- defN 24-Apr-02 08:59 gaea_operator/utils/registry.py
+-rw-r--r--  2.0 unx     1000 b- defN 24-Apr-02 08:59 gaea_operator/utils/tensor.py
+-rw-r--r--  2.0 unx      301 b- defN 24-Apr-02 08:59 gaea_operator/utils/time.py
+-rw-r--r--  2.0 unx    11390 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.data/data/classify.config/modify_train_parameter.py
+-rw-r--r--  2.0 unx     2315 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.data/data/classify.config/parameter.yaml
+-rw-r--r--  2.0 unx      130 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/__init__.py
+-rw-r--r--  2.0 unx    12621 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/modify_train_parameter.py
+-rw-r--r--  2.0 unx     4582 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/parameter.yaml
+-rw-r--r--  2.0 unx     4666 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/parameter_c.yaml
+-rw-r--r--  2.0 unx     4864 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.data/data/schema/object_detection.yaml
+-rw-r--r--  2.0 unx     2091 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx       14 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     9314 b- defN 24-Apr-02 08:59 gaea_operator-1.2.0.4.dist-info/RECORD
+92 files, 397087 bytes uncompressed, 111367 bytes compressed:  72.0%
```

## zipnote {}

```diff
@@ -237,41 +237,41 @@
 
 Filename: gaea_operator/utils/tensor.py
 Comment: 
 
 Filename: gaea_operator/utils/time.py
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.data/data/classify.config/modify_train_parameter.py
+Filename: gaea_operator-1.2.0.4.data/data/classify.config/modify_train_parameter.py
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.data/data/classify.config/parameter.yaml
+Filename: gaea_operator-1.2.0.4.data/data/classify.config/parameter.yaml
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/__init__.py
+Filename: gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/__init__.py
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/modify_train_parameter.py
+Filename: gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/modify_train_parameter.py
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/parameter.yaml
+Filename: gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/parameter.yaml
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/parameter_c.yaml
+Filename: gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/parameter_c.yaml
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.data/data/schema/object_detection.yaml
+Filename: gaea_operator-1.2.0.4.data/data/schema/object_detection.yaml
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.dist-info/METADATA
+Filename: gaea_operator-1.2.0.4.dist-info/METADATA
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.dist-info/WHEEL
+Filename: gaea_operator-1.2.0.4.dist-info/WHEEL
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.dist-info/top_level.txt
+Filename: gaea_operator-1.2.0.4.dist-info/top_level.txt
 Comment: 
 
-Filename: gaea_operator-1.2.0.3.dist-info/RECORD
+Filename: gaea_operator-1.2.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## gaea_operator/components/eval/ppyoloe_plus.py

```diff
@@ -6,15 +6,15 @@
 # @File    : eval_component.py
 """
 import json
 import os
 from argparse import ArgumentParser
 
 from gaea_tracker import ExperimentTracker
-from logit.base_logger import setup_logger
+from bcelogger.base_logger import setup_logger
 from windmilltrainingv1.client.training_api_job import parse_job_name
 from windmillclient.client.windmill_client import WindmillClient
 
 from gaea_operator.dataset import CocoDataset
 from gaea_operator.trainer import Trainer
 from gaea_operator.metric import update_metric_file
 from gaea_operator.utils import read_file
@@ -30,14 +30,17 @@
     parser.add_argument("--windmill-sk", type=str, default=os.environ.get("WINDMILL_SK"))
     parser.add_argument("--windmill-endpoint", type=str, default=os.environ.get("WINDMILL_ENDPOINT"))
     parser.add_argument("--project-name", type=str, default=os.environ.get("PROJECT_NAME"))
     parser.add_argument("--tracking-uri", type=str, default=os.environ.get("TRACKING_URI"))
     parser.add_argument("--experiment-name", type=str, default=os.environ.get("EXPERIMENT_NAME"))
     parser.add_argument("--experiment-kind", type=str, default=os.environ.get("EXPERIMENT_KIND"))
     parser.add_argument("--dataset-name", type=str, default=os.environ.get("DATASET_NAME"))
+    parser.add_argument("--advanced-parameters",
+                        type=str,
+                        default=os.environ.get("ADVANCED_PARAMETERS", "{}"))
 
     parser.add_argument("--input-model-uri", type=str, default=os.environ.get("INPUT_MODEL_URI"))
     parser.add_argument("--output-dataset-uri", type=str, default=os.environ.get("OUTPUT_DATASET_URI"))
     parser.add_argument("--output-uri", type=str, default=os.environ.get("OUTPUT_URI"))
 
     args, _ = parser.parse_known_args()
```

## gaea_operator/components/eval/resnet.py

```diff
@@ -5,15 +5,15 @@
 # @Author  : yanxiaodong
 # @File    : eval_component.py
 """
 import os
 from argparse import ArgumentParser
 
 from gaea_tracker import ExperimentTracker
-from logit.base_logger import setup_logger
+from bcelogger.base_logger import setup_logger
 from windmilltrainingv1.client.training_api_job import parse_job_name
 from windmillclient.client.windmill_client import WindmillClient
 
 from gaea_operator.dataset import ImageNetDataset
 from gaea_operator.trainer import Trainer
 from gaea_operator.metric import update_metric_file
 from gaea_operator.utils import read_file
```

## gaea_operator/components/inference/inference.py

```diff
@@ -5,16 +5,16 @@
 # @Author  : yanxiaodong
 # @File    : inference.py
 """
 import os
 from argparse import ArgumentParser
 
 from gaea_tracker import ExperimentTracker
-from logit.base_logger import setup_logger
-import logit
+from bcelogger.base_logger import setup_logger
+import bcelogger
 from windmilltrainingv1.client.training_api_dataset import parse_dataset_name
 from windmillclient.client.windmill_client import WindmillClient
 from tritonv2.evaluator import evaluate
 
 from gaea_operator.metric import InferenceMetricAnalysis, Metric
 from gaea_operator.utils import get_accelerator, read_file
 
@@ -56,15 +56,15 @@
                                        project_name=args.project_name)
     setup_logger(config=dict(file_name=os.path.join(args.output_uri, "worker.log")))
     response = read_file(input_dir=args.input_model_uri)
 
     ensemble_artifact_name = response["artifact"]["name"]
     output_model_uri = "/home/windmill/tmp/model"
     # 1. 下载ensemble template 模型
-    logit.info(f"Downloading ensemble from {ensemble_artifact_name}")
+    bcelogger.info(f"Downloading ensemble from {ensemble_artifact_name}")
     windmill_client.dump_models(artifact_name=ensemble_artifact_name,
                                 location_style="Triton",
                                 rename="ensemble",
                                 output_uri=output_model_uri)
 
     # 2.评估数据集
     response = read_file(input_dir=args.input_dataset_uri)
```

## gaea_operator/components/package/__init__.py

```diff
@@ -55,15 +55,16 @@
                    "ACCELERATOR": "{{accelerator}}",
                    "ENSEMBLE_MODEL_NAME": "{{ensemble_model_name}}",
                    "ENSEMBLE_MODEL_DISPLAY_NAME": "{{ensemble_model_display_name}}",
                    "SUB_MODEL_NAMES": "{{sub_model_names}}",
                    "SCENE": "{{scene}}"}
 
     package = ContainerStep(name="package",
-                            docker_env="iregistry.baidu-int.com/windmill-public/train/paddlepaddle-2.5.2-gpu-cuda12.0-cudnn8.9-trt8.6:v1.2.0",
+                            docker_env="iregistry.baidu-int.com/windmill-public/inference/"
+                                       "triton_r22_12_nvidia_infer_trt86:v1.2.0",
                             env=package_env,
                             parameters=package_params,
                             inputs={"input_model_uri": transform_step.outputs["output_model_uri"]},
                             outputs={"output_model_uri": Artifact(), "output_uri": Artifact()},
                             command=f'python3 -m gaea_operator.components.package.package '
                                     f'--algorithm={algorithm} '
                                     f'--input-model-uri={{{{input_model_uri}}}} '
```

## gaea_operator/components/package/package.py

```diff
@@ -6,16 +6,16 @@
 # @File    : transform_component.py
 """
 import json
 import os
 from argparse import ArgumentParser
 
 from gaea_tracker import ExperimentTracker
-from logit.base_logger import setup_logger
-import logit
+from bcelogger.base_logger import setup_logger
+import bcelogger
 from windmillmodelv1.client.model_api_model import parse_model_name
 from windmillartifactv1.client.artifact_api_artifact import get_name
 from windmillclient.client.windmill_client import WindmillClient
 
 from gaea_operator.config import Config
 from gaea_operator.model import format_name
 from gaea_operator.utils import read_file, \
@@ -73,29 +73,29 @@
                                        project_name=args.project_name)
     setup_logger(config=dict(file_name=os.path.join(args.output_uri, "worker.log")))
 
     response = read_file(input_dir=args.input_model_uri)
     metadata = response["artifact"]["metadata"]
 
     model_template = get_model_template(name=args.algorithm, model_store_name=args.public_model_store)
-    ppyoloe_plus_model_to_res = {model_template.suggest_template_model(): response}
+    ppyoloe_plus_model_to_res = {parse_model_name(model_template.suggest_template_model()).local_name: response}
 
     # 1. 下载ensemble template 模型
     ensemble_artifact_name = get_name(object_name=model_template.suggest_template_ensemble(), version="latest")
-    logit.info(f"Dumping model {ensemble_artifact_name} to {args.output_model_uri}")
+    bcelogger.info(f"Dumping model {ensemble_artifact_name} to {args.output_model_uri}")
     windmill_client.dump_models(artifact_name=ensemble_artifact_name,
                                 location_style="Triton",
                                 rename="ensemble",
                                 output_uri=args.output_model_uri)
 
     # 2. 生成打包配置文件
     modify_model_names = {
-        'preprocess': [model_template.suggest_template_preprocess()],
-        'postprocess': [model_template.suggest_template_postprocess()],
-        'ensemble': [model_template.suggest_template_ensemble()]
+        'preprocess': [parse_model_name(model_template.suggest_template_preprocess()).local_name],
+        'postprocess': [parse_model_name(model_template.suggest_template_postprocess()).local_name],
+        'ensemble': [parse_model_name(model_template.suggest_template_ensemble()).local_name]
     }
 
     model_name = parse_model_name(name=args.ensemble_model_name)
     workspace_id = model_name.workspace_id
     model_store_name = model_name.model_store_name
     ensemble_local_name = model_name.local_name
 
@@ -104,57 +104,61 @@
                                        ensemble_model_uri=args.output_model_uri,
                                        modify_model_names=modify_model_names,
                                        ensemble_local_name=ensemble_local_name)
 
     transform_local_name = response["localName"]
     transform_model_display_name = response["displayName"]
     # 3. 上传 Preprocess 模型
-    pre_model_uri = os.path.join(args.output_model_uri, model_template.suggest_template_preprocess())
+    pre_model_uri = os.path.join(args.output_model_uri,
+                                 parse_model_name(model_template.suggest_template_preprocess()).local_name)
     local_name = format_name(transform_local_name, "pre")
     display_name = format_name(transform_model_display_name, "预处理")
     response = windmill_client.create_model(workspace_id=workspace_id,
                                             model_store_name=model_store_name,
                                             local_name=local_name,
                                             display_name=display_name,
                                             category="Image/Preprocess",
                                             model_formats=["Python"],
                                             artifact_uri=find_dir(pre_model_uri))
-    logit.info(f"Model {local_name} created response: {response}")
-    ppyoloe_plus_model_to_res.update({model_template.suggest_template_preprocess(): json.loads(response.raw_data)})
+    bcelogger.info(f"Model {local_name} created response: {response}")
+    ppyoloe_plus_model_to_res.update(
+        {parse_model_name(model_template.suggest_template_preprocess()).local_name: json.loads(response.raw_data)})
 
     # 4. 上传 PostProcess 模型
-    post_model_uri = os.path.join(args.output_model_uri, model_template.suggest_template_postprocess())
+    post_model_uri = os.path.join(args.output_model_uri,
+                                  parse_model_name(model_template.suggest_template_postprocess()).local_name)
     local_name = format_name(transform_local_name, "post")
     display_name = format_name(transform_model_display_name, "后处理")
     response = windmill_client.create_model(workspace_id=workspace_id,
                                             model_store_name=model_store_name,
                                             local_name=local_name,
                                             display_name=display_name,
                                             category="Image/Postprocess",
                                             model_formats=["Python"],
                                             artifact_uri=find_dir(post_model_uri))
-    logit.info(f"Model {local_name} created response: {response}")
-    ppyoloe_plus_model_to_res.update({model_template.suggest_template_postprocess(): json.loads(response.raw_data)})
+    bcelogger.info(f"Model {local_name} created response: {response}")
+    ppyoloe_plus_model_to_res.update({
+        parse_model_name(model_template.suggest_template_postprocess()).local_name: json.loads(response.raw_data)})
 
     # 5. 修改 ensemble 配置文件
-    ensemble_model_uri = os.path.join(args.output_model_uri, model_template.suggest_template_ensemble())
+    ensemble_model_uri = os.path.join(args.output_model_uri, "ensemble")
     config.write_ensemble_config(ensemble_model_uri=ensemble_model_uri, model_config=ppyoloe_plus_model_to_res)
 
     # 6. 上传 ensemble 模型
     accelerator = get_accelerator(name=args.accelerator)
     response = windmill_client.create_model(
         workspace_id=workspace_id,
         model_store_name=model_store_name,
         local_name=ensemble_local_name,
         display_name=args.ensemble_model_display_name,
         prefer_model_server_parameters=accelerator.suggest_model_server_parameters(),
         category="Image/Ensemble",
         model_formats=["Python"],
         artifact_uri=find_dir(ensemble_model_uri))
-    logit.info(f"Model {ensemble_local_name} created response: {response}")
+    bcelogger.info(f"Model {ensemble_local_name} created response: {response}")
 
     # 4. 输出文件
     write_file(obj=json.loads(response.raw_data), output_dir=args.output_model_uri)
 
 
 if __name__ == "__main__":
     args = parse_args()
```

## gaea_operator/components/train/ppyoloe_plus.py

```diff
@@ -6,20 +6,20 @@
 # @File    : train_component.py
 """
 import os
 import json
 from argparse import ArgumentParser
 
 from gaea_tracker import ExperimentTracker
-from logit.base_logger import setup_logger
+from bcelogger.base_logger import setup_logger
 from windmillmodelv1.client.model_api_model import parse_model_name
 from windmillmodelv1.client.model_api_model import ModelName
 from windmillmodelv1.client.model_api_modelstore import parse_modelstore_name
 from windmillclient.client.windmill_client import WindmillClient
-import logit
+import bcelogger
 
 from gaea_operator.dataset import CocoDataset
 from gaea_operator.config import PPYOLOEPLUSMConfig
 from gaea_operator.metric.types.metric import LOSS_METRIC_NAME, \
     MAP_METRIC_NAME, \
     AP50_METRIC_NAME, \
     AR_METRIC_NAME, \
@@ -128,21 +128,21 @@
                                         metric_name=metric_name)
     best_score, version = Model(windmill_client=windmill_client). \
         get_best_model_score(model_name=args.model_name, metric_name=metric_name)
     tags = {metric_name: str(current_score)}
     alias = None
     if current_score >= best_score and version is not None:
         alias = ["best"]
-        logit.info(
+        bcelogger.info(
             f"{metric_name.capitalize()} current score {current_score} >= {best_score}, update [best]")
         tags.update(
             {"bestReason": f"current.score({current_score}) greater than {version}.score({best_score})"})
     if version is None:
         alias = ["best"]
-        logit.info(f"First alias [best] score: {current_score}")
+        bcelogger.info(f"First alias [best] score: {current_score}")
         tags.update({"bestReason": f"current.score({current_score})"})
 
     model_name = parse_model_name(args.model_name)
     workspace_id = model_name.workspace_id
     model_store_name = model_name.model_store_name
     local_name = model_name.local_name
     response = windmill_client.create_model(workspace_id=workspace_id,
@@ -151,15 +151,15 @@
                                             display_name=args.model_display_name,
                                             category="Image/ObjectDetection",
                                             model_formats=["PaddlePaddle"],
                                             artifact_alias=alias,
                                             artifact_tags=tags,
                                             artifact_metadata=config.metadata,
                                             artifact_uri=args.output_model_uri)
-    logit.info(f"Model {args.model_name} created response: {response}")
+    bcelogger.info(f"Model {args.model_name} created response: {response}")
 
     # 7. 输出文件
     write_file(obj=json.loads(response.raw_data), output_dir=args.output_model_uri)
 
 
 if __name__ == "__main__":
     args = parse_args()
```

## gaea_operator/components/train/resnet.py

```diff
@@ -6,20 +6,20 @@
 # @File    : classify.py
 """
 import os
 import json
 from argparse import ArgumentParser
 
 from gaea_tracker import ExperimentTracker
-from logit.base_logger import setup_logger
+from bcelogger.base_logger import setup_logger
 from windmillmodelv1.client.model_api_model import parse_model_name
 from windmillmodelv1.client.model_api_model import ModelName
 from windmillmodelv1.client.model_api_modelstore import parse_modelstore_name
 from windmillclient.client.windmill_client import WindmillClient
-import logit
+import bcelogger
 
 from gaea_operator.dataset import ImageNetDataset
 from gaea_operator.config import ResNetConfig
 from gaea_operator.metric.types.metric import LOSS_METRIC_NAME, \
     ACCURACY_METRIC_NAME, \
     CLASSIFICATION_ACCURACY_METRIC_NAME
 from gaea_operator.trainer import Trainer
@@ -111,21 +111,21 @@
                                         metric_name=metric_name)
     best_score, version = Model(windmill_client=windmill_client).get_best_model_score(
         model_name=args.model_name, metric_name=metric_name)
     tags = {metric_name: str(current_score)}
     alias = None
     if current_score >= best_score and version is not None:
         alias = ["best"]
-        logit.info(
+        bcelogger.info(
             f"{metric_name.capitalize()} current score {current_score} >= {best_score}, update [best]")
         tags.update(
             {f"bestReason": f"current.score({current_score}) greater than {version}.score({best_score})"})
     if version is None:
         alias = ["best"]
-        logit.info(f"First alias [best] score: {current_score}")
+        bcelogger.info(f"First alias [best] score: {current_score}")
         tags.update({f"bestReason": "current.score({current_score})"})
 
     model_name = parse_model_name(args.model_name)
     workspace_id = model_name.workspace_id
     model_store_name = model_name.model_store_name
     local_name = model_name.local_name
     response = windmill_client.create_model(workspace_id=workspace_id,
@@ -135,15 +135,15 @@
                                             category="Image/ImageClassification/MultiClass",
                                             model_formats=["PaddlePaddle"],
                                             artifact_alias=alias,
                                             artifact_tags=tags,
                                             artifact_metadata=config.metadata,
                                             artifact_uri=args.output_model_uri)
 
-    logit.info(f"Model {args.model_name} created response: {response}")
+    bcelogger.info(f"Model {args.model_name} created response: {response}")
 
     # 7. 输出文件
     write_file(obj=json.loads(response.raw_data), output_dir=args.output_model_uri)
 
 
 if __name__ == "__main__":
     args = parse_args()
```

## gaea_operator/components/transform/ppyoloe_plus.py

```diff
@@ -5,17 +5,17 @@
 # @Author  : yanxiaodong
 # @File    : transform_component.py
 """
 import os
 import json
 from argparse import ArgumentParser
 
-import logit
+import bcelogger
 from gaea_tracker import ExperimentTracker
-from logit.base_logger import setup_logger
+from bcelogger.base_logger import setup_logger
 from windmillmodelv1.client.model_api_model import parse_model_name
 from windmillclient.client.windmill_client import WindmillClient
 from gaea_operator.transform import Transform
 
 from gaea_operator.config import Config
 from gaea_operator.utils import write_file, read_file, get_model_template, ModelTemplate
 from gaea_operator.config.generate_transform_config import KEY_ACCELERATOR
@@ -105,15 +105,15 @@
                                             model_store_name=model_store_name,
                                             local_name=local_name,
                                             display_name=args.transform_model_display_name,
                                             category="Image/ObjectDetection",
                                             artifact_metadata=config.metadata,
                                             model_formats=[
                                                 Config.device_type2model_format[args.accelerator]])
-    logit.info(f"Model {args.transform_model_name} created response: {response}")
+    bcelogger.info(f"Model {args.transform_model_name} created response: {response}")
 
     # 4. 输出文件
     write_file(obj=json.loads(response.raw_data), output_dir=args.output_model_uri)
 
 
 if __name__ == "__main__":
     args = parse_args()
```

## gaea_operator/components/transform/resnet.py

```diff
@@ -5,17 +5,17 @@
 # @Author  : yanxiaodong
 # @File    : transform_component.py
 """
 import os
 import json
 from argparse import ArgumentParser
 
-import logit
+import bcelogger
 from gaea_tracker import ExperimentTracker
-from logit.base_logger import setup_logger
+from bcelogger.base_logger import setup_logger
 from windmillmodelv1.client.model_api_model import parse_model_name
 from windmillclient.client.windmill_client import WindmillClient
 from gaea_operator.transform import Transform
 
 from gaea_operator.config import Config
 from gaea_operator.utils import write_file, read_file, get_model_template, ModelTemplate
 from gaea_operator.config.generate_transform_config import KEY_ACCELERATOR
@@ -107,15 +107,15 @@
                                             model_store_name=model_store_name,
                                             local_name=local_name,
                                             display_name=args.transform_model_display_name,
                                             category="Image/ImageClassification/MultiClass",
                                             artifact_metadata=config.metadata,
                                             model_formats=[
                                                 Config.device_type2model_format[args.accelerator]])
-    logit.info(f"Model {args.transform_model_name} created response: {response}")
+    bcelogger.info(f"Model {args.transform_model_name} created response: {response}")
 
     # 6. 输出文件
     write_file(obj=json.loads(response.raw_data), output_dir=args.output_model_uri)
 
 
 if __name__ == "__main__":
     args = parse_args()
```

## gaea_operator/components/transform_eval/__init__.py

```diff
@@ -23,34 +23,37 @@
                         tracking_uri: str = "",
                         project_name: str = "",
                         accelerator: str = ""):
     """
     Transform eval step
     """
     transform_eval_params = {"flavour": "c4m16gpu1",
-                             "queue": "qv100",
+                             "queue": "qtrain",
                              "windmill_ak": windmill_ak,
                              "windmill_sk": windmill_sk,
                              "windmill_endpoint": windmill_endpoint,
                              "experiment_name": experiment_name,
                              "experiment_kind": experiment_kind,
                              "tracking_uri": tracking_uri,
                              "project_name": project_name,
                              "model_store_name": modelstore_name,
-                             "accelerator": accelerator}
+                             "accelerator": accelerator,
+                             "advanced_parameters": '{"conf_threshold":"0.1",'
+                                                    '"iou_threshold":"0.50"}'}
     transform_eval_env = {"PF_JOB_FLAVOUR": "{{flavour}}",
                           "PF_JOB_QUEUE_NAME": "{{queue}}",
                           "WINDMILL_AK": "{{windmill_ak}}",
                           "WINDMILL_SK": "{{windmill_sk}}",
                           "WINDMILL_ENDPOINT": "{{windmill_endpoint}}",
                           "EXPERIMENT_KIND": "{{experiment_kind}}",
                           "EXPERIMENT_NAME": "{{experiment_name}}",
                           "TRACKING_URI": "{{tracking_uri}}",
                           "PROJECT_NAME": "{{project_name}}",
-                          "ACCELERATOR": "{{accelerator}}"}
+                          "ACCELERATOR": "{{accelerator}}",
+                          "ADVANCED_PARAMETERS": "{{advanced_parameters}}"}
     accelerator = get_accelerator(name=accelerator)
     transform_eval_env.update(accelerator.suggest_env())
 
     transform_eval = ContainerStep(name="transform-eval",
                                    docker_env=accelerator.suggest_image(),
                                    env=transform_eval_env,
                                    parameters=transform_eval_params,
@@ -59,8 +62,9 @@
                                    outputs={"output_uri": Artifact()},
                                    command=f'python3 -m gaea_operator.components.transform_eval.transform_eval '
                                            f'--algorithm={algorithm} '
                                            f'--input-model-uri={{{{input_model_uri}}}} '
                                            f'--input-dataset-uri={{{{input_dataset_uri}}}} '
                                            f'--output-uri={{{{output_uri}}}}')
 
+
     return transform_eval
```

## gaea_operator/components/transform_eval/transform_eval.py

```diff
@@ -7,16 +7,16 @@
 """
 import os
 import shutil
 from argparse import ArgumentParser
 from typing import Dict
 
 from gaea_tracker import ExperimentTracker
-from logit.base_logger import setup_logger
-import logit
+from bcelogger.base_logger import setup_logger
+import bcelogger
 from windmilltrainingv1.client.training_api_dataset import parse_dataset_name
 from windmillmodelv1.client.model_api_model import parse_model_name
 from windmillartifactv1.client.artifact_api_artifact import get_name
 from windmillclient.client.windmill_client import WindmillClient
 from tritonv2.evaluator import evaluate
 
 from gaea_operator.config import Config
@@ -42,14 +42,17 @@
                         type=str,
                         default=os.environ.get("PUBLIC_MODEL_STORE", "workspaces/public/modelstores/public"))
     parser.add_argument("--tracking-uri", type=str, default=os.environ.get("TRACKING_URI"))
     parser.add_argument("--experiment-name", type=str, default=os.environ.get("EXPERIMENT_NAME"))
     parser.add_argument("--experiment-kind", type=str, default=os.environ.get("EXPERIMENT_KIND"))
     parser.add_argument("--accelerator", type=str, default=os.environ.get("ACCELERATOR", "t4"))
     parser.add_argument("--algorithm", type=str, default=os.environ.get("ALGORITHM", ""))
+    parser.add_argument("--advanced-parameters",
+                        type=str,
+                        default=os.environ.get("ADVANCED_PARAMETERS", "{}"))
 
     parser.add_argument("--input-model-uri", type=str, default=os.environ.get("INPUT_MODEL_URI"))
     parser.add_argument("--input-dataset-uri", type=str, default=os.environ.get("INPUT_DATASET_URI"))
     parser.add_argument("--output-uri", type=str, default=os.environ.get("OUTPUT_URI"))
 
     args, _ = parser.parse_known_args()
 
@@ -114,15 +117,15 @@
     response = read_file(input_dir=args.input_model_uri)
     metadata = response["artifact"]["metadata"]
 
     output_model_uri = "/home/windmill/tmp/model"
     # 1. 下载ensemble template 模型
     model_template = get_model_template(name=args.algorithm, model_store_name=args.public_model_store)
     ensemble_artifact_name = get_name(object_name=model_template.suggest_template_ensemble(), version="latest")
-    logit.info(f"Dumping model {ensemble_artifact_name} to {output_model_uri}")
+    bcelogger.info(f"Dumping model {ensemble_artifact_name} to {output_model_uri}")
     windmill_client.dump_models(artifact_name=ensemble_artifact_name,
                                 location_style="Triton",
                                 rename="ensemble",
                                 output_uri=output_model_uri)
 
     # 2. 基于模板文件组装转换后模型包
     package_model_by_template(windmill_client=windmill_client,
```

## gaea_operator/config/generate_transform_config.py

```diff
@@ -6,15 +6,15 @@
 """
 模型转换配置文件生成
 
 Authors: zhouwenlong(zhouwenlong01@baidu.com)
 Date:    2024/2/26 10:40
 """
 import yaml
-import logit
+import bcelogger
 import os
 from typing import Dict
 
 FRAMEWORK_LIST = ["onnx", "paddle"]
 ACCELERATOR_LIST = ["t4", "v100", "a100", "r200"]
 MODEL_NAME_DICT = {"ppyoloe": ["image", "scale_factor"],
                    "change-ppyoloe": ["image", "tmp_image", "scale_factor"],
@@ -80,28 +80,28 @@
     """
     generate input shape 
     """
     batch = -1 if max_batch_size > 1 else 1
     input_shape = {}
     if '_' in model_name:
         model_name = model_name[: -2]
-        logit.info('modify model name -> {}'.format(model_name))
+        bcelogger.info('modify model name -> {}'.format(model_name))
 
     if model_name not in MODEL_NAME_DICT.keys():
         raise ValueError("model name not in model list")
         return
     if "ppyoloe" in model_name:
         input_shape = {"image": [batch, 3, height, width], "scale_factor": [batch, 2]}
         if "change" in model_name:
             input_shape["tmp_image"] = [batch, 3, height, width]
     elif "vit-base" in model_name or "resnet" in model_name:
         input_shape["x"] = [batch, 3, height, width]
     elif "maskformer" in model_name or \
             "ocrnet" in model_name:
-        logit.warn("maskformer and ocrnet not support batchsize > 1")
+        bcelogger.warn("maskformer and ocrnet not support batchsize > 1")
         batch = 1
         input_shape['x'] = [batch, 3, height, width]
         if "change" in model_name:
             input_shape['x'] = [batch, 6, height, width]
     else:
         raise ValueError("model name not in model list")
         return
@@ -158,21 +158,21 @@
     onnx_op_flag = False
 
     if max_boxes is not None and conf_thres is not None and iou_thres is not None:
         nms_params = {"max_boxes": max_boxes,
                       "conf_thres": conf_thres,
                       "iou_thres": iou_thres,
                       "drop_nms": drop_nms}
-        logit.info("nms_params: {}".format(nms_params))
+        bcelogger.info("nms_params: {}".format(nms_params))
 
     # add OnnxToOnnx
     OnnxToOnnxParam = {}
     if mean is not None or std is not None or rename_list is not None or \
             transpose is not False or norm is not False or nms_params is not None:
-        logit.info('use onnx2onnx mode.')
+        bcelogger.info('use onnx2onnx mode.')
         onnx_op_flag = True
         OnnxToOnnxParam['type'] = 'OnnxToOnnx'
         OnnxToOnnxParam['input_shape'] = input_shape.copy()
         if 'scale_factor' in OnnxToOnnxParam['input_shape']:
             OnnxToOnnxParam['input_shape'].pop('scale_factor')
         if mean is not None and std is not None:
             OnnxToOnnxParam["mean"] = mean
@@ -180,30 +180,30 @@
         if rename_list is not None:
             OnnxToOnnxParam["rename_list"] = rename_list
         OnnxToOnnxParam["transpose"] = transpose
         OnnxToOnnxParam["norm"] = norm
         if nms_params is not None:
             OnnxToOnnxParam["nms_params"] = nms_params
 
-        logit.info("OnnxToOnnxParam = {}".format(OnnxToOnnxParam))
+        bcelogger.info("OnnxToOnnxParam = {}".format(OnnxToOnnxParam))
 
     # check args
     if source_framework not in FRAMEWORK_LIST:
         raise ValueError("{} is not supported".format(source_framework))
     if accelerator not in ACCELERATOR_LIST:
         raise ValueError("{} is not supported".format(accelerator))
 
     # convert_op_list = []
     # kunlun 
     if accelerator == "r200":
         OnnxToKunlunParam = {}
         if source_framework == "paddle":
             if onnx_op_flag and len(OnnxToOnnxParam) != 0 and nms_params is not None:
                 # raise ValueError("nms_params is not supported for paddle to kunlun")
-                logit.warning("ingore nms_params:[max_boxes, conf_thres ,iou_thres, drop_nms] for paddle to kunlun")
+                bcelogger.warning("ingore nms_params:[max_boxes, conf_thres ,iou_thres, drop_nms] for paddle to kunlun")
                 OnnxToOnnxParam["nms_params"] = None
 
             if onnx_op_flag and len(OnnxToOnnxParam) != 0:
                 PaddleToOnnxParam = {}
                 PaddleToOnnxParam["type"] = "PaddleToOnnx"
                 PaddleToOnnxParam["input_shape"] = input_shape
                 pipeline_cfg.append(PaddleToOnnxParam)
@@ -271,16 +271,19 @@
                 shape = str(b) + shape[1:]
                 start_param += name
                 start_param += ":"
                 start_param += shape
                 start_param += ","
             start_param = start_param[:-1]
             cmd += start_param + " "
-        cmd += f" --workspace=2048 --{advanced_parameters[KEY_PRECISION]}"
-        logit.info(f"OnnxToTensorrt cmd = {cmd}")
+        if min_batch == max_batch:
+            cmd = f" --workspace=2048 --{advanced_parameters[KEY_PRECISION]}"
+        else:
+            cmd += f" --workspace=2048 --{advanced_parameters[KEY_PRECISION]}"
+        bcelogger.info(f"OnnxToTensorrt cmd = {cmd}")
 
         OnnxToTensorrtParam["cmd"] = cmd
 
         pipeline_cfg.append(OnnxToTensorrtParam)
     # bitmain
     else:
         raise NotImplementedError("{} is not supported".format(accelerator))
@@ -342,15 +345,15 @@
     """
     model_name = advanced_parameters[KEY_MODEL_TYPE].lower()
     mean, std = get_mean_std(metadata, model_name)
     input_shape = gen_input_shape(model_name, int(advanced_parameters[KEY_EVAL_HEIGHT]), \
                                   int(advanced_parameters[KEY_EVAL_WIDTH]), \
                                   int(advanced_parameters[KEY_MAX_BATCH_SIZE]))
 
-    logit.info(str(input_shape))
+    bcelogger.info(str(input_shape))
     norm, transpose = get_norm_transpose(advanced_parameters[KEY_ACCELERATOR])
 
     create_common_config(advanced_parameters,
                          input_shape=input_shape,
                          mean=mean,
                          std=std,
                          norm=norm,
```

## gaea_operator/config/modify_package_files.py

```diff
@@ -7,15 +7,15 @@
 modify model config.pbtxt/parse.yaml in package step
 Authors: wanggaofei(wanggaofei03@baidu.com)
 Date:    2023-02-29
 """
 
 import yaml
 import os
-import logit
+import bcelogger
 from typing import Dict
 
 from .update_pbtxt import ModelConfig
 from .update_parse import ParseYamlConfig
 from gaea_operator.utils import find_dir
 
 KEY_PREPROCESS = 'preprocess'
@@ -60,15 +60,15 @@
         aim_files = self.retrive_path(find_dir(self.ensemble_model_uri), [KEY_PBTXT_NAME])
         if len(aim_files) > 0:
             ensemble_pbtxt_name = aim_files[0]
             ensemble_config = ModelConfig._create_from_file(ensemble_pbtxt_name)
             ensemble_config.set_ensemble_step_model_name_and_version(self.model_name_pairs, self.model_version_pairs)
             ensemble_config.write_config_to_file(ensemble_pbtxt_name)
         else:
-            logit.error('do NOT find {} in {}'.format(KEY_PBTXT_NAME, self.ensemble_model_uri))
+            bcelogger.error('do NOT find {} in {}'.format(KEY_PBTXT_NAME, self.ensemble_model_uri))
 
     def retrive_path(self, path, exts):
         """
         从指定路径中递归搜索，返回指定扩展名的文件列表
 
         Args:
         path (str): 指定的文件目录路径
@@ -81,15 +81,15 @@
         aim_files = []
         n = 0
         for home, dirs, files in os.walk(path):
             for _, f in enumerate(files):
                 if f in exts and not f.startswith("._"):
                     w_name = os.path.join(home, f)
                     n += 1
-                    logit.info("FIND" + str(n) + ":" + w_name)
+                    bcelogger.info("FIND" + str(n) + ":" + w_name)
                     aim_files.append(w_name)
 
         return aim_files
 
 
 class ModifyPackageFiles(object):
     """
@@ -138,16 +138,14 @@
             list (str) - 类别列表，每个元素为一个字符串，代表一个类别名称。
         """
         categories = []
         yaml_data = metadata
 
         if 'labels' in yaml_data:
             categories = yaml_data['labels']
-            # for n in yaml_data['labels']:
-            #     categories.append(n['name'])
         return categories
 
     def get_yaml(self, yaml_name):
         """
             read parse.yaml
         """
         if not os.path.exists(yaml_name):
@@ -172,15 +170,15 @@
         aim_files = []
         n = 0
         for home, dirs, files in os.walk(path):
             for _, f in enumerate(files):
                 if f in exts and not f.startswith("._"):
                     w_name = os.path.join(home, f)
                     n += 1
-                    logit.info("FIND" + str(n) + ":" + w_name)
+                    bcelogger.info("FIND" + str(n) + ":" + w_name)
                     aim_files.append(w_name)
 
         return aim_files
 
     def modify_ppyoloe(self, output_uri: str = None):
         """
         for ppyoloe
@@ -195,15 +193,15 @@
                     if len(aim_files) > 0:
                         preproc_pbtxt_name = aim_files[0]
                         preproc_config = ModelConfig._create_from_file(preproc_pbtxt_name)
                         preproc_config.set_preproc_width_height(idx=0, width=self.eval_width, \
                                                                 height=self.eval_height, is_nhwc=True)
                         preproc_config.write_config_to_file(preproc_pbtxt_name)
                     else:
-                        logit.error('do NOT find {} in {}'.format(KEY_PBTXT_NAME, m))
+                        bcelogger.error('do NOT find {} in {}'.format(KEY_PBTXT_NAME, m))
             elif model_type == KEY_MODEL:
                 # 2. model node modify max_batch_size/max_box_count
                 for m in models:
                     if output_uri is not None:
                         aim_files = [os.path.join(output_uri, KEY_PBTXT_NAME)]
                     else:
                         aim_files = self.retrive_path(find_dir(
@@ -211,15 +209,15 @@
                     if len(aim_files) > 0:
                         ppyoloe_pbtxt_name = aim_files[0]
                         ppyoloe_config = ModelConfig._create_from_file(ppyoloe_pbtxt_name)
                         ppyoloe_config.set_ppyoloe_output_max_box_count(val=self.max_box_count)
                         ppyoloe_config.set_field(KEY_MAX_BATCH_SIZE, self.max_batch_size)
                         ppyoloe_config.write_config_to_file(ppyoloe_pbtxt_name)
                     else:
-                        logit.error('do NOT find {} in {}'.format(KEY_PBTXT_NAME, m))
+                        bcelogger.error('do NOT find {} in {}'.format(KEY_PBTXT_NAME, m))
 
             elif model_type == KEY_POSTPROCESS:
                 for m in models:
                     # 4. modify parse.yaml
                     aim_files = self.retrive_path(find_dir(
                         os.path.join(self.ensemble_model_uri, m)), [KEY_PARSE_NAME])
                     if len(aim_files) > 0:
@@ -230,17 +228,17 @@
 
                         # 2. set categories
                         cfg.modify_categories(self.categories)
 
                         # 3. save
                         cfg.save_yaml(parse_name)
                     else:
-                        logit.error('do NOT find {} in {}'.format(KEY_PARSE_NAME, m))
+                        bcelogger.error('do NOT find {} in {}'.format(KEY_PARSE_NAME, m))
             else:
-                logit.error('do NOT support model_type: {}'.format(model_type))
+                bcelogger.error('do NOT support model_type: {}'.format(model_type))
 
     def modify_resnet(self, output_uri: str = None):
         """
         for classify
         """
         # 1. modify pbtxt
         for model_type, models in self.modify_model_names.items():
@@ -252,30 +250,30 @@
                     if len(aim_files) > 0:
                         preproc_pbtxt_name = aim_files[0]
                         preproc_config = ModelConfig._create_from_file(preproc_pbtxt_name)
                         preproc_config.set_preproc_width_height(idx=0, width=self.eval_width, \
                                                                 height=self.eval_height, is_nhwc=True)
                         preproc_config.write_config_to_file(preproc_pbtxt_name)
                     else:
-                        logit.error('do NOT find {} in {}'.format(KEY_PBTXT_NAME, m))
+                        bcelogger.error('do NOT find {} in {}'.format(KEY_PBTXT_NAME, m))
             elif model_type == KEY_MODEL:
                 # 2. model node modify max_batch_size
                 for m in models:
                     if output_uri is not None:
                         aim_files = [os.path.join(output_uri, KEY_PBTXT_NAME)]
                     else:
                         aim_files = self.retrive_path(find_dir(
                             os.path.join(self.ensemble_model_uri, m)), [KEY_PBTXT_NAME])
                     if len(aim_files) > 0:
                         classify_pbtxt_name = aim_files[0]
                         classify_config = ModelConfig._create_from_file(classify_pbtxt_name)
                         classify_config.set_field(KEY_MAX_BATCH_SIZE, self.max_batch_size)
                         classify_config.write_config_to_file(classify_pbtxt_name)
                     else:
-                        logit.error('do NOT find {} in {}'.format(KEY_PBTXT_NAME, m))
+                        bcelogger.error('do NOT find {} in {}'.format(KEY_PBTXT_NAME, m))
 
             elif model_type == KEY_POSTPROCESS:
                 for m in models:
                     # 4. modify parse.yaml
                     aim_files = self.retrive_path(find_dir(
                         os.path.join(self.ensemble_model_uri, m)), [KEY_PARSE_NAME])
                     if len(aim_files) > 0:
@@ -286,14 +284,14 @@
 
                         # 2. set categories
                         cfg.modify_categories(self.categories)
 
                         # 3. save
                         cfg.save_yaml(parse_name)
                     else:
-                        logit.error('do NOT find {} in {}'.format(KEY_PARSE_NAME, m))
+                        bcelogger.error('do NOT find {} in {}'.format(KEY_PARSE_NAME, m))
             else:
-                logit.error('do NOT support model_type: {}'.format(model_type))
+                bcelogger.error('do NOT support model_type: {}'.format(model_type))
 
 
 if __name__ == '__main__':
     pass
```

## gaea_operator/config/update_parse.py

```diff
@@ -8,15 +8,15 @@
 Authors: zhouwenlong(zhouwenlong01@baidu.com)
          wanggaofei(wanggaofei03@baidu.com)
 Date:    2023-03-16
 """
 import yaml
 import os
 import argparse
-import logit
+import bcelogger
 
 KEY_OUTPUTS = 'outputs'
 KEY_MODEL_NAME = 'model_name'
 
 class ParseYamlConfig(object):
     """
     解析修改模型包 parse.yaml
@@ -68,27 +68,27 @@
                 for cidx, class_name in enumerate(categories) :
                     new_categories.append({
                         "name": class_name["name"],
                         "id": class_name["id"]
                     })
 
                 single_map["categories"] = new_categories
-                logit.info('set categories. model_name: {} num: {}'.format(model_name, len(new_categories)))
+                bcelogger.info('set categories. model_name: {} num: {}'.format(model_name, len(new_categories)))
 
     def modify_ensemble_name(self, ensemble_name: str):
         """
             modify ensemble name
         """
         if KEY_OUTPUTS in self.yaml_data:
             for _, v in enumerate(self.yaml_data[KEY_OUTPUTS]):
                 if KEY_MODEL_NAME in v and 'ensemble' in v[KEY_MODEL_NAME]:
                     v[KEY_MODEL_NAME] = ensemble_name
-                    logit.info('modify ensemble name: {}'.format(ensemble_name))
+                    bcelogger.info('modify ensemble name: {}'.format(ensemble_name))
         else:
-            logit.error('do NOT find key in parse.yaml: {}'.format(KEY_OUTPUTS))
+            bcelogger.error('do NOT find key in parse.yaml: {}'.format(KEY_OUTPUTS))
 
     def save_yaml(self, yaml_name):
         """
         将字典数据保存为YAML格式的文件。
         
         Args:
             yaml_name (str): YAML文件名，包含路径。
```

## gaea_operator/config/update_pbtxt.py

```diff
@@ -16,15 +16,15 @@
 from copy import deepcopy
 from shutil import copytree
 from typing import Any, Dict, List, Optional
 
 from google.protobuf import json_format, text_format
 from google.protobuf.descriptor import FieldDescriptor
 from tritonclient.grpc import model_config_pb2
-import logit
+import bcelogger
 
 KEY_DIMS = 'dims'
 KEY_OUTPUT = 'output'
 KEY_OPTIMIZATION = 'optimization'
 KEY_EXECUTION_ACCELERATORS = 'executionAccelerators'
 KEY_CPU_EXECUTION_ACCELERATOR = 'cpuExecutionAccelerator'
 KEY_TYPE = 'type'
@@ -101,15 +101,15 @@
             The full path to config.pbtxt
 
         Returns
         -------
         ModelConfig
         """
         if not os.path.isfile(pbtxt_name):
-            logit.error(
+            bcelogger.error(
                 f'Path "{pbtxt_name}" does not exist.'
                 " Make sure that you have specified the correct model"
                 " repository and model name(s)."
             )
 
         with open(pbtxt_name, "r+") as f:
             config_str = f.read()
@@ -153,50 +153,50 @@
         """
         Returns
         -------
             List[str]: Sub-model names
         """
 
         if not self.is_ensemble():
-            logit.error(
+            bcelogger.error(
                 "Cannot find composing_models. Model platform is not ensemble."
             )
 
         try:
             composing_models = [
                 model["modelName"]
                 for model in self.to_dict()["ensembleScheduling"]["step"]
             ]
         except Exception:
-            logit.error(
+            bcelogger.error(
                 "Cannot find composing_models. Ensemble Scheduling and/or step is not present in config protobuf."
             )
 
         return composing_models
 
     def set_composing_model_variant_name(
         self, composing_model_name: str, variant_name: str
     ) -> None:
         """
         Replaces the Ensembles composing_model's name with the variant name
         """
 
         if not self.is_ensemble():
-            logit.error(
+            bcelogger.error(
                 "Cannot find composing_models. Model platform is not ensemble."
             )
 
         model_config_dict = self.to_dict()
 
         try:
             for composing_model in model_config_dict["ensembleScheduling"]["step"]:
                 if composing_model["modelName"] == composing_model_name:
                     composing_model["modelName"] = variant_name
         except Exception:
-            logit.error(
+            bcelogger.error(
                 "Cannot find composing_models. Ensemble Scheduling and/or step is not present in config protobuf."
             )
 
         self._model_config = self.from_dict(model_config_dict)._model_config
 
     def set_model_name(self, model_name: str) -> None:
         """
@@ -309,15 +309,15 @@
         self._model_config = self.from_dict(model_config_dict)._model_config
     
     def set_dict_dims(self, config_dict, idx: int, dims: list):
         """
             set array dims
         """
         if KEY_DIMS not in config_dict or len(dims) + idx > len(config_dict[KEY_DIMS]):
-            logit.error('do NOT find dims in config or dims invalid. idx: {} dims: {}'.format(idx, dims))
+            bcelogger.error('do NOT find dims in config or dims invalid. idx: {} dims: {}'.format(idx, dims))
         else:
             config_dict[KEY_DIMS][idx: idx + len(dims)] = dims
 
     def set_op_resize_width_height(self, width: int, height: int):
         """
             set all resize-type op width/height
         """
@@ -366,15 +366,15 @@
         """
         model_config_dict = self.to_dict()
         if KEY_ENSEMBLE_SCHEDULING in model_config_dict and KEY_STEP in model_config_dict[KEY_ENSEMBLE_SCHEDULING]:
             for v in model_config_dict[KEY_ENSEMBLE_SCHEDULING][KEY_STEP]:
                 if KEY_MODEL_NAME in v and v[KEY_MODEL_NAME] in names:
                     v[KEY_MODEL_VERSION] = versions[v[KEY_MODEL_NAME]]
                     v[KEY_MODEL_NAME] = names[v[KEY_MODEL_NAME]]
-                    logit.info('modify model name: {}'.format(v[KEY_MODEL_NAME]))
+                    bcelogger.info('modify model name: {}'.format(v[KEY_MODEL_NAME]))
         self._model_config = self.from_dict(model_config_dict)._model_config
 
     def get_field(self, name):
         """
         Get the value for the current field.
         """
```

## gaea_operator/config/ppyoloe_plus/template/modify_train_parameter.py

```diff
@@ -8,15 +8,15 @@
 Authors: wanggaofei(wanggaofei03@baidu.com)
 Date:    2023-02-29
 """
 import os
 import yaml
 import argparse
 
-import logit
+import bcelogger
 
 KEY_EPOCH = 'epoch'
 KEY_EVAL_HEIGHT = 'eval_height'
 KEY_EVAL_WIDTH = 'eval_width'
 KEY_EVAL_SIZE = 'eval_size'
 KEY_PRETRAIN_WEIGHTS = 'pretrain_weights'
 KEY_DEPTH_MULT = 'depth_mult'
@@ -95,19 +95,19 @@
     Returns:
         None - 无返回值，直接修改传入的yamldata参数。
     
     Raises:
         None - 该函数没有引发任何异常。
     """
     if key in yaml_data:
-        logit.info('old val. {} -> {}'.format(key, yaml_data[key]))
+        bcelogger.info('old val. {} -> {}'.format(key, yaml_data[key]))
         yaml_data[key] = val
-        logit.info('new val. {} -> {}'.format(key, yaml_data[key]))
+        bcelogger.info('new val. {} -> {}'.format(key, yaml_data[key]))
     else:
-        logit.error('do NOT find key: {}'.format(key))
+        bcelogger.error('do NOT find key: {}'.format(key))
 
 
 def get_value(yaml_data, key):
     """
     根据指定的键值获取对应的值，如果找不到则返回None。
     
     Args:
@@ -116,15 +116,15 @@
     
     Returns:
         Union[str, int, dict, list, None]: 如果找到该键值，则返回对应的值；否则返回None。
     """
     if key in yaml_data:
         return yaml_data[key]
     else:
-        logit.info('do NOT find key: {}'.format(key))
+        bcelogger.info('do NOT find key: {}'.format(key))
         return None
 
 
 def process_var(line, var_name, val):
     """
     处理变量
     """
@@ -183,15 +183,15 @@
     if snapshot_epoch is not None and int(snapshot_epoch) > val:
         set_value(yaml_data, KEY_SNAPSHOT_EPOCH, val)
     if KEY_LEARNING_RATE in yaml_data and KEY_SCHEDULERS in yaml_data[KEY_LEARNING_RATE]:
         for i in range(len(yaml_data[KEY_LEARNING_RATE][KEY_SCHEDULERS])):
             if KEY_MAX_EPOCHS in yaml_data[KEY_LEARNING_RATE][KEY_SCHEDULERS][i] and \
                     int(yaml_data[KEY_LEARNING_RATE][KEY_SCHEDULERS][i][KEY_MAX_EPOCHS]) > val:
                 yaml_data[KEY_LEARNING_RATE][KEY_SCHEDULERS][i][KEY_MAX_EPOCHS] = val
-                logit.info('set-epoch modify max_epochs. {}'.format(val))
+                bcelogger.info('set-epoch modify max_epochs. {}'.format(val))
 
 
 def convert_value_type(val):
     """
         convert string to real data type
     """
     if val.isdigit():
@@ -211,15 +211,15 @@
     for i, key in enumerate(keys):
         if key in config_dict:
             if i + 1 == len(keys):
                 config_dict[key] = convert_value_type(val)
             else:
                 config_dict = config_dict[key]
         else:
-            logit.error('do NOT find key: {} of {}'.format(key, multi_key))
+            bcelogger.error('do NOT find key: {} of {}'.format(key, multi_key))
             break
 
 
 def height_width_str2list(height_width_str):
     """
     字符串转列表
     """
@@ -233,15 +233,15 @@
             for i in range(len(s) // 2):
                 size_list.append([int(s[i * 2]), int(s[i * 2 + 1])])
         else:
             ws = s.split(',')
             for _, v in enumerate(ws):
                 size_list.append(int(v))
     else:
-        logit.error('invalid height-width-string. {}'.format(height_width_str))
+        bcelogger.error('invalid height-width-string. {}'.format(height_width_str))
     return size_list
 
 
 def set_target_size(yaml_data, eval_width, eval_height):
     """
     设置目标大小，如果不支持该大小则报错。
     如果配置文件中存在train reader和batch transforms，并且包含batch random resize，则修改其target size为指定的大小。
@@ -255,35 +255,35 @@
         None.
     
     Raises:
         ValueError: 如果不支持指定的目标大小。
     """
     eval_wh_str = str(eval_width) + ',' + str(eval_height)
     if eval_wh_str not in TARGET_SIZE_MAP:
-        logit.error('do NOT support target size. width: {}, height: {}'.format(eval_width, eval_height))
+        bcelogger.error('do NOT support target size. width: {}, height: {}'.format(eval_width, eval_height))
     else:
         if KEY_TRAIN_READER in yaml_data and KEY_BATCH_TRANSFORMS in yaml_data[KEY_TRAIN_READER]:
             for _, v in enumerate(yaml_data[KEY_TRAIN_READER][KEY_BATCH_TRANSFORMS]):
                 if KEY_BATCH_RANDOM_RESIZE in v and KEY_TARGET_SIZE in v[KEY_BATCH_RANDOM_RESIZE]:
                     v[KEY_BATCH_RANDOM_RESIZE][KEY_TARGET_SIZE] = height_width_str2list(TARGET_SIZE_MAP[eval_wh_str])
-                    logit.info('set target size: {}'.format(eval_wh_str))
+                    bcelogger.info('set target size: {}'.format(eval_wh_str))
 
 
 def set_model_type(yaml_data, val, pretrain_model_uri):
     """
     设置模型类型
     """
     model_type = val.strip().split('_')[-1]
     if model_type in PRETRAIN_MODEL_NAMES:
         name, depth_mult, width_mult = PRETRAIN_MODEL_NAMES[model_type]
         set_value(yaml_data, KEY_PRETRAIN_WEIGHTS, os.path.join(pretrain_model_uri, name))
         set_value(yaml_data, KEY_DEPTH_MULT, depth_mult)
         set_value(yaml_data, KEY_WIDTH_MULT, width_mult)
     else:
-        logit.error('do NOT known model type value. {}'.format(val))
+        bcelogger.error('do NOT known model type value. {}'.format(val))
 
 
 def generate_train_config(
         advanced_parameters: dict,
         pretrain_model_uri: str,
         train_config_name: str
 ):
@@ -296,15 +296,15 @@
     var_name_vals = [[KEY_EVAL_WIDTH, width], [KEY_EVAL_HEIGHT, height]]
 
     if advanced_parameters[KEY_MODEL_TYPE].startswith('change-'):
         input_yaml_name = 'parameter_c.yaml'
     else:
         input_yaml_name = 'parameter.yaml'
 
-    logit.info('train parameter name: {}'.format(input_yaml_name))
+    bcelogger.info('train parameter name: {}'.format(input_yaml_name))
 
     input_yaml_name = os.path.join(os.path.dirname(os.path.abspath(__file__)), input_yaml_name)
     yaml_data = modify_var_value(input_yaml_name, var_name_vals)
 
     # 1. set correlative parameters
     correlative_parameter_keys = [KEY_EVAL_WIDTH, KEY_EVAL_HEIGHT, KEY_EPOCH, KEY_MODEL_TYPE]
     # 1.1 target size
@@ -317,17 +317,17 @@
     set_model_type(yaml_data, advanced_parameters[KEY_MODEL_TYPE], pretrain_model_uri)
 
     # 2. set get-though parameters by key-value of dict
     for key, val in advanced_parameters.items():
         if key not in correlative_parameter_keys:
             set_multi_key_value(yaml_data, key, val)
 
-    logit.info('begin to save yaml. {}'.format(train_config_name))
+    bcelogger.info('begin to save yaml. {}'.format(train_config_name))
     save_yaml(yaml_data, train_config_name)
-    logit.info('write train config finish.')
+    bcelogger.info('write train config finish.')
 
 
 def parse_opt():
     """ parser opt
         Args:
 
         Returns:
@@ -351,10 +351,10 @@
         kv = v.split(':')
         param_dict[kv[0]] = kv[1]
     return param_dict
 
 
 if __name__ == "__main__":
     opt = parse_opt()
-    logit.info("args: {}".format(opt))
+    bcelogger.info("args: {}".format(opt))
 
     generate_train_config(str2dict(opt.advanced_parameters), opt.pretrain_model_uri, opt.train_config_name)
```

## gaea_operator/config/resnet/template/modify_train_parameter.py

```diff
@@ -9,15 +9,15 @@
 Date:    2023-02-29
 """
 
 import json
 import os
 import yaml
 import argparse
-import logit
+import bcelogger
 
 from windmillmodelv1.client.model_api_model import ModelMetadata, InputSize
 
 from gaea_operator.utils import DEFAULT_META_FILE_NAME
 
 KEY_EPOCH = 'Global.epochs'
 KEY_EVAL_HEIGHT = 'eval_height'
@@ -87,19 +87,19 @@
     Returns:
         None - 无返回值，直接修改传入的yamldata参数。
     
     Raises:
         None - 该函数没有引发任何异常。
     """
     if key in yaml_data:
-        logit.info('old val. {} -> {}'.format(key, yaml_data[key]))
+        bcelogger.info('old val. {} -> {}'.format(key, yaml_data[key]))
         yaml_data[key] = val
-        logit.info('new val. {} -> {}'.format(key, yaml_data[key]))
+        bcelogger.info('new val. {} -> {}'.format(key, yaml_data[key]))
     else:
-        logit.error('do NOT find key: {}'.format(key))
+        bcelogger.error('do NOT find key: {}'.format(key))
 
 
 def get_value(yaml_data, key):
     """
     根据指定的键值获取对应的值，如果找不到则返回None。
     
     Args:
@@ -108,15 +108,15 @@
     
     Returns:
         Union[str, int, dict, list, None]: 如果找到该键值，则返回对应的值；否则返回None。
     """
     if key in yaml_data:
         return yaml_data[key]
     else:
-        logit.info('do NOT find key: {}'.format(key))
+        bcelogger.info('do NOT find key: {}'.format(key))
         return None
 
 
 def convert_value_type(val):
     """
         convert string to real data type
     """
@@ -137,36 +137,36 @@
     for i, key in enumerate(keys):
         if key in config_dict:
             if i + 1 == len(keys):
                 config_dict[key] = convert_value_type(val)
             else:
                 config_dict = config_dict[key]
         else:
-            logit.error('do NOT find key: {} of {}'.format(key, multi_key))
+            bcelogger.error('do NOT find key: {} of {}'.format(key, multi_key))
             break
 
 
 def set_epoch(yaml_data, val):
     """
     set epoch
     """
     if KEY_GLOBAL in yaml_data:
         set_multi_key_value(yaml_data, KEY_EPOCH, str(val))
 
         save_interval = get_value(yaml_data[KEY_GLOBAL], KEY_SAVE_INTERVAL)
         if save_interval is None or int(save_interval) > int(val):
             set_value(yaml_data[KEY_GLOBAL], KEY_SAVE_INTERVAL, val)
-            logit.info('modify {}: {} -> {}'.format(KEY_SAVE_INTERVAL, save_interval, val))
+            bcelogger.info('modify {}: {} -> {}'.format(KEY_SAVE_INTERVAL, save_interval, val))
 
         eval_interval = get_value(yaml_data[KEY_GLOBAL], KEY_EVAL_INTERVAL)
         if eval_interval is None or int(eval_interval) > int(val):
             set_value(yaml_data[KEY_GLOBAL], KEY_EVAL_INTERVAL, val)
-            logit.info('modify {}: {} -> {}'.format(KEY_EVAL_INTERVAL, eval_interval, val))
+            bcelogger.info('modify {}: {} -> {}'.format(KEY_EVAL_INTERVAL, eval_interval, val))
     else:
-        logit.error('do NOT find key: {}'.format(KEY_GLOBAL))
+        bcelogger.error('do NOT find key: {}'.format(KEY_GLOBAL))
 
 
 def set_target_size(yaml_data, eval_width, eval_height):
     """
     设置目标大小，如果不支持该大小则报错。
     如果配置文件中存在train reader和batch transforms，并且包含batch random resize，则修改其target size为指定的大小。
     
@@ -189,34 +189,34 @@
     if KEY_DATALOADER in yaml_data and KEY_TRAIN in yaml_data[KEY_DATALOADER] \
             and KEY_DATASET in yaml_data[KEY_DATALOADER][KEY_TRAIN] \
             and KEY_TRANSFORM_OPS in yaml_data[KEY_DATALOADER][KEY_TRAIN][KEY_DATASET]:
         for i in range(len(yaml_data[KEY_DATALOADER][KEY_TRAIN][KEY_DATASET][KEY_TRANSFORM_OPS])):
             if KEY_RESIZE_IMAGE in yaml_data[KEY_DATALOADER][KEY_TRAIN][KEY_DATASET][KEY_TRANSFORM_OPS][i]:
                 set_value(yaml_data[KEY_DATALOADER][KEY_TRAIN][KEY_DATASET][KEY_TRANSFORM_OPS][i][KEY_RESIZE_IMAGE], \
                           KEY_SIZE, wh_list)
-                logit.info('set {}.{}.{}.{} size: {}'.format(KEY_DATALOADER, KEY_TRAIN, \
+                bcelogger.info('set {}.{}.{}.{} size: {}'.format(KEY_DATALOADER, KEY_TRAIN, \
                                                              KEY_DATASET, KEY_TRANSFORM_OPS, wh_list))
 
     if KEY_EVAL in yaml_data and KEY_DATASET in yaml_data[KEY_EVAL] and KEY_TRANSFORM_OPS \
             in yaml_data[KEY_EVAL][KEY_DATASET]:
         for i in range(yaml_data[KEY_EVAL][KEY_DATASET][KEY_TRANSFORM_OPS]):
             if KEY_RESIZE_IMAGE in yaml_data[KEY_EVAL][KEY_DATASET][KEY_TRANSFORM_OPS][i]:
                 set_value(yaml_data[KEY_EVAL][KEY_DATASET][KEY_TRANSFORM_OPS][i][KEY_RESIZE_IMAGE], KEY_SIZE, wh_list)
-                logit.info('set {}.{}.{}.{} size: {}'.format(KEY_EVAL, KEY_DATASET, \
+                bcelogger.info('set {}.{}.{}.{} size: {}'.format(KEY_EVAL, KEY_DATASET, \
                                                              KEY_TRANSFORM_OPS, KEY_RESIZE_IMAGE, wh_list))
 
 
 def get_pretrained_model_name(model_type, pretrained_model_path):
     """
         get pretrained model absolute path
     """
     if model_type == KEY_RESNET:
         return os.path.join(pretrained_model_path, PRETRAINED_MODEL_NAME_RESNET)
     else:
-        logit.error('do NOT support model type. {} use default pretrained model. {}'.format(model_type,
+        bcelogger.error('do NOT support model type. {} use default pretrained model. {}'.format(model_type,
                                                                                             PRETRAINED_MODEL_NAME_RESNET))
         return os.path.join(pretrained_model_path, PRETRAINED_MODEL_NAME_RESNET)
 
 
 def get_mean_std(yaml_data):
     """
         get train mean/std
@@ -258,17 +258,17 @@
             if KEY_PRETRAINED_MODEL == key:
                 val = get_pretrained_model_name(advanced_parameters[KEY_MODEL_TYPE],
                                                 advanced_parameters[KEY_PRETRAINED_MODEL])
             set_multi_key_value(yaml_data, key, val)
             if key in shadow_parameters:
                 set_multi_key_value(yaml_data, key.replace('Train', 'Eval'), val)
 
-    logit.info('begin to save yaml. {}'.format(train_config_name))
+    bcelogger.info('begin to save yaml. {}'.format(train_config_name))
     save_yaml(yaml_data, train_config_name)
-    logit.info('write train config finish.')
+    bcelogger.info('write train config finish.')
 
 
 def parse_opt():
     """ parser opt
         Args:
 
         Returns:
@@ -290,15 +290,15 @@
     for _, v in enumerate(s):
         kv = v.split(':')
         param_dict[kv[0]] = kv[1]
     return param_dict
 
 
 if __name__ == "__main__":
-    from logit.base_logger import setup_logger
+    from bcelogger.base_logger import setup_logger
 
     opt = parse_opt()
     setup_logger(config=dict(file_name=os.path.join('/ssd2/lyg/GAEA-PIPE/resnet18', "worker.log")))
     # labels: [{'name':, 'id': }, ...]
     labels = json.load(open('/ssd2/lyg/Dataset/zhiguan_clas/labels.json', "r"))
     advanced_parameters = {'Global.epochs': '1', 'Optimizer.lr.learning_rate': '0.001', \
                            'DataLoader.Train.loader.num_workers': '2', 'eval_height': '256', 'eval_width': '256', \
```

## gaea_operator/dataset/cityscape_dataset.py

```diff
@@ -4,15 +4,15 @@
 # @Time    : 2024/3/17
 # @Author  : yanxiaodong
 # @File    : imagenet_dataset.py
 """
 import os
 from typing import List, Any
 
-import logit
+import bcelogger
 from windmillclient.client.windmill_client import WindmillClient
 
 from .dataset import Dataset
 from gaea_operator.utils import get_filepaths_in_archive
 
 
 class CityscapesDataset(Dataset):
@@ -22,34 +22,34 @@
     usages = ["train.txt", "val.txt"]
 
     def __init__(self, windmill_client: WindmillClient, work_dir: str):
         super().__init__(windmill_client=windmill_client, work_dir=work_dir)
 
         self.image_prefix_path = ""
 
-    def _get_annotation(self, paths: List, fs_prefix: str, usage: str):
+    def _get_annotation(self, paths: List, base_uri: str, usage: str, work_di: str):
         annotation_file_list = []
         for path in paths:
             path = os.path.join(self.work_dir, path)
             annotation_file_list = get_filepaths_in_archive(path, self.decompress_output_uri, usage)
 
-        logit.info(f"[{usage.capitalize()}] annotation file list is: {annotation_file_list}")
+        bcelogger.info(f"[{usage.capitalize()}] annotation file list is: {annotation_file_list}")
 
         raw_data_list = []
         label_list = []
         for file in annotation_file_list:
             text_data = open(file, "r").read()
             raw_data = text_data.strip("\n").split("\n")
 
-            logit.info(f"Parse annotation file {file}, image num is {len(raw_data)}")
+            bcelogger.info(f"Parse annotation file {file}, image num is {len(raw_data)}")
 
             for idx in range(len(raw_data)):
                 img_file, label_file = raw_data[idx].rsplit(" ", 1)
-                img_file = self._file_name_cvt_abs(img_file, file, fs_prefix, 1)
-                label_file = self._file_name_cvt_abs(label_file, file, fs_prefix, 1)
+                img_file = self._file_name_cvt_abs(img_file, file, base_uri, 1, work_di)
+                label_file = self._file_name_cvt_abs(label_file, file, base_uri, 1, work_di)
                 raw_data[idx] = img_file + " " + label_file
 
             raw_data_list.append(raw_data)
 
             label_data = open(os.path.join(os.path.dirname(file), "labels.txt"), "r").read().strip("\n").split("\n")
             label_list.append(label_data)
 
@@ -88,15 +88,15 @@
             for item in raw_data:
                 fp.write(item + "\n")
 
     def _category_valid(self, label_list: List[List]):
         lengths = [len(label) for label in label_list]
 
         if len(set(lengths)) == 1:
-            logit.info(f"The number of labels is {lengths[0]}")
+            bcelogger.info(f"The number of labels is {lengths[0]}")
             for idx in range(1, len(lengths)):
                 for inner_idx, label in enumerate(label_list[idx]):
                     if label != label_list[0][inner_idx]:
                         raise ValueError(f"The labels name is not equal, please check {label_list}")
             self.labels = [{"id": idx, "name": name} for idx, name in enumerate(label_list[0])]
             return label_list[0]
         else:
```

## gaea_operator/dataset/coco_dataset.py

```diff
@@ -5,15 +5,15 @@
 # @Author  : yanxiaodong
 # @File    : coco_dataset.py
 """
 import json
 import os
 from typing import Any, Dict, List, Union, Tuple
 
-import logit
+import bcelogger
 from windmillclient.client.windmill_client import WindmillClient
 
 from .dataset import Dataset
 from gaea_operator.utils import get_filepaths_in_archive
 
 
 class CocoDataset(Dataset):
@@ -22,84 +22,91 @@
     """
     usages = [("train.json", "annotation.json"), ("val.json", "annotation.json")]
 
     def __init__(self, windmill_client: WindmillClient, work_dir: str, extra_work_dir: str = None):
         super().__init__(windmill_client=windmill_client, work_dir=work_dir, extra_work_dir=extra_work_dir)
         self.image_prefix_path = "images"
 
-    def _get_annotation(self, paths: List, fs_prefix: str, usage: Union[str, Tuple], work_dir: str) -> List:
+    def _get_annotation(self, paths: List, base_uri: str, usage: Union[str, Tuple], work_dir: str) -> List:
         annotation_file_list = []
         for path in paths:
             path = os.path.join(work_dir, path)
             annotation_file_list = get_filepaths_in_archive(path, self.decompress_output_uri, usage)
 
-        logit.info(f"Annotation file list is: {annotation_file_list}")
+        bcelogger.info(f"Annotation file list is: {annotation_file_list}")
 
         raw_data_list = []
         for file in annotation_file_list:
             json_data = json.load(open(file, "r"))
             images = json_data["images"]
 
-            logit.info(f"Parse annotation file {file}, image num is {len(images)}")
+            bcelogger.info(f"Parse annotation file {file}, image num is {len(images)}")
 
             for img in images:
-                img["file_name"] = self._file_name_cvt_abs(img["file_name"], file, fs_prefix, 2, work_dir)
+                img["file_name"] = self._file_name_cvt_abs(img["file_name"], file, base_uri, 2, work_dir)
                 self.image_set.add(img["file_name"])
 
             raw_data_list.append(json_data)
 
         return raw_data_list
 
     def _concat_annotation(self, raw_data_list: List):
-        if len(raw_data_list) > 1:
+        if len(raw_data_list) >= 1:
             raw_data_coco = self._coco_data_raw_concat(raw_data_list)
-        elif len(raw_data_list) == 1:
-            raw_data_coco = raw_data_list[0]
-            self.labels = [{"id": str(cat["id"]), "name": cat["name"]} for cat in raw_data_coco["categories"]]
         else:
             raw_data_coco = None
 
         return raw_data_coco
 
     def _write_annotation(self, output_dir: str, file_name: str, raw_data: Any):
         if raw_data is None:
             return
         if not os.path.exists(output_dir):
             os.makedirs(output_dir, exist_ok=True)
         file_path = os.path.join(output_dir, file_name)
         with open(file_path, "w") as fp:
-            json.dump(raw_data, fp, ensure_ascii=False, indent=4)
+            json.dump(raw_data, fp, separators=(",", ":"))
 
     def _coco_data_raw_concat(self, raw_data: List[Dict]):
         cat_name2id = self._category_valid(raw_data)
 
-        raw_data_coco = {"images": [], "annotations": [], "categories": raw_data[0]["categories"]}
-        max_im_id = 1
+        raw_data_coco = {"images": [], "annotations": [], "categories": []}
         max_ann_id = 1
+        new_im_id = 1
+
+        categories = []
+        for cat in raw_data[0]["categories"]:
+            cat["id"] = int(cat["id"])
+            categories.append(cat)
+        raw_data_coco["categories"] = categories
 
         for data in raw_data:
             old2new_im_id = {}
             old2new_cat_id = {}
             for img in data["images"]:
-                max_im_id = self._get_max_id(img["id"], max_im_id)
-                old2new_im_id[img["id"]] = max_im_id
-                img["id"] = max_im_id
+                old2new_im_id[img["id"]] = new_im_id
+                img["id"] = new_im_id
                 raw_data_coco["images"].append(img)
+                new_im_id += 1
 
             for cat in data["categories"]:
-                if cat_name2id[cat["name"]] != cat["id"]:
+                if cat_name2id[cat["name"]] != int(cat["id"]):
                     old2new_cat_id[cat["id"]] = cat_name2id[cat["name"]]
                     cat["id"] = cat_name2id[cat["name"]]
 
             for ann in data["annotations"]:
-                max_ann_id = self._get_max_id(ann["id"], max_ann_id)
+                max_ann_id = self._get_max_id(int(ann["id"]), max_ann_id)
                 if ann["image_id"] in old2new_im_id:
                     ann["image_id"] = old2new_im_id[ann["image_id"]]
+                else:
+                    ann["image_id"] = int(ann["image_id"])
                 if ann["category_id"] in old2new_cat_id:
                     ann["category_id"] = old2new_cat_id[ann["category_id"]]
+                else:
+                    ann["category_id"] = int(ann["category_id"])
                 ann["id"] = max_ann_id
                 raw_data_coco["annotations"].append(ann)
 
         return raw_data_coco
 
     def _get_max_id(self, im_id: int, max_im_id: int):
         if im_id > max_im_id:
@@ -108,17 +115,18 @@
         return max_im_id
 
     def _category_valid(self, raw_data: List[Dict]):
         categories_list = [data["categories"] for data in raw_data]
         lengths = [len(categories) for categories in categories_list]
 
         if len(set(lengths)) == 1:
-            logit.info(f"The number of categories is {lengths[0]}")
-            cat_name2id = {cat["name"]: cat["id"] for cat in categories_list[0]}
+            bcelogger.info(f"The number of categories is {lengths[0]}")
+            cat_name2id = {cat["name"]: int(cat["id"]) for cat in categories_list[0]}
             for idx in range(1, len(lengths)):
                 for cat in categories_list[idx]:
                     if cat["name"] not in cat_name2id:
                         raise ValueError(f"The categories name is not equal, please check {categories_list}")
             self.labels = [{"id": str(cat["id"]), "name": cat["name"]} for cat in categories_list[0]]
+            bcelogger.info(f"The labels is {self.labels}")
             return cat_name2id
         else:
             raise ValueError(f"The number of categories is not equal, please check {categories_list}")
```

## gaea_operator/dataset/dataset.py

```diff
@@ -6,15 +6,15 @@
 # @File    : dataset_concat.py
 """
 import json
 import os
 from abc import ABCMeta, abstractmethod
 from typing import Any, List, Union, Tuple
 
-import logit
+import bcelogger
 from windmillclient.client.windmill_client import WindmillClient
 
 from gaea_operator.utils import find_upper_level_folder, write_file
 
 
 class Dataset(metaclass=ABCMeta):
     """
@@ -39,53 +39,53 @@
                        usage: Union[str, Tuple],
                        base_dataset_name: str = None,
                        save_label: bool = False):
         """
         Concat dataset from artifact.
         """
         # 处理base dataset name
-        logit.info(f"Concat base dataset from dataset name {dataset_name}")
         base_raw_data_list = []
         if base_dataset_name is not None and len(base_dataset_name) > 0:
+            bcelogger.info(f"Concat base dataset from dataset name {base_dataset_name}")
             response = self.windmill_client.get_artifact(name=base_dataset_name)
             filesystem = self.windmill_client.suggest_first_filesystem(workspace_id=response.workspaceID,
                                                                        guest_name=response.parentName)
-            fs_prefix = self.windmill_client.build_base_uri(filesystem=filesystem)
-            base_paths = [os.path.relpath(_path, fs_prefix).rstrip('/') for _path in response.metadata["paths"]]
+            base_uri = self.windmill_client.build_base_uri(filesystem=filesystem)
+            base_paths = [os.path.relpath(_path, base_uri).rstrip('/') for _path in response.metadata["paths"]]
 
-            logit.info(f"Concat base dataset from path {base_paths}")
+            bcelogger.info(f"Concat base dataset from path {base_paths}")
             base_raw_data_list = self._get_annotation(paths=base_paths,
-                                                      fs_prefix=fs_prefix,
+                                                      base_uri=base_uri,
                                                       usage=usage,
                                                       work_dir=self.extra_work_dir)
 
-        logit.info(f"Concat dataset from dataset name {dataset_name}")
+        bcelogger.info(f"Concat dataset from dataset name {dataset_name}")
         response = self.windmill_client.get_artifact(name=dataset_name)
         write_file(json.loads(response.raw_data), output_dir=output_dir)
         filesystem = self.windmill_client.suggest_first_filesystem(workspace_id=response.workspaceID,
                                                                    guest_name=response.parentName)
-        fs_prefix = self.windmill_client.build_base_uri(filesystem=filesystem)
-        paths = [os.path.relpath(_path, fs_prefix).rstrip('/') for _path in response.metadata["paths"]]
-        logit.info(f"Concat dataset from path {paths}")
+        base_uri = self.windmill_client.build_base_uri(filesystem=filesystem)
+        paths = [os.path.relpath(_path, base_uri).rstrip('/') for _path in response.metadata["paths"]]
+        bcelogger.info(f"Concat dataset from path {paths}")
 
-        raw_data_list = self._get_annotation(paths=paths, fs_prefix=fs_prefix, usage=usage, work_dir=self.work_dir)
+        raw_data_list = self._get_annotation(paths=paths, base_uri=base_uri, usage=usage, work_dir=self.work_dir)
 
         raw_data_list += base_raw_data_list
         raw_data = self._concat_annotation(raw_data_list=raw_data_list)
         self._write_annotation(output_dir=output_dir,
                                file_name=usage if isinstance(usage, str) else usage[0],
                                raw_data=raw_data)
 
         self._warmup_image_meta()
 
         if usage == self.usages[0] or save_label:
             self._write_category(output_dir=output_dir)
 
     @abstractmethod
-    def _get_annotation(self, paths: List, fs_prefix: str, usage: str, work_dir: str) -> List:
+    def _get_annotation(self, paths: List, base_uri: str, usage: str, work_dir: str) -> List:
         pass
 
     @abstractmethod
     def _concat_annotation(self, raw_data_list: List):
         pass
 
     @abstractmethod
```

## gaea_operator/dataset/imagenet_dataset.py

```diff
@@ -4,15 +4,15 @@
 # @Time    : 2024/3/17
 # @Author  : yanxiaodong
 # @File    : imagenet_dataset.py
 """
 import os
 from typing import List, Any
 
-import logit
+import bcelogger
 from windmillclient.client.windmill_client import WindmillClient
 
 from .dataset import Dataset
 from gaea_operator.utils import get_filepaths_in_archive
 
 
 class ImageNetDataset(Dataset):
@@ -23,47 +23,44 @@
 
     def __init__(self, windmill_client: WindmillClient, work_dir: str):
         super().__init__(windmill_client=windmill_client, work_dir=work_dir)
 
         self.image_prefix_path = ""
         self.label_list = []
 
-    def _get_annotation(self, paths: List, fs_prefix: str, usage: str, work_dir: str):
+    def _get_annotation(self, paths: List, base_uri: str, usage: str, work_dir: str):
         annotation_file_list = []
         for path in paths:
             path = os.path.join(work_dir, path)
             annotation_file_list = get_filepaths_in_archive(path, self.decompress_output_uri, usage)
 
-        logit.info(f"Annotation file list is: {annotation_file_list}")
+        bcelogger.info(f"Annotation file list is: {annotation_file_list}")
 
         raw_data_list = []
         for file in annotation_file_list:
             text_data = open(file, "r").read()
             raw_data = text_data.strip("\n").split("\n")
 
-            logit.info(f"Parse annotation file {file}, image num is {len(raw_data)}")
+            bcelogger.info(f"Parse annotation file {file}, image num is {len(raw_data)}")
 
             for idx in range(len(raw_data)):
                 img_file, label = raw_data[idx].rsplit(" ", 1)
-                img_file = self._file_name_cvt_abs(img_file, file, fs_prefix, 1, work_dir)
+                img_file = self._file_name_cvt_abs(img_file, file, base_uri, 1, work_dir)
                 raw_data[idx] = img_file + " " + label
 
             raw_data_list.append(raw_data)
 
             label_data = open(os.path.join(os.path.dirname(file), "labels.txt"), "r").read().strip("\n").split("\n")
             self.label_list.append(label_data)
 
         return raw_data_list
 
     def _concat_annotation(self, raw_data_list: List):
-        if len(raw_data_list) > 1:
+        if len(raw_data_list) >= 1:
             raw_data_imagenet = self._imagenet_data_raw_concat(raw_data_list, self.label_list)
-        elif len(raw_data_list) == 1:
-            raw_data_imagenet = raw_data_list[0]
-            self.labels = [{"id": str(idx), "name": str(name)} for idx, name in enumerate(self.label_list[0])]
         else:
             raw_data_imagenet = None
 
         return raw_data_imagenet
 
     def _imagenet_data_raw_concat(self, raw_data: List[List], label_list: List[List]):
         label_name = self._category_valid(label_list)
@@ -92,15 +89,15 @@
             for item in raw_data:
                 fp.write(item + "\n")
 
     def _category_valid(self, label_list: List[List]):
         lengths = [len(label) for label in label_list]
 
         if len(set(lengths)) == 1:
-            logit.info(f"The number of labels is {lengths[0]}")
+            bcelogger.info(f"The number of labels is {lengths[0]}")
             for idx in range(1, len(lengths)):
                 for inner_idx, label in enumerate(label_list[idx]):
                     if label != label_list[0][inner_idx]:
                         raise ValueError(f"The labels name is not equal, please check {label_list}")
             self.labels = [{"id": str(idx), "name": str(name)} for idx, name in enumerate(label_list[0])]
             return label_list[0]
         else:
```

## gaea_operator/metric/metric.py

```diff
@@ -4,15 +4,15 @@
 # @Time    : 2024/3/7
 # @Author  : yanxiaodong
 # @File    : metric.py
 """
 import os
 from typing import Dict, List, Union, Optional
 
-import logit
+import bcelogger
 from gaea_tracker import ExperimentTracker
 from windmillclient.client.windmill_client import WindmillClient
 from windmilltrainingv1.client.training_api_project import parse_project_name
 
 from gaea_operator.utils import read_file, format_time, write_file, DEFAULT_METRIC_FILE_NAME
 from .types.metric import BaseMetric
 from gaea_operator.metric.analysis import EvalMetricAnalysis
@@ -94,31 +94,34 @@
                        tracker_client: ExperimentTracker,
                        dataset_name: str,
                        model_object_name: str,
                        model_artifact_name: str):
     """
     Update metric file.
     """
-    logit.info(f"Model artifact name is {model_artifact_name}")
-    base_metric = BaseMetric(artifactName=model_artifact_name, datasetName=dataset_name, updatedAt=format_time())
+    bcelogger.info(f"Model artifact name is {model_artifact_name}")
+    base_metric = BaseMetric(artifactName=model_artifact_name,
+                             datasetName=dataset_name,
+                             updatedAt=format_time(),
+                             baselineJobName=tracker_client.job_name)
 
     try:
         response = windmill_client.get_artifact(object_name=model_object_name, version="best")
-        logit.info(f"Get best artifact name {model_object_name} response {response}")
+        bcelogger.info(f"Get best artifact name {model_object_name} response {response}")
         if response.name != model_artifact_name:
-            logit.info(f"Get baseline model name {response.name}")
+            bcelogger.info(f"Get baseline model name {response.name}")
             baseline_model_name = response.name
             tags = [{"artifactName": baseline_model_name}, {"datasetName": dataset_name}]
             workspace_id, project_name = parse_project_name(tracker_client.project_name)
             response = windmill_client.list_job(workspace_id=workspace_id, project_name=project_name, tags=tags)
             if len(response.result) > 0:
                 base_metric.baselineJobName = response.result[0]["name"]
-            logit.info(f"Base metric dict is {base_metric.dict()}")
+            bcelogger.info(f"Base metric dict is {base_metric.dict()}")
     except Exception as e:
-        logit.error(f"Get best artifact name {model_object_name} error {e}")
+        bcelogger.error(f"Get best artifact name {model_object_name} error {e}")
 
     metric_dir = tracker_client.job_work_dir
     metric_data = read_file(input_dir=metric_dir, file_name=DEFAULT_METRIC_FILE_NAME)
     metric_data.update(base_metric.dict())
     write_file(obj=metric_data, output_dir=metric_dir, file_name=DEFAULT_METRIC_FILE_NAME)
```

## gaea_operator/model/model.py

```diff
@@ -1,15 +1,15 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 # @Time    : 2024/3/7
 # @Author  : yanxiaodong
 # @File    : model.py
 """
-import logit
+import bcelogger
 from windmillclient.client.windmill_client import WindmillClient
 
 from gaea_operator.metric.metric import get_score_from_file
 
 
 class Model:
     """
@@ -21,22 +21,22 @@
     def get_best_model_score(self, model_name: str, metric_name: str):
         """
         Get the best score of model.
         """
         try:
             best_alias = "best"
             response = self.windmill_client.get_artifact(object_name=model_name, version=best_alias)
-            logit.info(f"Get artifact {model_name} response: {response}")
+            bcelogger.info(f"Get artifact {model_name} response: {response}")
 
             self.windmill_client.download_artifact(object_name=model_name, version=best_alias, output_uri="./")
             best_score = get_score_from_file("./metric.json", metric_name=metric_name)
             assert best_score is not None, f"Can not get score from metric, please check metric name {metric_name}."
             version = response.version
         except Exception as e:
-            logit.error(f"Get artifact {model_name} error: {e}")
+            bcelogger.error(f"Get artifact {model_name} error: {e}")
             best_score = 0
             version = None
 
         return best_score, version
 
 
 def format_name(name: str, category: str):
```

## gaea_operator/trainer/trainer.py

```diff
@@ -1,15 +1,15 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """
 # @Time    : 2024/3/1
 # @Author  : yanxiaodong
 # @File    : Trainer.py
 """
-import logit
+import bcelogger
 from typing import List
 import os
 import time
 import threading
 
 from gaea_tracker import ExperimentTracker
 
@@ -43,15 +43,15 @@
         Launch the PaddleDetection training process.
         """
         from paddle.distributed.launch.main import launch
         os.environ["FLAGS_set_to_1d"] = "False"
         try:
             launch()
         except Exception as e:
-            logit.error(f"PaddleDetection training failed: {e}")
+            bcelogger.error(f"PaddleDetection training failed: {e}")
         finally:
             self.training_exit_flag = True
             if self.thread is not None:
                 self.thread.join()
 
     def paddledet_export(self, model_dir: str):
         """
@@ -69,15 +69,15 @@
         train_config = os.path.join(model_dir, DEFAULT_TRAIN_CONFIG_FILE_NAME)
         weights = os.path.join(model_dir, os.path.splitext(DEFAULT_PADDLEPADDLE_MODEL_FILE_NAME)[0])
         export_model.main(train_config=train_config, weights=weights, save_dir=model_dir)
 
     @classmethod
     def _framework_check(cls, framework: str):
         frameworks = [cls.framework_paddlepaddle, cls.framework_pytorch]
-        logit.info(f"framework: {framework}")
+        bcelogger.info(f"framework: {framework}")
 
         assert framework in frameworks, f"framework must be one of {frameworks}, but get framework {framework}"
 
     def _track_thread(self, metric_names: List):
         last_epoch, last_step = -1, -1
         while True:
             metric_filepath = os.path.join(self.tracker_client.job_work_dir, f"{self.tracker_client.run_id}.json")
@@ -94,18 +94,18 @@
                 metric_data = read_file(input_dir=os.path.dirname(filepath), file_name=os.path.basename(filepath))
                 epoch, step = metric_data["epoch"], metric_data["step"]
                 if epoch == last_epoch and step == last_step:
                     return epoch, step
                 for name in metric_names:
                     metric = get_score_from_metric_raw(metric_data=metric_data, metric_name=name)
                     if metric is not None:
-                        logit.info(f"Track metric {name} with value: {metric} on step {step} or epoch {epoch}")
+                        bcelogger.info(f"Track metric {name} with value: {metric} on step {step} or epoch {epoch}")
                         self.tracker_client.log_metrics(metrics={name: metric}, epoch=epoch, step=step)
             except Exception as e:
-                logit.error(f"Track metric failed: {e}")
+                bcelogger.error(f"Track metric failed: {e}")
 
             return epoch, step
         return last_epoch, last_step
 
     def track_model_score(self, metric_names):
         """
         Track the score of model.
```

## gaea_operator/transform/cvt_copy_model.py

```diff
@@ -8,15 +8,15 @@
 Authors: wanggaofei03
 Date:    2023-11-08
 """
 
 import argparse
 import os
 import yaml
-import logit
+import bcelogger
 import sys
 import shutil
 
 from gaea_transform.core.engine.local_test import Runner
 
 def get_yaml(yaml_name):
     """
@@ -79,15 +79,15 @@
 
     run = Runner(src_path=src_model_path, dst_path=dst_tmp_path, cfg_path=transform_config_path)
     run.run()
     
     # 2. copy dst model
     cvt_model_folder = get_cvt_model_folder(transform_config_path)
     if cvt_model_folder is None:
-        logit.error('transform config do NOT have output parameter. {}'.format(transform_config_path))
+        bcelogger.error('transform config do NOT have output parameter. {}'.format(transform_config_path))
     else:
         copy_all_files(os.path.join(dst_tmp_path, cvt_model_folder), dst_model_path)
 
 def parse_opt():
     """ parser opt
         Args:
 
@@ -101,9 +101,9 @@
     parser.add_argument('--dst_model_path', type=str, default="/root/dst_model", help='dst model path')
 
     option = parser.parse_args()
     return option
 
 if __name__ == '__main__':
     opt = parse_opt()
-    logit.info("args: {}".format(opt))
+    bcelogger.info("args: {}".format(opt))
     cvt_copy_model(opt.transform_config_path, opt.src_model_path, opt.dst_model_path)
```

## gaea_operator/utils/accelerator.py

```diff
@@ -69,15 +69,15 @@
 class NvidiaAccelerator(Accelerator):
     """
     Nvidia Accelerator
     """
 
     def __init__(self, name: str = "T4"):
         super().__init__(name=name)
-        self.image = "iregistry.baidu-int.com/windmill-public/inference/triton_r22_12_nvidia_infer_trt86:v1.2.0"
+        self.image = "iregistry.baidu-int.com/windmill-public/inference/nvidia:v1.2.0.1"
         self.args = {"backend-config": "tensorrt,plugins=/opt/tritonserver/lib/libmmdeploy_tensorrt_ops.so"}
         self.env = \
             {
                 "LD_LIBRARY_PATH":
                     "/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/tritonserver/lib",
                 "PATH": "/opt/tritonserver/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:"
                         "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin"
```

## gaea_operator/utils/compress.py

```diff
@@ -5,15 +5,15 @@
 # @Author  : yanxiaodong
 # @File    : compress.py
 """
 import os
 from typing import List, Union, Tuple
 import shutil
 
-import logit
+import bcelogger
 
 
 def is_compressed_file(file: str):
     """
     Check if the file is compressed.
     """
     compressed_extensions = ['.zip', '.tar', '.tar.gz', '.tar.bz2', '.tar.xz', '.gz', '.bz2', '.xz']
@@ -73,10 +73,10 @@
             else:
                 for t in target:
                     if path.endswith(t):
                         file_list.append(path)
     else:
         raise ValueError(f"Path {path} is not a file or directory")
 
-    logit.info(f"Found {len(file_list)} target file is {file_list}")
+    bcelogger.info(f"Found {len(file_list)} target file is {file_list}")
 
     return file_list
```

## Comparing `gaea_operator-1.2.0.3.data/data/classify.config/modify_train_parameter.py` & `gaea_operator-1.2.0.4.data/data/classify.config/modify_train_parameter.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 Date:    2023-02-29
 """
 
 import json
 import os
 import yaml
 import argparse
-import logit
+import bcelogger
 
 from windmillmodelv1.client.model_api_model import ModelMetadata, InputSize
 
 from gaea_operator.utils import DEFAULT_META_FILE_NAME
 
 KEY_EPOCH = 'Global.epochs'
 KEY_EVAL_HEIGHT = 'eval_height'
@@ -87,19 +87,19 @@
     Returns:
         None - 无返回值，直接修改传入的yamldata参数。
     
     Raises:
         None - 该函数没有引发任何异常。
     """
     if key in yaml_data:
-        logit.info('old val. {} -> {}'.format(key, yaml_data[key]))
+        bcelogger.info('old val. {} -> {}'.format(key, yaml_data[key]))
         yaml_data[key] = val
-        logit.info('new val. {} -> {}'.format(key, yaml_data[key]))
+        bcelogger.info('new val. {} -> {}'.format(key, yaml_data[key]))
     else:
-        logit.error('do NOT find key: {}'.format(key))
+        bcelogger.error('do NOT find key: {}'.format(key))
 
 
 def get_value(yaml_data, key):
     """
     根据指定的键值获取对应的值，如果找不到则返回None。
     
     Args:
@@ -108,15 +108,15 @@
     
     Returns:
         Union[str, int, dict, list, None]: 如果找到该键值，则返回对应的值；否则返回None。
     """
     if key in yaml_data:
         return yaml_data[key]
     else:
-        logit.info('do NOT find key: {}'.format(key))
+        bcelogger.info('do NOT find key: {}'.format(key))
         return None
 
 
 def convert_value_type(val):
     """
         convert string to real data type
     """
@@ -137,36 +137,36 @@
     for i, key in enumerate(keys):
         if key in config_dict:
             if i + 1 == len(keys):
                 config_dict[key] = convert_value_type(val)
             else:
                 config_dict = config_dict[key]
         else:
-            logit.error('do NOT find key: {} of {}'.format(key, multi_key))
+            bcelogger.error('do NOT find key: {} of {}'.format(key, multi_key))
             break
 
 
 def set_epoch(yaml_data, val):
     """
     set epoch
     """
     if KEY_GLOBAL in yaml_data:
         set_multi_key_value(yaml_data, KEY_EPOCH, str(val))
 
         save_interval = get_value(yaml_data[KEY_GLOBAL], KEY_SAVE_INTERVAL)
         if save_interval is None or int(save_interval) > int(val):
             set_value(yaml_data[KEY_GLOBAL], KEY_SAVE_INTERVAL, val)
-            logit.info('modify {}: {} -> {}'.format(KEY_SAVE_INTERVAL, save_interval, val))
+            bcelogger.info('modify {}: {} -> {}'.format(KEY_SAVE_INTERVAL, save_interval, val))
 
         eval_interval = get_value(yaml_data[KEY_GLOBAL], KEY_EVAL_INTERVAL)
         if eval_interval is None or int(eval_interval) > int(val):
             set_value(yaml_data[KEY_GLOBAL], KEY_EVAL_INTERVAL, val)
-            logit.info('modify {}: {} -> {}'.format(KEY_EVAL_INTERVAL, eval_interval, val))
+            bcelogger.info('modify {}: {} -> {}'.format(KEY_EVAL_INTERVAL, eval_interval, val))
     else:
-        logit.error('do NOT find key: {}'.format(KEY_GLOBAL))
+        bcelogger.error('do NOT find key: {}'.format(KEY_GLOBAL))
 
 
 def set_target_size(yaml_data, eval_width, eval_height):
     """
     设置目标大小，如果不支持该大小则报错。
     如果配置文件中存在train reader和batch transforms，并且包含batch random resize，则修改其target size为指定的大小。
     
@@ -189,34 +189,34 @@
     if KEY_DATALOADER in yaml_data and KEY_TRAIN in yaml_data[KEY_DATALOADER] \
             and KEY_DATASET in yaml_data[KEY_DATALOADER][KEY_TRAIN] \
             and KEY_TRANSFORM_OPS in yaml_data[KEY_DATALOADER][KEY_TRAIN][KEY_DATASET]:
         for i in range(len(yaml_data[KEY_DATALOADER][KEY_TRAIN][KEY_DATASET][KEY_TRANSFORM_OPS])):
             if KEY_RESIZE_IMAGE in yaml_data[KEY_DATALOADER][KEY_TRAIN][KEY_DATASET][KEY_TRANSFORM_OPS][i]:
                 set_value(yaml_data[KEY_DATALOADER][KEY_TRAIN][KEY_DATASET][KEY_TRANSFORM_OPS][i][KEY_RESIZE_IMAGE], \
                           KEY_SIZE, wh_list)
-                logit.info('set {}.{}.{}.{} size: {}'.format(KEY_DATALOADER, KEY_TRAIN, \
+                bcelogger.info('set {}.{}.{}.{} size: {}'.format(KEY_DATALOADER, KEY_TRAIN, \
                                                              KEY_DATASET, KEY_TRANSFORM_OPS, wh_list))
 
     if KEY_EVAL in yaml_data and KEY_DATASET in yaml_data[KEY_EVAL] and KEY_TRANSFORM_OPS \
             in yaml_data[KEY_EVAL][KEY_DATASET]:
         for i in range(yaml_data[KEY_EVAL][KEY_DATASET][KEY_TRANSFORM_OPS]):
             if KEY_RESIZE_IMAGE in yaml_data[KEY_EVAL][KEY_DATASET][KEY_TRANSFORM_OPS][i]:
                 set_value(yaml_data[KEY_EVAL][KEY_DATASET][KEY_TRANSFORM_OPS][i][KEY_RESIZE_IMAGE], KEY_SIZE, wh_list)
-                logit.info('set {}.{}.{}.{} size: {}'.format(KEY_EVAL, KEY_DATASET, \
+                bcelogger.info('set {}.{}.{}.{} size: {}'.format(KEY_EVAL, KEY_DATASET, \
                                                              KEY_TRANSFORM_OPS, KEY_RESIZE_IMAGE, wh_list))
 
 
 def get_pretrained_model_name(model_type, pretrained_model_path):
     """
         get pretrained model absolute path
     """
     if model_type == KEY_RESNET:
         return os.path.join(pretrained_model_path, PRETRAINED_MODEL_NAME_RESNET)
     else:
-        logit.error('do NOT support model type. {} use default pretrained model. {}'.format(model_type,
+        bcelogger.error('do NOT support model type. {} use default pretrained model. {}'.format(model_type,
                                                                                             PRETRAINED_MODEL_NAME_RESNET))
         return os.path.join(pretrained_model_path, PRETRAINED_MODEL_NAME_RESNET)
 
 
 def get_mean_std(yaml_data):
     """
         get train mean/std
@@ -258,17 +258,17 @@
             if KEY_PRETRAINED_MODEL == key:
                 val = get_pretrained_model_name(advanced_parameters[KEY_MODEL_TYPE],
                                                 advanced_parameters[KEY_PRETRAINED_MODEL])
             set_multi_key_value(yaml_data, key, val)
             if key in shadow_parameters:
                 set_multi_key_value(yaml_data, key.replace('Train', 'Eval'), val)
 
-    logit.info('begin to save yaml. {}'.format(train_config_name))
+    bcelogger.info('begin to save yaml. {}'.format(train_config_name))
     save_yaml(yaml_data, train_config_name)
-    logit.info('write train config finish.')
+    bcelogger.info('write train config finish.')
 
 
 def parse_opt():
     """ parser opt
         Args:
 
         Returns:
@@ -290,15 +290,15 @@
     for _, v in enumerate(s):
         kv = v.split(':')
         param_dict[kv[0]] = kv[1]
     return param_dict
 
 
 if __name__ == "__main__":
-    from logit.base_logger import setup_logger
+    from bcelogger.base_logger import setup_logger
 
     opt = parse_opt()
     setup_logger(config=dict(file_name=os.path.join('/ssd2/lyg/GAEA-PIPE/resnet18', "worker.log")))
     # labels: [{'name':, 'id': }, ...]
     labels = json.load(open('/ssd2/lyg/Dataset/zhiguan_clas/labels.json', "r"))
     advanced_parameters = {'Global.epochs': '1', 'Optimizer.lr.learning_rate': '0.001', \
                            'DataLoader.Train.loader.num_workers': '2', 'eval_height': '256', 'eval_width': '256', \
```

## Comparing `gaea_operator-1.2.0.3.data/data/classify.config/parameter.yaml` & `gaea_operator-1.2.0.4.data/data/classify.config/parameter.yaml`

 * *Files identical despite different names*

## Comparing `gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/modify_train_parameter.py` & `gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/modify_train_parameter.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 Authors: wanggaofei(wanggaofei03@baidu.com)
 Date:    2023-02-29
 """
 import os
 import yaml
 import argparse
 
-import logit
+import bcelogger
 
 KEY_EPOCH = 'epoch'
 KEY_EVAL_HEIGHT = 'eval_height'
 KEY_EVAL_WIDTH = 'eval_width'
 KEY_EVAL_SIZE = 'eval_size'
 KEY_PRETRAIN_WEIGHTS = 'pretrain_weights'
 KEY_DEPTH_MULT = 'depth_mult'
@@ -95,19 +95,19 @@
     Returns:
         None - 无返回值，直接修改传入的yamldata参数。
     
     Raises:
         None - 该函数没有引发任何异常。
     """
     if key in yaml_data:
-        logit.info('old val. {} -> {}'.format(key, yaml_data[key]))
+        bcelogger.info('old val. {} -> {}'.format(key, yaml_data[key]))
         yaml_data[key] = val
-        logit.info('new val. {} -> {}'.format(key, yaml_data[key]))
+        bcelogger.info('new val. {} -> {}'.format(key, yaml_data[key]))
     else:
-        logit.error('do NOT find key: {}'.format(key))
+        bcelogger.error('do NOT find key: {}'.format(key))
 
 
 def get_value(yaml_data, key):
     """
     根据指定的键值获取对应的值，如果找不到则返回None。
     
     Args:
@@ -116,15 +116,15 @@
     
     Returns:
         Union[str, int, dict, list, None]: 如果找到该键值，则返回对应的值；否则返回None。
     """
     if key in yaml_data:
         return yaml_data[key]
     else:
-        logit.info('do NOT find key: {}'.format(key))
+        bcelogger.info('do NOT find key: {}'.format(key))
         return None
 
 
 def process_var(line, var_name, val):
     """
     处理变量
     """
@@ -183,15 +183,15 @@
     if snapshot_epoch is not None and int(snapshot_epoch) > val:
         set_value(yaml_data, KEY_SNAPSHOT_EPOCH, val)
     if KEY_LEARNING_RATE in yaml_data and KEY_SCHEDULERS in yaml_data[KEY_LEARNING_RATE]:
         for i in range(len(yaml_data[KEY_LEARNING_RATE][KEY_SCHEDULERS])):
             if KEY_MAX_EPOCHS in yaml_data[KEY_LEARNING_RATE][KEY_SCHEDULERS][i] and \
                     int(yaml_data[KEY_LEARNING_RATE][KEY_SCHEDULERS][i][KEY_MAX_EPOCHS]) > val:
                 yaml_data[KEY_LEARNING_RATE][KEY_SCHEDULERS][i][KEY_MAX_EPOCHS] = val
-                logit.info('set-epoch modify max_epochs. {}'.format(val))
+                bcelogger.info('set-epoch modify max_epochs. {}'.format(val))
 
 
 def convert_value_type(val):
     """
         convert string to real data type
     """
     if val.isdigit():
@@ -211,15 +211,15 @@
     for i, key in enumerate(keys):
         if key in config_dict:
             if i + 1 == len(keys):
                 config_dict[key] = convert_value_type(val)
             else:
                 config_dict = config_dict[key]
         else:
-            logit.error('do NOT find key: {} of {}'.format(key, multi_key))
+            bcelogger.error('do NOT find key: {} of {}'.format(key, multi_key))
             break
 
 
 def height_width_str2list(height_width_str):
     """
     字符串转列表
     """
@@ -233,15 +233,15 @@
             for i in range(len(s) // 2):
                 size_list.append([int(s[i * 2]), int(s[i * 2 + 1])])
         else:
             ws = s.split(',')
             for _, v in enumerate(ws):
                 size_list.append(int(v))
     else:
-        logit.error('invalid height-width-string. {}'.format(height_width_str))
+        bcelogger.error('invalid height-width-string. {}'.format(height_width_str))
     return size_list
 
 
 def set_target_size(yaml_data, eval_width, eval_height):
     """
     设置目标大小，如果不支持该大小则报错。
     如果配置文件中存在train reader和batch transforms，并且包含batch random resize，则修改其target size为指定的大小。
@@ -255,35 +255,35 @@
         None.
     
     Raises:
         ValueError: 如果不支持指定的目标大小。
     """
     eval_wh_str = str(eval_width) + ',' + str(eval_height)
     if eval_wh_str not in TARGET_SIZE_MAP:
-        logit.error('do NOT support target size. width: {}, height: {}'.format(eval_width, eval_height))
+        bcelogger.error('do NOT support target size. width: {}, height: {}'.format(eval_width, eval_height))
     else:
         if KEY_TRAIN_READER in yaml_data and KEY_BATCH_TRANSFORMS in yaml_data[KEY_TRAIN_READER]:
             for _, v in enumerate(yaml_data[KEY_TRAIN_READER][KEY_BATCH_TRANSFORMS]):
                 if KEY_BATCH_RANDOM_RESIZE in v and KEY_TARGET_SIZE in v[KEY_BATCH_RANDOM_RESIZE]:
                     v[KEY_BATCH_RANDOM_RESIZE][KEY_TARGET_SIZE] = height_width_str2list(TARGET_SIZE_MAP[eval_wh_str])
-                    logit.info('set target size: {}'.format(eval_wh_str))
+                    bcelogger.info('set target size: {}'.format(eval_wh_str))
 
 
 def set_model_type(yaml_data, val, pretrain_model_uri):
     """
     设置模型类型
     """
     model_type = val.strip().split('_')[-1]
     if model_type in PRETRAIN_MODEL_NAMES:
         name, depth_mult, width_mult = PRETRAIN_MODEL_NAMES[model_type]
         set_value(yaml_data, KEY_PRETRAIN_WEIGHTS, os.path.join(pretrain_model_uri, name))
         set_value(yaml_data, KEY_DEPTH_MULT, depth_mult)
         set_value(yaml_data, KEY_WIDTH_MULT, width_mult)
     else:
-        logit.error('do NOT known model type value. {}'.format(val))
+        bcelogger.error('do NOT known model type value. {}'.format(val))
 
 
 def generate_train_config(
         advanced_parameters: dict,
         pretrain_model_uri: str,
         train_config_name: str
 ):
@@ -296,15 +296,15 @@
     var_name_vals = [[KEY_EVAL_WIDTH, width], [KEY_EVAL_HEIGHT, height]]
 
     if advanced_parameters[KEY_MODEL_TYPE].startswith('change-'):
         input_yaml_name = 'parameter_c.yaml'
     else:
         input_yaml_name = 'parameter.yaml'
 
-    logit.info('train parameter name: {}'.format(input_yaml_name))
+    bcelogger.info('train parameter name: {}'.format(input_yaml_name))
 
     input_yaml_name = os.path.join(os.path.dirname(os.path.abspath(__file__)), input_yaml_name)
     yaml_data = modify_var_value(input_yaml_name, var_name_vals)
 
     # 1. set correlative parameters
     correlative_parameter_keys = [KEY_EVAL_WIDTH, KEY_EVAL_HEIGHT, KEY_EPOCH, KEY_MODEL_TYPE]
     # 1.1 target size
@@ -317,17 +317,17 @@
     set_model_type(yaml_data, advanced_parameters[KEY_MODEL_TYPE], pretrain_model_uri)
 
     # 2. set get-though parameters by key-value of dict
     for key, val in advanced_parameters.items():
         if key not in correlative_parameter_keys:
             set_multi_key_value(yaml_data, key, val)
 
-    logit.info('begin to save yaml. {}'.format(train_config_name))
+    bcelogger.info('begin to save yaml. {}'.format(train_config_name))
     save_yaml(yaml_data, train_config_name)
-    logit.info('write train config finish.')
+    bcelogger.info('write train config finish.')
 
 
 def parse_opt():
     """ parser opt
         Args:
 
         Returns:
@@ -351,10 +351,10 @@
         kv = v.split(':')
         param_dict[kv[0]] = kv[1]
     return param_dict
 
 
 if __name__ == "__main__":
     opt = parse_opt()
-    logit.info("args: {}".format(opt))
+    bcelogger.info("args: {}".format(opt))
 
     generate_train_config(str2dict(opt.advanced_parameters), opt.pretrain_model_uri, opt.train_config_name)
```

## Comparing `gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/parameter.yaml` & `gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/parameter.yaml`

 * *Files identical despite different names*

## Comparing `gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/parameter_c.yaml` & `gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/parameter_c.yaml`

 * *Files identical despite different names*

## Comparing `gaea_operator-1.2.0.3.data/data/schema/object_detection.yaml` & `gaea_operator-1.2.0.4.data/data/schema/object_detection.yaml`

 * *Files identical despite different names*

## Comparing `gaea_operator-1.2.0.3.dist-info/METADATA` & `gaea_operator-1.2.0.4.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: gaea-operator
-Version: 1.2.0.3
+Version: 1.2.0.4
 Summary: A common operator library to help with training neural networks.
 Home-page: https://console.cloud.baidu-int.com/devops/icode/repos/baidu/mlops/gaea-operator/tree/master
 Author: liuyawen03
 Author-email: liuyawen03@baidu.com
 License: MIT
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
```

## Comparing `gaea_operator-1.2.0.3.dist-info/RECORD` & `gaea_operator-1.2.0.4.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,47 +1,47 @@
 gaea_operator/__init__.py,sha256=ByRuPwRErAhcYsPdR2RnVWtm35lq8yW-pzKVQkMlzIU,131
 gaea_operator/components/__init__.py,sha256=ByRuPwRErAhcYsPdR2RnVWtm35lq8yW-pzKVQkMlzIU,131
 gaea_operator/components/eval/__init__.py,sha256=I_nhvM_IU3swPZsE4m9lKQj50PABhRHZTVkiDjYZK5s,131
-gaea_operator/components/eval/ppyoloe_plus.py,sha256=5v6UXmxJB9x6l0bDi87utSjWAqbFe2Eat5ioERN4ao0,4104
-gaea_operator/components/eval/resnet.py,sha256=jNYmUCb3nf1FvdhXVZBq34lZdjPFUXBAIlzywYtTytc,4149
+gaea_operator/components/eval/ppyoloe_plus.py,sha256=IXw89UF6Dp4gzAuhL0jHZZmmQGHFDpk_rhIyo7g9ygQ,4268
+gaea_operator/components/eval/resnet.py,sha256=ZxVF6RtFbqOEL0sM1sIMFOCbTIRZD3FQm2k43otT2ak,4153
 gaea_operator/components/inference/__init__.py,sha256=_QcFYjp6eMXnpCXbbg-3O_12BHu-ckouK_d3_TkT2WA,2956
-gaea_operator/components/inference/inference.py,sha256=di7P9xRry3TT3plAGUchlGNvhtUHrYKltO_7C_Kdl70,4022
-gaea_operator/components/package/__init__.py,sha256=OFXvsMi3LOnTtg71LIamlQJJlk0e9bheEttwVp0w33k,3524
-gaea_operator/components/package/package.py,sha256=ekx43H0a7m0X5xhj_-aOhUsvWA7UV9N5K5y-_19-1K8,8066
+gaea_operator/components/inference/inference.py,sha256=NCZQqv4r-V8oNicecXGh8mgLaZIILcjV7VZa7NXO0jA,4034
+gaea_operator/components/package/__init__.py,sha256=jv6Cv1AhP1mhCbZ8D509u2Mhf_C1Deo18VA8-h5jExs,3555
+gaea_operator/components/package/package.py,sha256=H2K3k-SOCpueTq1F9vGf6MsDcNUEP_O3xbeECWmdbJQ,8375
 gaea_operator/components/train/__init__.py,sha256=I_nhvM_IU3swPZsE4m9lKQj50PABhRHZTVkiDjYZK5s,131
-gaea_operator/components/train/ppyoloe_plus.py,sha256=5MNCjEEOLrdLqtDLDU5t5pwHrF1zOYbNp1cUry9fVuk,8345
-gaea_operator/components/train/resnet.py,sha256=Us5vzM9toKnP7xnvBgqB2toUZMnBX7QVaWNnWHtx4l0,7370
+gaea_operator/components/train/ppyoloe_plus.py,sha256=pJ7XuhRhpYQnL1IRkwzxrtWYI8-EgS8LnwZY6a9w0XU,8365
+gaea_operator/components/train/resnet.py,sha256=1AsZtSTZyCA965Dasw9Ti9VHK_55mO1BRSyBE7MAHNc,7390
 gaea_operator/components/transform/__init__.py,sha256=I_nhvM_IU3swPZsE4m9lKQj50PABhRHZTVkiDjYZK5s,131
-gaea_operator/components/transform/ppyoloe_plus.py,sha256=wIdfgRmiGIyQHAxagtFyldGHBZDnjkJLjHsD96wHTOg,6066
-gaea_operator/components/transform/resnet.py,sha256=-QLMKjUv8Ujn2qIBUUza3rc3asvMW5ghYP-watWBTSY,6202
-gaea_operator/components/transform_eval/__init__.py,sha256=54S1xMo1e9BuTl5n3YsLgEutlXC82CFHf2e0gq9OAQ0,3323
-gaea_operator/components/transform_eval/transform_eval.py,sha256=JXQsEI-bJkghYm7ka7R8p1GSJN4GQ5QraEg1HlDwR30,7486
+gaea_operator/components/transform/ppyoloe_plus.py,sha256=WzwLtmTyvhd6iINlmXoh1zEX20RQahB2v9qZT3PsmU0,6078
+gaea_operator/components/transform/resnet.py,sha256=-FHg9_M5Sdt46U6HxfjSqHOUFA8jc6c0O6qgg7fk7aM,6214
+gaea_operator/components/transform_eval/__init__.py,sha256=tdT6-SVn3WI4TMpSTEjVhGJFu43_954ZRTt2snqBxKA,3559
+gaea_operator/components/transform_eval/transform_eval.py,sha256=KWuX1Rd9MzBORTnjjsgD2M3XOt4w4PGA8bQKwn4PcME,7658
 gaea_operator/config/__init__.py,sha256=RfkKuJH4iakBoz89Ni4KQKiuHeOg_XGTGuI-1-VVWA4,411
 gaea_operator/config/config.py,sha256=npxGcoNgTIU-pKnWpgqZBWLfWZm3GiYNTqhY3zeG_p0,4883
-gaea_operator/config/generate_transform_config.py,sha256=8TCe00iJ6VpbYGBojAHNm70qmG_KDEd0pN-iGa8VtJ0,13881
-gaea_operator/config/modify_package_files.py,sha256=O2MIqKE9d1T800IKBVmh27z2wF7g97ofZ38tiPZEDDM,11921
-gaea_operator/config/update_parse.py,sha256=OKeD26EX3MXRd0SvTQ3LqWItUyuWz5xOzdRzZ8AMLgw,3626
-gaea_operator/config/update_pbtxt.py,sha256=0wfVNF1dPv9IDfX9c_iQXTOlhWkNaGXw8yd__FOr8zk,16689
+gaea_operator/config/generate_transform_config.py,sha256=1IYQEGdgQNr0GG4h6jzS1qpH2WVkIuq-EHimJ-rrkck,14048
+gaea_operator/config/modify_package_files.py,sha256=9ggnaCxWQwiCzRUZ5LPzgDbyPSXuBh0wzLlEDswK9vA,11878
+gaea_operator/config/update_parse.py,sha256=JrqELK2dgIWCrtVz8gI6A6rH_5QbqZOHDNCDujnMqxs,3642
+gaea_operator/config/update_pbtxt.py,sha256=w3A5CRbnK1B4vKyUA8Qk2EDWLAqatM3ZkOw9Jxj1v9Q,16721
 gaea_operator/config/ppyoloe_plus/__init__.py,sha256=qIyiMMEuckOcc1g1DuA94Pa6HIwl9HHRcHbWtVeHPXc,130
 gaea_operator/config/ppyoloe_plus/ppyoloeplus_config.py,sha256=t1jN8ti8hpMM-l1U2Ukl8AS8xZsyKQh2pE5wNE1ew2U,4058
 gaea_operator/config/ppyoloe_plus/template/__init__.py,sha256=lX4deGvIgAQ1fTcskBUDqRfnOlOabgQjZXGwOJCVCkU,130
-gaea_operator/config/ppyoloe_plus/template/modify_train_parameter.py,sha256=Ghv9GRLa0c7xivLZqFk0A-42LzAYDLcRB8n7MjzAGb8,12561
+gaea_operator/config/ppyoloe_plus/template/modify_train_parameter.py,sha256=BePbG16yrxAJiwctUbc0s1VtKDDThsDtylndcIO8WPk,12621
 gaea_operator/config/ppyoloe_plus/template/parameter.yaml,sha256=pDZpMs6fsuVqwGIEv0T1oaasF3FQW_Rs0sneuoN54I0,4582
 gaea_operator/config/ppyoloe_plus/template/parameter_c.yaml,sha256=KVxZIotf24fk_SAI1v-CHNMC2ouG_lpMY0l_CtSXoRM,4666
 gaea_operator/config/resnet/__init__.py,sha256=qIyiMMEuckOcc1g1DuA94Pa6HIwl9HHRcHbWtVeHPXc,130
 gaea_operator/config/resnet/resnet_config.py,sha256=oJcrEEsIOe8waeijGNit9B215bNULXYw_xO9AIn6k9U,4283
-gaea_operator/config/resnet/template/modify_train_parameter.py,sha256=-NSpwxfCLtjPoH1MaWtdvdTyuTCQqfoic3grYBEIfI0,11330
+gaea_operator/config/resnet/template/modify_train_parameter.py,sha256=gjV6zhJQbQajBHyMa1YsNyJS4cfXJCWFXB7wf1z7jSQ,11390
 gaea_operator/config/resnet/template/parameter.yaml,sha256=aojj7ToIuE1KgAdxVtegP6fZqHylzdTUGMFoUwMq3h8,2315
 gaea_operator/dataset/__init__.py,sha256=2974_da8vxeY2jrbx1Z9HbP9NfihokTo-Terscm2pU8,260
-gaea_operator/dataset/cityscape_dataset.py,sha256=_WAEDb-eqMrDZ9HIWUh-62Z5njOB0p3JO6-Im33A0Xs,3924
-gaea_operator/dataset/coco_dataset.py,sha256=FR-hbBUBKt-O5G6cwNj02QUKMdazuLaeSlpetBBPQlw,4870
-gaea_operator/dataset/dataset.py,sha256=1uYlOu5DHrTnFhTVYZ3k8il3eV3r7R785cMOFVIiUdQ,4835
-gaea_operator/dataset/imagenet_dataset.py,sha256=VYo4hUo2r0EMdWqAPYugrxa-lKXXk2gaY3cKE2EhxvM,4095
+gaea_operator/dataset/cityscape_dataset.py,sha256=GklTQhBEzawUPurapS0cW389setw-zMQawJz9O7bdFw,3969
+gaea_operator/dataset/coco_dataset.py,sha256=C3-EQyHG9sRlrzGVEWJ-4D8-wz6bArhx5yF6w9bc7vA,5061
+gaea_operator/dataset/dataset.py,sha256=0CiSrlV-Bn88RfsGAEE2nQB274d4mX7kCE6UTBXKbOY,4855
+gaea_operator/dataset/imagenet_dataset.py,sha256=WiMIqweiEJ9WFo7FlcBbZe5gLXDkP88jSSwo8coGix0,3912
 gaea_operator/metric/__init__.py,sha256=fk0G_Cw3od-yzlYq5l48xDslTmqLFddrWoXOi3xqI48,584
-gaea_operator/metric/metric.py,sha256=FfBc5hZAjH1ZnQOn1tgnhY6iw4-KThE4jGIRzXecQis,5461
+gaea_operator/metric/metric.py,sha256=D_tp55AefLCqO0Ny0Zqt04ynn0rgRUjH9mJMN2GI9nQ,5613
 gaea_operator/metric/analysis/__init__.py,sha256=kEv9_MuQOFC83fCprxY19T7omFNXa9dAuxstXlsEsX8,433
 gaea_operator/metric/analysis/eval_metric_analysis.py,sha256=V8sTAs0LToO2oKdj2C08NIovP2RBrT0iNTU7KZNC8-E,14048
 gaea_operator/metric/analysis/inference_metric_analysis.py,sha256=aVFvGPr6i3tEn0mc1_KRQMpXLJ-cpxyGTO6po6NXl2I,5987
 gaea_operator/metric/analysis/label_statistics_metric_analysis.py,sha256=x7QlkLBWxIWkiv8ufVKFdrawUY-PXmje4LSbx6pTknY,8902
 gaea_operator/metric/operator/__init__.py,sha256=WwFRIAWpgRwzlu1O5rRQIqevbrDtQ3kB1yE6q0aWSDE,737
 gaea_operator/metric/operator/check.py,sha256=CTaAvD4VDpllUp3d7_1HD-HP_b3beuh-HyMwM2V6u4o,3195
 gaea_operator/metric/operator/metric.py,sha256=ia3NY6twXmFgr_ULfaudIcbmIqyJJwC51pakJdpvRhc,1923
@@ -59,34 +59,34 @@
 gaea_operator/metric/schema/object_detection.yaml,sha256=2V6AJBVa1nAE9FAosyz4z4Qv0ebw1IumYoRPUWo3Ebg,4864
 gaea_operator/metric/types/__init__.py,sha256=x5whaLpvnYR6XL2VB8IbYJ12jtYW7_zUnWks_JZY1A8,131
 gaea_operator/metric/types/image_classification_metric.py,sha256=Q-XVRRogS43yhuN0MHQY6viu-k9_r6ADK_Df2tBjPkU,1173
 gaea_operator/metric/types/metric.py,sha256=cG9oavFI6m1BoPXgP2KR5At7hCnau14fsJUqYRa8AKM,4874
 gaea_operator/metric/types/object_detection_metric.py,sha256=fS5aJZ8lHxRAcs69bfgVlvRijsbAAjuWSg2ZtuVtKA0,2362
 gaea_operator/metric/types/semantic_segmentation_metric.py,sha256=RVOUp2x8QGKwf3EepvBR-yaLvEj0OF6uM0aC0Ah-ASQ,1244
 gaea_operator/model/__init__.py,sha256=EssP2HXBMWO-4iXPbWjscBrXxSojJBOOHV3p-ioV6tA,214
-gaea_operator/model/model.py,sha256=oBjFdjFoQXqkEULaIaik0d1niVeGn7MtaKRaxIBd2ew,1424
+gaea_operator/model/model.py,sha256=pNswhABsGB5HHeExA4fOHH4ob2uRPqs97sCzRGPQwIo,1436
 gaea_operator/trainer/__init__.py,sha256=Kpisv4LZyJQy6vEce-8g3HS8bo8y6rwk8K3pEaF0na0,181
-gaea_operator/trainer/trainer.py,sha256=TWNMNqU87kiOCer-pdsYClZxbMpU8nG2XjlTEioEmCc,4310
+gaea_operator/trainer/trainer.py,sha256=Qvzuo7hFRdPGr6uXSyy0AzNc_ESmvgerKoaJu0cOeS0,4330
 gaea_operator/transform/__init__.py,sha256=1NtMabt2_F9fVeLuAM7g3i0yIac_4RP0lHMZzdyDDcY,187
-gaea_operator/transform/cvt_copy_model.py,sha256=dHud2yAeA4Ri3KFqT8VLbI1rGMEEVzYFIJ81lZ4TqAs,3295
+gaea_operator/transform/cvt_copy_model.py,sha256=jSWWgt2RT9ULPLNyOxypmpNuQ1kWH0gw2XNe6OLE7Sw,3307
 gaea_operator/transform/transform.py,sha256=kDZ4m4QUPbuDYSwT6lKFH7T0GEADNL3f6T-L4IUjG0g,776
 gaea_operator/utils/__init__.py,sha256=DmxN4SMbqaCs_ZHTJBF9eMitaXKLIi1Qm6TKHT_NY0Q,1788
-gaea_operator/utils/accelerator.py,sha256=Pi8mlhZviOvN-TmSJFbl66bCdPB8gncG3Gw6dC3E_Q8,5077
-gaea_operator/utils/compress.py,sha256=YLszQdYn5S4n2iuGai0j8BF0bNrFW6lNsA7AVvHJ6OY,2709
+gaea_operator/utils/accelerator.py,sha256=eR5pbm_fC7Q-Z_N8pAH9Vz4pkkNNWUeoH_K3B17R1Ik,5053
+gaea_operator/utils/compress.py,sha256=iVH0usAYQidWYT2Mo8a6uJa5uGl8knwSn64CYmfkIss,2717
 gaea_operator/utils/consts.py,sha256=0TsyISenFn_x8eRNIH1fdXPLw8zSac-IAfiCxKbhySw,426
 gaea_operator/utils/file.py,sha256=9g9fFq8aw4t0I8D3wkN9Bao3tfaN3X3itZmO4LF6MsU,1948
 gaea_operator/utils/import_module.py,sha256=W1mKFqBClRsYm9MNqParN-03PUFIwgMJJAgSAvD9m38,1533
 gaea_operator/utils/model_template.py,sha256=CsmItvK3IUhTqFOmmn2gVfh6xg5jWEpJAoRK3NWrsXM,4919
 gaea_operator/utils/registry.py,sha256=QchfsCrwhk3i8gmtS16PAF4029TcdOdEeWUdo1ruAps,1680
 gaea_operator/utils/tensor.py,sha256=BLnnyKz79hOqZy-L7IeA4ffDX5DoWP9NvLzStdI6C8w,1000
 gaea_operator/utils/time.py,sha256=xNKc-rzCCwhh-hNlNO05J_TA6rCMKMepuZ3qzvcScLs,301
-gaea_operator-1.2.0.3.data/data/classify.config/modify_train_parameter.py,sha256=-NSpwxfCLtjPoH1MaWtdvdTyuTCQqfoic3grYBEIfI0,11330
-gaea_operator-1.2.0.3.data/data/classify.config/parameter.yaml,sha256=aojj7ToIuE1KgAdxVtegP6fZqHylzdTUGMFoUwMq3h8,2315
-gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/__init__.py,sha256=lX4deGvIgAQ1fTcskBUDqRfnOlOabgQjZXGwOJCVCkU,130
-gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/modify_train_parameter.py,sha256=Ghv9GRLa0c7xivLZqFk0A-42LzAYDLcRB8n7MjzAGb8,12561
-gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/parameter.yaml,sha256=pDZpMs6fsuVqwGIEv0T1oaasF3FQW_Rs0sneuoN54I0,4582
-gaea_operator-1.2.0.3.data/data/ppyoloe_plus.config/parameter_c.yaml,sha256=KVxZIotf24fk_SAI1v-CHNMC2ouG_lpMY0l_CtSXoRM,4666
-gaea_operator-1.2.0.3.data/data/schema/object_detection.yaml,sha256=2V6AJBVa1nAE9FAosyz4z4Qv0ebw1IumYoRPUWo3Ebg,4864
-gaea_operator-1.2.0.3.dist-info/METADATA,sha256=rOVw588xrNoJjSo6j4oDAXhdfx3J8gCwP1TJZ9JNc_o,2091
-gaea_operator-1.2.0.3.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-gaea_operator-1.2.0.3.dist-info/top_level.txt,sha256=1-ONMzqexKnQ2qOurelrROodwfO1B8jTzpzbERvNycY,14
-gaea_operator-1.2.0.3.dist-info/RECORD,,
+gaea_operator-1.2.0.4.data/data/classify.config/modify_train_parameter.py,sha256=gjV6zhJQbQajBHyMa1YsNyJS4cfXJCWFXB7wf1z7jSQ,11390
+gaea_operator-1.2.0.4.data/data/classify.config/parameter.yaml,sha256=aojj7ToIuE1KgAdxVtegP6fZqHylzdTUGMFoUwMq3h8,2315
+gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/__init__.py,sha256=lX4deGvIgAQ1fTcskBUDqRfnOlOabgQjZXGwOJCVCkU,130
+gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/modify_train_parameter.py,sha256=BePbG16yrxAJiwctUbc0s1VtKDDThsDtylndcIO8WPk,12621
+gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/parameter.yaml,sha256=pDZpMs6fsuVqwGIEv0T1oaasF3FQW_Rs0sneuoN54I0,4582
+gaea_operator-1.2.0.4.data/data/ppyoloe_plus.config/parameter_c.yaml,sha256=KVxZIotf24fk_SAI1v-CHNMC2ouG_lpMY0l_CtSXoRM,4666
+gaea_operator-1.2.0.4.data/data/schema/object_detection.yaml,sha256=2V6AJBVa1nAE9FAosyz4z4Qv0ebw1IumYoRPUWo3Ebg,4864
+gaea_operator-1.2.0.4.dist-info/METADATA,sha256=hGFEkz3Vgmw-QcMTGlymVtWu_iqA-DUkKDIYh6BFK44,2091
+gaea_operator-1.2.0.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+gaea_operator-1.2.0.4.dist-info/top_level.txt,sha256=1-ONMzqexKnQ2qOurelrROodwfO1B8jTzpzbERvNycY,14
+gaea_operator-1.2.0.4.dist-info/RECORD,,
```

