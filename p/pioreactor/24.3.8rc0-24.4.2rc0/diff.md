# Comparing `tmp/pioreactor-24.3.8rc0-py3-none-any.whl.zip` & `tmp/pioreactor-24.4.2rc0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,89 +1,90 @@
-Zip file size: 227039 bytes, number of entries: 87
--rw-r--r--  2.0 unx      117 b- defN 24-Mar-08 21:26 pioreactor/__init__.py
--rw-r--r--  2.0 unx     6785 b- defN 24-Mar-08 21:26 pioreactor/config.py
--rw-r--r--  2.0 unx      265 b- defN 24-Mar-08 21:26 pioreactor/error_codes.py
--rw-r--r--  2.0 unx      443 b- defN 24-Mar-08 21:26 pioreactor/exc.py
--rw-r--r--  2.0 unx     3854 b- defN 24-Mar-08 21:26 pioreactor/hardware.py
--rw-r--r--  2.0 unx     6633 b- defN 24-Mar-08 21:26 pioreactor/logging.py
--rw-r--r--  2.0 unx    15532 b- defN 24-Mar-08 21:26 pioreactor/mureq.py
--rw-r--r--  2.0 unx    12004 b- defN 24-Mar-08 21:26 pioreactor/pubsub.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-08 21:26 pioreactor/py.typed
--rw-r--r--  2.0 unx     6386 b- defN 24-Mar-08 21:26 pioreactor/structs.py
--rw-r--r--  2.0 unx     3304 b- defN 24-Mar-08 21:26 pioreactor/types.py
--rw-r--r--  2.0 unx     2922 b- defN 24-Mar-08 21:26 pioreactor/version.py
--rw-r--r--  2.0 unx     4305 b- defN 24-Mar-08 21:26 pioreactor/whoami.py
--rw-r--r--  2.0 unx      540 b- defN 24-Mar-08 21:26 pioreactor/actions/__init__.py
--rw-r--r--  2.0 unx     8851 b- defN 24-Mar-08 21:26 pioreactor/actions/led_intensity.py
--rw-r--r--  2.0 unx     8967 b- defN 24-Mar-08 21:26 pioreactor/actions/od_blank.py
--rw-r--r--  2.0 unx    24380 b- defN 24-Mar-08 21:26 pioreactor/actions/od_calibration.py
--rw-r--r--  2.0 unx    19734 b- defN 24-Mar-08 21:26 pioreactor/actions/pump.py
--rw-r--r--  2.0 unx    22449 b- defN 24-Mar-08 21:26 pioreactor/actions/pump_calibration.py
--rw-r--r--  2.0 unx    20194 b- defN 24-Mar-08 21:26 pioreactor/actions/self_test.py
--rw-r--r--  2.0 unx     5493 b- defN 24-Mar-08 21:26 pioreactor/actions/stirring_calibration.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-08 21:26 pioreactor/actions/leader/__init__.py
--rw-r--r--  2.0 unx     4897 b- defN 24-Mar-08 21:26 pioreactor/actions/leader/backup_database.py
--rw-r--r--  2.0 unx    22875 b- defN 24-Mar-08 21:26 pioreactor/actions/leader/experiment_profile.py
--rw-r--r--  2.0 unx     6582 b- defN 24-Mar-08 21:26 pioreactor/actions/leader/export_experiment_data.py
--rw-r--r--  2.0 unx      146 b- defN 24-Mar-08 21:26 pioreactor/automations/__init__.py
--rw-r--r--  2.0 unx     1000 b- defN 24-Mar-08 21:26 pioreactor/automations/base.py
--rw-r--r--  2.0 unx      459 b- defN 24-Mar-08 21:26 pioreactor/automations/dosing/__init__.py
--rw-r--r--  2.0 unx    29392 b- defN 24-Mar-08 21:26 pioreactor/automations/dosing/base.py
--rw-r--r--  2.0 unx     1404 b- defN 24-Mar-08 21:26 pioreactor/automations/dosing/chemostat.py
--rw-r--r--  2.0 unx     1497 b- defN 24-Mar-08 21:26 pioreactor/automations/dosing/fed_batch.py
--rw-r--r--  2.0 unx     4962 b- defN 24-Mar-08 21:26 pioreactor/automations/dosing/pid_morbidostat.py
--rw-r--r--  2.0 unx      476 b- defN 24-Mar-08 21:26 pioreactor/automations/dosing/silent.py
--rw-r--r--  2.0 unx     4613 b- defN 24-Mar-08 21:26 pioreactor/automations/dosing/turbidostat.py
--rw-r--r--  2.0 unx      510 b- defN 24-Mar-08 21:26 pioreactor/automations/events/__init__.py
--rw-r--r--  2.0 unx      567 b- defN 24-Mar-08 21:26 pioreactor/automations/led/__init__.py
--rw-r--r--  2.0 unx    12023 b- defN 24-Mar-08 21:26 pioreactor/automations/led/base.py
--rw-r--r--  2.0 unx     3875 b- defN 24-Mar-08 21:26 pioreactor/automations/led/light_dark_cycle.py
--rw-r--r--  2.0 unx      154 b- defN 24-Mar-08 21:26 pioreactor/automations/temperature/__init__.py
--rw-r--r--  2.0 unx     9317 b- defN 24-Mar-08 21:26 pioreactor/automations/temperature/base.py
--rw-r--r--  2.0 unx      561 b- defN 24-Mar-08 21:26 pioreactor/automations/temperature/only_record_temperature.py
--rw-r--r--  2.0 unx     4235 b- defN 24-Mar-08 21:26 pioreactor/automations/temperature/thermostat.py
--rw-r--r--  2.0 unx      715 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/__init__.py
--rw-r--r--  2.0 unx    44846 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/base.py
--rw-r--r--  2.0 unx     7081 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/dosing_control.py
--rw-r--r--  2.0 unx    22044 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/growth_rate_calculating.py
--rw-r--r--  2.0 unx     5814 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/led_control.py
--rw-r--r--  2.0 unx    29180 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/monitor.py
--rw-r--r--  2.0 unx    51941 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/od_reading.py
--rw-r--r--  2.0 unx    17963 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/stirring.py
--rw-r--r--  2.0 unx    24229 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/temperature_control.py
--rw-r--r--  2.0 unx      605 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/leader/__init__.py
--rw-r--r--  2.0 unx    16897 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/leader/mqtt_to_db_streaming.py
--rw-r--r--  2.0 unx     4345 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/leader/watchdog.py
--rw-r--r--  2.0 unx     1094 b- defN 24-Mar-08 21:26 pioreactor/background_jobs/subjobs/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-08 21:26 pioreactor/cli/__init__.py
--rw-r--r--  2.0 unx    34487 b- defN 24-Mar-08 21:26 pioreactor/cli/pio.py
--rw-r--r--  2.0 unx    26942 b- defN 24-Mar-08 21:26 pioreactor/cli/pios.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-08 21:26 pioreactor/experiment_profiles/__init__.py
--rw-r--r--  2.0 unx     5306 b- defN 24-Mar-08 21:26 pioreactor/experiment_profiles/parser.py
--rw-r--r--  2.0 unx     2952 b- defN 24-Mar-08 21:26 pioreactor/experiment_profiles/profile_struct.py
--rw-r--r--  2.0 unx      197 b- defN 24-Mar-08 21:26 pioreactor/experiment_profiles/sly/__init__.py
--rw-r--r--  2.0 unx    16273 b- defN 24-Mar-08 21:26 pioreactor/experiment_profiles/sly/lex.py
--rw-r--r--  2.0 unx    83015 b- defN 24-Mar-08 21:26 pioreactor/experiment_profiles/sly/yacc.py
--rw-r--r--  2.0 unx     3807 b- defN 24-Mar-08 21:26 pioreactor/plugin_management/__init__.py
--rw-r--r--  2.0 unx     1316 b- defN 24-Mar-08 21:26 pioreactor/plugin_management/install_plugin.py
--rw-r--r--  2.0 unx     1179 b- defN 24-Mar-08 21:26 pioreactor/plugin_management/list_plugins.py
--rw-r--r--  2.0 unx     1657 b- defN 24-Mar-08 21:26 pioreactor/plugin_management/uninstall_plugin.py
--rw-r--r--  2.0 unx     1194 b- defN 24-Mar-08 21:26 pioreactor/plugin_management/utils.py
--rw-r--r--  2.0 unx    13665 b- defN 24-Mar-08 21:26 pioreactor/utils/__init__.py
--rw-r--r--  2.0 unx     4240 b- defN 24-Mar-08 21:26 pioreactor/utils/adcs.py
--rw-r--r--  2.0 unx     1930 b- defN 24-Mar-08 21:26 pioreactor/utils/dacs.py
--rw-r--r--  2.0 unx      976 b- defN 24-Mar-08 21:26 pioreactor/utils/gpio_helpers.py
--rw-r--r--  2.0 unx     3311 b- defN 24-Mar-08 21:26 pioreactor/utils/math_helpers.py
--rw-r--r--  2.0 unx     5173 b- defN 24-Mar-08 21:26 pioreactor/utils/mock.py
--rw-r--r--  2.0 unx     3622 b- defN 24-Mar-08 21:26 pioreactor/utils/networking.py
--rw-r--r--  2.0 unx    10265 b- defN 24-Mar-08 21:26 pioreactor/utils/pwm.py
--rw-r--r--  2.0 unx     3393 b- defN 24-Mar-08 21:26 pioreactor/utils/rpi_bad_power.py
--rw-r--r--  2.0 unx     7889 b- defN 24-Mar-08 21:26 pioreactor/utils/sqlite_worker.py
--rw-r--r--  2.0 unx    18416 b- defN 24-Mar-08 21:26 pioreactor/utils/streaming_calculations.py
--rw-r--r--  2.0 unx     5827 b- defN 24-Mar-08 21:26 pioreactor/utils/timing.py
--rw-r--r--  2.0 unx     1067 b- defN 24-Mar-08 21:26 pioreactor-24.3.8rc0.dist-info/LICENSE
--rw-r--r--  2.0 unx     3713 b- defN 24-Mar-08 21:26 pioreactor-24.3.8rc0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-08 21:26 pioreactor-24.3.8rc0.dist-info/WHEEL
--rw-r--r--  2.0 unx       79 b- defN 24-Mar-08 21:26 pioreactor-24.3.8rc0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       11 b- defN 24-Mar-08 21:26 pioreactor-24.3.8rc0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     8023 b- defN 24-Mar-08 21:26 pioreactor-24.3.8rc0.dist-info/RECORD
-87 files, 779474 bytes uncompressed, 214149 bytes compressed:  72.5%
+Zip file size: 231023 bytes, number of entries: 88
+-rw-r--r--  2.0 unx      117 b- defN 24-Apr-02 19:15 pioreactor/__init__.py
+-rw-r--r--  2.0 unx     5868 b- defN 24-Apr-02 19:15 pioreactor/config.py
+-rw-r--r--  2.0 unx      265 b- defN 24-Apr-02 19:15 pioreactor/error_codes.py
+-rw-r--r--  2.0 unx      958 b- defN 24-Apr-02 19:15 pioreactor/exc.py
+-rw-r--r--  2.0 unx     3854 b- defN 24-Apr-02 19:15 pioreactor/hardware.py
+-rw-r--r--  2.0 unx     6861 b- defN 24-Apr-02 19:15 pioreactor/logging.py
+-rw-r--r--  2.0 unx    15658 b- defN 24-Apr-02 19:15 pioreactor/mureq.py
+-rw-r--r--  2.0 unx    12014 b- defN 24-Apr-02 19:15 pioreactor/pubsub.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-02 19:15 pioreactor/py.typed
+-rw-r--r--  2.0 unx     6386 b- defN 24-Apr-02 19:15 pioreactor/structs.py
+-rw-r--r--  2.0 unx     3304 b- defN 24-Apr-02 19:15 pioreactor/types.py
+-rw-r--r--  2.0 unx     2912 b- defN 24-Apr-02 19:15 pioreactor/version.py
+-rw-r--r--  2.0 unx     5462 b- defN 24-Apr-02 19:15 pioreactor/whoami.py
+-rw-r--r--  2.0 unx      540 b- defN 24-Apr-02 19:15 pioreactor/actions/__init__.py
+-rw-r--r--  2.0 unx     8865 b- defN 24-Apr-02 19:15 pioreactor/actions/led_intensity.py
+-rw-r--r--  2.0 unx     9013 b- defN 24-Apr-02 19:15 pioreactor/actions/od_blank.py
+-rw-r--r--  2.0 unx    24316 b- defN 24-Apr-02 19:15 pioreactor/actions/od_calibration.py
+-rw-r--r--  2.0 unx    19799 b- defN 24-Apr-02 19:15 pioreactor/actions/pump.py
+-rw-r--r--  2.0 unx    22400 b- defN 24-Apr-02 19:15 pioreactor/actions/pump_calibration.py
+-rw-r--r--  2.0 unx    20149 b- defN 24-Apr-02 19:15 pioreactor/actions/self_test.py
+-rw-r--r--  2.0 unx     5339 b- defN 24-Apr-02 19:15 pioreactor/actions/stirring_calibration.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-02 19:15 pioreactor/actions/leader/__init__.py
+-rw-r--r--  2.0 unx     4902 b- defN 24-Apr-02 19:15 pioreactor/actions/leader/backup_database.py
+-rw-r--r--  2.0 unx    24791 b- defN 24-Apr-02 19:15 pioreactor/actions/leader/experiment_profile.py
+-rw-r--r--  2.0 unx     6544 b- defN 24-Apr-02 19:15 pioreactor/actions/leader/export_experiment_data.py
+-rw-r--r--  2.0 unx      146 b- defN 24-Apr-02 19:15 pioreactor/automations/__init__.py
+-rw-r--r--  2.0 unx     1000 b- defN 24-Apr-02 19:15 pioreactor/automations/base.py
+-rw-r--r--  2.0 unx      459 b- defN 24-Apr-02 19:15 pioreactor/automations/dosing/__init__.py
+-rw-r--r--  2.0 unx    29496 b- defN 24-Apr-02 19:15 pioreactor/automations/dosing/base.py
+-rw-r--r--  2.0 unx     1424 b- defN 24-Apr-02 19:15 pioreactor/automations/dosing/chemostat.py
+-rw-r--r--  2.0 unx     1497 b- defN 24-Apr-02 19:15 pioreactor/automations/dosing/fed_batch.py
+-rw-r--r--  2.0 unx     4962 b- defN 24-Apr-02 19:15 pioreactor/automations/dosing/pid_morbidostat.py
+-rw-r--r--  2.0 unx      476 b- defN 24-Apr-02 19:15 pioreactor/automations/dosing/silent.py
+-rw-r--r--  2.0 unx     4613 b- defN 24-Apr-02 19:15 pioreactor/automations/dosing/turbidostat.py
+-rw-r--r--  2.0 unx      510 b- defN 24-Apr-02 19:15 pioreactor/automations/events/__init__.py
+-rw-r--r--  2.0 unx      567 b- defN 24-Apr-02 19:15 pioreactor/automations/led/__init__.py
+-rw-r--r--  2.0 unx    12023 b- defN 24-Apr-02 19:15 pioreactor/automations/led/base.py
+-rw-r--r--  2.0 unx     3875 b- defN 24-Apr-02 19:15 pioreactor/automations/led/light_dark_cycle.py
+-rw-r--r--  2.0 unx      154 b- defN 24-Apr-02 19:15 pioreactor/automations/temperature/__init__.py
+-rw-r--r--  2.0 unx     9317 b- defN 24-Apr-02 19:15 pioreactor/automations/temperature/base.py
+-rw-r--r--  2.0 unx      561 b- defN 24-Apr-02 19:15 pioreactor/automations/temperature/only_record_temperature.py
+-rw-r--r--  2.0 unx     4235 b- defN 24-Apr-02 19:15 pioreactor/automations/temperature/thermostat.py
+-rw-r--r--  2.0 unx      715 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/__init__.py
+-rw-r--r--  2.0 unx    44487 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/base.py
+-rw-r--r--  2.0 unx     7043 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/dosing_control.py
+-rw-r--r--  2.0 unx    22120 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/growth_rate_calculating.py
+-rw-r--r--  2.0 unx     5806 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/led_control.py
+-rw-r--r--  2.0 unx    29062 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/monitor.py
+-rw-r--r--  2.0 unx    52689 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/od_reading.py
+-rw-r--r--  2.0 unx    17978 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/stirring.py
+-rw-r--r--  2.0 unx    24449 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/temperature_control.py
+-rw-r--r--  2.0 unx      605 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/leader/__init__.py
+-rw-r--r--  2.0 unx    17001 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/leader/mqtt_to_db_streaming.py
+-rw-r--r--  2.0 unx     4573 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/leader/watchdog.py
+-rw-r--r--  2.0 unx     1094 b- defN 24-Apr-02 19:15 pioreactor/background_jobs/subjobs/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-02 19:15 pioreactor/cli/__init__.py
+-rw-r--r--  2.0 unx    26719 b- defN 24-Apr-02 19:15 pioreactor/cli/pio.py
+-rw-r--r--  2.0 unx    26008 b- defN 24-Apr-02 19:15 pioreactor/cli/pios.py
+-rw-r--r--  2.0 unx    10177 b- defN 24-Apr-02 19:15 pioreactor/cluster_management/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-02 19:15 pioreactor/experiment_profiles/__init__.py
+-rw-r--r--  2.0 unx     5488 b- defN 24-Apr-02 19:15 pioreactor/experiment_profiles/parser.py
+-rw-r--r--  2.0 unx     2896 b- defN 24-Apr-02 19:15 pioreactor/experiment_profiles/profile_struct.py
+-rw-r--r--  2.0 unx      197 b- defN 24-Apr-02 19:15 pioreactor/experiment_profiles/sly/__init__.py
+-rw-r--r--  2.0 unx    16273 b- defN 24-Apr-02 19:15 pioreactor/experiment_profiles/sly/lex.py
+-rw-r--r--  2.0 unx    83015 b- defN 24-Apr-02 19:15 pioreactor/experiment_profiles/sly/yacc.py
+-rw-r--r--  2.0 unx     3807 b- defN 24-Apr-02 19:15 pioreactor/plugin_management/__init__.py
+-rw-r--r--  2.0 unx     1446 b- defN 24-Apr-02 19:15 pioreactor/plugin_management/install_plugin.py
+-rw-r--r--  2.0 unx     1179 b- defN 24-Apr-02 19:15 pioreactor/plugin_management/list_plugins.py
+-rw-r--r--  2.0 unx     1789 b- defN 24-Apr-02 19:15 pioreactor/plugin_management/uninstall_plugin.py
+-rw-r--r--  2.0 unx     1194 b- defN 24-Apr-02 19:15 pioreactor/plugin_management/utils.py
+-rw-r--r--  2.0 unx    21357 b- defN 24-Apr-02 19:15 pioreactor/utils/__init__.py
+-rw-r--r--  2.0 unx     4240 b- defN 24-Apr-02 19:15 pioreactor/utils/adcs.py
+-rw-r--r--  2.0 unx     1930 b- defN 24-Apr-02 19:15 pioreactor/utils/dacs.py
+-rw-r--r--  2.0 unx      976 b- defN 24-Apr-02 19:15 pioreactor/utils/gpio_helpers.py
+-rw-r--r--  2.0 unx     3311 b- defN 24-Apr-02 19:15 pioreactor/utils/math_helpers.py
+-rw-r--r--  2.0 unx     5211 b- defN 24-Apr-02 19:15 pioreactor/utils/mock.py
+-rw-r--r--  2.0 unx     3622 b- defN 24-Apr-02 19:15 pioreactor/utils/networking.py
+-rw-r--r--  2.0 unx    10149 b- defN 24-Apr-02 19:15 pioreactor/utils/pwm.py
+-rw-r--r--  2.0 unx     3393 b- defN 24-Apr-02 19:15 pioreactor/utils/rpi_bad_power.py
+-rw-r--r--  2.0 unx     7889 b- defN 24-Apr-02 19:15 pioreactor/utils/sqlite_worker.py
+-rw-r--r--  2.0 unx    18416 b- defN 24-Apr-02 19:15 pioreactor/utils/streaming_calculations.py
+-rw-r--r--  2.0 unx     5827 b- defN 24-Apr-02 19:15 pioreactor/utils/timing.py
+-rw-r--r--  2.0 unx     1067 b- defN 24-Apr-02 19:15 pioreactor-24.4.2rc0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3713 b- defN 24-Apr-02 19:15 pioreactor-24.4.2rc0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-02 19:15 pioreactor-24.4.2rc0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       79 b- defN 24-Apr-02 19:15 pioreactor-24.4.2rc0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       11 b- defN 24-Apr-02 19:15 pioreactor-24.4.2rc0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     8122 b- defN 24-Apr-02 19:15 pioreactor-24.4.2rc0.dist-info/RECORD
+88 files, 792847 bytes uncompressed, 217975 bytes compressed:  72.5%
```

## zipnote {}

```diff
@@ -168,14 +168,17 @@
 
 Filename: pioreactor/cli/pio.py
 Comment: 
 
 Filename: pioreactor/cli/pios.py
 Comment: 
 
+Filename: pioreactor/cluster_management/__init__.py
+Comment: 
+
 Filename: pioreactor/experiment_profiles/__init__.py
 Comment: 
 
 Filename: pioreactor/experiment_profiles/parser.py
 Comment: 
 
 Filename: pioreactor/experiment_profiles/profile_struct.py
@@ -237,26 +240,26 @@
 
 Filename: pioreactor/utils/streaming_calculations.py
 Comment: 
 
 Filename: pioreactor/utils/timing.py
 Comment: 
 
-Filename: pioreactor-24.3.8rc0.dist-info/LICENSE
+Filename: pioreactor-24.4.2rc0.dist-info/LICENSE
 Comment: 
 
-Filename: pioreactor-24.3.8rc0.dist-info/METADATA
+Filename: pioreactor-24.4.2rc0.dist-info/METADATA
 Comment: 
 
-Filename: pioreactor-24.3.8rc0.dist-info/WHEEL
+Filename: pioreactor-24.4.2rc0.dist-info/WHEEL
 Comment: 
 
-Filename: pioreactor-24.3.8rc0.dist-info/entry_points.txt
+Filename: pioreactor-24.4.2rc0.dist-info/entry_points.txt
 Comment: 
 
-Filename: pioreactor-24.3.8rc0.dist-info/top_level.txt
+Filename: pioreactor-24.4.2rc0.dist-info/top_level.txt
 Comment: 
 
-Filename: pioreactor-24.3.8rc0.dist-info/RECORD
+Filename: pioreactor-24.4.2rc0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pioreactor/config.py

```diff
@@ -168,34 +168,8 @@
 
 
 @cache
 def get_mqtt_address() -> str:
     return get_config().get("mqtt", "broker_address", fallback=get_leader_address())
 
 
-def check_firstboot_successful() -> bool:
-    from pioreactor.whoami import is_testing_env
-
-    if is_testing_env():
-        return True
-    return os.path.isfile("/usr/local/bin/firstboot.sh.done")
-
-
-def get_active_workers_in_inventory() -> tuple[str, ...]:
-    # note that this rehydrates conifg.ini from disk before checking.
-    # because we are not using config.getboolean here, values like "0" are seen as true,
-    # hence we use the built in config.BOOLEAN_STATES to determine truthiness
-    config = get_config()
-    return tuple(
-        str(unit)
-        for (unit, available) in config["cluster.inventory"].items()
-        if config.BOOLEAN_STATES[available]
-    )
-
-
-def get_workers_in_inventory() -> tuple[str, ...]:
-    # note that this rehydrates config.ini from disk before checking.
-    config = get_config()
-    return tuple(str(unit) for (unit, available) in config["cluster.inventory"].items())
-
-
 config = get_config()
```

## pioreactor/exc.py

```diff
@@ -20,7 +20,37 @@
     """
 
 
 class CalibrationError(Exception):
     """
     An issue with calibration (pump, stirring, OD, etc.)
     """
+
+
+class NotActiveWorkerError(Exception):
+    """
+    if a worker is not active, things shouldn't happen.
+    """
+
+
+class NotAssignedAnExperimentError(Exception):
+    """
+    if a worker is assigned an experiment, and something is trying to run
+    """
+
+
+class NoWorkerFoundError(Exception):
+    """
+    Worker isn't present in the leader's inventory
+    """
+
+
+class BashScriptError(Exception):
+    """
+    Some external bash script failed
+    """
+
+
+class RoleError(Exception):
+    """
+    Leader vs worker?
+    """
```

## pioreactor/logging.py

```diff
@@ -4,20 +4,20 @@
 import logging
 from logging import handlers
 from typing import Optional
 
 from json_log_formatter import JSONFormatter  # type: ignore
 
 from pioreactor.config import config
+from pioreactor.exc import NotAssignedAnExperimentError
 from pioreactor.pubsub import Client
 from pioreactor.pubsub import create_client
 from pioreactor.pubsub import publish_to_pioreactor_cloud
 from pioreactor.utils.timing import current_utc_timestamp
-from pioreactor.whoami import am_I_active_worker
-from pioreactor.whoami import get_latest_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
 from pioreactor.whoami import get_unit_name
 from pioreactor.whoami import UNIVERSAL_EXPERIMENT
 
 
 logging.raiseExceptions = False
 
 
@@ -60,14 +60,20 @@
 add_logging_level("NOTICE", NOTICE)
 
 
 class CustomLogger(logging.LoggerAdapter):
     def notice(self, msg, *args, **kwargs):
         self.log(NOTICE, msg, *args, **kwargs)
 
+    def clean_up(self):
+        handlers = self.logger.handlers[:]
+        for handler in handlers:
+            self.logger.removeHandler(handler)
+            handler.close()
+
 
 class CustomisedJSONFormatter(JSONFormatter):
     def json_record(self, message: str, extra: dict, record: logging.LogRecord) -> dict:
         extra["message"] = message
         extra["level"] = record.levelname
         extra["task"] = record.name
         extra["timestamp"] = current_utc_timestamp()
@@ -154,15 +160,18 @@
 
     if unit is None:
         unit = get_unit_name()
 
     if experiment is None:
         # this fails if we aren't able to connect to leader, hence the to_mqtt check
         if to_mqtt:
-            experiment = get_latest_experiment_name()
+            try:
+                experiment = get_assigned_experiment_name(unit)
+            except NotAssignedAnExperimentError:
+                experiment = UNIVERSAL_EXPERIMENT
         else:
             experiment = UNIVERSAL_EXPERIMENT
 
     # file handler
     file_handler = handlers.WatchedFileHandler(config["logging"]["log_file"])
     file_handler.setLevel(logging.DEBUG)
     file_handler.setFormatter(
@@ -198,16 +207,14 @@
             pub_client = create_client(
                 client_id=f"{name}-logging-{unit}-{experiment}",
                 max_connection_attempts=2,
                 keepalive=15 * 60,
             )
         assert pub_client is not None
 
-        experiment = experiment if am_I_active_worker() else UNIVERSAL_EXPERIMENT
-
         # create MQTT handlers for logs table
         topic = f"pioreactor/{unit}/{experiment}/logs/{source}"
         mqtt_to_db_handler = MQTTHandler(topic, pub_client)
         mqtt_to_db_handler.setLevel(logging.DEBUG)
         mqtt_to_db_handler.setFormatter(CustomisedJSONFormatter())
 
         # add MQTT/remote log handlers
```

## pioreactor/mureq.py

```diff
@@ -7,37 +7,38 @@
 0BSD ("zero-clause BSD") license.
 """
 # mypy: ignore-errors
 from __future__ import annotations
 
 import contextlib
 import io
-import json as jsonlib
 import os.path
 import socket
 import ssl
 import urllib.parse
 from base64 import b64encode
 from http.client import HTTPConnection
 from http.client import HTTPException
 from http.client import HTTPMessage
 from http.client import HTTPResponse
 from http.client import HTTPSConnection
 from typing import Generator
 from typing import Optional
 
-from pioreactor.config import config
+from msgspec.json import decode as loads
+from msgspec.json import encode as dumps
 
 
 DEFAULT_TIMEOUT = 15.0
 DEFAULT_UA = "Python/Pioreactor"
-DEFAULT_AUTH = f'Basic {config.get("ui_basic_auth", "api_key", fallback="")}'
 
+JSON_HEADERS = {"Content-Type": "application/json"}
 
-def basic_auth(username: str, password: str):
+
+def basic_auth(username: str, password: str) -> str:
     # get(..., headers={ 'Authorization' : f'Basic {basic_auth(username, password)}'})
     token = b64encode(f"{username}:{password}".encode("utf-8")).decode("ascii")
     return token
 
 
 def request(method, url, *, read_limit=None, **kwargs) -> Response:
     """request performs an HTTP request and reads the entire response body.
@@ -227,17 +228,17 @@
 
     def raise_for_status(self):
         """raise_for_status checks the response's success code, raising an
         exception for error codes."""
         if not self.ok:
             raise HTTPErrorStatus(self.status_code)
 
-    def json(self):
+    def json(self) -> dict:
         """Attempts to deserialize the response body as UTF-8 encoded JSON."""
-        return jsonlib.loads(self.body)
+        return loads(self.body)
 
     def _debugstr(self):
         buf = io.StringIO()
         print("HTTP", self.status_code, file=buf)
         for k, v in self.headers.items():
             print(f"{k}: {v}", file=buf)
         print(file=buf)
@@ -333,14 +334,18 @@
             parsed_location.query,
             parsed_location.fragment,
         )
     )
 
 
 def _prepare_outgoing_headers(headers):
+    from pioreactor.config import config
+
+    DEFAULT_AUTH = f'Basic {config.get("ui_basic_auth", "api_key", fallback="")}'
+
     if headers is None:
         headers = HTTPMessage()
     elif not isinstance(headers, HTTPMessage):
         new_headers = HTTPMessage()
         if hasattr(headers, "items"):
             iterator = headers.items()
         else:
@@ -376,15 +381,15 @@
     if body is not None:
         if not isinstance(body, bytes):
             raise TypeError("body must be bytes or None", type(body))
         return body
 
     if json is not None:
         _setdefault_header(headers, "Content-Type", _JSON_CONTENTTYPE)
-        return jsonlib.dumps(json).encode("utf-8")
+        return dumps(json)
 
     if form is not None:
         _setdefault_header(headers, "Content-Type", _FORM_CONTENTTYPE)
         return urllib.parse.urlencode(form, doseq=True)
 
     return None
 
@@ -425,15 +430,16 @@
     port = 443 if is_https else 80
     if parsed_url.port:
         port = parsed_url.port
 
     if is_unix and unix_socket is None:
         unix_socket = urllib.parse.unquote(parsed_url.netloc)
 
-    path = parsed_url.path
+    path = urllib.parse.quote(parsed_url.path)
+
     if parsed_url.query:
         if enc_params:
             path = f"{path}?{parsed_url.query}&{enc_params}"
         else:
             path = f"{path}?{parsed_url.query}"
     else:
         if enc_params:
```

## pioreactor/pubsub.py

```diff
@@ -6,14 +6,15 @@
 import string
 import threading
 from time import sleep
 from typing import Any
 from typing import Callable
 from typing import Optional
 
+from msgspec.json import decode as loads
 from paho.mqtt.client import Client as PahoClient
 
 from pioreactor.config import config
 from pioreactor.config import mqtt_address
 from pioreactor.types import MQTTMessage
 
 
@@ -334,22 +335,21 @@
         self.log_level = log_level.upper()
         # set the unit and experiment we are looking for
         self.unit = unit
         self.experiment = experiment
         # create a bucket for the logs
         self.bucket: list[dict] = []
         # subscribe to the logs
+
         self.client: Client = subscribe_and_callback(
             self._collect_logs_into_bucket,
             str(PIOREACTOR / self.unit / self.experiment / "logs" / "app"),
         )
 
     def _collect_logs_into_bucket(self, message):
-        from json import loads
-
         # load the message
         log = loads(message.payload)
         # if the log level matches, add it to the bucket
         if log["level"] == self.log_level:
             self.bucket.append(log)
 
     def __enter__(self) -> list[dict]:
```

## pioreactor/version.py

```diff
@@ -3,15 +3,15 @@
 
 import os
 
 # pioreactor version
 # Append ".dev0" if a dev version
 # Append "rc0" if a rc version
 # No zero padding!
-__version__ = "24.3.8rc0"
+__version__ = "24.4.2rc0"
 
 
 def get_hardware_version() -> tuple[int, int] | tuple[int, int, str]:
     if os.environ.get("HARDWARE") is not None:
         # ex: > HARDWARE=1.1 pio ...
         return int(os.environ["HARDWARE"].split(".")[0]), int(os.environ["HARDWARE"].split(".")[1])
 
@@ -48,15 +48,15 @@
 
 
 def get_rpi_machine() -> str:
     try:
         with open("/proc/device-tree/model") as f:
             return f.read().strip().rstrip("\x00")
     except FileNotFoundError:
-        return "Raspberry Pi 3 - testing"
+        return "Raspberry Pi 3"
 
 
 def get_firmware_version() -> tuple[int, int]:
     if os.environ.get("FIRMWARE") is not None:
         # ex: > FIRMWARE=1.1 pio ...
 
         return tuple(int(_) for _ in os.environ["FIRMWARE"].split("."))  # type: ignore
```

## pioreactor/whoami.py

```diff
@@ -1,36 +1,47 @@
 # -*- coding: utf-8 -*-
 from __future__ import annotations
 
 import os
 import sys
 import time
+import warnings
 from functools import cache
 
-from msgspec.json import decode
-
-from pioreactor.structs import ExperimentMetadata
+from pioreactor import mureq
+from pioreactor.exc import NotAssignedAnExperimentError
+from pioreactor.exc import NoWorkerFoundError
 from pioreactor.version import serial_number
 
+
 UNIVERSAL_IDENTIFIER = "$broadcast"
 UNIVERSAL_EXPERIMENT = "$experiment"
 NO_EXPERIMENT = "$no_experiment_present"
 
 
-def get_latest_testing_experiment_name() -> str:
-    exp = get_latest_experiment_name()
-    return f"_testing_{exp}"
+def get_testing_experiment_name() -> str:
+    try:
+        exp = get_assigned_experiment_name(get_unit_name())
+        return f"_testing_{exp}"
+    except NotAssignedAnExperimentError:
+        return f"_testing_{NO_EXPERIMENT}"
 
 
 @cache
 def get_latest_experiment_name() -> str:
-    return _get_latest_experiment_name()
+    warnings.warn("Use whoami.get_assigned_experiment_name(unit) instead", DeprecationWarning, stacklevel=2)
+    return get_assigned_experiment_name(get_unit_name())
 
 
-def _get_latest_experiment_name() -> str:
+@cache
+def get_assigned_experiment_name(unit_name: str) -> str:
+    return _get_assigned_experiment_name(unit_name)
+
+
+def _get_assigned_experiment_name(unit_name: str) -> str:
     from pioreactor.logging import create_logger
     from pioreactor import mureq
 
     if os.environ.get("EXPERIMENT") is not None:
         return os.environ["EXPERIMENT"]
     elif is_testing_env():
         return "_testing_experiment"
@@ -38,22 +49,25 @@
     from pioreactor.config import leader_address
 
     retries = 6
     exit_reason = ""
 
     for attempt in range(retries):
         try:
-            result = mureq.get(f"http://{leader_address}/api/experiments/latest")
+            result = mureq.get(f"http://{leader_address}/api/workers/{unit_name}/experiment")
             result.raise_for_status()
-            return decode(result.body, type=ExperimentMetadata).experiment
+            data = result.json()
+            return data["experiment"]
         except mureq.HTTPErrorStatus as e:
             if e.status_code == 401:
                 # auth error, something is wrong
                 exit_reason = "auth"
                 break
+            elif e.status_code == 404:
+                raise NotAssignedAnExperimentError("Worker is not assigned to an experiment")
         except mureq.HTTPException:
             exit_reason = "connection_refused"
         except Exception:
             # some other error? Keep trying
             pass
         time.sleep(0.5 * attempt)
     else:
@@ -62,25 +76,51 @@
     logger = create_logger("pioreactor", experiment=UNIVERSAL_EXPERIMENT, to_mqtt=False)
 
     if exit_reason == "auth":
         logger.warning(
             f"Error in authentication to UI. Check http://{leader_address} and config.ini for api_key."
         )
     elif exit_reason == "timeout":
-        logger.warning(
-            f"Not able to access experiments in UI. Check http://{leader_address}/api/experiments/latest"
-        )
+        logger.warning(f"Not able to access experiments in UI. Check http://{leader_address}/api/experiments")
     elif exit_reason == "connection_refused":
         logger.warning(
             f"Not able to access experiments in UI. Check http://{leader_address} is online and check network."
         )
+    elif exit_reason == "unassigned":
+        logger.warning(f"Worker {unit_name} not found or not assigned to any experiment.")
     return NO_EXPERIMENT
 
 
 @cache
+def is_active(unit_name: str) -> bool:
+    from pioreactor.config import leader_address
+
+    if os.environ.get("ACTIVE") == "1":
+        return True
+    elif os.environ.get("ACTIVE") == "0":
+        return False
+
+    if is_testing_env():
+        return True
+
+    try:
+        result = mureq.get(f"http://{leader_address}/api/workers/{unit_name}")
+        result.raise_for_status()
+        data = result.json()
+        return bool(data["is_active"])
+    except mureq.HTTPErrorStatus as e:
+        if e.status_code == 404:
+            raise NoWorkerFoundError("Worker is not present in leader's inventory")
+        else:
+            raise e
+    except mureq.HTTPException as e:
+        raise e
+
+
+@cache
 def is_testing_env() -> bool:
     return ("pytest" in sys.modules) or (os.environ.get("TESTING") is not None)
 
 
 def get_hostname() -> str:
     import socket
 
@@ -108,22 +148,16 @@
         return True
 
     from pioreactor.config import leader_hostname
 
     return get_unit_name() == leader_hostname
 
 
-@cache
 def am_I_active_worker() -> bool:
-    if is_testing_env():
-        return True
-
-    from pioreactor.config import get_active_workers_in_inventory
-
-    return get_unit_name() in get_active_workers_in_inventory()
+    return is_active(get_unit_name())
 
 
 @cache
 def get_hashed_serial_number() -> str:
     from hashlib import md5
 
     return md5(serial_number.encode()).hexdigest()
@@ -133,21 +167,20 @@
     try:
         with open("/home/pioreactor/.pioreactor/.image_info") as f:
             return f.read().strip().split("=")[1]
     except OSError:  # catch FileNotFoundError, PermissionError, and other file-related exceptions
         return "<Failed to fetch>"
 
 
-if is_testing_env():
-    # mock out gpiozero's pins
-    # from gpiozero import Device
-    # from gpiozero.pins.mock import MockFactory
-    # from gpiozero.pins.mock import MockPWMPin
+def check_firstboot_successful() -> bool:
+    if is_testing_env():
+        return True
+    return os.path.isfile("/usr/local/bin/firstboot.sh.done")
 
-    # Device.pin_factory = MockFactory(pin_class=MockPWMPin)
 
+if is_testing_env():
     # allow Blinka to think we are an Rpi:
     # https://github.com/adafruit/Adafruit_Python_PlatformDetect/blob/75f69806222fbaf8535130ed2eacd07b06b1a298/adafruit_platformdetect/board.py
     os.environ["BLINKA_FORCECHIP"] = "BCM2XXX"  # RaspberryPi
     os.environ["BLINKA_FORCEBOARD"] = "RASPBERRY_PI_3A_PLUS"  # Raspberry Pi 3 Model A Plus Rev 1.0
     os.environ["FIRMWARE"] = "1.0"
     os.environ["HARDWARE"] = "1.2"
```

## pioreactor/actions/led_intensity.py

```diff
@@ -17,15 +17,15 @@
 from pioreactor.pubsub import Client
 from pioreactor.pubsub import create_client
 from pioreactor.pubsub import QOS
 from pioreactor.types import LedChannel
 from pioreactor.types import LedIntensityValue
 from pioreactor.utils import local_intermittent_storage
 from pioreactor.utils.timing import current_utc_datetime
-from pioreactor.whoami import get_latest_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
 from pioreactor.whoami import get_unit_name
 from pioreactor.whoami import is_testing_env
 
 ALL_LED_CHANNELS: list[LedChannel] = ["A", "B", "C", "D"]
 
 
 @contextmanager
@@ -128,16 +128,16 @@
     Notes
     -------
     State is updated in MQTT and the temporary cache `leds`:
 
         pioreactor/<unit>/<experiment>/leds/intensity    {'A': intensityA, 'B': intensityB, ...}
 
     """
-    experiment = experiment or get_latest_experiment_name()
     unit = unit or get_unit_name()
+    experiment = experiment or get_assigned_experiment_name(unit)
 
     logger = create_logger("led_intensity", experiment=experiment, unit=unit, pub_client=pubsub_client)
     updated_successfully = True
 
     if not is_testing_env():
         from pioreactor.utils.dacs import DAC
     else:
@@ -254,15 +254,15 @@
     source_of_event: Optional[str] = None,
     no_log: bool = False,
 ) -> bool:
     """
     Modify the intensity of LED channel(s)
     """
     unit = get_unit_name()
-    experiment = get_latest_experiment_name()
+    experiment = get_assigned_experiment_name(unit)
 
     state: dict[LedChannel, LedIntensityValue] = {}
     if a is not None:
         state["A"] = a
     if b is not None:
         state["B"] = b
     if c is not None:
```

## pioreactor/actions/od_blank.py

```diff
@@ -16,16 +16,16 @@
 from pioreactor import structs
 from pioreactor import types as pt
 from pioreactor import whoami
 from pioreactor.config import config
 from pioreactor.logging import create_logger
 from pioreactor.utils import is_pio_job_running
 from pioreactor.utils import local_persistant_storage
+from pioreactor.utils import managed_lifecycle
 from pioreactor.utils import math_helpers
-from pioreactor.utils import publish_ready_to_disconnected_state
 from pioreactor.utils.timing import current_utc_datetime
 
 
 def od_statistics(
     od_stream: Iterator,
     action_name: str,
     experiment: Optional[str] = None,
@@ -37,30 +37,30 @@
     Compute a sample statistics of the photodiodes attached.
 
     There's a variance w.r.t. the rotation of the vial that we can't control.
     """
 
     logger = logger or create_logger(action_name)
     unit = unit or whoami.get_unit_name()
-    experiment = experiment or whoami.get_latest_experiment_name()
+    experiment = experiment or whoami.get_assigned_experiment_name(unit)
 
     logger.info(
-        f"Starting to compute statistics from OD readings. Collecting {n_samples} data points."
+        f"Starting to compute statistics from OD readings. Collecting {n_samples} data points. This may take a while."
     )
 
     # turn on stirring if not already on
     if not is_pio_job_running("stirring"):
         from pioreactor.background_jobs.stirring import start_stirring
 
         logger.info("Starting stirring.")
         st = start_stirring(
             unit=unit,
             experiment=experiment,
         )
-        st.block_until_rpm_is_close_to_target(timeout=40)  # wait for maximum 2 minutes
+        st.block_until_rpm_is_close_to_target(timeout=40)  # wait for stirring to be reasonable.
     else:
         st = nullcontext()  # type: ignore
 
     with st:
         readings = defaultdict(list)
         angles = {}
 
@@ -109,15 +109,15 @@
 
         return means, variances
 
 
 def delete_od_blank(unit=None, experiment=None):
     action_name = "od_blank"
     unit = unit or whoami.get_unit_name()
-    experiment = experiment or whoami.get_latest_experiment_name()
+    experiment = experiment or whoami.get_assigned_experiment_name(unit)
 
     with local_persistant_storage(action_name) as cache:
         if experiment not in cache:
             return
 
         pubsub.publish(
             f"pioreactor/{unit}/{experiment}/{action_name}/means",
@@ -144,27 +144,27 @@
     n_samples: int = 20,
     unit=None,
     experiment=None,
 ) -> dict[pt.PdChannel, float]:
     action_name = "od_blank"
     logger = create_logger(action_name)
     unit = unit or whoami.get_unit_name()
-    experiment = experiment or whoami.get_latest_experiment_name()
-    testing_experiment = whoami.get_latest_testing_experiment_name()
+    experiment = experiment or whoami.get_assigned_experiment_name(unit)
+    testing_experiment = whoami.get_testing_experiment_name()
 
     from pioreactor.background_jobs.od_reading import start_od_reading
     from pioreactor.background_jobs.stirring import start_stirring
 
-    with publish_ready_to_disconnected_state(unit, experiment, action_name):
+    with managed_lifecycle(unit, experiment, action_name):
         with start_od_reading(
             od_angle_channel1,
             od_angle_channel2,
             unit=unit,
             interval=1.5,
-            experiment=testing_experiment,
+            experiment=testing_experiment,  # use testing experiment to not pollute the database (and they would show up in the UI)
             fake_data=whoami.is_testing_env(),
         ) as od_stream, start_stirring(
             unit=unit,
             experiment=testing_experiment,
         ) as st:
             # warm up OD reader
             for count, _ in enumerate(od_stream, start=0):
@@ -189,17 +189,15 @@
                 pubsub.publish(
                     f"pioreactor/{unit}/{experiment}/{action_name}/mean/{channel}",
                     encode(
                         structs.ODReading(
                             timestamp=current_utc_datetime(),
                             channel=channel,
                             od=means[channel],
-                            angle=config.get(
-                                "od_config.photodiode_channel", channel, fallback=None
-                            ),
+                            angle=config.get("od_config.photodiode_channel", channel, fallback=None),
                         )
                     ),
                     qos=pubsub.QOS.AT_LEAST_ONCE,
                     retain=True,
                 )
 
             # publish to UI
@@ -238,15 +236,15 @@
     help="Number of samples",
 )
 def click_od_blank(ctx, od_angle_channel1, od_angle_channel2, n_samples: int) -> None:
     """
     Compute statistics about the blank OD time series
     """
     unit = whoami.get_unit_name()
-    experiment = whoami.get_latest_experiment_name()
+    experiment = whoami.get_assigned_experiment_name(unit)
 
     if ctx.invoked_subcommand is None:
         od_blank(
             od_angle_channel1,
             od_angle_channel2,
             n_samples=n_samples,
             unit=unit,
@@ -257,10 +255,10 @@
 @click_od_blank.command(name="delete")
 @click.option(
     "--experiment",
     help="delete particular experiment",
 )
 def click_delete_od_blank(experiment):
     unit = whoami.get_unit_name()
-    experiment = experiment or whoami.get_latest_experiment_name()
+    experiment = experiment or whoami.get_assigned_experiment_name(unit)
 
     delete_od_blank(unit, experiment)
```

## pioreactor/actions/od_calibration.py

```diff
@@ -27,18 +27,18 @@
 from pioreactor.background_jobs.stirring import Stirrer
 from pioreactor.config import config
 from pioreactor.config import leader_address
 from pioreactor.mureq import patch
 from pioreactor.mureq import put
 from pioreactor.utils import is_pio_job_running
 from pioreactor.utils import local_persistant_storage
-from pioreactor.utils import publish_ready_to_disconnected_state
+from pioreactor.utils import managed_lifecycle
 from pioreactor.utils.timing import current_utc_datestamp
 from pioreactor.utils.timing import current_utc_datetime
-from pioreactor.whoami import get_latest_testing_experiment_name
+from pioreactor.whoami import get_testing_experiment_name
 from pioreactor.whoami import get_unit_name
 from pioreactor.whoami import is_testing_env
 
 
 def green(string):
     return style(string, fg="green")
 
@@ -159,15 +159,15 @@
         pass
 
     echo("Starting stirring and blocking until near target RPM.")
 
     st = stirring(
         target_rpm=config.getfloat("stirring", "target_rpm"),
         unit=get_unit_name(),
-        experiment=get_latest_testing_experiment_name(),
+        experiment=get_testing_experiment_name(),
     )
     st.block_until_rpm_is_close_to_target(abs_tolerance=120)
     return st
 
 
 def plot_data(
     x,
@@ -225,15 +225,15 @@
 
     with start_od_reading(
         cast(pt.PdAngleOrREF, config.get("od_config.photodiode_channel", "1")),
         cast(pt.PdAngleOrREF, config.get("od_config.photodiode_channel", "2")),
         interval=None,
         unit=get_unit_name(),
         fake_data=is_testing_env(),
-        experiment=get_latest_testing_experiment_name(),
+        experiment=get_testing_experiment_name(),
         use_calibration=False,
     ) as od_reader:
 
         def get_voltage_from_adc() -> pt.Voltage:
             od_readings1 = od_reader.record_from_adc()
             od_readings2 = od_reader.record_from_adc()
             return 0.5 * (od_readings1.ods[pd_channel].od + od_readings2.ods[pd_channel].od)
@@ -498,20 +498,20 @@
     angle = data.get("angle", str(config["od_config.photodiode_channel"][pd_channel]))
 
     return pd_channel, angle, ods, voltages
 
 
 def od_calibration(data_file: str | None) -> None:
     unit = get_unit_name()
-    experiment = get_latest_testing_experiment_name()
+    experiment = get_testing_experiment_name()
 
     if any(is_pio_job_running(["stirring", "od_reading"])):
         raise ValueError("Stirring and OD reading should be turned off.")
 
-    with publish_ready_to_disconnected_state(unit, experiment, "od_calibration"):
+    with managed_lifecycle(unit, experiment, "od_calibration"):
         introduction()
         name = get_name_from_user()
 
         if data_file is None:
             (
                 initial_od600,
                 minimum_od600,
```

## pioreactor/actions/pump.py

```diff
@@ -23,15 +23,15 @@
 from pioreactor.logging import CustomLogger
 from pioreactor.pubsub import Client
 from pioreactor.pubsub import QOS
 from pioreactor.utils.pwm import PWM
 from pioreactor.utils.timing import catchtime
 from pioreactor.utils.timing import current_utc_datetime
 from pioreactor.utils.timing import default_datetime_for_pioreactor
-from pioreactor.whoami import get_latest_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
 from pioreactor.whoami import get_unit_name
 
 DEFAULT_PWM_CALIBRATION = structs.PumpCalibration(
     # TODO: provide better estimates for duration_ and bias_ based on some historical data.
     # it can even be a function of voltage
     name="default",
     pioreactor_unit=get_unit_name(),
@@ -230,26 +230,27 @@
     source_of_event: Optional[str] = None,
     calibration: Optional[structs.AnyPumpCalibration] = None,
     continuously: bool = False,
     config=config,  # techdebt, don't use
     manually: bool = False,
     mqtt_client: Optional[Client] = None,
     logger: Optional[CustomLogger] = None,
+    job_source: Optional[str] = None,
 ) -> pt.mL:
     """
     Returns the mL cycled. However,
     If calibration is not defined or available on disk, returns gibberish.
     """
     if not ((ml is not None) or (duration is not None) or continuously):
         raise ValueError("either ml or duration must be set")
     if (ml is not None) and (duration is not None):
         raise ValueError("Only select ml or duration")
 
-    experiment = experiment or get_latest_experiment_name()
     unit = unit or get_unit_name()
+    experiment = experiment or get_assigned_experiment_name(unit)
 
     action_name = _get_pump_action(pump_type)
 
     if logger is None:
         logger = create_logger(action_name, experiment=experiment, unit=unit)
 
     try:
@@ -260,21 +261,22 @@
 
     if calibration is None:
         try:
             calibration = _get_calibration(pump_type)
         except exc.CalibrationError:
             pass
 
-    with utils.publish_ready_to_disconnected_state(
+    with utils.managed_lifecycle(
         unit,
         experiment,
         action_name,
         mqtt_client=mqtt_client,
         exit_on_mqtt_disconnect=True,
         mqtt_client_kwargs={"keepalive": 10},
+        job_source=job_source,
     ) as state:
         mqtt_client = state.mqtt_client
 
         with PWMPump(
             unit, experiment, pin, calibration=calibration, mqtt_client=mqtt_client, logger=logger
         ) as pump:
             if manually:
@@ -376,16 +378,16 @@
     :param pump_type: A string that specifies the type of pump to be used for the liquid circulation.
     :param unit: (Optional) A string that specifies the unit name. If not provided, the unit name will be obtained.
     :param experiment: (Optional) A string that specifies the experiment name. If not provided, the latest experiment name
                        will be obtained.
     :return: None
     """
     action_name = f"circulate_{pump_type}"
-    experiment = experiment or get_latest_experiment_name()
     unit = unit or get_unit_name()
+    experiment = experiment or get_assigned_experiment_name(unit)
     duration = float(duration)
 
     if logger is None:
         logger = create_logger(action_name, experiment=experiment, unit=unit)
 
     waste_pin, media_pin = _get_pin("waste", config), _get_pin(pump_type, config)
 
@@ -408,15 +410,15 @@
         if media_calibration.duration_ > waste_calibration.duration_:
             ratio = min(waste_calibration.duration_ / media_calibration.duration_, ratio)
     else:
         logger.warning(
             "Calibrations don't exist for pump(s). Keep an eye on the liquid level to avoid overflowing!"
         )
 
-    with utils.publish_ready_to_disconnected_state(
+    with utils.managed_lifecycle(
         unit,
         experiment,
         action_name,
         mqtt_client=mqtt_client,
         exit_on_mqtt_disconnect=True,
         mqtt_client_kwargs={"keepalive": 10},
     ) as state:
@@ -490,15 +492,15 @@
     source_of_event: Optional[str],
     manually: bool,
 ) -> float:
     """
     Remove waste/media from unit
     """
     unit = get_unit_name()
-    experiment = get_latest_experiment_name()
+    experiment = get_assigned_experiment_name(unit)
 
     return add_alt_media(
         ml=ml,
         duration=duration,
         continuously=continuously,
         source_of_event=source_of_event,
         unit=unit,
@@ -525,15 +527,15 @@
     source_of_event: Optional[str],
     manually: bool,
 ) -> float:
     """
     Remove waste/media from unit
     """
     unit = get_unit_name()
-    experiment = get_latest_experiment_name()
+    experiment = get_assigned_experiment_name(unit)
 
     return remove_waste(
         ml=ml,
         duration=duration,
         continuously=continuously,
         source_of_event=source_of_event,
         unit=unit,
@@ -560,15 +562,15 @@
     source_of_event: Optional[str],
     manually: bool,
 ) -> float:
     """
     Add media to unit
     """
     unit = get_unit_name()
-    experiment = get_latest_experiment_name()
+    experiment = get_assigned_experiment_name(unit)
 
     return add_media(
         ml=ml,
         duration=duration,
         continuously=continuously,
         source_of_event=source_of_event,
         unit=unit,
```

## pioreactor/actions/pump_calibration.py

```diff
@@ -28,21 +28,21 @@
 from pioreactor.config import config
 from pioreactor.config import leader_address
 from pioreactor.hardware import voltage_in_aux
 from pioreactor.logging import create_logger
 from pioreactor.mureq import patch
 from pioreactor.mureq import put
 from pioreactor.utils import local_persistant_storage
-from pioreactor.utils import publish_ready_to_disconnected_state
+from pioreactor.utils import managed_lifecycle
 from pioreactor.utils.math_helpers import correlation
 from pioreactor.utils.math_helpers import simple_linear_regression_with_forced_nil_intercept
 from pioreactor.utils.timing import current_utc_datestamp
 from pioreactor.utils.timing import current_utc_datetime
-from pioreactor.whoami import get_latest_experiment_name
-from pioreactor.whoami import get_latest_testing_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
+from pioreactor.whoami import get_testing_experiment_name
 from pioreactor.whoami import get_unit_name
 
 
 def green(string):
     return style(string, fg="green")
 
 
@@ -195,15 +195,15 @@
     )
 
     try:
         execute_pump(
             continuously=True,
             source_of_event="pump_calibration",
             unit=get_unit_name(),
-            experiment=get_latest_testing_experiment_name(),
+            experiment=get_testing_experiment_name(),
             calibration=structs.PumpCalibration(
                 name="calibration",
                 created_at=current_utc_datetime(),
                 pump=pump_type,
                 duration_=1.0,
                 hz=hz,
                 dc=dc,
@@ -323,15 +323,15 @@
             while not confirm(style(green(f"Ready to test {duration:.2f}s?"))):
                 pass
 
             execute_pump(
                 duration=duration,
                 source_of_event="pump_calibration",
                 unit=get_unit_name(),
-                experiment=get_latest_testing_experiment_name(),
+                experiment=get_testing_experiment_name(),
                 calibration=empty_calibration,
             )
 
             r = prompt(
                 style(green("Enter amount of water expelled (g or ml), or REDO")),
                 confirmation_prompt=style(green("Repeat for confirmation")),
             )
@@ -432,20 +432,20 @@
     assert len(durations) == len(volumes), "data must be the same length."
 
     return durations, volumes, hz, dc
 
 
 def pump_calibration(min_duration: float, max_duration: float, json_file: str | None) -> None:
     unit = get_unit_name()
-    experiment = get_latest_experiment_name()
+    experiment = get_assigned_experiment_name(unit)
 
     logger = create_logger("pump_calibration", unit=unit, experiment=experiment)
     logger.info("Starting pump calibration.")
 
-    with publish_ready_to_disconnected_state(unit, experiment, "pump_calibration"):
+    with managed_lifecycle(unit, experiment, "pump_calibration"):
         clear()
         if json_file is None:
             introduction()
 
         pump_type, execute_pump = which_pump_are_you_calibrating()
         name = get_metadata_from_user(pump_type)
```

## pioreactor/actions/self_test.py

```diff
@@ -37,23 +37,23 @@
 from pioreactor.logging import create_logger
 from pioreactor.logging import CustomLogger
 from pioreactor.pubsub import Client
 from pioreactor.types import LedChannel
 from pioreactor.types import PdChannel
 from pioreactor.utils import is_pio_job_running
 from pioreactor.utils import local_persistant_storage
-from pioreactor.utils import publish_ready_to_disconnected_state
+from pioreactor.utils import managed_lifecycle
 from pioreactor.utils import SummableDict
 from pioreactor.utils.math_helpers import correlation
 from pioreactor.utils.math_helpers import mean
 from pioreactor.utils.math_helpers import trimmed_mean
 from pioreactor.utils.math_helpers import variance
 from pioreactor.version import hardware_version_info
-from pioreactor.whoami import get_latest_experiment_name
-from pioreactor.whoami import get_latest_testing_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
+from pioreactor.whoami import get_testing_experiment_name
 from pioreactor.whoami import get_unit_name
 from pioreactor.whoami import is_testing_env
 
 
 def test_pioreactor_HAT_present(client: Client, logger: CustomLogger, unit: str, experiment: str) -> None:
     assert is_HAT_present(), "HAT is not connected, or i2c is not working."
 
@@ -127,41 +127,43 @@
     # what's up with this order? We originally did a shuffle() of list(range(20, 55, 3))
     # so as to reduce the effects of temperature.
     # the problem is that if an LED is directly across from a PD, a high intensity will quickly
     # saturate it and fail the test. So we try low intensities first, and if we exceed some threshold
     # we exit before moving to the high intensities.
     INTENSITIES = [20, 23, 26, 53, 44, 38, 35, 29, 47, 50, 32, 41]
 
-    current_experiment_name = get_latest_experiment_name()
     results: dict[tuple[LedChannel, PdChannel], float] = {}
 
     adc_reader = ADCReader(
         channels=ALL_PD_CHANNELS, dynamic_gain=False, fake_data=is_testing_env(), penalizer=0.0
     )
     adc_reader.setup_adc()
 
+    ir_led_channel = cast(LedChannel, config["leds_reverse"][IR_keyword])
+
     # set all to 0, but use original experiment name, since we indeed are setting them to 0.
     led_intensity(
         {channel: 0 for channel in ALL_LED_CHANNELS},
         unit=unit,
-        experiment=current_experiment_name,
+        experiment=experiment,
         verbose=False,
         source_of_event="self_test",
     )
 
-    for led_channel in ALL_LED_CHANNELS:
+    # for led_channel in ALL_LED_CHANNELS: # we use to check all LED channels, but most users don't need to check all, also https://github.com/Pioreactor/pioreactor/issues/445
+    for led_channel in [ir_led_channel]:  # fast to just check IR
         varying_intensity_results: dict[PdChannel, list[float]] = {
             pd_channel: [] for pd_channel in ALL_PD_CHANNELS
         }
         for intensity in INTENSITIES:
             # turn on the LED to set intensity
             led_intensity(
                 {led_channel: intensity},
                 unit=unit,
-                experiment=current_experiment_name,
+                experiment=experiment,
                 verbose=False,
                 source_of_event="self_test",
             )
 
             # record from ADC, we'll average them
             avg_reading = average_over_pd_channel_to_voltages(
                 adc_reader.take_reading(), adc_reader.take_reading()
@@ -183,15 +185,15 @@
             results[(led_channel, pd_channel)] = measured_correlation
             logger.debug(f"Corr({led_channel}, {pd_channel}) = {measured_correlation}")
 
         # set back to 0
         led_intensity(
             {led_channel: 0},
             unit=unit,
-            experiment=current_experiment_name,
+            experiment=experiment,
             verbose=False,
             source_of_event="self_test",
         )
         adc_reader.clear_batched_readings()
 
     logger.debug(f"Correlations between LEDs and PD:\n{pformat(results)}")
     detected_relationships = []
@@ -214,36 +216,33 @@
     # correlation with the IR led
     pd_channels_to_test: list[PdChannel] = []
     for channel, angle_or_ref in config["od_config.photodiode_channel"].items():
         if angle_or_ref != "":
             channel = cast(PdChannel, channel)
             pd_channels_to_test.append(channel)
 
-    ir_led_channel = cast(LedChannel, config["leds_reverse"][IR_keyword])
-
     for ir_pd_channel in pd_channels_to_test:
         assert results[(ir_led_channel, ir_pd_channel)] > 0.9, f"missing {ir_led_channel} ⇝ {ir_pd_channel}"
 
 
 def test_ambient_light_interference(client: Client, logger: CustomLogger, unit: str, experiment: str) -> None:
     # test ambient light IR interference. With all LEDs off, and the Pioreactor not in a sunny room, we should see near 0 light.
     assert is_HAT_present()
     adc_reader = ADCReader(
         channels=ALL_PD_CHANNELS,
         dynamic_gain=False,
         fake_data=is_testing_env(),
     )
 
     adc_reader.setup_adc()
-    current_experiment_name = get_latest_experiment_name()
     led_intensity(
         {channel: 0 for channel in ALL_LED_CHANNELS},
         unit=unit,
         source_of_event="self_test",
-        experiment=current_experiment_name,
+        experiment=experiment,
         verbose=False,
     )
 
     readings = adc_reader.take_reading()
 
     if hardware_version_info < (1, 1):
         assert all([readings[pd_channel] < 0.005 for pd_channel in ALL_PD_CHANNELS]), readings
@@ -265,20 +264,19 @@
         ir_intensity = float(config_ir_intensity)
 
     adc_reader = ADCReader(
         channels=[reference_channel], dynamic_gain=False, fake_data=is_testing_env(), penalizer=0.0
     )
     adc_reader.setup_adc()
 
-    current_experiment_name = get_latest_experiment_name()
     with change_leds_intensities_temporarily(
         {ir_channel: ir_intensity},
         unit=unit,
         source_of_event="self_test",
-        experiment=current_experiment_name,
+        experiment=experiment,
         verbose=False,
     ):
         samples = []
 
         for i in range(6):
             samples.append(adc_reader.take_reading()[reference_channel])
 
@@ -424,15 +422,15 @@
             test_name = test.__name__
 
             try:
                 test(client, logger, unit, experiment_name)
                 res = True
             except Exception as e:
                 logger.debug(e, exc_info=True)
-                logger.error(e)
+                logger.warning(f"{test_name.replace('_', ' ')}: {e}")
 
             logger.debug(f"{test_name}: {'✅' if res else '❌'}")
 
             self.count_tested += 1
             self.count_passed += int(res)
 
             client.publish(
@@ -445,16 +443,16 @@
 @click.command(name="self_test")
 @click.option("-k", help="see pytest's -k argument", type=str)
 def click_self_test(k: Optional[str]) -> int:
     """
     Test the input/output in the Pioreactor
     """
     unit = get_unit_name()
-    testing_experiment = get_latest_testing_experiment_name()
-    experiment = get_latest_experiment_name()
+    testing_experiment = get_testing_experiment_name()
+    experiment = get_assigned_experiment_name(unit)
     logger = create_logger("self_test", unit=unit, experiment=experiment)
 
     A_TESTS = [
         test_pioreactor_HAT_present,
         test_detect_heating_pcb,
         test_positive_correlation_between_temperature_and_heating,
         test_aux_power_is_not_too_high,
@@ -464,15 +462,15 @@
         test_ambient_light_interference,
         test_REF_is_lower_than_0_dot_256_volts,
         test_REF_is_in_correct_position,
         test_PD_is_near_0_volts_for_blank,
         test_positive_correlation_between_rpm_and_stirring,
     ]
 
-    with publish_ready_to_disconnected_state(unit, testing_experiment, "self_test") as state:
+    with managed_lifecycle(unit, testing_experiment, "self_test") as state:
         client = state.mqtt_client
         if any(
             is_pio_job_running(
                 ["od_reading", "temperature_control", "stirring", "dosing_control", "led_control"]
             )
         ):
             logger.error(
```

## pioreactor/actions/stirring_calibration.py

```diff
@@ -13,35 +13,33 @@
 
 from pioreactor.background_jobs import stirring
 from pioreactor.config import config
 from pioreactor.logging import create_logger
 from pioreactor.pubsub import publish
 from pioreactor.utils import is_pio_job_running
 from pioreactor.utils import local_persistant_storage
-from pioreactor.utils import publish_ready_to_disconnected_state
+from pioreactor.utils import managed_lifecycle
 from pioreactor.utils.math_helpers import simple_linear_regression
 from pioreactor.utils.timing import current_utc_timestamp
-from pioreactor.whoami import get_latest_experiment_name
-from pioreactor.whoami import get_latest_testing_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
+from pioreactor.whoami import get_testing_experiment_name
 from pioreactor.whoami import get_unit_name
 
 
 def stirring_calibration(min_dc: int, max_dc: int) -> None:
     unit = get_unit_name()
-    experiment = get_latest_testing_experiment_name()
+    experiment = get_testing_experiment_name()
     action_name = "stirring_calibration"
     logger = create_logger(action_name)
 
-    with publish_ready_to_disconnected_state(unit, get_latest_experiment_name(), action_name):
+    with managed_lifecycle(unit, get_assigned_experiment_name(unit), action_name):
         logger.info("Starting stirring calibration.")
 
         if is_pio_job_running("stirring"):
-            logger.error(
-                "Make sure Stirring job is off before running stirring calibration. Exiting."
-            )
+            logger.error("Make sure Stirring job is off before running stirring calibration. Exiting.")
             return
 
         measured_rpms = []
 
         # go up and down to observe any hysteresis.
         dcs = (
             list(range(max_dc, min_dc, -3))
@@ -75,17 +73,15 @@
                     f"pioreactor/{unit}/{experiment}/{action_name}/percent_progress",
                     count / n_samples * 100,
                 )
                 logger.debug(f"Progress: {count/n_samples:.0%}")
 
         # drop any 0 in RPM, too little DC
         try:
-            filtered_dcs, filtered_measured_rpms = zip(
-                *filter(lambda d: d[1] > 0, zip(dcs, measured_rpms))
-            )
+            filtered_dcs, filtered_measured_rpms = zip(*filter(lambda d: d[1] > 0, zip(dcs, measured_rpms)))
         except ValueError:
             # the above can fail if all measured rpms are 0
             logger.error("No RPMs were measured. Is the stirring spinning?")
             return
 
         if len(filtered_dcs) <= n_samples * 0.75:
             # the above can fail if all measured rpms are 0
@@ -98,17 +94,15 @@
         # set x=measure_rpms, y=dcs
         (rpm_coef, rpm_coef_std), (intercept, intercept_std) = simple_linear_regression(
             filtered_measured_rpms, filtered_dcs
         )
         logger.debug(f"{rpm_coef=}, {rpm_coef_std=}, {intercept=}, {intercept_std=}")
 
         if rpm_coef <= 0:
-            logger.warning(
-                "Something went wrong - detected negative correlation between RPM and stirring."
-            )
+            logger.warning("Something went wrong - detected negative correlation between RPM and stirring.")
             return
 
         elif intercept <= 0:
             logger.warning("Something went wrong - the intercept should be greater than 0.")
             return
 
         with local_persistant_storage(action_name) as cache:
@@ -142,16 +136,14 @@
     """
     Generate a lookup between stirring and voltage
     """
 
     if max_dc is None and min_dc is None:
         # seed with initial_duty_cycle
         config_initial_duty_cycle = config.getfloat("stirring", "initial_duty_cycle")
-        min_dc, max_dc = round(config_initial_duty_cycle * 0.75), round(
-            config_initial_duty_cycle * 1.33
-        )
+        min_dc, max_dc = round(config_initial_duty_cycle * 0.75), round(config_initial_duty_cycle * 1.33)
     elif (max_dc is not None) and (min_dc is not None):
         assert min_dc < max_dc, "min_dc >= max_dc"
     else:
         raise ValueError("min_dc and max_dc must both be set.")
 
     stirring_calibration(min_dc, max_dc)
```

## pioreactor/actions/leader/backup_database.py

```diff
@@ -1,18 +1,18 @@
 # -*- coding: utf-8 -*-
 from __future__ import annotations
 
 import click
 
+from pioreactor.cluster_management import get_active_workers_in_inventory
 from pioreactor.config import config
-from pioreactor.config import get_active_workers_in_inventory
 from pioreactor.logging import create_logger
 from pioreactor.pubsub import subscribe
 from pioreactor.utils import local_persistant_storage
-from pioreactor.utils import publish_ready_to_disconnected_state
+from pioreactor.utils import managed_lifecycle
 from pioreactor.utils.networking import add_local
 from pioreactor.utils.timing import current_utc_timestamp
 from pioreactor.whoami import get_unit_name
 from pioreactor.whoami import UNIVERSAL_EXPERIMENT
 
 
 def count_writes_occurring(unit: str) -> int:
@@ -44,15 +44,15 @@
 
     import sqlite3
     from sh import ErrorReturnCode, rsync  # type: ignore
 
     unit = get_unit_name()
     experiment = UNIVERSAL_EXPERIMENT
 
-    with publish_ready_to_disconnected_state(unit, experiment, "backup_database"):
+    with managed_lifecycle(unit, experiment, "backup_database", ignore_is_active_state=True):
         logger = create_logger(
             "backup_database", experiment=experiment, unit=unit, to_mqtt=False
         )  # the backup would take so long that the mqtt client would disconnect. We also don't want to write to the db.
 
         logger.debug(f"Starting backup of database to {output_file}")
 
         if not force and count_writes_occurring(unit) >= 2:
```

## pioreactor/actions/leader/experiment_profile.py

```diff
@@ -8,26 +8,24 @@
 from typing import Callable
 from typing import Optional
 
 import click
 from msgspec.json import encode
 from msgspec.yaml import decode
 
-from pioreactor.config import get_active_workers_in_inventory
+from pioreactor.cluster_management import get_active_workers_in_experiment
 from pioreactor.config import leader_address
 from pioreactor.experiment_profiles import profile_struct as struct
-from pioreactor.experiment_profiles.parser import check_syntax
-from pioreactor.experiment_profiles.parser import parse_profile_expression
-from pioreactor.experiment_profiles.parser import parse_profile_expression_to_bool
 from pioreactor.logging import create_logger
 from pioreactor.logging import CustomLogger
 from pioreactor.mureq import put
 from pioreactor.pubsub import publish
-from pioreactor.utils import publish_ready_to_disconnected_state
-from pioreactor.whoami import get_latest_experiment_name
+from pioreactor.utils import ClusterJobManager
+from pioreactor.utils import managed_lifecycle
+from pioreactor.whoami import get_assigned_experiment_name
 from pioreactor.whoami import get_unit_name
 
 bool_expression = str | bool
 
 
 def wrap_in_try_except(func, logger: CustomLogger) -> Callable:
     def inner_function(*args, **kwargs) -> None:
@@ -56,41 +54,47 @@
 
 
 def evaluate_options(options: dict, unit: str) -> dict:
     """
     Users can provide options like {'target_rpm': '${{ bioreactor_A:stirring:target_rpm + 10 }}'}, and the latter
     should be evaluated
     """
+    from pioreactor.experiment_profiles.parser import parse_profile_expression
+
     options_expressed = {}
     for key, value in options.items():
         if is_bracketed_expression(value):
             expression = strip_expression_brackets(value)
             # replace :: placeholder with unit
             expression = expression.replace("::", f"{unit}:", 1)
             options_expressed[key] = parse_profile_expression(expression)
         else:
             options_expressed[key] = value
     return options_expressed
 
 
 def evaluate_bool_expression(bool_expression: bool_expression, unit: str) -> bool:
+    from pioreactor.experiment_profiles.parser import parse_profile_expression_to_bool
+
     if isinstance(bool_expression, bool):
         return bool_expression
 
     if is_bracketed_expression(bool_expression):
         bool_expression = strip_expression_brackets(bool_expression)
 
     # replace :: placeholder with unit
     bool_expression = bool_expression.replace("::", f"{unit}:", 1)
 
     # bool_expression is a str
     return parse_profile_expression_to_bool(bool_expression)
 
 
 def check_syntax_of_bool_expression(bool_expression: bool_expression) -> bool:
+    from pioreactor.experiment_profiles.parser import check_syntax
+
     if isinstance(bool_expression, bool):
         return True
 
     if is_bracketed_expression(bool_expression):
         bool_expression = strip_expression_brackets(bool_expression)
 
     # in a common expressions, users can use ::word:work which is technically not allowed. For checking, we replace with garbage
@@ -211,15 +215,15 @@
     job_name: str,
     logger: CustomLogger,
     schedule: scheduler,
     action: struct.Action,
     dry_run: bool = False,
 ) -> Callable[..., None]:
     actions_to_execute = []
-    for worker in get_active_workers_in_inventory():
+    for worker in get_active_workers_in_experiment(experiment):
         actions_to_execute.append(
             wrapped_execute_action(worker, experiment, job_name, logger, schedule, action, dry_run)
         )
 
     return chain_functions(*actions_to_execute)
 
 
@@ -234,14 +238,18 @@
     while_: Optional[bool_expression],
     repeat_every_hours: float,
     max_hours: Optional[float],
     actions: list[struct.ActionWithoutRepeat],
     schedule: scheduler,
 ):
     def _callable() -> None:
+        # first check if the Pioreactor is still part of the experiment.
+        if get_assigned_experiment_name(unit) != experiment:
+            return
+
         if ((if_ is None) or evaluate_bool_expression(if_, unit)) and (
             ((while_ is None) or evaluate_bool_expression(while_, unit))
         ):
             for action in actions:
                 if action.hours_elapsed > repeat_every_hours:
                     logger.warning(
                         f"Action {action} hours_elapsed is greater than the repeat's repeat_every_hours. Skipping."
@@ -288,14 +296,17 @@
     job_name: str,
     options: struct._LogOptions,
     dry_run: bool,
     if_: Optional[str | bool],
     logger: CustomLogger,
 ) -> Callable[..., None]:
     def _callable() -> None:
+        # first check if the Pioreactor is still part of the experiment.
+        if get_assigned_experiment_name(unit) != experiment:
+            return
         if (if_ is None) or evaluate_bool_expression(if_, unit):
             level = options.level.lower()
             getattr(logger, level)(options.message.format(unit=unit, job=job_name, experiment=experiment))
         else:
             logger.debug(f"Action's `if` condition, `{if_}`, evaluated False. Skipping action.")
 
     return wrap_in_try_except(_callable, logger)
@@ -308,21 +319,30 @@
     options: dict,
     args: list,
     dry_run: bool,
     if_: Optional[str | bool],
     logger: CustomLogger,
 ) -> Callable[..., None]:
     def _callable() -> None:
+        # first check if the Pioreactor is still part of the experiment.
+        if get_assigned_experiment_name(unit) != experiment:
+            return
+
         if (if_ is None) or evaluate_bool_expression(if_, unit):
             if dry_run:
                 logger.info(f"Dry-run: Starting {job_name} on {unit} with options {options} and args {args}.")
             else:
                 publish(
                     f"pioreactor/{unit}/{experiment}/run/{job_name}",
-                    encode({"options": evaluate_options(options, unit), "args": args}),
+                    encode(
+                        {
+                            "options": evaluate_options(options, unit) | {"job_source": "experiment_profile"},
+                            "args": args,
+                        }
+                    ),
                 )
         else:
             logger.debug(f"Action's `if` condition, `{if_}`, evaluated False. Skipping action.")
 
     return wrap_in_try_except(_callable, logger)
 
 
@@ -331,14 +351,18 @@
     experiment: str,
     job_name: str,
     dry_run: bool,
     if_: Optional[str | bool],
     logger: CustomLogger,
 ) -> Callable[..., None]:
     def _callable() -> None:
+        # first check if the Pioreactor is still part of the experiment.
+        if get_assigned_experiment_name(unit) != experiment:
+            return
+
         if (if_ is None) or evaluate_bool_expression(if_, unit):
             if dry_run:
                 logger.info(f"Dry-run: Pausing {job_name} on {unit}.")
             else:
                 publish(f"pioreactor/{unit}/{experiment}/{job_name}/$state/set", "sleeping")
         else:
             logger.debug(f"Action's `if` condition, `{if_}`, evaluated False. Skipping action.")
@@ -351,14 +375,17 @@
     experiment: str,
     job_name: str,
     dry_run: bool,
     if_: Optional[str | bool],
     logger: CustomLogger,
 ) -> Callable[..., None]:
     def _callable() -> None:
+        # first check if the Pioreactor is still part of the experiment.
+        if get_assigned_experiment_name(unit) != experiment:
+            return
         if (if_ is None) or evaluate_bool_expression(if_, unit):
             if dry_run:
                 logger.info(f"Dry-run: Resuming {job_name} on {unit}.")
             else:
                 publish(f"pioreactor/{unit}/{experiment}/{job_name}/$state/set", "ready")
         else:
             logger.debug(f"Action's `if` condition, `{if_}`, evaluated False. Skipping action.")
@@ -371,14 +398,17 @@
     experiment: str,
     job_name: str,
     dry_run: bool,
     if_: Optional[str | bool],
     logger: CustomLogger,
 ) -> Callable[..., None]:
     def _callable() -> None:
+        # first check if the Pioreactor is still part of the experiment.
+        if get_assigned_experiment_name(unit) != experiment:
+            return
         if (if_ is None) or evaluate_bool_expression(if_, unit):
             if dry_run:
                 logger.info(f"Dry-run: Stopping {job_name} on {unit}.")
             else:
                 publish(f"pioreactor/{unit}/{experiment}/{job_name}/$state/set", "disconnected")
         else:
             logger.debug(f"Action's `if` condition, `{if_}`, evaluated False. Skipping action.")
@@ -392,14 +422,17 @@
     job_name: str,
     options: dict,
     dry_run: bool,
     if_: Optional[str | bool],
     logger: CustomLogger,
 ) -> Callable[..., None]:
     def _callable() -> None:
+        # first check if the Pioreactor is still part of the experiment.
+        if get_assigned_experiment_name(unit) != experiment:
+            return
         if (if_ is None) or evaluate_bool_expression(if_, unit):
             if dry_run:
                 for setting, value in options.items():
                     logger.info(f"Dry-run: Updating {setting} to {value} in {job_name} on {unit}.")
 
             else:
                 for setting, value in evaluate_options(options, unit).items():
@@ -410,15 +443,15 @@
     return wrap_in_try_except(_callable, logger)
 
 
 def hours_to_seconds(hours: float) -> float:
     return hours * 60 * 60
 
 
-def _verify_experiment_profile(profile: struct.Profile) -> struct.Profile:
+def _verify_experiment_profile(profile: struct.Profile) -> bool:
     # things to check for:
     # 1. Don't "stop" or "start" any *_automations
     # 2. Don't change generic settings on *_controllers, (Ex: changing target temp on temp_controller is wrong)
     # 3. check syntax of if statements
 
     actions_per_job = defaultdict(list)
 
@@ -469,33 +502,33 @@
             if (
                 isinstance(action, struct.Repeat)
                 and action.while_
                 and not check_syntax_of_bool_expression(action.while_)
             ):
                 raise SyntaxError(f"Syntax error in {action}: `{action.while_}`")
 
-    return profile
+    return True
 
 
 def _load_experiment_profile(profile_filename: str) -> struct.Profile:
     with open(profile_filename) as f:
         return decode(f.read(), type=struct.Profile)
 
 
 def load_and_verify_profile(profile_filename: str) -> struct.Profile:
     profile = _load_experiment_profile(profile_filename)
-    _verify_experiment_profile(profile)
+    assert _verify_experiment_profile(profile), "profile is incorrect"
     return profile
 
 
-def push_labels_to_ui(labels_map: dict[str, str]) -> None:
+def push_labels_to_ui(experiment, labels_map: dict[str, str]) -> None:
     try:
         for unit_name, label in labels_map.items():
             put(
-                f"http://{leader_address}/api/unit_labels/current",
+                f"http://{leader_address}/api/experiments/{experiment}/unit_labels",
                 encode({"unit": unit_name, "label": label}),
                 headers={"Content-Type": "application/json"},
             )
     except Exception:
         pass
 
 
@@ -536,20 +569,19 @@
         else:
             not_installed.append(plugin)
 
     if not_installed:
         raise ImportError(f"Missing packages {not_installed}")
 
 
-def execute_experiment_profile(profile_filename: str, dry_run: bool = False) -> None:
+def execute_experiment_profile(profile_filename: str, experiment: str, dry_run: bool = False) -> None:
     unit = get_unit_name()
-    experiment = get_latest_experiment_name()
     action_name = "experiment_profile"
-    logger = create_logger(action_name)
-    with publish_ready_to_disconnected_state(unit, experiment, action_name) as state:
+    logger = create_logger(action_name, unit=unit, experiment=experiment)
+    with managed_lifecycle(unit, experiment, action_name, ignore_is_active_state=True) as state:
         try:
             profile = load_and_verify_profile(profile_filename)
         except Exception as e:
             logger.error(e)
             raise e
 
         state.mqtt_client.publish(
@@ -593,15 +625,15 @@
                 )
 
         # process specific pioreactors
         for unit_ in profile.pioreactors:
             pioreactor_specific_block = profile.pioreactors[unit_]
             if pioreactor_specific_block.label is not None:
                 label = pioreactor_specific_block.label
-                push_labels_to_ui({unit_: label})
+                push_labels_to_ui(experiment, {unit_: label})
 
             for job_name, job in pioreactor_specific_block.jobs.items():
                 for action in job.actions:
                     s.enter(
                         delay=hours_to_seconds(action.hours_elapsed),
                         priority=get_simple_priority(action),
                         action=wrapped_execute_action(
@@ -611,62 +643,73 @@
                             logger,
                             s,
                             action,
                             dry_run,
                         ),
                     )
 
-        logger.debug("Starting execution actions.")
+        logger.debug("Starting execution of actions.")
 
         try:
             # try / finally to handle keyboard interrupts
 
             # the below is so the schedule can be canceled by setting the event.
             while not state.exit_event.wait(timeout=0):
                 next_event_in = s.run(blocking=False)
                 if next_event_in is not None:
                     time.sleep(min(0.5, next_event_in))
                 else:
                     break
         finally:
-            state.mqtt_client.publish(
-                f"pioreactor/{unit}/{experiment}/{action_name}/experiment_profile_name",
-                None,
-                retain=True,
-            )
-
             if state.exit_event.is_set():
                 # ended early
-                logger.notice(f"Exiting profile {profile.experiment_profile_name} early: {len(s.queue)} actions not started.")  # type: ignore
+
+                # stop all jobs started
+                # we can use active workers in experiment, since if a worker leaves an experiment or goes inactive, it's jobs are stopped
+                workers = get_active_workers_in_experiment(experiment)
+                with ClusterJobManager(workers) as jm:
+                    jm.kill_jobs(experiment=experiment, job_source="experiment_profile")
+
+                logger.notice(f"Stopping profile {profile.experiment_profile_name} early: {len(s.queue)} actions not started, and stopping all started actions.")  # type: ignore
+
             else:
                 if dry_run:
                     logger.notice(  # type: ignore
                         f"Finished executing DRY-RUN of profile {profile.experiment_profile_name}."
                     )
 
                 else:
                     logger.notice(f"Finished executing profile {profile.experiment_profile_name}.")  # type: ignore
 
+            state.mqtt_client.publish(
+                f"pioreactor/{unit}/{experiment}/{action_name}/experiment_profile_name",
+                None,
+                retain=True,
+            )
+
+            logger.clean_up()
+
 
 @click.group(name="experiment_profile")
 def click_experiment_profile():
     """
     (leader only) Run and manage experiment profiles
     """
     pass
 
 
 @click_experiment_profile.command(name="execute")
 @click.argument("filename", type=click.Path())
+@click.argument("experiment", type=str)
 @click.option("--dry-run", is_flag=True, help="Don't actually execute, just print to screen")
-def click_execute_experiment_profile(filename: str, dry_run: bool) -> None:
+def click_execute_experiment_profile(filename: str, experiment, dry_run: bool) -> None:
     """
     (leader only) Run an experiment profile.
     """
-    execute_experiment_profile(filename, dry_run)
+    execute_experiment_profile(filename, experiment, dry_run)
 
 
 @click_experiment_profile.command(name="verify")
 @click.argument("filename", type=click.Path())
 def click_verify_experiment_profile(filename: str) -> None:
     """
     (leader only) Verify an experiment profile for correctness.
```

## pioreactor/actions/leader/export_experiment_data.py

```diff
@@ -52,22 +52,21 @@
 
 
 def export_experiment_data(
     experiment: Optional[str], output: str, partition_by_unit: bool, tables: list
 ) -> None:
     """
     Set an experiment, else it defaults to the entire table.
-
     """
     import sqlite3
     import zipfile
     import csv
 
     if not output.endswith(".zip"):
-        print("output should end with .zip")
+        click.echo("output should end with .zip")
         raise click.Abort()
 
     logger = create_logger("export_experiment_data")
     logger.info(f"Starting export of table{'s' if len(tables) > 1 else ''}: {', '.join(tables)}.")
 
     time = datetime.now().strftime("%Y%m%d%H%M%S")
 
@@ -118,26 +117,25 @@
             else:
                 if experiment is not None:
                     query = f"SELECT {timestamp_to_localtimestamp_clause} * from {table} WHERE experiment=:experiment ORDER BY :order_by"
                     cursor.execute(query, {"experiment": experiment, "order_by": order_by})
                 else:
                     query = f"SELECT {timestamp_to_localtimestamp_clause} * from {table} ORDER BY :order_by"
                     cursor.execute(query, {"order_by": order_by})
-                    experiment = "all_experiments"
 
                 headers = [_[0] for _ in cursor.description]
                 iloc_pioreactor_unit = headers.index("pioreactor_unit")
                 filenames = []
                 unit_to_writer_map = {}
 
                 with ExitStack() as stack:
                     for row in cursor:
                         unit = row[iloc_pioreactor_unit]
                         if unit not in unit_to_writer_map:
-                            filename = f"{experiment}-{table}-{unit}-{time}.csv"
+                            filename = f"{experiment or 'exp'}-{table}-{unit}-{time}.csv"
                             filenames.append(filename)
                             path_to_file = os.path.join(os.path.dirname(output), filename)
                             unit_to_writer_map[unit] = csv.writer(
                                 stack.enter_context(open(path_to_file, "w")), delimiter=","
                             )
                             unit_to_writer_map[unit].writerow(headers)
```

## pioreactor/automations/dosing/base.py

```diff
@@ -543,20 +543,26 @@
         ):
             self._latest_settings_ended_at = current_utc_datetime()
             self._send_details_to_mqtt()
             self._latest_settings_started_at = current_utc_datetime()
             self._latest_settings_ended_at = None
 
     def _set_growth_rate(self, message: pt.MQTTMessage) -> None:
+        if not message.payload:
+            return
+
         self.previous_growth_rate = self._latest_growth_rate
         payload = decode(message.payload, type=structs.GrowthRate)
         self._latest_growth_rate = payload.growth_rate
         self.latest_growth_rate_at = payload.timestamp
 
     def _set_normalized_od(self, message: pt.MQTTMessage) -> None:
+        if not message.payload:
+            return
+
         self.previous_normalized_od = self._latest_normalized_od
         payload = decode(message.payload, type=structs.ODFiltered)
         self._latest_normalized_od = payload.od_filtered
         self.latest_normalized_od_at = payload.timestamp
 
     def _set_ods(self, message: pt.MQTTMessage) -> None:
         if not message.payload:
```

## pioreactor/automations/dosing/chemostat.py

```diff
@@ -19,17 +19,17 @@
     }
 
     def __init__(self, volume: float | str, **kwargs) -> None:
         super().__init__(**kwargs)
 
         with local_persistant_storage("current_pump_calibration") as cache:
             if "media" not in cache:
-                raise CalibrationError("Media pump calibration must be performed first.")
+                raise CalibrationError("Media and waste pump calibration must be performed first.")
             elif "waste" not in cache:
-                raise CalibrationError("Waste pump calibration must be performed first.")
+                raise CalibrationError("Media and waste pump calibration must be performed first.")
 
         self.volume = float(volume)
 
     def execute(self) -> events.DilutionEvent:
         volume_actually_cycled = self.execute_io_action(media_ml=self.volume, waste_ml=self.volume)
         return events.DilutionEvent(
             f"exchanged {volume_actually_cycled['waste_ml']}mL",
```

## pioreactor/background_jobs/base.py

```diff
@@ -2,36 +2,38 @@
 from __future__ import annotations
 
 import atexit
 import signal
 import threading
 import typing as t
 from copy import copy
+from os import environ
 from os import getpid
 from time import sleep
 from time import time
 
 from msgspec.json import decode as loads
 from msgspec.json import encode as dumps
 
 from pioreactor import structs
 from pioreactor import types as pt
 from pioreactor.config import config
 from pioreactor.config import leader_hostname
+from pioreactor.exc import NotActiveWorkerError
 from pioreactor.logging import create_logger
 from pioreactor.pubsub import Client
 from pioreactor.pubsub import create_client
 from pioreactor.pubsub import MQTT_TOPIC
 from pioreactor.pubsub import QOS
 from pioreactor.pubsub import subscribe
 from pioreactor.utils import append_signal_handlers
 from pioreactor.utils import is_pio_job_running
-from pioreactor.utils import local_intermittent_storage
-from pioreactor.utils.timing import current_utc_timestamp
+from pioreactor.utils import JobManager
 from pioreactor.utils.timing import RepeatedTimer
+from pioreactor.whoami import is_active
 from pioreactor.whoami import is_testing_env
 from pioreactor.whoami import UNIVERSAL_IDENTIFIER
 
 
 T = t.TypeVar("T")
 BJT = t.TypeVar("BJT", bound="_BackgroundJob")
 
@@ -258,14 +260,17 @@
             raise ValueError("Job name not allowed.")
         if not self.job_name.islower():
             raise ValueError("Job name should be all lowercase.")
 
         self.experiment = experiment
         self.unit = unit
         self._source = source
+        self._job_source = environ.get(
+            "JOB_SOURCE", default="user"
+        )  # ex: could be JOB_SOURCE=experiment_profile, or JOB_SOURCE=external_provider
 
         # why do we need two clients? Paho lib can't publish a message in a callback,
         # but this is critical to our usecase: listen for events, and fire a response (ex: state change)
         # so we split the listening and publishing. I've tried combining them and got stuck a lot
         # https://github.com/Pioreactor/pioreactor/blob/cb54974c9be68616a7f4fb45fe60fdc063c81238/pioreactor/background_jobs/base.py
         # See issue: https://github.com/eclipse/paho.mqtt.python/issues/527
         # The order we add them to the list is important too, as disconnects occur async,
@@ -330,30 +335,18 @@
         C.__init__() # risk of job failing here
         P.__post__init__()  # write metadata to disk
         P.on_init_to_ready()  # default noop - can be overwritten in sub.
         P.ready()
         C.on_ready()
         """
 
-        with local_intermittent_storage(f"job_metadata_{self.job_name}") as cache:
-            # we set the "lock" in ready as then we know the __init__ finished successfully. Previously,
-            # __init__ might fail, and not clean up pio_job_* correctly.
-            # the catch is that there is a window where two jobs can be started, see growth_rate_calculating.
-            # sol for authors: move the long-running parts to the on_init_to_ready function.
-            cache["started_at"] = current_utc_timestamp()
-            cache["is_running"] = "1"
-            cache["source"] = self._source
-            cache["experiment"] = self.experiment
-            cache["unit"] = self.unit
-            cache["leader_hostname"] = leader_hostname
-            cache["pid"] = getpid()
-            cache["ended_at"] = ""  # populated later
-
-        with local_intermittent_storage("pio_jobs_running") as cache:
-            cache[self.job_name] = getpid()
+        with JobManager() as jm:
+            self._jm_key = jm.register_and_set_running(
+                self.unit, self.experiment, self.job_name, self._job_source, getpid(), leader_hostname
+            )
 
         self.set_state(self.READY)
 
     def start_passive_listeners(self) -> None:
         # overwrite this to in subclasses to subscribe to topics in MQTT
         # using this handles reconnects correctly.
         pass
@@ -782,28 +775,21 @@
 
         self._log_state(self.state)
 
         # we "set" the internal event, which will cause any event.waits to finishing blocking.
         self._blocking_event.set()
 
     def _remove_from_cache(self) -> None:
-        with local_intermittent_storage(f"job_metadata_{self.job_name}") as cache:
-            cache["is_running"] = "0"
-            cache["ended_at"] = current_utc_timestamp()
-
-        with local_intermittent_storage("pio_jobs_running") as cache:
-            cache.pop(self.job_name)
+        if hasattr(self, "_jm_key"):
+            with JobManager() as jm:
+                jm.set_not_running(self._jm_key)
 
     def _disconnect_from_loggers(self) -> None:
         # clean up logger handlers
-
-        handlers = self.logger.logger.handlers[:]
-        for handler in handlers:
-            self.logger.logger.removeHandler(handler)
-            handler.close()
+        self.logger.clean_up()
 
     def _disconnect_from_mqtt_clients(self) -> None:
         # disconnect from MQTT
         self.sub_client.loop_stop()
         self.sub_client.disconnect()
 
         # this HAS to happen last, because this contains our publishing client
@@ -969,20 +955,33 @@
     def __enter__(self: BJT) -> BJT:
         return self
 
     def __exit__(self, *args) -> None:
         self.clean_up()
 
 
+class LongRunningBackgroundJob(_BackgroundJob):
+    """
+    This doesn't check for is_active, so should be used for jobs like monitor, etc.
+    """
+
+    def __init__(self, unit: str, experiment: str) -> None:
+        super().__init__(unit, experiment, source="app")
+
+
 class BackgroundJob(_BackgroundJob):
     """
-    Native jobs should inherit from this class.
+    Worker jobs should inherit from this class.
     """
 
     def __init__(self, unit: str, experiment: str) -> None:
+        if not is_active(unit):
+            raise NotActiveWorkerError(
+                f"{unit} is not active. Make active in leader, or set ACTIVE=1 in the environment: ACTIVE=1 pio run ... "
+            )
         super().__init__(unit, experiment, source="app")
 
 
 class BackgroundJobContrib(_BackgroundJob):
     """
     Plugin jobs should inherit from this class.
     """
```

## pioreactor/background_jobs/dosing_control.py

```diff
@@ -74,17 +74,15 @@
             )
 
         self.automation = DosingAutomation(automation_name=automation_name, args=kwargs)
         self.automation_name = self.automation.automation_name
         self.logger.info(f"Starting {self.automation}.")
 
         try:
-            self.automation_job = automation_class(
-                unit=self.unit, experiment=self.experiment, **kwargs
-            )
+            self.automation_job = automation_class(unit=self.unit, experiment=self.experiment, **kwargs)
         except Exception as e:
             self.logger.error(e)
             self.logger.debug(e, exc_info=True)
             self.clean_up()
             raise e
 
     def set_automation(self, algo_metadata: DosingAutomation) -> None:
@@ -102,17 +100,15 @@
             # sometimes the user will change the job too fast before the dosing job is created, let's protect against that.
             time.sleep(1)
             self.set_automation(algo_metadata)
 
         try:
             klass = self.available_automations[algo_metadata.automation_name]
             self.logger.info(f"Starting {algo_metadata}.")
-            self.automation_job = klass(
-                unit=self.unit, experiment=self.experiment, **algo_metadata.args
-            )
+            self.automation_job = klass(unit=self.unit, experiment=self.experiment, **algo_metadata.args)
             self.automation = algo_metadata
             self.automation_name = self.automation.automation_name
         except KeyError:
             self.logger.debug(
                 f"Unable to find automation {algo_metadata.automation_name}. Available automations are {list(self.available_automations.keys())}. Note: You need to restart this job to have access to newly-added automations.",
                 exc_info=True,
             )
@@ -147,17 +143,18 @@
     automation_name: str,
     duration: Optional[Union[float, str]] = None,
     skip_first_run: bool = False,
     unit: Optional[str] = None,
     experiment: Optional[str] = None,
     **kwargs,
 ) -> DosingController:
+    unit = unit or whoami.get_unit_name()
     return DosingController(
-        unit=unit or whoami.get_unit_name(),
-        experiment=experiment or whoami.get_latest_experiment_name(),
+        unit=unit,
+        experiment=experiment or whoami.get_assigned_experiment_name(unit),
         automation_name=automation_name,
         duration=duration,
         skip_first_run=skip_first_run,
         **kwargs,
     )
```

## pioreactor/background_jobs/growth_rate_calculating.py

```diff
@@ -37,15 +37,14 @@
 from __future__ import annotations
 
 from collections import defaultdict
 from datetime import datetime
 from json import dumps
 from json import loads
 from typing import Generator
-from typing import Optional
 
 import click
 from msgspec import DecodeError
 from msgspec.json import decode
 
 from pioreactor import structs
 from pioreactor import types as pt
@@ -179,15 +178,15 @@
             angle for (_, angle) in config["od_config.photodiode_channel"].items() if angle in VALID_PD_ANGLES
         ]
 
         self.logger.debug(f"{angles=}")
         ekf_outlier_std_threshold = config.getfloat(
             "growth_rate_calculating.config",
             "ekf_outlier_std_threshold",
-            fallback=5.0,
+            fallback=3.0,
         )
         if ekf_outlier_std_threshold <= 2.0:
             raise ValueError(
                 "outlier_std_threshold should not be less than 2.0 - that's eliminating too many data points."
             )
 
         self.logger.debug(f"{ekf_outlier_std_threshold=}")
@@ -372,46 +371,43 @@
                 interval = float(msg.payload)
             else:
                 interval = 5
             self.ekf.scale_OD_variance_for_next_n_seconds(factor, minutes * (12 * interval))
         else:
             self.ekf.scale_OD_variance_for_next_n_seconds(factor, minutes * 60)
 
-    def scale_raw_observations(
-        self, observations: dict[pt.PdChannel, float]
-    ) -> Optional[dict[pt.PdChannel, float]]:
+    def scale_raw_observations(self, observations: dict[pt.PdChannel, float]) -> dict[pt.PdChannel, float]:
         def _scale_and_shift(obs, shift, scale) -> float:
             return (obs - shift) / (scale - shift)
 
         scaled_signals = {
             channel: _scale_and_shift(
                 raw_signal, self.od_blank[channel], self.od_normalization_factors[channel]
             )
             for channel, raw_signal in observations.items()
         }
 
         if any(v <= 0.0 for v in scaled_signals.values()):
-            self.logger.warning(
-                f"Negative normalized value(s) observed: {scaled_signals}. Did your blank have inoculant in it?"
-            )
             self.logger.debug(f"od_normalization_factors: {self.od_normalization_factors}")
             self.logger.debug(f"od_blank: {dict(self.od_blank)}")
-            return None
+            raise ValueError(
+                f"Negative normalized value(s) observed: {scaled_signals}. Did your blank have inoculant in it?"
+            )
 
         return scaled_signals
 
     def respond_to_od_readings_from_mqtt(self, message: pt.MQTTMessage) -> None:
         if self.state != self.READY:
             return
 
         try:
             od_readings = decode(message.payload, type=structs.ODReadings)
             self.update_state_from_observation(od_readings)
         except DecodeError:
-            pass
+            self.logger.debug(f"Decode error in `{message.payload.decode()}` to structs.ODReadings")
 
         return
 
     def update_state_from_observation(
         self, od_readings: structs.ODReadings
     ) -> tuple[structs.GrowthRate, structs.ODFiltered, structs.KalmanFilterOutput]:
         """
@@ -421,15 +417,15 @@
             (
                 self.growth_rate,
                 self.od_filtered,
                 self.kalman_filter_outputs,
             ) = self._update_state_from_observation(od_readings)
         except Exception as e:
             self.logger.debug(e, exc_info=True)
-            self.logger.warning(f"Updating Kalman Filter failed with {str(e)}")
+            self.logger.warning(f"Updating Kalman Filter failed with {e}")
             # just return the previous data
             return self.growth_rate, self.od_filtered, self.kalman_filter_outputs
 
         # save to cache
         with local_persistant_storage("growth_rate") as cache:
             cache[self.experiment] = self.growth_rate.growth_rate
 
@@ -438,20 +434,18 @@
 
         return self.growth_rate, self.od_filtered, self.kalman_filter_outputs
 
     def _update_state_from_observation(
         self, od_readings: structs.ODReadings
     ) -> tuple[structs.GrowthRate, structs.ODFiltered, structs.KalmanFilterOutput]:
         timestamp = od_readings.timestamp
+
         scaled_observations = self.scale_raw_observations(
             self._batched_raw_od_readings_to_dict(od_readings.ods)
         )
-        if scaled_observations is None:
-            # exit early
-            raise ValueError()
 
         if whoami.is_testing_env():
             # when running a mock script, we run at an accelerated rate, but want to mimic
             # production.
             dt = self.expected_dt
         else:
             if self.time_of_previous_observation is not None:
@@ -459,15 +453,15 @@
                     (timestamp - self.time_of_previous_observation).total_seconds() / 60 / 60
                 )  # delta time in hours
 
                 if dt < 0:
                     self.logger.debug(
                         f"Late arriving data: {timestamp=}, {self.time_of_previous_observation=}"
                     )
-                    raise ValueError()
+                    raise ValueError("Late arriving data: {timestamp=}, {self.time_of_previous_observation=}")
 
             else:
                 dt = 0.0
 
             self.time_of_previous_observation = timestamp
 
         updated_state_, covariance_ = self.ekf.update(list(scaled_observations.values()), dt)
@@ -566,25 +560,29 @@
     Start calculating growth rate
     """
     if ctx.invoked_subcommand is None:
         import os
 
         os.nice(1)
 
+        unit = whoami.get_unit_name()
+        experiment = whoami.get_assigned_experiment_name(unit)
+
         calculator = GrowthRateCalculator(  # noqa: F841
             ignore_cache=ignore_cache,
-            unit=whoami.get_unit_name(),
-            experiment=whoami.get_latest_experiment_name(),
+            unit=unit,
+            experiment=experiment,
         )
         calculator.block_until_disconnected()
 
 
 @click_growth_rate_calculating.command(name="clear_cache")
 def click_clear_cache() -> None:
-    experiment = whoami.get_latest_experiment_name()
+    unit = whoami.get_unit_name()
+    experiment = whoami.get_assigned_experiment_name(unit)
 
     with local_persistant_storage("od_filtered") as cache:
         cache.pop(experiment)
     with local_persistant_storage("growth_rate") as cache:
         cache.pop(experiment)
     with local_persistant_storage("od_normalization_mean") as cache:
         cache.pop(experiment)
```

## pioreactor/background_jobs/led_control.py

```diff
@@ -15,15 +15,15 @@
 
 import click
 
 from pioreactor import exc
 from pioreactor import hardware
 from pioreactor.background_jobs.base import BackgroundJob
 from pioreactor.structs import LEDAutomation
-from pioreactor.whoami import get_latest_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
 from pioreactor.whoami import get_unit_name
 
 
 class LEDController(BackgroundJob):
     # this is automagically populated
     available_automations = {}  # type: ignore
     job_name = "led_control"
@@ -47,17 +47,15 @@
             self.logger.error(msg)
             self.clean_up()
             raise KeyError(msg)
 
         self.automation = LEDAutomation(automation_name=automation_name, args=kwargs)
         self.logger.info(f"Starting {self.automation}.")
         try:
-            self.automation_job = automation_class(
-                unit=self.unit, experiment=self.experiment, **kwargs
-            )
+            self.automation_job = automation_class(unit=self.unit, experiment=self.experiment, **kwargs)
         except Exception as e:
             self.logger.error(e)
             self.logger.debug(e, exc_info=True)
             self.clean_up()
             raise e
         self.automation_name = self.automation.automation_name
 
@@ -70,17 +68,15 @@
             # sometimes the user will change the job too fast before the job is created, let's protect against that.
             time.sleep(1)
             self.set_automation(algo_metadata)
 
         try:
             klass = self.available_automations[algo_metadata.automation_name]
             self.logger.info(f"Starting {algo_metadata}.")
-            self.automation_job = klass(
-                unit=self.unit, experiment=self.experiment, **algo_metadata.args
-            )
+            self.automation_job = klass(unit=self.unit, experiment=self.experiment, **algo_metadata.args)
             self.automation = algo_metadata
             self.automation_name = self.automation.automation_name
         except KeyError:
             self.logger.debug(
                 f"Unable to find automation {algo_metadata.automation_name}. Available automations are {list(self.available_automations.keys())}. Note: You need to restart this job to have access to newly-added automations.",
                 exc_info=True,
             )
@@ -110,17 +106,19 @@
     automation_name: str,
     duration: Optional[float] = None,
     skip_first_run=False,
     unit: Optional[str] = None,
     experiment: Optional[str] = None,
     **kwargs,
 ) -> LEDController:
+    unit = unit or get_unit_name()
+    experiment = experiment or get_assigned_experiment_name(unit)
     return LEDController(
-        unit=unit or get_unit_name(),
-        experiment=experiment or get_latest_experiment_name(),
+        unit=unit,
+        experiment=experiment,
         automation_name=automation_name,
         skip_first_run=skip_first_run,
         duration=duration,
         **kwargs,
     )
```

## pioreactor/background_jobs/monitor.py

```diff
@@ -14,18 +14,19 @@
 import lgpio
 from msgspec.json import decode as loads
 
 from pioreactor import error_codes
 from pioreactor import utils
 from pioreactor import version
 from pioreactor import whoami
-from pioreactor.background_jobs.base import BackgroundJob
+from pioreactor.background_jobs.base import LongRunningBackgroundJob
 from pioreactor.config import config
 from pioreactor.config import get_config
-from pioreactor.config import mqtt_address
+from pioreactor.config import get_mqtt_address
+from pioreactor.exc import NotAssignedAnExperimentError
 from pioreactor.hardware import GPIOCHIP
 from pioreactor.hardware import is_HAT_present
 from pioreactor.hardware import PCB_BUTTON_PIN as BUTTON_PIN
 from pioreactor.hardware import PCB_LED_PIN as LED_PIN
 from pioreactor.hardware import TEMP
 from pioreactor.mureq import get
 from pioreactor.pubsub import QOS
@@ -39,15 +40,15 @@
 from pioreactor.utils.timing import to_datetime
 
 if whoami.is_testing_env():
     from pioreactor.utils.mock import MockCallback
     from pioreactor.utils.mock import MockHandle
 
 
-class Monitor(BackgroundJob):
+class Monitor(LongRunningBackgroundJob):
     """
     This job starts at Rpi startup, and isn't connected to any experiment. It has the following roles:
 
      1. Reports metadata (voltage, CPU usage, etc.) about the Rpi / Pioreactor to the leader
      2. Controls the LED / Button interaction. Plus any additional callbacks to the button down/up.
      3. Correction after a restart
      4. Check database backup if leader
@@ -87,14 +88,15 @@
     published_settings = {
         "computer_statistics": {"datatype": "json", "settable": False},
         "button_down": {"datatype": "boolean", "settable": False},
         "versions": {"datatype": "json", "settable": True},
         "voltage_on_pwm_rail": {"datatype": "Voltage", "settable": False},
         "ipv4": {"datatype": "string", "settable": False},
         "wlan_mac_address": {"datatype": "string", "settable": False},
+        "eth_mac_address": {"datatype": "string", "settable": False},
     }
     computer_statistics: Optional[dict] = None
     led_in_use: bool = False
     _pre_button: list[Callable] = []
     _post_button: list[Callable] = []
 
     def __init__(self, unit: str, experiment: str) -> None:
@@ -110,21 +112,17 @@
             "hat_serial": version.serial_number,
             "rpi_machine": version.rpi_version_info,
             "timestamp": current_utc_timestamp(),
         }
 
         self.logger.debug(f"Pioreactor software version: {self.versions['app']}")
         self.logger.debug(f"Raspberry Pi: {self.versions['rpi_machine']}")
-
-        if whoami.am_I_active_worker():
-            self.logger.debug(f"Pioreactor HAT version: {self.versions['hat']}")
-
-            self.logger.debug(f"Pioreactor firmware version: {self.versions['firmware']}")
-
-            self.logger.debug(f"Pioreactor HAT serial number: {self.versions['hat_serial']}")
+        self.logger.debug(f"Pioreactor HAT version: {self.versions['hat']}")
+        self.logger.debug(f"Pioreactor firmware version: {self.versions['firmware']}")
+        self.logger.debug(f"Pioreactor HAT serial number: {self.versions['hat_serial']}")
 
         self.button_down = False
         # set up GPIO for accessing the button and changing the LED
 
         try:
             # if these fail, don't kill the entire job - sucks for onboarding.
             self._setup_GPIO()
@@ -184,30 +182,41 @@
             self._button_callback = MockCallback()
             self._handle = MockHandle()
 
     def check_for_network(self) -> None:
         if whoami.is_testing_env():
             self.ipv4 = "127.0.0.1"
             self.wlan_mac_address = "d8:3a:dd:61:01:59"
+            self.eth_mac_address = "d8:3a:dd:61:01:60"
         else:
             ipv4 = get_ip()
             while ipv4 == "127.0.0.1" or ipv4 is None:
-                # no wifi connection? Sound the alarm.
+                # no connection? Sound the alarm.
                 self.logger.warning("Unable to connect to network...")
                 self.flicker_led_with_error_code(error_codes.NO_NETWORK_CONNECTION)
                 sleep(1)
                 ipv4 = get_ip()
 
             self.ipv4 = ipv4
 
-            with open("/sys/class/net/wlan0/address", "r") as f:
-                self.wlan_mac_address = f.read().strip()
+            try:
+                with open("/sys/class/net/wlan0/address", "r") as f:
+                    self.wlan_mac_address = f.read().strip()
+            except FileNotFoundError:
+                self.wlan_mac_address = "NA"
+
+            try:
+                with open("/sys/class/net/eth0/address", "r") as f:
+                    self.eth_mac_address = f.read().strip()
+            except FileNotFoundError:
+                self.eth_mac_address = "NA"
 
         self.logger.debug(f"IPv4 address: {self.ipv4}")
         self.logger.debug(f"WLAN MAC address: {self.wlan_mac_address}")
+        self.logger.debug(f"Ethernet MAC address: {self.wlan_mac_address}")
 
     def self_checks(self) -> None:
         # check active network connection
         self.check_for_network()
 
         # watch for undervoltage problems
         self.check_for_power_problems()
@@ -392,15 +401,15 @@
             try:
                 error_code_sc = self.sub_client.reconnect()
             except Exception:
                 pass
 
             self.logger.warning(
                 f"""Not able to connect MQTT clients to leader.
-1. Is the {mqtt_address=}, in config.ini correct?
+1. Is the mqtt_adress={get_mqtt_address()}, in config.ini correct?
 2. Is the Pioreactor leader online and responsive?
 """
             )  # remember, this doesn't get published to leader...
             self.logger.debug(f"{error_code_pc=}, {error_code_sc=}")
 
             # self.set_state(self.LOST)
             self.flicker_led_with_error_code(error_codes.MQTT_CLIENT_NOT_CONNECTED_TO_LEADER)
@@ -409,62 +418,25 @@
         with utils.local_persistant_storage("database_backups") as cache:
             if cache.get("latest_backup_timestamp"):
                 latest_backup_at = to_datetime(cache["latest_backup_timestamp"])
 
                 if (current_utc_datetime() - latest_backup_at).days > 30:
                     self.logger.warning("Database hasn't been backed up in over 30 days.")
 
-    def check_state_of_jobs_on_machine(self) -> None:
-        """
-        This compares jobs that are current running on the machine, vs
-        what MQTT says. In the case of a restart on leader, MQTT can get out
-        of sync. We only need to run this check on startup.
-
-        See answer here: https://iot.stackexchange.com/questions/5784/does-mosquito-broker-persist-lwt-messages-to-disk-so-they-may-be-recovered-betw
-        """
-        latest_exp = whoami._get_latest_experiment_name()
-
-        def check_against_processes_running(msg: MQTTMessage) -> None:
-            job = msg.topic.split("/")[3]
-            if (msg.payload.decode() in (self.READY, self.INIT, self.SLEEPING)) and (
-                not utils.is_pio_job_running(job)
-            ):
-                self.publish(
-                    f"pioreactor/{self.unit}/{latest_exp}/{job}/$state",
-                    self.LOST,
-                    retain=True,
-                )
-                self.logger.debug(f"Manually changing {job} state in MQTT.")
-
-        self.subscribe_and_callback(
-            check_against_processes_running,
-            f"pioreactor/{self.unit}/{latest_exp}/+/$state",
-        )
-
-        # let the above code run...
-        sleep(2.5)
-
-        # unsubscribe
-        self.sub_client.message_callback_remove(f"pioreactor/{self.unit}/{latest_exp}/+/$state")
-        self.sub_client.unsubscribe(f"pioreactor/{self.unit}/{latest_exp}/+/$state")
-
-        return
-
     def on_ready(self) -> None:
         self.flicker_led_response_okay()
         self.logger.notice(f"{self.unit} is online and ready.")  # type: ignore
 
         # we can delay this check until ready.
-        self.check_state_of_jobs_on_machine()
 
     def on_disconnected(self) -> None:
         self.led_off()
         with suppress(AttributeError):
-            lgpio.gpiochip_close(self._handle)
             self._button_callback.cancel()
+            lgpio.gpiochip_close(self._handle)
 
         set_gpio_availability(BUTTON_PIN, True)
         set_gpio_availability(LED_PIN, True)
 
     def led_on(self) -> None:
         if not whoami.is_testing_env():
             lgpio.gpio_write(self._handle, LED_PIN, 1)
@@ -629,55 +601,74 @@
         effectively runs:
         > pio run job_name arg1 arg2 --option-A value1 --option-B value2 --flag
 
         """
 
         # we use a thread below since we want to exit this callback without blocking it.
         # a blocked callback can disconnect from MQTT broker, prevent other callbacks, etc.
+        # TODO: we should this entire code into a thread...
 
-        job_name = msg.topic.split("/")[-1]
-        payload = loads(msg.payload) if msg.payload else {"options": {}, "args": []}
+        topic_parts = msg.topic.split("/")
 
-        # if "options" not in payload:
-        #    self.logger.debug("`options` key missing from payload. You should provide an empty dictionary.")
+        job_name = topic_parts[-1]
+        experiment = topic_parts[2]
 
-        options = payload.get("options", {})
+        if experiment != whoami.UNIVERSAL_EXPERIMENT:
+            # we put this into two if statements to minimize chances we have to fetch data.
+            try:
+                assigned_experiment = whoami._get_assigned_experiment_name(self.unit)
+            except NotAssignedAnExperimentError:
+                assigned_experiment = whoami.NO_EXPERIMENT
 
-        # if "args" not in payload:
-        #    self.logger.debug("`args` key missing from payload. You should provide an empty list.")
+            # make sure I'm assigned to the correct experiment
+            if experiment != assigned_experiment:
+                return
+
+        payload = loads(msg.payload) if msg.payload else {"options": {}, "args": []}
+
+        options = payload.get("options", {})
 
         args = payload.get("args", [])
 
         # this is a performance hack and should be changed later...
         if job_name == "led_intensity":
+            # TODO: this needs to check if active / assigned
+            # the below would work, but is very slow for a callback
+            # putting it in led_intensity makes everything else slow (ex: od_reading)
+            # if not whoami.is_active(self.unit):
+            #    return
+
             from pioreactor.actions.led_intensity import led_intensity, ALL_LED_CHANNELS
 
             state = {ch: options.pop(ch) for ch in ALL_LED_CHANNELS if ch in options}
             options["pubsub_client"] = self.pub_client
             options["unit"] = self.unit
-            options["experiment"] = whoami._get_latest_experiment_name()  # techdebt
+            options["experiment"] = experiment  # techdebt
+            options.pop("job_source", "")  # techdebt, led_intensity doesn't use job_source
             Thread(
                 target=utils.boolean_retry,
                 args=(led_intensity, (state,), options),
                 kwargs={"sleep_for": 0.4, "retries": 5},
             ).start()
 
         elif job_name in {
             "add_media",
             "add_alt_media",
             "remove_waste",
             "circulate_media",
             "circulate_alt_media",
         }:
+            # is_active is checked in the lifecycle block
+
             from pioreactor.actions import pump as pump_actions
 
             pump_action = getattr(pump_actions, job_name)
 
             options["unit"] = self.unit
-            options["experiment"] = whoami._get_latest_experiment_name()  # techdebt
+            options["experiment"] = experiment  # techdebt
             options["config"] = get_config()  # techdebt
             Thread(target=pump_action, kwargs=options, daemon=True).start()
             self.logger.debug(f"Running `{job_name}` from monitor job.")
 
         else:
             command = self._job_options_and_args_to_shell_command(job_name, args, options)
             Thread(
@@ -689,25 +680,26 @@
             self.logger.debug(f"Running `{command}` from monitor job.")
 
     @staticmethod
     def _job_options_and_args_to_shell_command(
         job_name: str, args: list[str], options: dict[str, Any]
     ) -> str:
         core_command = ["pio", "run", job_name]
+        env = [f'JOB_SOURCE={options.pop("job_source", "user")}']
 
         list_of_options: list[str] = []
         for option, value in options.items():
             list_of_options.append(f"--{option.replace('_', '-')}")
             if value is not None:
                 # this handles flag arguments, like --dry-run
                 list_of_options.append(str(value))
 
         # shell-escaped to protect against injection vulnerabilities, see join docs
         # we don't escape the suffix.
-        return f"nohup {join(core_command + args + list_of_options)} >/dev/null 2>&1 &"
+        return join(env + ["nohup"] + core_command + args + list_of_options) + " >/dev/null 2>&1 &"
 
     def flicker_error_code_from_mqtt(self, message: MQTTMessage) -> None:
         if self.led_in_use:
             return
 
         error_code = int(message.payload)
         Thread(target=self.flicker_led_with_error_code, args=(error_code,), daemon=True).start()
```

## pioreactor/background_jobs/od_reading.py

```diff
@@ -233,21 +233,24 @@
             self.adc_offsets[channel] = self.adc.from_voltage_to_raw_precise(blank_reading)
 
         self.logger.debug(
             f"ADC offsets: {self.adc_offsets}, and in voltage: { {c: self.adc.from_raw_to_voltage(i) for c, i in  self.adc_offsets.items()}}"
         )
 
     def check_on_max(self, value: pt.Voltage) -> None:
-        if value > 3.2:
+        unit = whoami.get_unit_name()
+        exp = whoami.get_assigned_experiment_name(unit)
+
+        if value <= 3.0:
+            return
+        elif value > 3.2:
             self.logger.error(
                 f"An ADC channel is recording a very high voltage, {round(value, 2)}V. We are shutting down components and jobs to keep the ADC safe."
             )
 
-            unit, exp = whoami.get_unit_name(), whoami.get_latest_experiment_name()
-
             with local_intermittent_storage("led_locks") as cache:
                 for c in led_utils.ALL_LED_CHANNELS:
                     cache.pop(c)
 
             # turn off all LEDs that might be causing problems
             # however, ODReader may turn on the IR LED again.
             led_utils.led_intensity(
@@ -271,15 +274,15 @@
             return
 
         elif value > 3.0:
             self.logger.warning(
                 f"An ADC channel is recording a very high voltage, {round(value, 2)}V. It's recommended to keep it less than 3.0V. Suggestion: decrease the IR intensity, or change the PD angle to a lower angle."
             )
             publish(
-                f"pioreactor/{whoami.get_unit_name()}/{whoami.get_latest_experiment_name()}/monitor/flicker_led_with_error_code",
+                f"pioreactor/{unit}/{exp}/monitor/flicker_led_with_error_code",
                 error_codes.ADC_INPUT_TOO_HIGH,
             )
             return
 
     def _sin_regression_with_known_freq(
         self,
         x: list[float],
@@ -897,64 +900,83 @@
             pubsub_client=self.pub_client,
             verbose=False,
         ):
             with led_utils.lock_leds_temporarily(self.non_ir_led_channels):
                 # IR led is on
                 self.start_ir_led()
                 sleep(0.10)
-                testing_signals = self.adc_reader.setup_adc()  # determine best gain, max-signal, etc.
 
-                if determine_best_ir_led_intensity:
-                    self.ir_led_intensity = self._determine_best_ir_led_intensity(testing_signals)
+                on_reading = self.adc_reader.setup_adc()  # determine best gain, max-signal, etc.
 
                 # IR led is off so we can set blanks
                 self.stop_ir_led()
                 sleep(0.10)
 
-                avg_blank_reading = average_over_pd_channel_to_voltages(
+                blank_reading = average_over_pd_channel_to_voltages(
                     self.adc_reader.take_reading(),
                     self.adc_reader.take_reading(),
                 )
-                self.adc_reader.set_offsets(avg_blank_reading)  # set dark offset
+                self.adc_reader.set_offsets(blank_reading)  # set dark offset
 
                 # clear the history in adc_reader, so that we don't blank readings in later inference.
                 self.adc_reader.clear_batched_readings()
 
+                if determine_best_ir_led_intensity:
+                    self.ir_led_intensity = self._determine_best_ir_led_intensity(on_reading, blank_reading)
+
         if (self.interval is not None) and self.interval > 0:
             if self.interval <= 1.0:
                 self.logger.warning(
                     f"Recommended to have the interval between readings be larger than 1.0 second. Currently {self.interval} s."
                 )
 
             self.record_from_adc_timer = timing.RepeatedTimer(
                 self.interval,
                 self.record_from_adc,
                 job_name=self.job_name,
                 run_immediately=True,
             ).start()
 
         self.logger.debug(
-            f"Starting od_reading with PD channels {channel_angle_map}, with IR LED intensity {self.ir_led_intensity}% from channel {self.ir_channel}."
+            f"Starting od_reading with PD channels {channel_angle_map}, with IR LED intensity {self.ir_led_intensity}% from channel {self.ir_channel}, every {self.interval} seconds"
         )
 
-    def _determine_best_ir_led_intensity(self, signals: PdChannelToVoltage) -> float:
+    def _determine_best_ir_led_intensity(
+        self, on_reading: PdChannelToVoltage, blank_reading: PdChannelToVoltage
+    ) -> float:
         for pd_channel in self.channel_angle_map:
-            signals.pop(pd_channel)
+            culture_on_signal = on_reading.pop(pd_channel)
+            blank_on_signal = blank_reading.pop(pd_channel)
+
+            # if the blank signal is too close to the culture signal, its possible the culture is very sparse
+            # this could create poor lower sensitivity, so we bump up the IR LED slightly.
+            # 1.5 and 0.1 are arbitrary!
+            if culture_on_signal / blank_on_signal < 1.5:
+                sparse_signal_factor = 0.1
+            else:
+                sparse_signal_factor = 0.0
 
-        if len(signals) == 0:
+        if len(on_reading) == 0:
             # no op, didn't specify a REF, so we can't do much.
             return self.ir_led_intensity
-        elif len(signals) > 1:
+        elif len(on_reading) > 1:
             raise ValueError("Too many REFs?")
         else:
             # only element of the dict is our REF signal
-            _, signal_voltage = signals.popitem()
+            _, signal_voltage = on_reading.popitem()
             return clamp(
-                20.0, round(self.TARGET_REF_VOLTAGE * (self.ir_led_intensity / signal_voltage), 2), 80.0
-            )  # more than 80% is a bad idea for this LED
+                20.0,
+                round(
+                    self.TARGET_REF_VOLTAGE
+                    * (self.ir_led_intensity / signal_voltage)
+                    * (1 + sparse_signal_factor),
+                    2,
+                ),
+                80.0,
+            )  # more than 80% is a bad idea for IR LED
 
     def _prepare_post_callbacks(self) -> list[Callable]:
         callbacks: list[Callable] = []
 
         # user created callbacks, this binds the callback to the instance so def cb(self, ... ) makes sense.
         for func in self._post_read:
             setattr(self, func.__name__, types.MethodType(func, self))
@@ -1198,15 +1220,15 @@
     then the correct syntax is `start_od_reading("REF", "90").
 
     """
     if interval is not None:
         assert interval > 0, "interval must be positive."
 
     unit = unit or whoami.get_unit_name()
-    experiment = experiment or whoami.get_latest_experiment_name()
+    experiment = experiment or whoami.get_assigned_experiment_name(unit)
 
     ir_led_reference_channel = find_ir_led_reference(od_angle_channel1, od_angle_channel2)
     channel_angle_map = create_channel_angle_map(od_angle_channel1, od_angle_channel2)
     channels = list(channel_angle_map.keys())
 
     # use IR LED reference to normalize?
     if ir_led_reference_channel is not None:
```

## pioreactor/background_jobs/stirring.py

```diff
@@ -25,15 +25,15 @@
 from pioreactor.utils import local_persistant_storage
 from pioreactor.utils.gpio_helpers import set_gpio_availability
 from pioreactor.utils.pwm import PWM
 from pioreactor.utils.streaming_calculations import PID
 from pioreactor.utils.timing import catchtime
 from pioreactor.utils.timing import current_utc_datetime
 from pioreactor.utils.timing import RepeatedTimer
-from pioreactor.whoami import get_latest_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
 from pioreactor.whoami import get_unit_name
 from pioreactor.whoami import is_testing_env
 
 if is_testing_env():
     from pioreactor.utils.mock import MockRpmCalculator
     from pioreactor.utils.mock import MockCallback
     from pioreactor.utils.mock import MockHandle
@@ -432,15 +432,15 @@
 
         if self.rpm_calculator is None:  # or is_testing_env():
             # can't block if we aren't recording the RPM
             return False
 
         sleep_time = 0.2
         poll_time = 2  # usually 4, but we don't need high accuracy here,
-        self.logger.debug(f"Stirring is blocking until RPM is near {self.target_rpm}.")
+        self.logger.debug(f"{self.job_name} is blocking until RPM is near {self.target_rpm}.")
 
         self.rpm_check_repeated_thread.pause()
 
         with catchtime() as time_waiting:
             sleep(2)  # on init, the stirring is too fast from the initial "kick"
             self.poll_and_update_dc(poll_time)
 
@@ -465,15 +465,15 @@
 def start_stirring(
     target_rpm: float = config.getfloat("stirring", "target_rpm", fallback=400),
     unit: Optional[str] = None,
     experiment: Optional[str] = None,
     use_rpm: bool = config.getboolean("stirring", "use_rpm", fallback="true"),
 ) -> Stirrer:
     unit = unit or get_unit_name()
-    experiment = experiment or get_latest_experiment_name()
+    experiment = experiment or get_assigned_experiment_name(unit)
 
     if use_rpm and not is_testing_env():
         rpm_calculator = RpmFromFrequency()
     elif use_rpm and is_testing_env():
         rpm_calculator = MockRpmCalculator()  # type: ignore
     else:
         rpm_calculator = None
```

## pioreactor/background_jobs/temperature_control.py

```diff
@@ -205,15 +205,15 @@
     def _read_external_temperature(self) -> float:
         """
         Read the current temperature from our sensor, in Celsius
         """
         running_sum, running_count = 0.0, 0
         try:
             # check temp is fast, let's do it a few times to reduce variance.
-            for i in range(5):
+            for i in range(6):
                 running_sum += self.heating_pcb_tmp_driver.get_temperature()
                 running_count += 1
                 sleep(0.05)
 
         except OSError as e:
             self.logger.debug(e, exc_info=True)
             raise exc.HardwareNotFoundError(
@@ -278,15 +278,17 @@
             )
             self.automation = algo_metadata
             self.automation_name = algo_metadata.automation_name
 
             # since we are changing automations inside a controller, we know that the latest temperature reading is recent, so we can
             # pass it on to the new automation.
             # this is most useful when temp-control is initialized with only_record_temperature, and then quickly switched over to thermostat.
-            self.automation_job._set_latest_temperature(self.temperature)
+            if hasattr(self, "temperature"):
+                # sometimes self doesn't have temperature, see conditional near self.seconds_since_last_active_heating()
+                self.automation_job._set_latest_temperature(self.temperature)
 
         except KeyError:
             self.logger.debug(
                 f"Unable to find automation {algo_metadata.automation_name}. Available automations are {list(self.available_automations.keys())}. Note: You need to restart this job to have access to newly-added automations.",
                 exc_info=True,
             )
             self.logger.warning(
@@ -552,17 +554,19 @@
 
 def start_temperature_control(
     automation_name: str,
     unit: Optional[str] = None,
     experiment: Optional[str] = None,
     **kwargs,
 ) -> TemperatureController:
+    unit = unit or whoami.get_unit_name()
+    experiment = experiment or whoami.get_assigned_experiment_name(unit)
     return TemperatureController(
-        unit=unit or whoami.get_unit_name(),
-        experiment=experiment or whoami.get_latest_experiment_name(),
+        unit=unit,
+        experiment=experiment,
         automation_name=automation_name,
         **kwargs,
     )
 
 
 @click.command(
     name="temperature_control",
```

## pioreactor/background_jobs/leader/mqtt_to_db_streaming.py

```diff
@@ -10,15 +10,15 @@
 
 import click
 from msgspec import Struct
 from msgspec.json import decode as msgspec_loads
 
 from pioreactor import structs
 from pioreactor import types as pt
-from pioreactor.background_jobs.base import BackgroundJob
+from pioreactor.background_jobs.base import LongRunningBackgroundJob
 from pioreactor.config import config
 from pioreactor.hardware import PWM_TO_PIN
 from pioreactor.pubsub import MQTT_TOPIC
 from pioreactor.pubsub import QOS
 from pioreactor.utils.sqlite_worker import Sqlite3Worker
 from pioreactor.utils.timing import current_utc_datetime
 from pioreactor.utils.timing import RepeatedTimer
@@ -51,15 +51,15 @@
 
 
 class TopicToCallback(Struct):
     topic: str | MQTT_TOPIC | list[str | MQTT_TOPIC]
     callback: Callable[[pt.MQTTMessage], None]
 
 
-class MqttToDBStreamer(BackgroundJob):
+class MqttToDBStreamer(LongRunningBackgroundJob):
     job_name = "mqtt_to_db_streaming"
     published_settings = {
         "inserts_in_last_60s": {"datatype": "integer", "settable": False},
     }
 
     inserts_in_last_60s = 0
     _inserts_in_last_60s = 0
@@ -221,17 +221,19 @@
         "volume_change_ml": dosing_event.volume_change,
         "event": dosing_event.event,
         "source_of_event": dosing_event.source_of_event,
     }
 
 
 def parse_experiment_profile_runs(topic: str, payload: pt.MQTTMessagePayload) -> dict:
+    metadata = produce_metadata(topic)
     return {
         "started_at": current_utc_datetime(),
         "experiment_profile_name": payload.decode("utf-8"),
+        "experiment": metadata.experiment,
     }
 
 
 def parse_led_change_events(topic: str, payload: pt.MQTTMessagePayload) -> dict:
     led_event = msgspec_loads(payload, type=structs.LEDChangeEvent)
     metadata = produce_metadata(topic)
```

## pioreactor/background_jobs/leader/watchdog.py

```diff
@@ -2,25 +2,25 @@
 from __future__ import annotations
 
 import threading
 import time
 
 import click
 
-from pioreactor.background_jobs.base import BackgroundJob
+from pioreactor.background_jobs.base import LongRunningBackgroundJob
+from pioreactor.cluster_management import get_workers_in_inventory
 from pioreactor.config import get_leader_hostname
-from pioreactor.config import get_workers_in_inventory
 from pioreactor.pubsub import subscribe
 from pioreactor.types import MQTTMessage
 from pioreactor.utils.networking import discover_workers_on_network
 from pioreactor.whoami import get_unit_name
 from pioreactor.whoami import UNIVERSAL_EXPERIMENT
 
 
-class WatchDog(BackgroundJob):
+class WatchDog(LongRunningBackgroundJob):
     job_name = "watchdog"
 
     def __init__(self, unit: str, experiment: str) -> None:
         super(WatchDog, self).__init__(unit=unit, experiment=experiment)
 
         self.start_passive_listeners()
 
@@ -45,16 +45,23 @@
                     )
 
     def watch_for_lost_state(self, state_message: MQTTMessage) -> None:
         # generally, I hate this code below...
 
         unit = state_message.topic.split("/")[1]
 
+        # don't check workers that aren't part of the cluster
+        current_workers = get_workers_in_inventory()
+
         # ignore if leader is "lost"
-        if (state_message.payload.decode() == self.LOST) and (unit != self.unit):
+        if (
+            (state_message.payload.decode() == self.LOST)
+            and (unit != self.unit)
+            and (unit in current_workers)
+        ):
             # TODO: this song-and-dance works for monitor, why not extend it to other jobs...
 
             self.logger.warning(f"{unit} seems to be lost. Trying to re-establish connection...")
             time.sleep(5)
 
             if self.state != self.READY:
                 # when the entire Rpi shuts down, ex via sudo reboot, monitor can publish a lost. This code will halt the shutdown.
```

## pioreactor/cli/pio.py

```diff
@@ -4,66 +4,41 @@
 
 > pio run stirring --ignore-rpm
 > pio logs
 """
 from __future__ import annotations
 
 import subprocess
-from concurrent.futures import ThreadPoolExecutor
-from json import dumps
-from json import loads
 from os import geteuid
 from shlex import quote
-from sys import exit
 from time import sleep
 from typing import Optional
 
 import click
+from msgspec.json import decode as loads
+from msgspec.json import encode as dumps
 
 import pioreactor
-import pioreactor.utils.networking as networking
 from pioreactor import actions
 from pioreactor import background_jobs as jobs
+from pioreactor import config
+from pioreactor import exc
 from pioreactor import plugin_management
 from pioreactor import pubsub
 from pioreactor import whoami
-from pioreactor.config import check_firstboot_successful
-from pioreactor.config import config
-from pioreactor.config import get_leader_hostname
 from pioreactor.logging import create_logger
 from pioreactor.mureq import get
 from pioreactor.mureq import HTTPException
+from pioreactor.utils import JobManager
 from pioreactor.utils import local_intermittent_storage
 from pioreactor.utils import local_persistant_storage
-from pioreactor.utils.networking import add_local
 from pioreactor.utils.networking import is_using_local_access_point
-from pioreactor.utils.timing import catchtime
 from pioreactor.utils.timing import current_utc_timestamp
 
 
-JOBS_TO_SKIP_KILLING = [
-    # this is used in `pio kill --all-jobs`, but accessible so that plugins can edit it.
-    # don't kill our permanent jobs
-    "monitor",
-    "watchdog",
-    "mqtt_to_db_streaming",
-    # don't kill automations, let the parent controller do it.
-    # probably all BackgroundSubJob should be here.
-    "temperature_automation",
-    "dosing_automation",
-    "led_automation",
-    # pumping jobs are created by a thread in monitor, and inherit the same PID. We don't want to `kill PID`,
-    # so skip killing using `kill`, and instead use MQTT to kill.
-    "add_media",
-    "remove_waste",
-    "add_alt_media",
-    "led_intensity",
-]
-
-
 @click.group(invoke_without_command=True)
 @click.pass_context
 def pio(ctx) -> None:
     """
     Execute commands on this Pioreactor.
 
     Configuration available: /home/pioreactor/.pioreactor/config.ini
@@ -75,15 +50,15 @@
 
     # if a user runs `pio`, we want the check_firstboot_successful to run, hence the invoke_without_command
     # https://click.palletsprojects.com/en/8.1.x/commands/#group-invocation-without-command
     if ctx.invoked_subcommand is None:
         click.echo(ctx.get_help())
 
     # this check could go somewhere else. TODO This check won't execute if calling pioreactor from a script.
-    if not check_firstboot_successful():
+    if not whoami.check_firstboot_successful():
         raise SystemError(
             "/usr/local/bin/firstboot.sh found on disk. firstboot.sh likely failed. Try looking for errors in `sudo systemctl status firstboot.service`."
         )
 
     if geteuid() == 0:
         raise SystemError("Don't run as root!")
 
@@ -124,15 +99,15 @@
                     line += tmp
                     if line.endswith("\n") and count > (n_lines - n):
                         yield line
                     line = ""
                 else:
                     sleep(sleep_sec)
 
-    for line in follow(config["logging"]["log_file"]):
+    for line in follow(config.config["logging"]["log_file"]):
         click.echo(line, nl=False)
 
 
 @pio.command(name="log", short_help="logs a message from the CLI")
 @click.option("-m", "--message", required=True, type=str, help="the message to append to the log")
 @click.option(
     "-l",
@@ -170,97 +145,32 @@
     pubsub.publish(
         f"pioreactor/{whoami.get_unit_name()}/{whoami.UNIVERSAL_EXPERIMENT}/monitor/flicker_led_response_okay",
         1,
     )
 
 
 @pio.command(name="kill", short_help="kill job(s)")
-@click.argument("job", nargs=-1)
+@click.option("--name", type=click.STRING)
+@click.option("--experiment", type=click.STRING)
+@click.option("--job-source", type=click.STRING)
 @click.option("--all-jobs", is_flag=True, help="kill all Pioreactor jobs running")
-def kill(job: list[str], all_jobs: bool) -> None:
+def kill(name: str | None, experiment: str | None, job_source: str | None, all_jobs: bool) -> None:
     """
     stop job(s).
     """
-
-    from sh import kill  # type: ignore
-    from pioreactor.actions.led_intensity import led_intensity
-
-    def safe_kill(*args: int) -> None:
-        try:
-            kill(*args)
-        except Exception:
-            pass
-
-    if all_jobs:
-        # kill all pumping
-        with pubsub.create_client() as client:
-            client.publish(
-                f"pioreactor/{whoami.UNIVERSAL_IDENTIFIER}/{whoami.UNIVERSAL_EXPERIMENT}/add_media/$state/set",
-                "disconnected",
-                qos=pubsub.QOS.AT_LEAST_ONCE,
-            )
-            client.publish(
-                f"pioreactor/{whoami.UNIVERSAL_IDENTIFIER}/{whoami.UNIVERSAL_EXPERIMENT}/remove_waste/$state/set",
-                "disconnected",
-                qos=pubsub.QOS.AT_LEAST_ONCE,
-            )
-            client.publish(
-                f"pioreactor/{whoami.UNIVERSAL_IDENTIFIER}/{whoami.UNIVERSAL_EXPERIMENT}/add_alt_media/$state/set",
-                "disconnected",
-                qos=pubsub.QOS.AT_LEAST_ONCE,
-            )
-
-        # kill all running pioreactor processes
-        jobs_killed_already = []
-        with local_intermittent_storage("pio_jobs_running") as cache:
-            for j in cache:
-                if j not in JOBS_TO_SKIP_KILLING:
-                    pid = cache[j]
-                    if pid not in jobs_killed_already:
-                        safe_kill(int(pid))
-                        jobs_killed_already.append(pid)
-
-        # kill all LEDs
-        sleep(0.25)
-        try:
-            # non-workers won't have this hardware, so just skip it
-            led_intensity({"A": 0.0, "B": 0.0, "C": 0.0, "D": 0.0}, verbose=False, experiment="_test")
-        except Exception:
-            pass
-
-        # assert everything is off
-        with local_intermittent_storage("pwm_dc") as cache:
-            for pin in cache:
-                if cache[pin] != 0.0:
-                    print(f"pin {pin} is not off!")
-
-        # assert everything is off
-        with local_intermittent_storage("leds") as cache:
-            for led in cache:
-                if cache[led] != 0.0:
-                    print(f"LED {led} is not off!")
-
-    else:
-        jobs_killed_already = []
-        with local_intermittent_storage("pio_jobs_running") as cache:
-            for j in cache:
-                if j in job:
-                    pid = cache[j]
-                    if pid not in jobs_killed_already:
-                        safe_kill(int(pid))
-                        jobs_killed_already.append(pid)
+    with JobManager() as jm:
+        count = jm.count_jobs(all_jobs=all_jobs, name=name, experiment=experiment, job_source=job_source)
+        jm.kill_jobs(all_jobs=all_jobs, name=name, experiment=experiment, job_source=job_source)
+    click.echo(f"Killed {count} job(s).")
 
 
 @pio.group(short_help="run a job")
-def run() -> None:
-    if not (whoami.am_I_active_worker() or whoami.am_I_leader()):
-        click.echo(
-            f"Running `pio` on a non-active Pioreactor. Do you need to change `{whoami.get_unit_name()}` in `cluster.inventory` section in `config.ini`?"
-        )
-        raise click.Abort()
+@click.option("--source", "-s", default="user", help="source of command")
+def run(source) -> None:
+    pass
 
 
 @pio.command(name="version", short_help="print the Pioreactor software version")
 @click.option("--verbose", "-v", is_flag=True, help="show more system information")
 def version(verbose: bool) -> None:
     if verbose:
         import platform
@@ -367,16 +277,16 @@
     ----------
 
     > pio update-settings stirring --target_rpm 500
     > pio update-settings stirring --target-rpm 500
     > pio update-settings dosing_control --automation '{"type": "dosing", "automation_name": "silent", "args": {}}
 
     """
-    exp = whoami.get_latest_experiment_name()
     unit = whoami.get_unit_name()
+    exp = whoami.get_assigned_experiment_name(unit)
 
     extra_args = {ctx.args[i][2:]: ctx.args[i + 1] for i in range(0, len(ctx.args), 2)}
 
     assert len(extra_args) > 0
 
     for setting, value in extra_args.items():
         setting = setting.replace("-", "_")
@@ -494,15 +404,15 @@
                     (f"rm -rf {tmp_release_folder}", -3),
                     (f"unzip {source} -d {tmp_release_folder}", -2),
                     (f"unzip {tmp_release_folder}/wheels_{version_installed}.zip -d {tmp_release_folder}/wheels", 0),
                     (f"mv {tmp_release_folder}/pioreactorui_*.tar.gz {tmp_dir}/pioreactorui_archive || :", 0.5),  # move ui folder to be accessed by a `pio update ui`
                     (f"sudo bash {tmp_release_folder}/pre_update.sh || :", 1),
                     (f"sudo pip install --no-index --find-links={tmp_release_folder}/wheels/ {tmp_release_folder}/pioreactor-{version_installed}-py3-none-any.whl", 2),
                     (f"sudo bash {tmp_release_folder}/update.sh || :", 3),
-                    (f'sudo sqlite3 {config["storage"]["database"]} < {tmp_release_folder}/update.sql || :', 4),
+                    (f'sudo sqlite3 {config.config["storage"]["database"]} < {tmp_release_folder}/update.sql || :', 4),
                     (f"sudo bash {tmp_release_folder}/post_update.sh || :", 5),
                     (f"rm -rf {tmp_release_folder}", 6),
                 ]
             )
             # fmt: on
         elif source.endswith(".whl"):
             # provided a whl
@@ -572,15 +482,15 @@
                         ("sudo bash /tmp/update.sh", 4),
                     ]
                 )
             elif asset_name == "update.sql":
                 commands_and_priority.extend(
                     [
                         (f"wget -O /tmp/update.sql {url}", 5),
-                        (f'sudo sqlite3 {config["storage"]["database"]} < /tmp/update.sql', 6),
+                        (f'sudo sqlite3 {config.config["storage"]["database"]} < /tmp/update.sql', 6),
                     ]
                 )
             elif asset_name == "post_update.sh":
                 commands_and_priority.extend(
                     [
                         (f"wget -O /tmp/post_update.sh {url}", 99),
                         ("sudo bash /tmp/post_update.sh", 100),
@@ -670,205 +580,86 @@
 pio.add_command(plugin_management.click_uninstall_plugin)
 pio.add_command(plugin_management.click_list_plugins)
 
 # this runs on both leader and workers
 run.add_command(jobs.monitor.click_monitor)
 
 
-if whoami.am_I_active_worker():
-    run.add_command(jobs.growth_rate_calculating.click_growth_rate_calculating)
-    run.add_command(jobs.stirring.click_stirring)
-    run.add_command(jobs.od_reading.click_od_reading)
-    run.add_command(jobs.dosing_control.click_dosing_control)
-    run.add_command(jobs.led_control.click_led_control)
-    run.add_command(jobs.temperature_control.click_temperature_control)
-
-    run.add_command(actions.led_intensity.click_led_intensity)
-    run.add_command(actions.pump.click_add_alt_media)
-    run.add_command(actions.pump.click_add_media)
-    run.add_command(actions.pump.click_remove_waste)
-    run.add_command(actions.od_blank.click_od_blank)
-    run.add_command(actions.self_test.click_self_test)
-    run.add_command(actions.stirring_calibration.click_stirring_calibration)
-    run.add_command(actions.pump_calibration.click_pump_calibration)
-    run.add_command(actions.od_calibration.click_od_calibration)
-
-    # TODO: this only adds to `pio run` - what if users want to add a high level command? Examples?
-    for plugin in pioreactor.plugin_management.get_plugins().values():
-        for possible_entry_point in dir(plugin.module):
-            if possible_entry_point.startswith("click_"):
-                run.add_command(getattr(plugin.module, possible_entry_point))
+try:
+    # this can fail if the server isn't online yet
+    # TODO: kinda sucks we are doing an HTTP request each time pio is invoked...
+    am_worker = whoami.am_I_active_worker()
+except HTTPException:
+    am_worker = True
+
+
+run.add_command(jobs.growth_rate_calculating.click_growth_rate_calculating)
+run.add_command(jobs.stirring.click_stirring)
+run.add_command(jobs.od_reading.click_od_reading)
+run.add_command(jobs.dosing_control.click_dosing_control)
+run.add_command(jobs.led_control.click_led_control)
+run.add_command(jobs.temperature_control.click_temperature_control)
+
+run.add_command(actions.led_intensity.click_led_intensity)
+run.add_command(actions.pump.click_add_alt_media)
+run.add_command(actions.pump.click_add_media)
+run.add_command(actions.pump.click_remove_waste)
+run.add_command(actions.od_blank.click_od_blank)
+run.add_command(actions.self_test.click_self_test)
+run.add_command(actions.stirring_calibration.click_stirring_calibration)
+run.add_command(actions.pump_calibration.click_pump_calibration)
+run.add_command(actions.od_calibration.click_od_calibration)
+
+# TODO: this only adds to `pio run` - what if users want to add a high level command? Examples?
+for plugin in pioreactor.plugin_management.get_plugins().values():
+    for possible_entry_point in dir(plugin.module):
+        if possible_entry_point.startswith("click_"):
+            run.add_command(getattr(plugin.module, possible_entry_point))
 
 
 if whoami.am_I_leader():
+    from pioreactor.cluster_management import add_worker
+    from pioreactor.cluster_management import remove_worker
+    from pioreactor.cluster_management import assign_worker_to_experiment
+    from pioreactor.cluster_management import unassign_worker_from_experiment
+    from pioreactor.cluster_management import update_active
+    from pioreactor.cluster_management import discover_workers
+    from pioreactor.cluster_management import cluster_status
+
     run.add_command(jobs.mqtt_to_db_streaming.click_mqtt_to_db_streaming)
     run.add_command(jobs.watchdog.click_watchdog)
     run.add_command(actions.export_experiment_data.click_export_experiment_data)
     run.add_command(actions.backup_database.click_backup_database)
     run.add_command(actions.experiment_profile.click_experiment_profile)
 
+    @pio.group(short_help="manage workers")
+    def workers():
+        pass
+
+    workers.add_command(add_worker)
+    workers.add_command(remove_worker)
+    workers.add_command(assign_worker_to_experiment)
+    workers.add_command(unassign_worker_from_experiment)
+    workers.add_command(update_active)
+    workers.add_command(discover_workers)
+    workers.add_command(cluster_status)
+
     @pio.command(short_help="access the db CLI")
     def db() -> None:
         import os
 
-        os.system(f"sqlite3 {config['storage']['database']} -column -header")
+        os.system(f"sqlite3 {config.config['storage']['database']} -column -header")
 
     @pio.command(short_help="tail MQTT")
     @click.option("--topic", "-t", default="pioreactor/#")
     def mqtt(topic: str) -> None:
         import os
 
         os.system(f"""mosquitto_sub -v -t '{topic}' -F "%19.19I  |  %t   %p" -u pioreactor -P raspberry""")
 
-    @pio.command(name="add-pioreactor", short_help="add a new Pioreactor to cluster")
-    @click.argument("hostname")
-    @click.option("--password", "-p", default="raspberry")
-    def add_pioreactor(hostname: str, password: str) -> None:
-        """
-        Add a new pioreactor worker to the cluster. The pioreactor should already have the worker image installed and is turned on.
-        """
-        # TODO: move this to its own file
-        import socket
-
-        logger = create_logger(
-            "add_pioreactor",
-            unit=whoami.get_unit_name(),
-            experiment=whoami.UNIVERSAL_EXPERIMENT,
-        )
-        logger.info(f"Adding new pioreactor {hostname} to cluster.")
-
-        hostname = hostname.removesuffix(".local")
-        hostname_dot_local = hostname + ".local"
-
-        # check to make sure <hostname>.local is on network
-        checks, max_checks = 0, 15
-        sleep_time = 3
-
-        with catchtime() as elapsed:
-            while not networking.is_hostname_on_network(hostname_dot_local):
-                checks += 1
-                try:
-                    socket.gethostbyname(hostname_dot_local)
-                except socket.gaierror:
-                    sleep(sleep_time)
-                    click.echo(f"`{hostname}` not found on network - checking again.")
-                    if checks >= max_checks:
-                        logger.error(
-                            f"`{hostname}` not found on network after {round(elapsed())} seconds. Check that you provided the right i) WiFi credentials to the network, ii) the hostname is correct, the iii) worker is turned on."
-                        )
-                        raise click.Abort()
-
-        res = subprocess.run(
-            ["bash", "/usr/local/bin/add_new_pioreactor_worker_from_leader.sh", hostname, password],
-            capture_output=True,
-            text=True,
-        )
-        if res.returncode == 0:
-            logger.notice(f"New pioreactor {hostname} successfully added to cluster.")  # type: ignore
-        else:
-            logger.error(res.stderr)
-            raise click.Abort()
-
-    @pio.command(
-        name="discover-workers",
-        short_help="discover all pioreactor workers on the network",
-    )
-    @click.option(
-        "-t",
-        "--terminate",
-        is_flag=True,
-        help="Terminate after dumping a more or less complete list",
-    )
-    def discover_workers(terminate: bool) -> None:
-        from pioreactor.utils.networking import discover_workers_on_network
-
-        for hostname in discover_workers_on_network(terminate):
-            click.echo(hostname)
-
-    @pio.command(name="cluster-status", short_help="report information on the cluster")
-    def cluster_status() -> None:
-        """
-        Note that this only looks at the current cluster as defined in config.ini.
-        """
-        import socket
-
-        def get_metadata(hostname):
-            # get ip
-            if whoami.get_unit_name() == hostname:
-                ip = networking.get_ip()
-            else:
-                try:
-                    ip = socket.gethostbyname(add_local(hostname))
-                except OSError:
-                    ip = "unknown"
-
-            # get state
-            result = pubsub.subscribe(
-                f"pioreactor/{hostname}/{whoami.UNIVERSAL_EXPERIMENT}/monitor/$state",
-                timeout=1,
-                name="CLI",
-            )
-            if result:
-                state = result.payload.decode()
-            else:
-                state = "unknown"
-
-            # get version
-            result = pubsub.subscribe(
-                f"pioreactor/{hostname}/{whoami.UNIVERSAL_EXPERIMENT}/monitor/versions",
-                timeout=1,
-                name="CLI",
-            )
-            if result:
-                versions = loads(result.payload.decode())
-            else:
-                versions = {"hat": "unknown", "hat_serial": "unknown"}
-
-            # is reachable?
-            reachable = networking.is_reachable(add_local(hostname))
-
-            return ip, state, reachable, versions
-
-        def display_data_for(hostname_status: tuple[str, str]) -> bool:
-            hostname, status = hostname_status
-
-            ip, state, reachable, versions = get_metadata(hostname)
-
-            statef = click.style(f"{state:15s}", fg="green" if state in ("ready", "init") else "red")
-            ipf = f"{ip if (ip is not None) else 'unknown':20s}"
-
-            is_leaderf = f"{('Y' if hostname==get_leader_hostname() else 'N'):15s}"
-            hostnamef = f"{hostname:20s}"
-            reachablef = (
-                f"{(click.style('Y', fg='green') if reachable       else click.style('N', fg='red')):23s}"
-            )
-            statusf = (
-                f"{(click.style('Y', fg='green') if (status == '1') else click.style('N', fg='red')):23s}"
-            )
-            versionf = f"{versions['hat']:15s}"
-
-            click.echo(f"{hostnamef} {is_leaderf} {ipf} {statef} {reachablef} {statusf} {versionf}")
-            return reachable & (state == "ready")
-
-        worker_statuses = list(config["cluster.inventory"].items())
-        n_workers = len(worker_statuses)
-
-        click.secho(
-            f"{'Unit / hostname':20s} {'Is leader?':15s} {'IP address':20s} {'State':15s} {'Reachable?':14s} {'Active?':14s} {'HAT version':15s}",
-            bold=True,
-        )
-        if n_workers == 0:
-            return
-
-        with ThreadPoolExecutor(max_workers=n_workers) as executor:
-            results = executor.map(display_data_for, worker_statuses)
-
-        if not all(results):
-            exit(1)
-
     @update.command(name="ui")
     @click.option("-b", "--branch", help="install from a branch on github")
     @click.option(
         "-r",
         "--repo",
         help="install from a repo on github. Format: username/project",
         default="pioreactor/pioreactorui",
@@ -922,14 +713,10 @@
                 command,
                 universal_newlines=True,
                 stdout=subprocess.DEVNULL,
                 stderr=subprocess.PIPE,
             )
             if p.returncode != 0:
                 logger.error(p.stderr)
-                raise click.Abort()
+                raise exc.BashScriptError(p.stderr)
 
         logger.notice(f"Updated PioreactorUI to version {version_installed}.")  # type: ignore
-
-
-if __name__ == "__main__":
-    pio()
```

## pioreactor/cli/pios.py

```diff
@@ -5,24 +5,25 @@
 from __future__ import annotations
 
 from concurrent.futures import ThreadPoolExecutor
 from typing import Optional
 
 import click
 
+from pioreactor.cluster_management import get_active_workers_in_inventory
+from pioreactor.cluster_management import get_workers_in_inventory
 from pioreactor.config import config
-from pioreactor.config import get_active_workers_in_inventory
 from pioreactor.config import get_leader_hostname
-from pioreactor.config import get_workers_in_inventory
 from pioreactor.logging import create_logger
+from pioreactor.utils import ClusterJobManager
 from pioreactor.utils.networking import add_local
 from pioreactor.utils.networking import cp_file_across_cluster
 from pioreactor.utils.timing import current_utc_timestamp
 from pioreactor.whoami import am_I_leader
-from pioreactor.whoami import get_latest_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
 from pioreactor.whoami import get_unit_name
 from pioreactor.whoami import is_testing_env
 from pioreactor.whoami import UNIVERSAL_EXPERIMENT
 from pioreactor.whoami import UNIVERSAL_IDENTIFIER
 
 
 @click.group()
@@ -34,18 +35,14 @@
 
     Report errors or feedback here: https://github.com/Pioreactor/pioreactor/issues
     """
     if not am_I_leader() and not is_testing_env():
         click.echo("workers cannot run `pios` commands. Try `pio` instead.", err=True)
         raise click.Abort()
 
-    if len(get_active_workers_in_inventory()) == 0:
-        logger = create_logger("CLI", unit=get_unit_name(), experiment=UNIVERSAL_EXPERIMENT)
-        logger.warning("No active workers. See `cluster.inventory` section in config.ini.")
-
 
 if am_I_leader():
 
     def universal_identifier_to_all_active_workers(units: tuple[str, ...]) -> tuple[str, ...]:
         if units == (UNIVERSAL_IDENTIFIER,):
             units = get_active_workers_in_inventory()
         return units
@@ -418,25 +415,36 @@
         for unit in units:
             results.append(_thread_function(unit))
 
         if not all(results):
             raise click.Abort()
 
     @pios.command("kill", short_help="kill a job(s) on workers")
-    @click.argument("job", nargs=-1)
+    @click.option("--job")
     @click.option(
         "--units",
         multiple=True,
         default=(UNIVERSAL_IDENTIFIER,),
         type=click.STRING,
         help="specify a hostname, default is all active units",
     )
     @click.option("--all-jobs", is_flag=True, help="kill all worker jobs")
+    @click.option("--experiment", type=click.STRING)
+    @click.option("--job-source", type=click.STRING)
+    @click.option("--name", type=click.STRING)
     @click.option("-y", is_flag=True, help="skip asking for confirmation")
-    def kill(job: str, units: tuple[str, ...], all_jobs: bool, y: bool) -> None:
+    def kill(
+        job: str | None,
+        units: tuple[str, ...],
+        all_jobs: bool,
+        experiment: str | None,
+        job_source: str | None,
+        name: str | None,
+        y: bool,
+    ) -> None:
         """
         Send a SIGTERM signal to JOB. JOB can be any Pioreactor job name, like "stirring".
         Example:
 
         > pios kill stirring
 
 
@@ -447,52 +455,25 @@
 
         Kill all worker jobs (i.e. this excludes leader jobs like watchdog). Ignores `job` argument.
 
         > pios kill --all-jobs -y
 
 
         """
-        from sh import ssh  # type: ignore
-        from sh import ErrorReturnCode_255  # type: ignore
-        from sh import ErrorReturnCode_1  # type: ignore
+
+        units = universal_identifier_to_all_active_workers(units)
 
         if not y:
-            confirm = input(
-                f"Confirm killing {str(job) if (not all_jobs) else 'all jobs'} on {units}? Y/n: "
-            ).strip()
+            confirm = input(f"Confirm killing jobs on {units}? Y/n: ").strip()
             if confirm != "Y":
                 raise click.Abort()
 
-        command = f"pio kill {' '.join(job)}"
-        command += "--all-jobs" if all_jobs else ""
-
-        logger = create_logger("CLI", unit=get_unit_name(), experiment=UNIVERSAL_EXPERIMENT)
-
-        def _thread_function(unit: str):
-            logger.debug(f"Executing `{command}` on {unit}.")
-            try:
-                ssh(add_local(unit), command)
-                return True
-
-            except ErrorReturnCode_255 as e:
-                logger.debug(e, exc_info=True)
-                logger.error(f"Unable to connect to unit {unit}. {e.stderr.decode()}")
-                return False
-            except ErrorReturnCode_1 as e:
-                logger.error(f"Error occurred: {e}. See logs for more.")
-                logger.debug(e, exc_info=True)
-                logger.debug(e.stderr, exc_info=True)
-                return False
-
-        units = universal_identifier_to_all_active_workers(units)
-        with ThreadPoolExecutor(max_workers=len(units)) as executor:
-            results = executor.map(_thread_function, units)
-
-        if not all(results):
-            raise click.Abort()
+        with ClusterJobManager(units) as cm:
+            if not cm.kill_jobs(all_jobs=all_jobs, experiment=experiment, job_source=job_source, name=name):
+                raise click.Abort()
 
     @pios.command(
         name="run",
         context_settings=dict(ignore_unknown_options=True, allow_extra_args=True),
         short_help="run a job on workers",
     )
     @click.argument("job", type=click.STRING)
@@ -699,28 +680,28 @@
         Examples
         ---------
         > pios update-settings stirring --target_rpm 500 --units worker1
         > pios update-settings dosing_control --automation '{"type": "dosing", "automation_name": "silent", "args": {}}
 
         """
 
-        exp = get_latest_experiment_name()
         extra_args = {ctx.args[i][2:]: ctx.args[i + 1] for i in range(0, len(ctx.args), 2)}
 
         if "unit" in extra_args:
             click.echo("Did you mean to use 'units' instead of 'unit'? Exiting.", err=True)
             raise click.Abort()
 
         assert len(extra_args) > 0
 
         from pioreactor.pubsub import publish
 
         def _thread_function(unit: str) -> bool:
+            experiment = get_assigned_experiment_name(unit)
             for setting, value in extra_args.items():
-                publish(f"pioreactor/{unit}/{exp}/{job}/{setting}/set", value)
+                publish(f"pioreactor/{unit}/{experiment}/{job}/{setting}/set", value)
             return True
 
         units = universal_identifier_to_all_active_workers(units)
         with ThreadPoolExecutor(max_workers=len(units)) as executor:
             results = executor.map(_thread_function, units)
 
         if not all(results):
```

## pioreactor/experiment_profiles/parser.py

```diff
@@ -5,15 +5,16 @@
 
 from msgspec import DecodeError
 from msgspec.json import decode
 
 from .sly import Lexer
 from .sly import Parser
 from pioreactor.pubsub import subscribe
-from pioreactor.whoami import get_latest_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
+from pioreactor.whoami import is_active
 
 
 def convert_string(input_str: str) -> bool | float | str:
     # Try to convert to float
     try:
         return float(input_str)
     except ValueError:
@@ -157,16 +158,21 @@
     def expr(self, p):
         return float(p.NUMBER)
 
     @_("UNIT_JOB_SETTING")
     def expr(self, p) -> bool | float | str:
         unit, job, setting_keys = p.UNIT_JOB_SETTING.split(":")
         setting, *keys = setting_keys.split(".")
-        experiment = get_latest_experiment_name()
-        result = subscribe(f"pioreactor/{unit}/{experiment}/{job}/{setting}", timeout=2)
+
+        experiment = get_assigned_experiment_name(unit)
+
+        if not is_active(unit):
+            raise NotActiveWorkerError(f"Worker {unit} is not active.")
+
+        result = subscribe(f"pioreactor/{unit}/{experiment}/{job}/{setting}", timeout=3)
         if result:
             # error handling here
             try:
                 data_blob = decode(result.payload)
             except DecodeError:
                 # just a string?
                 return convert_string(result.payload.decode())
@@ -177,15 +183,15 @@
                 # its a nested json object, iteratively nest into it.
                 for key in keys:
                     value = value[key]
 
             return convert_string(value)
 
         else:
-            raise ValueError(f"{p.UNIT_JOB_SETTING} does not exist.")
+            raise ValueError(f"{p.UNIT_JOB_SETTING} does not exist for experiment {experiment}")
 
 
 def parse_profile_expression_to_bool(profile_string: str) -> bool:
     result = parse_profile_expression(profile_string)
     if result is None:
         # syntax error or something funky.
         raise SyntaxError(profile_string)
```

## pioreactor/experiment_profiles/profile_struct.py

```diff
@@ -111,12 +111,11 @@
     jobs: Jobs = {}
 
 
 class Profile(Struct, forbid_unknown_fields=True):
     experiment_profile_name: str
     metadata: Metadata = field(default_factory=Metadata)
     plugins: list[Plugin] = []
-    stop_on_exit: bool = False  # TODO: not implemented
     common: CommonBlock = field(
         default_factory=CommonBlock
     )  # later this might expand to include other fields
     pioreactors: dict[PioreactorUnitName, PioreactorSpecificBlock] = {}
```

## pioreactor/plugin_management/install_plugin.py

```diff
@@ -2,14 +2,15 @@
 from __future__ import annotations
 
 import subprocess
 from shlex import quote
 
 import click
 
+from pioreactor.exc import BashScriptError
 from pioreactor.logging import create_logger
 from pioreactor.whoami import UNIVERSAL_EXPERIMENT
 
 
 def install_plugin(name_of_plugin: str, source: str | None = None) -> None:
     logger = create_logger("install_plugin", experiment=UNIVERSAL_EXPERIMENT)
     logger.debug(f"Installing plugin {name_of_plugin}.")
@@ -25,14 +26,15 @@
 
     if result.returncode == 0:
         logger.notice(f"Successfully installed plugin {name_of_plugin}.")  # type: ignore
     else:
         logger.error(f"Failed to install plugin {name_of_plugin}. See logs.")
         logger.debug(result.stdout)
         logger.debug(result.stderr)
+        raise BashScriptError(f"Failed to install plugin {name_of_plugin}. See logs.")
 
 
 @click.command(name="install-plugin", short_help="install a plugin")
 @click.argument("name-of-plugin")
 @click.option(
     "--source",
     type=str,
```

## pioreactor/plugin_management/uninstall_plugin.py

```diff
@@ -2,14 +2,15 @@
 from __future__ import annotations
 
 import subprocess
 from shlex import quote
 
 import click
 
+from pioreactor.exc import BashScriptError
 from pioreactor.logging import create_logger
 from pioreactor.plugin_management.utils import discover_plugins_in_local_folder
 from pioreactor.whoami import UNIVERSAL_EXPERIMENT
 
 
 def uninstall_plugin(name_of_plugin: str) -> None:
     logger = create_logger("uninstall_plugin", experiment=UNIVERSAL_EXPERIMENT)
@@ -34,14 +35,15 @@
         logger.warning(f"Unable to uninstall: plugin {name_of_plugin} is not installed.")
     elif result.returncode == 0:
         logger.notice(f"Successfully uninstalled plugin {name_of_plugin}.")  # type: ignore
     else:
         logger.error(f"Failed to uninstall plugin {name_of_plugin}. See logs.")
         logger.debug(result.stdout)
         logger.debug(result.stderr)
+        raise BashScriptError(f"Failed to uninstall plugin {name_of_plugin}. See logs.")
 
     return
 
 
 @click.command(name="uninstall-plugin", short_help="uninstall an existing plugin")
 @click.argument("name-of-plugin")
 def click_uninstall_plugin(name_of_plugin: str) -> None:
```

## pioreactor/utils/__init__.py

```diff
@@ -1,34 +1,44 @@
 # -*- coding: utf-8 -*-
 from __future__ import annotations
 
 import os
 import signal
+import sqlite3
 import tempfile
 import time
+from concurrent.futures import ThreadPoolExecutor
 from contextlib import contextmanager
 from functools import wraps
+from os import getpid
 from threading import Event
 from typing import Any
 from typing import Callable
 from typing import cast
 from typing import Generator
 from typing import Optional
 from typing import overload
 from typing import Sequence
 
 from diskcache import Cache  # type: ignore
 
 from pioreactor import structs
 from pioreactor import types as pt
 from pioreactor import whoami
+from pioreactor.exc import NotActiveWorkerError
+from pioreactor.exc import RoleError
 from pioreactor.pubsub import Client
 from pioreactor.pubsub import create_client
 from pioreactor.pubsub import QOS
 from pioreactor.pubsub import subscribe_and_callback
+from pioreactor.utils.networking import add_local
+from pioreactor.utils.timing import current_utc_timestamp
+
+
+JobMetadataKey = int
 
 
 class callable_stack:
     """
     A class for managing a stack of callable objects in Python.
 
     Example:
@@ -93,54 +103,64 @@
 
 
 def append_signal_handlers(signal_value: signal.Signals, new_callbacks: list[Callable]) -> None:
     for callback in new_callbacks:
         append_signal_handler(signal_value, callback)
 
 
-class publish_ready_to_disconnected_state:
+class managed_lifecycle:
     """
     Wrap a block of code to have "state" in MQTT. See od_normalization, self_test, pump
 
     You can use send a "disconnected" to "pioreactor/<unit>/<exp>/<name>/$state/set" to stop/disconnect it.
 
     Example
     ----------
 
-    > with publish_ready_to_disconnected_state(unit, experiment, "self_test"): # publishes "ready" to mqtt
+    > with managed_lifecycle(unit, experiment, "self_test"): # publishes "ready" to mqtt
     >    do_work()
     >
     > # on close of block, a "disconnected" is fired to MQTT, regardless of how that end is achieved (error, return statement, etc.)
 
 
-    If the program is required to know if it's killed, publish_ready_to_disconnected_state contains an event (see pump.py code)
+    If the program is required to know if it's killed, managed_lifecycle contains an event (see pump.py code)
 
-    > with publish_ready_to_disconnected_state(unit, experiment, "self_test") as state:
+    > with managed_lifecycle(unit, experiment, "self_test") as state:
     >    do_work()
     >
     >    state.block_until_disconnected()
     >    # or state.exit_event.is_set() or state.exit_event.wait(...) are other options.
     >
 
+    For now, it's possible to run multiple jobs with the same name using this tool.
+
     """
 
     def __init__(
         self,
         unit: str,
         experiment: str,
         name: str,
         mqtt_client: Optional[Client] = None,
         exit_on_mqtt_disconnect: bool = False,
         mqtt_client_kwargs: Optional[dict] = None,
+        ignore_is_active_state=False,  # hack and kinda gross
+        source: str = "app",
+        job_source: str | None = None,
     ) -> None:
+        if not ignore_is_active_state and not whoami.is_active(unit):
+            raise NotActiveWorkerError(f"{unit} is not active.")
+
         self.unit = unit
         self.experiment = experiment
         self.name = name
         self.state = "init"
         self.exit_event = Event()
+        self._source = source
+        self._job_source = job_source or os.environ.get("JOB_SOURCE") or "user"
 
         last_will = {
             "topic": f"pioreactor/{self.unit}/{self.experiment}/{self.name}/$state",
             "payload": b"lost",
             "qos": QOS.EXACTLY_ONCE,
             "retain": True,
         }
@@ -166,15 +186,15 @@
     def _exit(self, *args) -> None:
         # recall: we can't publish in a callback!
         self.exit_event.set()
 
     def _on_disconnect(self, *args):
         self._exit()
 
-    def __enter__(self) -> publish_ready_to_disconnected_state:
+    def __enter__(self) -> managed_lifecycle:
         try:
             # this only works on the main thread.
             append_signal_handler(signal.SIGTERM, self._exit)
             append_signal_handler(signal.SIGINT, self._exit)
         except ValueError:
             pass
 
@@ -182,16 +202,18 @@
         self.mqtt_client.publish(
             f"pioreactor/{self.unit}/{self.experiment}/{self.name}/$state",
             self.state,
             qos=QOS.AT_LEAST_ONCE,
             retain=True,
         )
 
-        with local_intermittent_storage("pio_jobs_running") as cache:
-            cache[self.name] = os.getpid()
+        with JobManager() as jm:
+            self._jm_key = jm.register_and_set_running(
+                self.unit, self.experiment, self.name, self._job_source, getpid(), ""
+            )
 
         return self
 
     def __exit__(self, *args) -> None:
         self.state = "disconnected"
         self.mqtt_client.publish(
             f"pioreactor/{self.unit}/{self.experiment}/{self.name}/$state",
@@ -199,16 +221,17 @@
             qos=QOS.AT_LEAST_ONCE,
             retain=True,
         )
         if not self._externally_provided_client:
             self.mqtt_client.loop_stop()
             self.mqtt_client.disconnect()
 
-        with local_intermittent_storage("pio_jobs_running") as cache:
-            cache.pop(self.name)
+        with JobManager() as jm:
+            jm.set_not_running(self._jm_key)
+
         return
 
     def exit_from_mqtt(self, message: pt.MQTTMessage) -> None:
         if message.payload == b"disconnected":
             self._exit()
 
     def start_passive_listeners(self) -> None:
@@ -308,20 +331,18 @@
     > result = is_pio_job_running(["od_reading", "stirring"])
     > # [True, False]
     """
     if isinstance(target_jobs, str):
         target_jobs = [target_jobs]
 
     results = []
-    with local_intermittent_storage("pio_jobs_running") as cache:
+
+    with JobManager() as jm:
         for job in target_jobs:
-            if job not in cache:
-                results.append(False)
-            else:
-                results.append(True)
+            results.append(jm.is_job_running(job))
 
     if len(target_jobs) == 1:
         return results[0]
     else:
         return results
 
 
@@ -443,7 +464,219 @@
     for i in range(retries):
         try:
             return func(*args, **kwargs)
         except Exception as e:
             if i == retries - 1:  # If this was the last attempt
                 raise e
             time.sleep(sleep_for)
+
+
+def safe_kill(*args: int) -> None:
+    from sh import kill  # type: ignore
+
+    try:
+        kill(*args)
+    except Exception:
+        pass
+
+
+class ShellKill:
+    def __init__(self):
+        self.list_of_pids = []
+
+    def append(self, pid):
+        self.list_of_pids.append(pid)
+
+    def kill(self):
+        safe_kill(*self.list_of_pids)
+
+
+class MQTTKill:
+    def __init__(self):
+        self.list_of_job_names = []
+
+    def append(self, name):
+        self.list_of_job_names.append(name)
+
+    def kill(self):
+        with create_client() as client:
+            for i, name in enumerate(self.list_of_job_names):
+                msg = client.publish(
+                    f"pioreactor/{whoami.get_unit_name()}/{whoami.UNIVERSAL_EXPERIMENT}/{name}/$state/set",
+                    "disconnected",
+                    qos=QOS.AT_LEAST_ONCE,
+                )
+
+                if (i + 1) == len(self.list_of_job_names):
+                    # list one
+                    msg.wait_for_publish(2)
+
+
+class JobManager:
+    AUTOMATION_JOBS = ("temperature_automation", "dosing_automation", "led_automation")
+    PUMPING_JOBS = (
+        "add_media",
+        "remove_waste",
+        "add_alt_media",
+        "circulate_media",
+        "circulate_alt_media",
+    )
+    LONG_RUNNING_JOBS = ("monitor", "mqtt_to_db_streaming", "watchdog")
+
+    def __init__(self) -> None:
+        self.db_path = f"{tempfile.gettempdir()}/pio_jobs_metadata.db"
+        self.conn = sqlite3.connect(self.db_path)
+        self.cursor = self.conn.cursor()
+        self._create_table()
+
+    def _create_table(self) -> None:
+        create_table_query = """
+            CREATE TABLE IF NOT EXISTS pio_job_metadata (
+            id           INTEGER PRIMARY KEY AUTOINCREMENT,
+            unit         TEXT NOT NULL,
+            experiment   TEXT NOT NULL,
+            name         TEXT NOT NULL,
+            job_source   TEXT NOT NULL,
+            started_at   TEXT NOT NULL,
+            is_running   INTEGER NOT NULL,
+            leader       TEXT NOT NULL,
+            pid          INTEGER NOT NULL,
+            ended_at     TEXT
+        );
+        """
+        self.cursor.execute(create_table_query)
+        self.conn.commit()
+
+    def register_and_set_running(
+        self, unit: str, experiment: str, name: str, job_source: str | None, pid: int, leader: str
+    ) -> JobMetadataKey:
+        insert_query = "INSERT INTO pio_job_metadata (started_at, is_running, job_source, experiment, unit, name, leader, pid, ended_at) VALUES (STRFTIME('%Y-%m-%dT%H:%M:%f000Z', 'NOW'), 1, :job_source, :experiment, :unit, :name, :leader, :pid, NULL);"
+        self.cursor.execute(
+            insert_query,
+            {
+                "unit": unit,
+                "experiment": experiment,
+                "job_source": job_source,
+                "pid": pid,
+                "leader": leader,
+                "name": name,
+            },
+        )
+        self.conn.commit()
+        assert isinstance(self.cursor.lastrowid, int)
+        return self.cursor.lastrowid
+
+    def set_not_running(self, job_metadata_key: JobMetadataKey) -> None:
+        update_query = "UPDATE pio_job_metadata SET is_running=0, ended_at=STRFTIME('%Y-%m-%dT%H:%M:%f000Z', 'NOW') WHERE id=(?)"
+        self.cursor.execute(update_query, (job_metadata_key,))
+        self.conn.commit()
+        return
+
+    def is_job_running(self, job_name: str) -> bool:
+        select_query = """SELECT pid FROM pio_job_metadata WHERE name=(?) and is_running=1"""
+        self.cursor.execute(select_query, (job_name,))
+        return len(self.cursor.fetchall()) > 0
+
+    def _get_jobs(self, all_jobs: bool = False, **query) -> list[tuple[str, int]]:
+        if not all_jobs:
+            # Construct the WHERE clause based on the query parameters
+            where_clause = " AND ".join([f"{key} = :{key}" for key in query.keys() if query[key] is not None])
+
+            # Construct the SELECT query
+            select_query = f"SELECT name, pid FROM pio_job_metadata WHERE {where_clause} AND is_running=1;"
+
+            # Execute the query and fetch the results
+            self.cursor.execute(select_query, query)
+
+        else:
+            # Construct the SELECT query
+            select_query = f"SELECT name, pid FROM pio_job_metadata WHERE is_running=1 AND name NOT IN {self.LONG_RUNNING_JOBS}"
+
+            # Execute the query and fetch the results
+            self.cursor.execute(select_query)
+
+        return self.cursor.fetchall()
+
+    def count_jobs(self, all_jobs: bool = False, **query) -> int:
+        return len(self._get_jobs(all_jobs, **query))
+
+    def kill_jobs(self, all_jobs: bool = False, **query) -> None:
+        # ex: kill_jobs(experiment="testing_exp") should return end all jobs with experiment='testing_exp'
+
+        mqtt_kill = MQTTKill()
+        shell_kill = ShellKill()
+
+        for job, pid in self._get_jobs(all_jobs, **query):
+            if job in self.PUMPING_JOBS:
+                mqtt_kill.append(job)
+            elif job == "led_intensity":
+                # led_intensity doesn't register with the JobManager, probably should somehow.
+                pass
+            elif job in self.AUTOMATION_JOBS:
+                # don't kill them, the parent will.
+                pass
+            else:
+                shell_kill.append(pid)
+
+        mqtt_kill.kill()
+        shell_kill.kill()
+
+    def __enter__(self) -> JobManager:
+        return self
+
+    def __exit__(self, *args) -> None:
+        self.conn.close()
+        return
+
+
+class ClusterJobManager:
+    def __init__(self, units: tuple[str, ...]) -> None:
+        if not whoami.am_I_leader():
+            raise RoleError("Must be leader to use this. Maybe you want JobManager?")
+
+        self.units = units
+
+    def kill_jobs(
+        self,
+        all_jobs: bool = False,
+        experiment: str | None = None,
+        name: str | None = None,
+        job_source: str | None = None,
+    ) -> bool:
+        from shlex import join
+        from sh import ssh  # type: ignore
+        from sh import ErrorReturnCode_255  # type: ignore
+        from sh import ErrorReturnCode_1  # type: ignore
+
+        command_pieces = ["pio", "kill"]
+        if experiment:
+            command_pieces.extend(["--experiment", experiment])
+        if name:
+            command_pieces.extend(["--name", name])
+        if job_source:
+            command_pieces.extend(["--job-source", job_source])
+        if all_jobs:
+            command_pieces.append("--all-jobs")
+
+        command = join(command_pieces)
+
+        def _thread_function(unit: str) -> bool:
+            try:
+                ssh(add_local(unit), command)
+                return True
+
+            except (ErrorReturnCode_255, ErrorReturnCode_1):
+                return False
+
+        if whoami.is_testing_env():
+            return True
+
+        with ThreadPoolExecutor(max_workers=len(self.units)) as executor:
+            results = executor.map(_thread_function, self.units)
+
+        return all(results)
+
+    def __enter__(self) -> ClusterJobManager:
+        return self
+
+    def __exit__(self, *args) -> None:
+        return
```

## pioreactor/utils/mock.py

```diff
@@ -156,15 +156,16 @@
 
 class MockCallback:
     def cancel(self):
         pass
 
 
 class MockHandle:
-    pass
+    def __and__(self, other):
+        return 1
 
 
 class MockRpmCalculator:
     ALWAYS_RETURN_RPM = config.getfloat("stirring", "target_rpm")
 
     def setup(self):
         pass
```

## pioreactor/utils/pwm.py

```diff
@@ -19,15 +19,15 @@
 from pioreactor.pubsub import Client
 from pioreactor.pubsub import create_client
 from pioreactor.types import GpioPin
 from pioreactor.utils import clamp
 from pioreactor.utils import gpio_helpers
 from pioreactor.utils import local_intermittent_storage
 from pioreactor.version import rpi_version_info
-from pioreactor.whoami import get_latest_experiment_name
+from pioreactor.whoami import get_assigned_experiment_name
 from pioreactor.whoami import get_unit_name
 from pioreactor.whoami import is_testing_env
 
 if is_testing_env():
     from pioreactor.utils.mock import MockPWMOutputDevice
     from pioreactor.utils.mock import MockHardwarePWM as HardwarePWM
 else:
@@ -90,19 +90,15 @@
 
     def start(self, initial_dc: pt.FloatBetween0and100) -> None:
         self._started = True
         self.dc = initial_dc
         lgpio.tx_pwm(self._handle, self.pin, self.frequency, self.dc)
 
     def off(self) -> None:
-        try:
-            self.dc = 0.0
-        except lgpio.error:
-            # see issue #435
-            pass
+        self.dc = 0.0
 
     @property
     def dc(self) -> pt.FloatBetween0and100:
         return self._dc
 
     @dc.setter
     def dc(self, dc: pt.FloatBetween0and100) -> None:
@@ -173,15 +169,15 @@
         unit: Optional[str] = None,
         experiment: Optional[str] = None,
         always_use_software: bool = False,
         pubsub_client: Optional[Client] = None,
         logger: Optional[CustomLogger] = None,
     ) -> None:
         self.unit = unit or get_unit_name()
-        self.experiment = experiment or get_latest_experiment_name()
+        self.experiment = experiment or get_assigned_experiment_name(unit)
 
         if pubsub_client is None:
             self._external_client = False
             self.pubsub_client = create_client(client_id=f"pwm-{unit}-{experiment}-{pin}")
         else:
             self._external_client = True
             self.pubsub_client = pubsub_client
@@ -275,16 +271,15 @@
         self.duty_cycle = round(float(duty_cycle), 5)
 
         self._serialize()
 
     def clean_up(self) -> None:
         with suppress(ValueError):
             # this is thrown if the _pwm hasn't started yet.
-            self._pwm.dc = 0
-            self._pwm.off()
+            self.stop()
 
         self._pwm.close()
 
         self.unlock()
 
         with local_intermittent_storage("pwm_dc") as cache:
             cache.pop(self.pin)
```

## Comparing `pioreactor-24.3.8rc0.dist-info/LICENSE` & `pioreactor-24.4.2rc0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pioreactor-24.3.8rc0.dist-info/METADATA` & `pioreactor-24.4.2rc0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pioreactor
-Version: 24.3.8rc0
+Version: 24.4.2rc0
 Summary: The core Python app of the Pioreactor. Control your bioreactor through Python.
 Home-page: https://github.com/pioreactor/pioreactor
 Author: Pioreactor
 Author-email: hello@pioreactor.com
 License: MIT
 Keywords: microbiology,bioreactor,turbidostat,raspberry pi,education,research
 Classifier: Topic :: Scientific/Engineering
```

## Comparing `pioreactor-24.3.8rc0.dist-info/RECORD` & `pioreactor-24.4.2rc0.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,87 +1,88 @@
 pioreactor/__init__.py,sha256=YBDDaFMxxsG_1Dg9ss9llZcW95gAh8WqpcdCjhF0T-E,117
-pioreactor/config.py,sha256=73ykaApjcqwL4n60oZij7r5tcjGqFfbWAPkpBm84pa8,6785
+pioreactor/config.py,sha256=QqeRVVJoqMOtMszp6M5nnbEPkjM1pCO7yrI1uFcgqas,5868
 pioreactor/error_codes.py,sha256=XDfT3fPTVKN9wIGM9DdrXdvrEUn7lQkf6CJd8Xj_vFo,265
-pioreactor/exc.py,sha256=GvNp0vvgeAclxB2eIGZMnhYU3bFIYKz_AGOU9hKYstM,443
+pioreactor/exc.py,sha256=9fnJVIpg9Yyxq2e1qoWrE1ekMu7QcMHkjdRRkc7Pk3w,958
 pioreactor/hardware.py,sha256=9kdFrdLxgNPMFgXZSijV8zjVJy26zPET0K9axVb-tmI,3854
-pioreactor/logging.py,sha256=smsAV-pjKV6R6zQcQNM1l5St2F-Rvx3pg7smy-nKxx0,6633
-pioreactor/mureq.py,sha256=dAm8AjiNLrNeybYDc3Q3JXCbhpS7Zr-6UmkWHAOfsLo,15532
-pioreactor/pubsub.py,sha256=d5bGbcEXno85_0DMUzckS5KnRHOxVTSqGFAR9CGwATo,12004
+pioreactor/logging.py,sha256=Zy27M-YfxucqVzmKsDEYi-VH-ddGAUbxNUcjPuMGbzc,6861
+pioreactor/mureq.py,sha256=a9gCrKeNND8F3Trekq6_7Sdk3BUMcilgvx_l1KwB0G8,15658
+pioreactor/pubsub.py,sha256=psqCl4R2DeOLeZmgHo-pZ3lJgzwEs2tvb9WGNmDO30E,12014
 pioreactor/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pioreactor/structs.py,sha256=Z6mr5s9T6rSrhPAVKjSbKJJmc3oFZs8tfYY0_PrGjgw,6386
 pioreactor/types.py,sha256=Dh5nDPrqQQ4maMtMeqrUeXRzwHsJIzKFM2QfR9ZPLQ8,3304
-pioreactor/version.py,sha256=yhLVbU2874qYLJyrWCQ62oNE-KODJ8sxhvTw6Pw3dpM,2922
-pioreactor/whoami.py,sha256=QdZosF9FOIsZHk16MCSMOCD7T3J-_ty0j_cyBcfBsHs,4305
+pioreactor/version.py,sha256=TqV16NL4b58IEKIrKJ8Op6ewum7EfW43ydXjr5WEWZ4,2912
+pioreactor/whoami.py,sha256=tkUJb0pcY_Bq2XXQH1VwA8TKm7dUjg_dce8KVjZHSTw,5462
 pioreactor/actions/__init__.py,sha256=VuwIL8llFFqBspvBRgC9yNm-Blu9tsE2kwgIJEs786o,540
-pioreactor/actions/led_intensity.py,sha256=UTL9b87K-T21HyyFWGyuiUpdgp-9dpeGCjafyO0_sGk,8851
-pioreactor/actions/od_blank.py,sha256=9ySpdVElopiRCaCIXuTZXzSXtcamiIZXW5y5DMtJjGk,8967
-pioreactor/actions/od_calibration.py,sha256=f9OkhfxLVMvcs4OYwOQJ7Yn0JVbgOZj2jrhbjyWinzs,24380
-pioreactor/actions/pump.py,sha256=hqt0JAfO25WvtrzzgVk2IKyiRRoopGptoMnYeHN8hgg,19734
-pioreactor/actions/pump_calibration.py,sha256=fA4UtLIO0Pv1nfMZmgLLZ8TLfSJtXhmtnjlwdnxqOKc,22449
-pioreactor/actions/self_test.py,sha256=teJ5WG_WPqgioMMjAmIFGXLuxtq9HA-ZYM3sSFrCmb4,20194
-pioreactor/actions/stirring_calibration.py,sha256=qGZmjLKN2XTXxw5IfnyEmA0yFUIm0EQXcR2EmETio9U,5493
+pioreactor/actions/led_intensity.py,sha256=UOzsJ5kBI25_RSNOz4wG9c6U1QSHDpPif2XisREc6nU,8865
+pioreactor/actions/od_blank.py,sha256=flKxWK0BySZzdJwW8oqn4RPiALIVRScI2yy5lpwlC2E,9013
+pioreactor/actions/od_calibration.py,sha256=xIvjnWYivquED1DTDzk_iyZGC9mP24WyHOGWBpI5kyM,24316
+pioreactor/actions/pump.py,sha256=6seq-0Do6CZSh25kaCKOhfs6RDGYmNWtcv72U9tbjeA,19799
+pioreactor/actions/pump_calibration.py,sha256=yKvc-pi1OQZejZRkenA2_LebLRu4B7nG5w_PTAKRLw0,22400
+pioreactor/actions/self_test.py,sha256=AeUspX_wQAvtFw_Jw-yDjvFJm-yvnIlII4rPbD6zZLU,20149
+pioreactor/actions/stirring_calibration.py,sha256=i2IGq2pUfKC6l63MNFgEwopouuy-ts7sJxuaGmCWpA4,5339
 pioreactor/actions/leader/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pioreactor/actions/leader/backup_database.py,sha256=bexC02qKUhuBdtTcrtT-NUQPfbtQalDPAPrgFDMvl0k,4897
-pioreactor/actions/leader/experiment_profile.py,sha256=p2MUw3js7G9ECnxPnbaP37LiY5eQ3czNGiAcs_Zb1Gk,22875
-pioreactor/actions/leader/export_experiment_data.py,sha256=cZG1uSoT1nt6FjUEDzHg4ZFJY7UKX0-m0of-9JTcgvI,6582
+pioreactor/actions/leader/backup_database.py,sha256=G3SMLyMgR9Hrfo7qyjvfEVtVc9EcfcimuvsYBKfoAEI,4902
+pioreactor/actions/leader/experiment_profile.py,sha256=Zp1kT0aB2-Yli0FtrbrfFQrWhN_Rmkq5gKCS2frglAk,24791
+pioreactor/actions/leader/export_experiment_data.py,sha256=qU-ZTC8RXtZZp6mzqjcyMM9JdKS5bP2zHmmF6cySZ8M,6544
 pioreactor/automations/__init__.py,sha256=PTQQNnA-xrcDLHa0a0ZVM1fYw--rwRSrcoXRvJDHSN0,146
 pioreactor/automations/base.py,sha256=ZCRXAMvz7rtER8zT6wFLxuw6KfimsMhf9f1G9k3y24o,1000
 pioreactor/automations/dosing/__init__.py,sha256=dqTWOveaUiOvaWZ_U90UtN2w_pqDbM1qBIn5X9dat_E,459
-pioreactor/automations/dosing/base.py,sha256=qmg0gpnJEm3ISn39Ysf3gMSYkMeSvT2XHlBl2Ct6v78,29392
-pioreactor/automations/dosing/chemostat.py,sha256=Vs7nu3IVTUmq4XUOPvKi058Ho5gLUcyrcZNJQr6MBsc,1404
+pioreactor/automations/dosing/base.py,sha256=rxWhJ4GqjPlLGnMm-vjfl_HphYGMXEFK4EaAJ0aarFc,29496
+pioreactor/automations/dosing/chemostat.py,sha256=bf3a3gAe_6OZztB7RsNuYFinimdZty5tR5WwJhYGBks,1424
 pioreactor/automations/dosing/fed_batch.py,sha256=YZv5h0PHRwXT4dpEELWGQnNbvgKypzE1CuxiUWuVl_k,1497
 pioreactor/automations/dosing/pid_morbidostat.py,sha256=obaEebppEb_ZUW0UUMc_I68XN8WA2Hp4gLm603URs4I,4962
 pioreactor/automations/dosing/silent.py,sha256=UfYL7mVBMH71rqd35eZ9pDDWikz7DmCnkJGfiSXy6qA,476
 pioreactor/automations/dosing/turbidostat.py,sha256=jIrAtK6cs5ABHDtWdKI6-Z3upaxCTNgzRXguBQsYT_U,4613
 pioreactor/automations/events/__init__.py,sha256=WxPoBeae7fD2SGOLEZ-rQf3IM83Je4SS4CztddADqlw,510
 pioreactor/automations/led/__init__.py,sha256=BhqlUc01Kj2Wbm5tf5C9y7X8AHo4XZv6eDixeakQNLc,567
 pioreactor/automations/led/base.py,sha256=YeVMwzeWthExN7Boxd2cetkrKIgq57fDgtGY6g8spY8,12023
 pioreactor/automations/led/light_dark_cycle.py,sha256=GX50pON5HbZxIUxQMl_WfeYKV_ERtyiJjxQIgbfzNsE,3875
 pioreactor/automations/temperature/__init__.py,sha256=rxAitEYRWPQqFWvGUl_XIe9FwyzwRVQTQmaReafOfBM,154
 pioreactor/automations/temperature/base.py,sha256=jYxG-uG5VFtsyYmPp_HuVbCxemYnmgedaYQusxLPTew,9317
 pioreactor/automations/temperature/only_record_temperature.py,sha256=DB-6Dxn_CQ54YoNKsD46F6EnKi2o5kJDKkhfqB0ekKI,561
 pioreactor/automations/temperature/thermostat.py,sha256=ziOOEH2ghX2MnuLT9-vArUrBRV722gYpQgUUJs3XrG0,4235
 pioreactor/background_jobs/__init__.py,sha256=pn23vImMyfiHjbdKObOWpAwzNUbE1xnHWUYk4j4KSJM,715
-pioreactor/background_jobs/base.py,sha256=_gkMPhytZPBAFw9Am5KYZEzUSX2Z1TBJd5qmDcCZtfw,44846
-pioreactor/background_jobs/dosing_control.py,sha256=pQKMwpe-gKVBMt2juNz0osTatH2VbfBCihX1stoJ7h4,7081
-pioreactor/background_jobs/growth_rate_calculating.py,sha256=vng5JmoP6phDXPwVFkWldMnbdq3874ZIf4z_kHTfoc0,22044
-pioreactor/background_jobs/led_control.py,sha256=S-CkNsua1IqrjEJYJGXDWbYChEYi3n7kWIPxqYPI_1U,5814
-pioreactor/background_jobs/monitor.py,sha256=7YPA0FMeFuxUe_69kppAJK1Vvv4R0ar_ZNNKp24KPb8,29180
-pioreactor/background_jobs/od_reading.py,sha256=Bd6dJLfOEwTYwjlGOLCLtUSTi5qoPX2z9kmqZbCvXiw,51941
-pioreactor/background_jobs/stirring.py,sha256=5BYfTVJkLK7hoAFb5iEHVReK0ZKQKElz9Jn-tTD54UQ,17963
-pioreactor/background_jobs/temperature_control.py,sha256=SO578DsaLfXl3VW8muYGNXRApJYqEJ3hNwiKNMftsbc,24229
+pioreactor/background_jobs/base.py,sha256=CpMxUFyh5wBuUX1-qXTzKB289-23HHFXFSVDaugiJkw,44487
+pioreactor/background_jobs/dosing_control.py,sha256=PSohU2cgQANjMJbpS2i7Z5lobQUPgzBD-AB9Ordfma4,7043
+pioreactor/background_jobs/growth_rate_calculating.py,sha256=Ls-DdmcQA9QEjQFqUA9J58MeYUD6WTsLo_KIr97SSt0,22120
+pioreactor/background_jobs/led_control.py,sha256=oS2rZtHlq8Ct2WZ_3y2L_coqbtIJGvQeqBdmwiYeG4o,5806
+pioreactor/background_jobs/monitor.py,sha256=YP5hW2p_OOA_xB0TCRtO_9xiRjXO-XfZquKMtyjBAm8,29062
+pioreactor/background_jobs/od_reading.py,sha256=59vcJZgZaewFZv7wvpclcHLyinXOodCgEeWARF3HUEc,52689
+pioreactor/background_jobs/stirring.py,sha256=syCS5abapM3y1OVDDS02kmvsL2rSTZLvwWbMkkR4Syw,17978
+pioreactor/background_jobs/temperature_control.py,sha256=os8uN0zfE4Wnfs9xpZ_JjXpOHEgf8CIIdEWZ2r8JvlQ,24449
 pioreactor/background_jobs/leader/__init__.py,sha256=Uhl2Xksfkf06lmfmz2scTxmBWDBnwkA9rSil_V94aWQ,605
-pioreactor/background_jobs/leader/mqtt_to_db_streaming.py,sha256=OtBTh4W8jJIZ_FZvqnWmTueGbCn4cyRNKyQ6UwjTvQU,16897
-pioreactor/background_jobs/leader/watchdog.py,sha256=BQVou_f44gp9Et_idxXA4_nZAiNQgYt51MTS7Mgqx6Q,4345
+pioreactor/background_jobs/leader/mqtt_to_db_streaming.py,sha256=ZYGNrf281NvtAQCVRQ20RNS3efVktWl7SSehyItHEMw,17001
+pioreactor/background_jobs/leader/watchdog.py,sha256=Lh7ZUp7M5rxGmbSSCykSZEJBgXF6Um-Ur7KwsipkNco,4573
 pioreactor/background_jobs/subjobs/__init__.py,sha256=RP7U5Q5cWqmmlmiVwAbVnwGyEnhqJM6fk71VP-3Y0c0,1094
 pioreactor/cli/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pioreactor/cli/pio.py,sha256=5eU5ho8CykXigaTNiie9rUY0LXPKawgyqXGBqmuCLzU,34487
-pioreactor/cli/pios.py,sha256=UuUpYeqdmnMbl4Pkdsfx3ndt5CrPrnHzt88y9yykKGQ,26942
+pioreactor/cli/pio.py,sha256=KwVtrJ1Y4rEB1xqdAT3waeLgklYPRsa-ipzoddHTwng,26719
+pioreactor/cli/pios.py,sha256=klmcDpLom4rCHr9Pb6x5sTKDxnRiy7iAz3WwHBzMNOk,26008
+pioreactor/cluster_management/__init__.py,sha256=QKl9pA6Pws5DJM2wanMxSWMCMztHQ4gtTiFikd7ac3Y,10177
 pioreactor/experiment_profiles/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-pioreactor/experiment_profiles/parser.py,sha256=N3wRPjcAuHX4tZcN1doRkWfo0Uu5IGQ5HTm95hxqlJk,5306
-pioreactor/experiment_profiles/profile_struct.py,sha256=9WT7-LnDmTmc4y-owCYIis8Ic4P8DBjEvbkCR9WhDMk,2952
+pioreactor/experiment_profiles/parser.py,sha256=lYDWjhTirYWiWAx6o7T_K044FwQsmPNGmqQYmXT50sg,5488
+pioreactor/experiment_profiles/profile_struct.py,sha256=OIok66XPpbba8ghUqzQL9EkT9OqbAzO5gcXSohHP6oY,2896
 pioreactor/experiment_profiles/sly/__init__.py,sha256=Z7rV-XpX5Nz29NqaaJ7ln8hHgY6WYqC5t-59YYt8xqs,197
 pioreactor/experiment_profiles/sly/lex.py,sha256=Dwt9dJqVw92BpmfwWR0PzqduLpD5qE-ILgSj_vXUlRg,16273
 pioreactor/experiment_profiles/sly/yacc.py,sha256=EfzAO8Wg4Cb8wOPBaf8pK37MDsk0KFjTEN8BHgDMXi0,83015
 pioreactor/plugin_management/__init__.py,sha256=ivql-g9QTAWn_xvFCoFYj0BzUBcWxYp-AQKKifUdNXQ,3807
-pioreactor/plugin_management/install_plugin.py,sha256=i3KchEps5cafvuhADOLacaziL9UpuoDvMnvDVBaTCuw,1316
+pioreactor/plugin_management/install_plugin.py,sha256=o6LDEjw2PLZpaH4hqfGdNnFx7hxChsSv4AQ4m4F1ljw,1446
 pioreactor/plugin_management/list_plugins.py,sha256=oXTY0zfPkTR-PjZiCyuil9DAnPjQ9Z-GsFQev3b-5ak,1179
-pioreactor/plugin_management/uninstall_plugin.py,sha256=Wg9DDwPsjWvmjC4z1EoBWdXuTGSkBI7nlpC9eVlWoN0,1657
+pioreactor/plugin_management/uninstall_plugin.py,sha256=cIwFK6WUGU2k4jwAywttXWGsxARYagE1x9CQtPbjtCg,1789
 pioreactor/plugin_management/utils.py,sha256=Cferq4vLR0JuXjZY135tm_fcszm5_8bNP4eToOH2Slw,1194
-pioreactor/utils/__init__.py,sha256=ZLG-wtBSwtlij4wjFY0EJco36PHAKsjuzGXoEMHlsCU,13665
+pioreactor/utils/__init__.py,sha256=Epli_0_xEvgWA6nDEUAOMYvVY7z85HedYxbFC0xYVgE,21357
 pioreactor/utils/adcs.py,sha256=8tLRJuZFUB0JFLCnQ7WFkx171DlYgigLcCzo3xy-pwQ,4240
 pioreactor/utils/dacs.py,sha256=gPAEjRseVA3-Ta5MTHpwi0HRdbWVd47pD21FcCaQwFY,1930
 pioreactor/utils/gpio_helpers.py,sha256=9lAKuEMuLIqXtTk6wRisKABo_Ab0n-zGcBjtN8gjX2I,976
 pioreactor/utils/math_helpers.py,sha256=lHAaO2X6tH8oP5Znk7CoO5l-phdPpgAR1AQTecVZr88,3311
-pioreactor/utils/mock.py,sha256=lef7UvBq4652ti1SpTAWz_93W9NzOdTrYIjKLJax1dg,5173
+pioreactor/utils/mock.py,sha256=o5tNfE40supfTBDurMCgbXp2dqtsCsXeK3Hl6LotJow,5211
 pioreactor/utils/networking.py,sha256=6I3sykzSFPfrS1erIRFu33Cq8x-25I_UFdJTdsn9KOg,3622
-pioreactor/utils/pwm.py,sha256=mLjZ-jf5sbqYb3HRIFvA3ueuFxZ4oLHviP6mXekQ7QM,10265
+pioreactor/utils/pwm.py,sha256=33tYIbip7C6XILNw6y8wM2F7R70llepO4CirWx3Gpyw,10149
 pioreactor/utils/rpi_bad_power.py,sha256=CbtzIi9x8pvtVAX6aID8MG5YXNzkeIRFYfhri4qI-Xo,3393
 pioreactor/utils/sqlite_worker.py,sha256=TKgohPrZu3HsttrzH7HDmwkk5KWSEkFT1sFTB7jQkBc,7889
 pioreactor/utils/streaming_calculations.py,sha256=RP2ZIG7oylkJipXRvCn8uxd2JO979MSrI1U6jFxHM80,18416
 pioreactor/utils/timing.py,sha256=x_CPXqm4pkaaz5I3XUkTM49XJbjTETXKya3YRkbCwSk,5827
-pioreactor-24.3.8rc0.dist-info/LICENSE,sha256=V9lTmv9cMeiSZ_9ezl7s5LVGXkZ4t7PUzVxIeVqkk7k,1067
-pioreactor-24.3.8rc0.dist-info/METADATA,sha256=N9bgJCQBdWM7FIJigX8vjHb5F_SwYdVDWQvAg275BsM,3713
-pioreactor-24.3.8rc0.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-pioreactor-24.3.8rc0.dist-info/entry_points.txt,sha256=1vQa-58PTH44hOQBeYFJdO3Tdfzea7_pYDxv5KQWvZ4,79
-pioreactor-24.3.8rc0.dist-info/top_level.txt,sha256=xhd14Ee_KR74whX88OzvljqlGXmfpBUHOSIqDrbs9_0,11
-pioreactor-24.3.8rc0.dist-info/RECORD,,
+pioreactor-24.4.2rc0.dist-info/LICENSE,sha256=V9lTmv9cMeiSZ_9ezl7s5LVGXkZ4t7PUzVxIeVqkk7k,1067
+pioreactor-24.4.2rc0.dist-info/METADATA,sha256=lJ-SLTAPFwEkwBOmm4v2GCB6Wh7snATXtprtHMrL9xc,3713
+pioreactor-24.4.2rc0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+pioreactor-24.4.2rc0.dist-info/entry_points.txt,sha256=1vQa-58PTH44hOQBeYFJdO3Tdfzea7_pYDxv5KQWvZ4,79
+pioreactor-24.4.2rc0.dist-info/top_level.txt,sha256=xhd14Ee_KR74whX88OzvljqlGXmfpBUHOSIqDrbs9_0,11
+pioreactor-24.4.2rc0.dist-info/RECORD,,
```

